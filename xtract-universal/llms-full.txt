# Xtract Universal HelpCenter

> Optimal data flow between SAP and databases, BI or cloud solutions – stable and fully automated. Learn how to use Xtract Universal by browsing our user documentation, articles and more.

Full: https://helpcenter.theobald-software.com/xtract-universal/llms-full.txt

# Getting Started

This section shows how to install and set up Xtract Universal for the first time.

### Video Introduction

Get started with Xtract Universal by watching our onboarding video or following the step-by-step instructions below.

### Installation

Follow the steps below to install Xtract Universal for the first time:

1. [Download](https://theobald-software.com/en/download-trial/) a 30 day trial version of Xtract Universal.
1. Run the Xtract Universal executable (XtractUniversalSetup.exe) to install the [Xtract Universal Designer and the Xtract Universal Server](../documentation/introduction/#software-architecture). For information on system requirements, see [Requirements](../documentation/setup/requirements/).
1. Make sure that the Xtract Universal Service is running on your windows system and that the default port 8065 is not blocked by your firewall.
1. Open the Xtract Universal Designer application and click **[ Connect]** to [connect to the Xtract Universal Server](../documentation/designer/#connect-the-designer-to-a-server) using the default settings.

For more information, see [Setup](../documentation/setup/).

### Connect to SAP

There are two types of SAP connections:

- connect via [RFC protocol](../documentation/sap-connection/settings/#source-type-rfc) (default)
- connect via [OData protocol](../documentation/sap-connection/settings/#source-type-odata)

Follow the steps below to connect to SAP using the standard RFC protocol.

1. Before connecting to SAP via RFC for the first time, set up an SAP dialog user with the necessary [SAP user rights](../documentation/setup-in-sap/sap-authority-objects/#general-authorization-objects).
1. In the main window of the Designer, click **[New]**. The window "Change Source" opens.
1. Enter a name for the SAP connection in the field **Name**, e.g., *s4hana*, *bw*, etc.
1. In the *General* tab, enter the system details of your SAP system. Input values for the SAP connection can be found in the *Properties* of the SAP Logon Pad or they can be requested from the SAP Basis team.
1. In the *Authentication* tab, enter the SAP credentials of the SAP dialog user.
1. Click **[Test designer connection]** to validate the connection between the Xtract Universal Designer and the SAP system.
1. Click **[Test server connection]** to validate the connection between the Xtract Universal Server and the SAP system.
1. Click **[OK]** to save the SAP source.

For more information, see [SAP Connection](../documentation/sap-connection/).

Tip

To edit a source or to create new sources, navigate to **Server > Manage Sources** in the menu bar.

### Create an Extraction

Extractions are the main entities of Xtract Universal. They define what data to extract from SAP and where to write the data. Follow the steps below to create a new extraction:

1. In the main window of the Designer, click **[New]**. The window "Create Extraction" opens.

1. Select an SAP Connection from the drop-down menu in **Source** .

1. Enter a name for the extraction .

1. Select one of the following extraction types :

   | Extraction Type | Connection Type | Description | | --- | --- | --- | | [BAPI](../documentation/bapi/) | RFC | Execute BAPIs and Function Modules. | | [BWCube](../documentation/bwcube/) | RFC | Extract data from SAP BW InfoCubes and BEx Queries. | | [BW Hierarchy](../documentation/hierarchy/) | RFC | Extract Hierarchies from an SAP BW / BI system. | | [DeltaQ](../documentation/deltaq/) | RFC | Extract data from DataSources (OLTP) and extractors from ERP and ECC systems. | | [OData](../documentation/odata/) | OData | Extract data via SAP OData services. | | [ODP(OData)](../documentation/odp-odata/) | OData | Extract ODP-based data using OData services. | | [ODP](../documentation/odp/) | RFC | Extract data via the SAP Operational Data Provisioning (ODP) framework. | | [OHS](../documentation/ohs/) | RFC | Extract data from InfoSpokes and OHS destinations. | | [Query](../documentation/query/) | RFC | Extract data from ERP queries. **Note: BEx queries are covered by BWCube**. | | [Report](../documentation/report/) | RFC | Extract data from SAP ABAP reports. | | [Table](../documentation/table/) | RFC | Extract data from SAP tables and views. | | [Table CDC](../documentation/table-cdc/) | RFC | Extract delta data from SAP tables and views. |

1. Click **[OK]**. The main window of the extraction type opens automatically.\
   Follow the instructions in the documentation of the selected extraction type to set up the extraction.

#### A Simple Extraction for Beginners

Follow the steps below to extract customer master data from the SAP table KNA1:

1. [Create an extraction](#create-an-extraction) that uses the Table extraction type.
1. In the main window of the extraction type, click **[Add]** to look up an SAP table. The window "Table Lookup" opens.
1. In the field **Table Name**, enter the name of the table to extract (KNA1) . Use wildcards (\*) if needed.
1. Click **[]** . Search results are displayed.
1. Select the table KNA1 and click **[OK]**. The application returns to the main window of the extraction type.
1. Optional: Select the table columns to extract. By default all columns are extracted. For more information on filter options and advanced settings, see [Define the Table Extraction Type](../documentation/table/#define-the-table-extraction-type)
1. Click **[Load live preview]** to display a live preview of the first 100 records.
1. Click **[OK]** to save the extraction type.

The extraction is now listed in the main window of the Designer. To edit an extraction, double-click the extraction.

### Run an Extraction

Extractions can be run directly in the Xtract Universal Designer or via [web service](../web-api/) and [command line](../documentation/execute-and-automate/call-via-commandline/). Follow the steps below to testrun your extraction in the Designer:

1. In the main window of the Designer, select an extraction and click **[Run]** . The window "Run Extraction" opens.
1. Click **[Run]** to execute the extraction. The status in the subsection *General Info* indicates if the extraction finished successfully.
1. Open the *Output* tab to view the extracted data .

For more information, see [Execute and Automate](../documentation/execute-and-automate/).

### Write Data to a Target Environment

Xtract Universal allows you to load data to a wide range of target environments, including databases, cloud storages, BI tools, etc. By default, extractions use the [**http-csv**](../documentation/destinations/csv-via-http/) destination as a target environment.

Follow the steps below to add a new destination to Xtract Universal:

1. In the main window of the Designer, select an extraction.

1. Click **[Destination]**. The window “Destination Settings” opens.

1. In the “Destination Settings” window, click **[+]** to add a new destination.

   Note

   To write data to an existing destination, select the destination from the **Destination** dropdown list. [http-csv](../documentation/destinations/csv-via-http/) and [http-json](../documentation/destinations/json-via-http/) are available by default.

1. In the window "Destination Details", enter a name for the destination.

1. Select a destination type from the drop-down menu. A list of connection details opens.

1. Fill out the destination details to connect to the destination. Destination details vary depending on the destination type. For more information about destination details, select your destination:

   Select a destination Alteryx Amazon S3 Amazon Redshift Dataiku EXASolution Flat File CSV Flat File JSON Flat File Parquet Google Cloud Storage HTTP CSV HTTP JSON Huawei Cloud OBS IBM Db2 KNIME Microsoft Azure Storage Microsoft Azure Synapse Analytics Microsoft Fabric (OneLake) Microsoft Fabric Open Mirroring Microsoft Power BI Microsoft Power BI Report Server Microsoft SharePoint Microsoft SQL Server MySQL Oracle PostgreSQL QlikSense and QlikView Salesforce SAP HANA Snowflake Tableau

1. Click **[OK]** to confirm your input. The window "Destination Details" closes.

1. Optional: change the default destination settings. Destination settings are specific to the selected extraction and vary depending on the destination type. For more information about destination settings, select your destination:

   Select a destination Alteryx Amazon S3 Amazon Redshift Dataiku EXASolution Flat File CSV Flat File JSON Flat File Parquet Google Cloud Storage HTTP CSV HTTP JSON Huawei Cloud OBS IBM Db2 KNIME Microsoft Azure Storage Microsoft Azure Synapse Analytics Microsoft Fabric (OneLake) Microsoft Fabric Open Mirroring Microsoft Power BI Microsoft Power BI Report Server Microsoft SharePoint Microsoft SQL Server MySQL Oracle PostgreSQL QlikSense and QlikView Salesforce SAP HANA Snowflake Tableau

1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination. For more information on available destinations, see [Destinations](../documentation/destinations/).
# User Documentation

The user documentation of Theobald Software is designed to introduce readers to the main functions of Xtract Universal.

Theobald Software's claim is to keep the user documentation up to date according to the latest product version. Information concerning the older version is removed gradually from the documentation content. It is generally recommended to always install the latest version of the product.

Theobald Software's claim is to always update all the used screenshots of other software vendors, nevertheless this cannot be guaranteed.

©2024 Theobald Software GmbH. All rights reserved.

### Target group and audience

This documentation is created for all users of Xtract Universal. The user documentation offers an overview of the interface, of the navigation and of the basic information to the users who never or rarely worked with the product. Experienced users can find more detailed information on more complex topics and use the user documentation for reference.

Reading the "Getting Started" section is the prerequisite for working with the product. The documentation can also be useful during the evaluation phase.

### Typographical conventions

| Convention | Used for marking: | Example | | --- | --- | --- | | bold & square brackets | Buttons | **[Edit]** | | bold | URL buttons | **Subscriptions** | | bold | Fields within a window, tab names | **Name** | | italics | Input values | *MATNR* | | italics | Drop-down menu options | *TextAndCode* |

#### Notes and warning messages

There are three main types of warning messages with the corresponding colors.

| Signal word | Color | Severity and meaning | | --- | --- | --- | | Note | Blue | Additional information | | Warning | Yellow | Information that is important for executing an error free procedure | | Tip | Green | Tips | | Recommendation | Green | Theobald Software recommendations and best practices advice |

Warning Messages comply with EN 82079 and formulated according the SAFE-method that is derived from German. The SAFE method is a procedure for systematically designing safety instructions. The severity of the danger as well as the source of the danger.

**"SAFE"** stands for:

**S**chwere der Gefahr (Signalwort) **A**rt und Quelle der Gefahr **F**olgen bei Missachtung der Gefahr **E**ntkommen (Maßnahmen zur Abwehr der Gefahr)

Translation: Severity of the danger (Signal word) Type and source of the danger Consequences of disregarding the danger Escape (measures to avert the danger)

Example:

Warning

**RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table**\
To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.

Note

The corresponding SQL command is generated dynamically and executed on the SAP server.

This page shows how to connect the Xtract Universal Designer with a Xtract Universal Server.

## Connect the Designer to a Server

When starting the Xtract Universal Designer, the window "Connect to Xtract Universal Server" opens.\
Enter connection and user details to connect the Designer to a Xtract Universal Server.

1. Launch the Xtract Universal Designer.

1. When the Xtract Universal Server is a local server, the server address is *localhost*.\
   The default port (8064) may vary depending on the configuration.

1. If the service is not running on default port 8064, specify the port by adding :[port] after the host name. The default port can be configured in the [server settings](../server/server-settings/).

1. Select an authentication method. Once logged in, you can activate or deactivate methods of authentication, see [Access Management](../access-restrictions/#authentication-between-designer-and-server).

1. Click **[ Connect]** to connect the Designer to the Server. The main window of the Designer opens.

1. Launch the Xtract Universal Designer.

1. When the Xtract Universal Designer and the Xtract Universal Server run on different machines, enter the host name of the Xtract Universal Server. Make sure the port is not blocked by your firewall.

1. If the service does not run on default port 8064, specify the port by adding :[port] after the host name. The default port can be configured in the [server settings](../server/server-settings/).

1. Select an authentication method. Once logged in, you can activate or deactivate methods of authentication, see [Access Management](../access-restrictions/#authentication-between-designer-and-server).

1. Click **[ Connect]** to connect the Designer to the Server. The main window of the Designer opens.

Xtract Universal offers different methods of authenticating and securing the connection between the Xtract Universal Designer and the Xtract Universal Server. The default authentication methods are authentication via *Windows credentials (current user)* and *Anonymous (no encryption)*.

## Main Window of the Designer

The Designer features different functionalities to design and configure extractions.

Tip

Press `F1` to open the documentation page for particular windows.

### Main Menu Bar

| Icon | Menu Item | Description | Details | | --- | --- | --- | --- | | | Reset Preferences | Reset connection settings to server | [Connect the Designer to a Server](#connect-the-designer-to-a-server) | | | Disconnect | Log off the server | - | | | Exit | Close the Designer | - |

| Icon | Menu Item | Description | Details | | --- | --- | --- | --- | | | New | Create a new extraction | [Create an Extraction](../../getting-started/#create-an-extraction) | | | Edit | Edit an existing extraction | - | | | Delete | Delete an existing extraction | - | | | Clone | Clone an existing extraction | - | | | Add/Remove Keywords | Define keywords of selected extractions | [General Settings](../table/general-settings/) | | | Source | Select an existing SAP source system | [SAP Connection](../sap-connection/) | | | Destination | Select a destination | [Destinations](../destinations/) | | | Log | Open extraction log | [Extraction Logs](../logs/) | | | Run | Run a selected extraction | [Run an Extraction](../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer) | | | Abort | Abort a selected extraction | - | | | Clear Result Cache | Clear extraction cache | [Cache results](../table/general-settings/#misc-tab) | | | Refresh | Refresh extraction status | - | | | Filter | Define filters or keywords | [Define Keywords](../table/general-settings/#misc-tab) |

| Icon | Menu Item | Description | Details | | --- | --- | --- | --- | | | Logs | Open server logs | [Server Logs](../logs/) | | | Settings | Open server settings | [Server Settings](../server/server-settings/) | | | Manage Sources | Edit connection to source system | [SAP Connection](../sap-connection/) | | | Manage Destinations | Edit or delete destinations | [Destinations](../destinations/). |

| Icon | Menu Item | Description | Details | | --- | --- | --- | --- | | | Set User Password | Set or change user password | [User Managements](../access-restrictions/user-management/) | | | Manage Users | Manage user groups | [User Groups](../access-restrictions/user-management/) |

| Icon | Menu Item | Description | Details | | --- | --- | --- | --- | | | Online Help (EN) | Open the HelpCenter | - | | | Submit Support Ticket | Open the Ticket Portal | [Support Portal](https://support.theobald-software.com/helpdesk/User/Register) | | | Download latest version | Link to My Theobald Software | [Customer Portal](https://my.theobald-software.com/) | | | Info | Product information | - | | | Version History | List of recent software changes | [Changelog](../../changelog/) |

### GUI Buttons

| Icon | Button | Description | Details | | --- | --- | --- | --- | | | New | Create a new extraction | [Create an Extraction](../../getting-started/#create-an-extraction) | | | Edit | Edit existing extraction | - | | | Delete | Delete existing extraction | - | | | Clone | Clone existing extraction | - | | | Refresh | Update of the extraction status | - | | | Search | Filter extraction names | [List of Extractions](#list-of-extractions) | | | Source | Select existing SAP source system | [SAP Connection](../sap-connection/) | | | Destination | Select destination | [Destinations](../destinations/) | | | Log | Open extraction log | [Extraction Logs](../logs/) | | | Run | Run a selected extraction | [Run an Extraction](../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer) |

### Extraction Groups

Extractions can be organized in groups, see [Organize Extractions](../organize-extractions/). The treeview in the main window of the Designer displays all extraction groups and reflects the folder structure in the following directory: `C:\Program Files\XtractUniversal\config\extractions`.

Select a group to display the list of extractions that are located in the corresponding directory.

### List of Extractions

List of all extractions in the currently selected extraction group divided by name, type, modification and creation date etc.

Tip

Use the search bar above the list to filter extractions. Wildcards (\*) are not supported.\
More extensive filter options are available using the [GUI Buttons](#gui-buttons) .

Tip

To adjust the settings of multiple extractions at once, select multiple extractions using `Ctrl`+`Left Button`.\
You can now adjust the **[ Destination]** and **[ Source]** settings for all selected extractions, see [GUI Buttons](#gui-buttons).

**Double Click:** Double-clicking an extraction works as a shortcut to the **[ Edit]** button.\
**Right-click:** Right-clicking an extraction in the list opens the following menu:

| Icon | Menu Item | Description | Details | | --- | --- | --- | --- | | | Edit | Edit an existing extraction | - | | | Delete | Delete an existing extraction | - | | | Clone | Clone an existing extraction | - | | | Add/Remove Keywords | Define keywords of selected extractions | [General Settings](../table/general-settings/) | | | Log | Open extraction log | [Extraction Logs](../logs/) | | | Source settings | Select an existing SAP source system | - | | | Destination settings | Select a destination | [Destinations](../destinations/) | | | Run | Run a selected extraction | [Run an Extraction](../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer) | | | Run in browser | Run a selected extraction in the default web browser | - | | | Run in xu.exe | Run a selected extraction in the command-line tool xu.exe | [Execute Extractions with xu.exe](../execute-and-automate/call-via-commandline/) | | | Abort | Abort a selected extraction | - |

### Status Bar

The status bar contains information about the following:

- Number of defined extractions in the selected folder, the overall number of extractions and the number of licensed extractions
- Login username
- Server authentication method
- Designer access rights
- License type and validity period
- Connected XU server version
- Connected XU client version

This section contains user documentation for Xtract Universal.

Find more information on how to use and navigate through the user documentation in the section ["About this Documentation"](../about-this-documentation/).

### About Xtract Universal

Xtract Universal is an SAP Connector for data extraction from SAP to various target environments. The extracted data can than be further processed e.g., in the context of business intelligence, data integration and data analytics.

### About Extractions

The main entities in Xtract Universal are called "extractions". An extraction is a combination of the following components:

- A defined [extraction type](#extraction-types), e.g., Table, Query, Report, etc.
- A connection to a [source](../setup/requirements/#supported-sap-systems-and-releases) system, e.g., SAP S/4HANA, SAP BW, etc.
- A connection to a [destination](../destinations/) / target environment, e.g., SQL Server, Azure, etc.

The graphic on the right depicts a practical example of an extraction and its components.

For more information on how to create and maintain extractions in the Xtract Universal Designer, see [create extractions](../../getting-started/#create-an-extraction).

### Software Architecture

Xtract Universal consists of two components:

- A [Designer](../designer/) to design and configure extractions.\
  When designing extractions in the Designer, the user can create and modify extractions, sources, destinations and alter the server settings.
- A [Server](../server/) to execute extractions.\
  During the execution phase, the user can execute the extractions that were designed in the Designer. The execution of the extractions takes place on the server.

Depending on the target environment, an extraction can be executed by the [Xtract Universal command line tool - xu.exe / xu.elf](../execute-and-automate/call-via-commandline/) or by the consuming destination, see [Pull and Push Destinations](../destinations/).

### Extraction Types

Xtract Universal offers the following extraction types to cover a wide range of data extraction scenarios.

Xtract Universal logs all steps performed on a system in log files.\
This page shows how to access server and extraction logs in the Designer. Logs can also be queried using the Xtract Universal [Web API](../../web-api/).

### Log Levels

Each log entry is assigned one of the following message types:

| Type of log entry | Description | | --- | --- | | *E - Errors* | Error messages issued during the extraction process. | | *I - Information* | Status messages, about processes that do not lead to an error. | | *W - Warnings* | Information about problems that do not lead to an extraction error, e.g., authentication errors. | | *D - Debug Details* | Detailed information that helps finding error cause. |

### Access Server Logs

To open the server logs, navigate to **Server > Log (Web Worker)** in the main window of the Designer.

The logs are created per TCP connection. To switch between logs, use the list of timestamps on the left. Server logs are automatically deleted after a defined period of time, see [server settings](../server/server-settings/).

For information on how to query server logs using the web API, see [Web API - Get Server Logs](../../web-api/#get-server-logs).

### Access Extraction Logs

To open the extraction logs, select an extraction from the list of extractions and click **[ Logs]** in the main window of the Designer.

The timestamp for each extraction process is displayed in the left part of the "View Extraction Log" window. Extraction logs are automatically deleted after a defined period of time, see [server settings](../server/server-settings/).

For information on how to query extraction logs using the web API, see [Web API - Get Extraction Logs](../../web-api/#get-extraction-logs).

- To filter the execution date of the logs, enter a time period in .
- To filter log levels, use the checkboxes underneath the log display .
- To copy the current log to the clipboard, click **[]** .

### Activate Tracing

The RFC communication with SAP can be recorded by tracing for troubleshooting purposes.\
Standard logging is always active and is independent of the tracing setting.

1. Open the [SAP connection settings](../sap-connection/settings/) of the source that is assigned to the failing extraction.
1. In the tab *RFC Options*, enter the path to an existing directory in the field **Trace directory** or create a new directory for the trace files.
1. Reproduce the error. XML files with the tracing information are created in the specified directory.
1. Compress the folder to a zip file ("Trace.zip") and send the zip file to the Theobald [support team](https://support.theobald-software.com/helpdesk/).
1. Reopen the SAP connection settings and delete the path in the field **Trace directory**.

Warning

**Increase of used hard drive memory.**\
A big amount of information is collected when debug logging is activated. This can decrease the capacity of your hard drives dramatically. Activate the debug logging only when necessary e.g., upon request of the support team.

### Generated Log Files

The log files are stored in the installation directory of Xtract Universal, e.g., `C:\Program Files\XtractUniversal\logs`. The following log files are created:

| Type | File Name | Description | Location path | | --- | --- | --- | --- | | Server | ServiceLog.txt | Contains the activities of XtractService.exe. | `C:\Program Files\XtractUniversal\logs` | | Server | WebServer-Log: yyyyMMddTHHmmss.fffZ.log, e.g., 20201013T055455.465Z.log | The name contains the timestamp in UTC. A new file is created when the server is started, additionally a new log file is created every 24 hours. [Theobald.XU.Web.Listener.exe](../server/server-tasks/#theobaldxuweblistenerexe) is the corresponding process. | `C:\Program Files\XtractUniversal\logs\servers\web\listener` | | Server | WebWorker-Logs: yyyyMMddTHHmmss.fffZ.log, e.g., 20201013T055455.465Z.log | The name contains the timestamp in UTC. A new file is created when a TCP connection is accepted.Theobald.XU.Web.Worker.exe is the corresponding process. | `C:\Program Files\XtractUniversal\logs\servers\web\worker` | | Extraction | Extraction logs: yyyyMMddTHHmmss.fffZ.log, e.g., 20201013T055455.465Z.log | The name contains the timestamp in UTC. A new file is created to start an extraction. Theobald.XU.Web.Worker.exe is the corresponding process. | `C:\Program Files\XtractUniversal\logs\extractions\[Name_der_Extaktion]` |

For more information on the server processes, see [Server Tasks](../server/server-tasks/).

Tip

To redirect logs to another location, symlink the logs folder of the installation directory to a custom directory.\
Example for PowerShell:

```shell
New-Item -ItemType SymbolicLink -Target "E:\log" -Path "C:\Program Files\XtractUniversal\logs"

```

### Read Extraction Logs

Read the logs written in understandable language to better understand the procedures of Xtract Universal. Ihe depicted example log belongs to an extraction that writes data into an SQL destination:

General technical information is displayed.

The [SQL destination](../destinations/microsoft-sql-server/) is prepared for receiving data.

The license check is performed including entity check and other relevant information.

Connection to SAP is established.

[Runtime parameters](../parameters/) are logged.

Data from SAP is requested.

Package no. 1 is received from SAP.

Package no. 1 is written to the SQL server.

Extraction is completed.

This page shows how to organize extractions in groups to improve maintenance.

### Group Extractions

Follow the steps below to create a new group and add extractions to the group.

1. In the tree view of the Designer, right-click the *extractions* folder. The context menu opens.
1. Click **New subfolder**. The window "Create new subdirectory" opens.
1. Enter a unique name for the extraction group and click **[OK]**. The new subfolder is displayed in the tree view. Nested groups are supported.
1. To add extractions to the subfolder, open the list of extractions in the *extractions* folder (or any other subfolder) and select all relevant extractions. Use **[CTRL]+[left mouse button]** to select multiple extractions.
1. Drag and drop extractions from the list of extractions into the new subfolder.

The treeview in the main window of the Designer reflects the folder structure in the directory `C:\Program Files\XtractUniversal\config\extractions`.

Tip

You can assign an SAP source system or a destination to all extractions within an extraction group by using **[Ctrl]+[A]** to select all extractions in the group. To assign an SAP source system, use the GUI button **[Source]**. To assign a destination, use the GUI button **[Destination]**.

### Rename or Delete Groups

Right-click a group to open the context menu.

- Click **Rename** to rename the group.
- Click **Delete** to delete the group and its content.

### Look Up Extractions

Use the search bar above the extraction list to filter the displayed extractions. Wildcards (\*) are not supported.

Follow the steps below to look up an extraction in all extraction groups:

1. In the main window of the Designer, click **[Search]**. The window "Search Extractions" opens.
1. Enter the name of a group, extraction, extraction type, source system, destination or a keyword in the search bar.
1. Click . Search results are displayed.
1. Select an extraction from search results.
1. Click **[Go]**.

The corresponding extraction group opens and the extraction is is selected.

This section contains information on how to set up access restrictions to the Designer and the Server.

- Restrictions to the Designer affect the creation or modification of destinations, sources and extractions
- Restrictions to the Server affect the execution of extractions

There are two types of users and user groups, access can be restricted to:

- Windows AD users (Kerberos authentication)
- [Custom users](user-management/) (Basic authentication)

### Authentication Between Designer and Server

The connection between the Designer and the Server can be established using different authentication and encryption methods. The authentication method guarantees the verification of the identity of the logged in user.

Note

When running extractions in the Designer, the executing user is always the Windows account that runs the Designer, not the login user. To run extractions under a different user, start the Xtract Universal Designer application as a different user (`Shift` + ).

Note

To use Kerberos transport encryption or authenticate an Active Directory (AD) user, a Kerberos Target Principal Name (TPN) is required. TPN can either be a User Principal Name (UPN) or a Service Principal Name (SPN). For more information on TPN, see [Knowledge Base Article: Target Principal Field (TPN)](../../knowledge-base/target-principal-TPN/).

The following combinations of transport encryption and authentication are available:

| Authentication Method | Description | TPN required | | --- | --- | --- | | Windows credentials (current user) | The AD user, who runs the Designer authenticates themselves towards the Server via Kerberos. All data exchanged between Designer & Server is encrypted using Kerberos. | | | Windows credentials (different users) | The AD user, whose user name and password are entered in the login window, authenticates themselves to the XU server via Kerberos. All data exchanged between Designer & Server is encrypted using Kerberos. | | | Custom credentials (TLS encryption) | The custom user, whose user name and password are entered in the login window, authenticates themselves to the Server. All data exchanged between Designer & Server is encrypted via TLS. To use TLS transport encryption, an [X.509 server certificate](install-x.509-certificate/) is required for the Service (can be stored in the [server settings](../server/server-settings/). In the login window, the DNS hostname of the server for which the certificate is issued needs to be entered into the *Server* field. | | | Custom credentials (Kerberos encryption) | The custom user, whose user name and password are entered in the login window, authenticates themselves to the Server. All data exchanged between Designer & Server is encrypted using Kerberos. | | | Anonymous (no encryption) | There is no authentication. The data exchanged between Designer & Server is transferred in plain text without transport encryption. | |

### Activate / Deactivate Authentication

Follow the steps below to activate or deactivate authentication methods:

1. Open **Server > Settings** from the main window of the Designer.
1. In the tab *Configuration Server*, activate or deactivate your authentication methods.
1. Click **[OK]** to confirm your input. If prompted, restart the server.

When [starting the Designer](../designer/), only the activated authentication methods are available for the server connection.

The following article shows how to install an X.509 certificate for transport encryption.\
The installation of an X.509 certificate is required to use [Transport Layer Security (TLS)](https://learn.microsoft.com/en-us/windows/win32/secauthn/transport-layer-security-protocol) and secure authentication with Xtract Universal.

### About X.509 Certificates

There are two main approaches for creating an X.509 certificate:

- Certificate released by an (internal) certification authority (CA)
- Self-signed certificate

On test environments you can use a self-signed certificate. For production environment it is recommended to use a certificate released by an (internal) certificate authority (CA).

### Create and Import the X.509 Certificate

Make sure to have a TLS certificate issued by your IT network team considering the following points:

1. The certificate property “Subject Alternative Name” contains the DNS name of the server that runs the XtractUniversal Windows service. When activating TLS, the *Subject Alternative Name* is used as the new hostname.
1. The certificate common name (CN) attribute contains the DNS name of the server. To display the Common Name (CN) of the certificate, double-click the certificate in the Cetrificate Manager and navigate to the *Details* tab.
1. Import the certificate to the [Windows Certificate Store](<https://technet.microsoft.com/en-us/ms788967(v=vs.91)>) of the machine, that runs the XtractUniversal Windows service using the Microsoft Management Console (mmc.exe). The depicted example uses the server name "TODD":
1. Right-click the certificate and navigate to **All Tasks > Manage private keys** to add a new permission entry for the Windows user that runs the XtractUniversal Windows service.
1. Enter the object name "NT Service\\XtractUniversal Service" and click **[Check Names]** before applying the changes.

The certificate is now available on your machine.

Note

The Windows Certificate Store works with most browsers. NMozilla Firefox offers its own certificate storage. Configure your Firefox browser to trust certificates in the Windows certificate store or import the certificate via an enterprise policy, see [Mozilla Support: Setting Up Certificate Authorities (CAs) in Firefox](https://support.mozilla.org/en-US/kb/setting-certificate-authorities-firefox).

### Integrate the X.509 Certificate

1. Open **Server > Settings** from the main window of the Designer.
1. In the tab *Web Server*, click **[Select X.509 certificate]**. The window "Edit certificate location" opens.
1. Select the X.509 certificate created for your machine under **Local Machine > Personal**.
1. Click **[OK]** to confirm your input. If prompted, restart the server.

The Xtract Universal server is now accessible via https protocol.

______________________________________________________________________

#### Related Links

- [Knowledge Base Article: Enable Secure Network Communication (SNC) via X.509 certificate](../../../knowledge-base/enable-snc-using-pse-file/)
- [Knowledge Base Article: Certificate Renewal for TLS](../../../knowledge-base/certificate-renewal/)
- [Change Service Account](../../server/service-account/)

This page shows how to restrict access to Xtract Universal's Designer to Windows AD users or [custom users](../user-management/#create-custom-users). Access restrictions on the Designer ensure that only dedicated users can create or edit destinations, sources and extractions. Access restrictions can be performed on several levels.

### Restrict Access to the Designer

Follow the steps below to restrict the access to the Designer to specific [users or user groups](../user-management/). By default, the access restrictions also apply to extractions and sources.

1. Open **Server > Settings** from the main window of the Designer.
1. In the tab *Configuration Server*, activate the option **Restrict Designer access to the following users/groups**.
1. Activate or deactivate authentication methods for the users or user groups, see [Authentication Between Designer and Server](../#authentication-between-designer-and-server).
1. Click **[Add]** to add existing Windows AD users or [custom users](../user-management/#create-custom-users).
1. Assign access rights to the users or user groups.

After access restrictions are set up, only designated users can connect to the Xtract Universal server when [starting the Designer](../../designer/).

### Restriction Levels

A user group can have one of the following access rights. These rights only concern actions (read, create, modify) that can be performed within the Designer.

| Restriction Level | Description | | --- | --- | | Read | Members of this group have read access, but cannot edit extractions. | | Modify | Members of this group have the same access rights as users with "read" rights. Furthermore, users with "modify" rights can edit existing extractions, but cannot create or clone extractions. | | Create | Members of this group have identical access rights as the users with "modify" rights. In addition, users with "create" rights can create and clone extractions, but cannot perform any admin activities. | | Admin | Members of this group have all rights, no restrictions and can perform admin tasks. Admin activities include changing server settings, accessing server logs, or editing users and connections (SAP and target environments). Access restrictions on extractions or the source system are ignored. |

### Restrict Access to Extractions

Access control can be performed at extraction level.\
By default, the Designer access restrictions also apply to extractions and sources.

1. Double-click an extraction. The main window of the extraction type opens.
1. Click **[General Settings]**. The window "General Settings" opens.
1. In the tab *Security*, add or remove existing users and assign access rights to the users.

The access control on the extraction level overrides the settings at server level.

### Restrict Access to Sources

Access control can be performed at the SAP source level.\
By default, the Designer access restrictions also apply to extractions and sources.

1. Open **Server > Manage Sources** from the main window. The window "Manage Sources" opens.
1. Click **[]** to edit an existing connection. The window "Change Source" opens.
1. In the tab *Access Control*, add or remove existing users and assign access rights to the users.

The access control on the source level overrides the settings at server level.

This page shows how to restrict access to Xtract Universal's built in web server to Windows AD users or [custom users](../user-management/#create-custom-users). Access restrictions on the web server ensure that only dedicated users can execute extractions. Windows AD credentials or credentials of a custom user must be submitted when running an extraction.

### Activate TLS Encryption

Access restrictions require users to access the web server through an https connection (TLS encryption). This requires the installation of an X.509 certificate. If the certificate is not listed in the Windows certificate store, [install the X.509 certificate](../install-x.509-certificate/#create-and-import-the-x509-certificate).

1. Open **Server > Settings** from the main window of the Designer.

1. In the tab *Web Server* tab, select one of the following protocols:

   - **HTTPS - Restricted to AD users with Designer read access**
   - **HTTPS - Restricted to custom users with Designer read access**

1. Click **[Select X.509 certificate]**. The "Edit certificate location" window opens.

1. Select the X.509 certificate created for your machine under **Local Machine > Personal**.

1. Click **[OK]** to confirm your input. The window closes.

1. Optional: Change the port number of the *HTTPS port*.

1. Click **[OK]** to confirm your input. If prompted, restart the server.

### Restrict Access to Windows AD Users (Kerberos Authentication)

Follow the steps below to limit the execution of extractions to users that pass Windows AD credentials, when calling extractions. The caller must have at least *Read access* to the Designer.

1. Assign a Windows service account under which the Xtract Universal service runs, see [Change Service Account](../../server/service-account/).
1. [Activate TLS encryption](#activate-tls-encryption).
1. Open **Server > Settings** from the main window of the Designer.
1. In the tab *Web Server*, select **HTTPS - Restricted to AD users with Designer read access**.
1. In the tab *Configuration Server*, add the custom users or groups that are allowed to execute extractions. For more information, see [Designer Access](../restrict-designer-access/#restrict-access-to-the-designer).
1. Assign at least *Read* permission to the Windows AD users.
1. Close all windows with **[OK]**. If prompted, restart the server.

Note

This type of authentication uses Kerberos authentication via SPNEGO. NTLM is not supported.

### Restrict Access to Custom Users (Basic Authentication)

Follow the steps below to limit the execution of extractions to users that pass [custom credentials](../user-management/#create-custom-users), when calling extractions. The custom user must have at least *Read access* to the Designer.

1. [Activate TLS encryption](#activate-tls-encryption).
1. Open **Server > Settings** from the main window of the Designer.
1. In the tab *Web Server*, select **HTTPS - Restricted to custom users with Designer read access**.
1. In the tab *Configuration Server*, add the custom users or groups that are allowed to execute an extraction. For more information, see [Designer Access](../restrict-designer-access/#restrict-access-to-the-designer).
1. Assign at least *Read* permission to the custom users.
1. Close all windows with **[OK]**. If prompted, restart the server.

Note

For information on how to call an extraction with Basic Authentication using the *xu.exe*, see [Basic Authentication via Commandline](../../execute-and-automate/call-via-commandline/#basic-authentication-via-commandline).

______________________________________________________________________

#### Related Links

- [Wikipedia: SPNEGO](https://en.wikipedia.org/wiki/SPNEGO)
- [Knowledge Base Article: Enable Secure Network Communication (SNC) via X.509 certificate](../../../knowledge-base/enable-snc-using-pse-file/)
- [Knowledge Base Article: Certificate Renewal for TLS](../../../knowledge-base/certificate-renewal/)
- [Server Settings](../../server/server-settings/)

This page shows how to create custom users and user groups in the Xtract Universal Designer. To restrict access to the Xtract Universal Designer or Server, you can use already existing [Windows AD Users](https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/manage/understand-default-user-accounts) and Groups or create your own custom users and groups.

Note

Windows AD Users and Groups are created outside of the Designer. Be careful to only add Windows AD *Security Groups* to the Designer. Users that are assigned to *Distribution Groups* are denied access at logon.

### Create Custom Users

Follow the steps below to create custom users:

1. Open **Server > Manage Users** from the main window of the Designer.
1. Click **[Add]** to create a new user.
1. Enter a username and a password for the user.
1. Click **[OK]** to confirm your input.

At the time of user creation, no rights need to be assigned. For information on how to assign access rights, see [Designer Access](../restrict-designer-access/).

### Assign Custom Users to User Groups

Follow the steps below to assign custom users to user groups. It is recommended to assign users to groups and grant access to particular actions instead of adding single users and granting them access one by one.

1. Open **Server > Settings**.
1. In the tab *Configuration Server*, activate the option **Restrict Designer access to the following users/groups**.
1. Click **[New]** to create a custom user group.
1. Click **[OK]** to close the server settings. If prompted, restart the Designer.
1. Open **Server > Manage Users** from the main window of the Designer.
1. Select an existing custom user and click **[Edit]**. The "Edit user" window opens.
1. Use the arrows **[\<]** and **[>]** to assign and remove users to and from groups.
1. Click **[OK]** to confirm your input.

Custom users can only be assigned to custom user groups. Windows AD users can only be assigned to Windows AD groups, but not to custom user groups.

### User Groups after Migration

As custom users can only be assigned to custom user groups and Windows AD users can only be assigned to Windows AD groups, the created custom user groups may disappear when migrating to a newer product version. This does not affect access management, but the access at user level is resolved.

To grant access at group level, the Windows AD users need to be assigned to new Windows AD groups.

This page shows how to use the BAPI extraction type.\
The BAPI extraction type can be used to parameterize and execute SAP function modules and BAPIs for automation.

### About Function Modules / BAPIs

Function modules are procedures that encapsulate and reuse global functions in the SAP system. SAP systems contain several predefined functions modules that can be called from any ABAP program. A Business Application Programming Interface (BAPI) is a remote function module that can access business data and processes of an SAP system from different systems.

### Custom BAPIs

The use of custom BAPIs (Z function modules) is possible. Issues specific to Z function modules are not included in the scope of support provided by Theobald Software.

### Prerequisites

- A connection to an SAP system is available, see [SAP Connection](../sap-connection/).
- The SAP user has sufficient user rights, see [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#bapi).

Warning

**Missing Authorization.**\
To use the BAPI extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust [SAP Authority Objects: BAPI](../setup-in-sap/sap-authority-objects/#bapi) accordingly.

### Create a BAPI Extraction

1. In the main window of the Designer, click **[New]** to create a new extraction. The window "Create Extraction" opens.
1. Select an [SAP Connection](../sap-connection/) of type **RFC** from the drop-down menu **Source**.
1. Enter a unique name for your extraction.
1. Select the extraction type **BAPI**.
1. Click **[OK]**. The main window of the extraction type opens automatically.

The majority of the functions of the extraction type can be accessed in the main window.

### Look up a Function Module / BAPI

1. In the main window of the extraction type, click **[]** to look up a Function Module / BAPI. The window "Function Module Lookup" opens.
1. In the window "Function Module Lookup" enter the name of the function module or BAPI . Use wildcards (\*) if needed.
1. Click **[]** . Search results are displayed.
1. Select a Function Module / BAPI and click **[OK]**.

The application returns to the main window of the extraction type.

### Define the BAPI Extraction Type

The BAPI extraction type offers the following options for defining parameters of a Function Module / BAPI:

1. Add input parameters (data you want to send to SAP) in **Imports**, see [Import Parameters](input-and-output/#import-parameters).\
   You can enter scalar values or structures .

1. Add output parameters (data you want to receive from SAP) in **Exports**, see [Export Parameters](input-and-output/#export-parameters).\
   Select output by activating the checkbox next to the items.

1. Optional: If available, define input and output parameters in **Changings**, see [Changings Parameters](input-and-output/#changing-parameters).

1. Add tables to the output of the extraction type or add table parameters to the input of the extraction type in **Tables**, see [Table Parameters](input-and-output/#table-parameters).

   - Click **[]** to check the names and data types of the table fields .
   - Activate the checkbox in the output column to add items to the output .
   - Click **[]** to edit the content of the table .

1. Optional: If available, define which exceptions thrown by the Function Module / BAPI are ignored during runtime, see [Exceptions](input-and-output/#exceptions).

1. Check the [General Settings](general-settings/) before running the extraction.

1. Click **[OK]** to save the extraction type.

You can now run the extraction, see [Execute and Automate Extractions](../execute-and-automate/).

______________________________________________________________________

#### Related Links

- [Knowledge Base: Access Data in the SAP Public Cloud](../../knowledge-base/access-data-in-the-sap-public-cloud/)
- [Knowledge Base: Read Data from Cluster Fields in Tables PCL1 and PCL2 (Payroll)](../../knowledge-base/read-data-from-cluster-fields-in-the-tables-pcl1-and-pcl2-payroll/)

Runtime parameters are are placeholders for values that are passed at runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom).

## Create Runtime Parameters

There are two types of runtime parameters:

- [Scalar parameters](#scalar-parameters) represent a single value.
- [List parameters](#list-parameters) represent multiple values.

### Scalar Parameters

Follow the steps below to create a scalar runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add Scalar]** to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.

   | Type | Description | | --- | --- | | *Text* | Can be used for any type of SAP selection field. | | *Number* | Can be used for numeric SAP selection fields. | | *Flag* | Can only be used for SAP selection fields THAT require an ‘X’ (true) or a blank ‘‘ (false) as input value. |

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

### List Parameters

Follow the steps below to create a list runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add List]** to define list parameters that contain multiple values separated by commas e.g., 1,10 or “1”, “10”. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

## Assign Runtime Parameters

Follow the steps below to assign runtime parameters to input fields of import, changings or table parameters of a Function Module / BAPI:

1. Navigate to the item you want to parameterize in the **Imports**, **Changings** or **Tables** tab.
1. Click the icon button next to the item to switch from static values () to runtime parameters (). If no icon is available, [create a runtime parameter](#create-runtime-parameters).
1. Select a runtime parameter from the dropdown-list.

Pass values during runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom).

This page contains an overview of the settings in the window "General Settings".\
To open the general settings, click **General Settings** in the main window of the extraction type.

### Misc. Tab

The *Misc.* tab covers cache settings, column encryption and keywords of an extraction type.

#### Cache results

The *Cache results* option is only available in [pull destinations](../../destinations/), e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times. To decrease the SAP server load, you can select the **Cache results** option, this way the pull destination pulls the data from cache and not from the SAP.

Cache results increases the performance and limits the impact on the SAP system. If this behavior is undesirable (for example, because the data must be always 100% up-to-date), the cache option must be explicitly disabled.

#### Keywords

One or more keywords (tags) can be assigned to an extraction. Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the "Search Extractions" window. To open the "Search Extractions" window, click **[ Search]** in the main window of the Designer.

Tip

To add keywords to multiple extractions at once, select the extractions in the main window of the Designer.\
Right-click + **Add/Remove keywords** opens the window "Add/Remove Keywords To/From Multiple Extractions".

### Primary Key Tab

Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.

The depicted example shows the SAP object *MAKT* with its primary key inherited from SAP in the general settings of the Designer. In this example the primary key consists of *MANDT*, *MATNR* and *SPRAS*. The primary key is also taken over in the destination.

Note

A defined primary key field in a table is a prerequisite for merging data.

#### Generate Surrogate Key Column

If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs. The surrogate keys are hash values of type signed 8 byte integer, e.g., `#-3008591679982390000`. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.

### Security Tab

Restrict user access to the extraction. For more information, see [Restrict Designer Access](../../access-restrictions/restrict-designer-access/).

### Columns Order Tab

The "Columns Order" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns. Index 0 defines the first column in the result set, index 1 the second columns, etc.

| Option | Description | | --- | --- | | **Enabled** | When this option is active, the defined column order is applied when running the extraction. | | **Swap** | Swaps the index of 2 columns. All other columns keep their indexes. | | **Insert** | Inserts the selected column into the selected index. All other indexes are recalculated. | | **Reset default** | Restores the original column order. |

This page contains an overview and description of the Import, Export, Changings and Tables parameter in the BAPI extraction type.

Each Import, Export, Changings and Tables parameter can have one of the following representations:

- A scalar field (e.g., characters, date, time, number, amount etc.).
- A structure consisting of several components.
- A table (tabular array of data) consisting of columns (data values of the same type) and rows (data records).

## Import Parameters

Import parameters represent the input values sent from the client to SAP. In the tab **Imports** you can define import parameters that can be presented as scalar values or structures .

To use the filtering function, enter text in the headers of the columns **Name** and **Description**.

### Scalar Parameters

Assign single values to an import parameter by using one of the following options:

- If the input field is empty, enter a static value.
- If a checkbox is displayed in the input field, the parameter is predefined in SAP. The value in SAP is displayed in a light gray font. To disable the predefined value, activate the checkbox and leave the field empty or enter a new value.
- If **[]** is displayed in the input field, enter a static value. Click **[]** to switch between static values and dynamic values that are set at runtime.
- If **[]** is displayed in the input field, select a [runtime parameter](../edit-runtime-parameters/). Click **[]** to switch between dynamic values and static values.

When using runtime parameters, make sure the data type of the input matches the data type in SAP.

### Structure Parameters

When a Function Module / BAPI uses structures as import parameters, you can assign structure elements (i.e. fields) similarly to single [scalar fields](#scalar-parameters). Setting a single value or a parameter for the whole structure is not possible.

1. Click **[]**. The window "Edit Structures" opens.
1. Enter static values ( **[]** icon or no icon) or assign runtime parameters ( **[]** icon).

Tip

It is possible to use tables as input parameters, see [Tables Parameters](#table-parameters).

## Export Parameters

Export parameters represent the output values sent from SAP back to the client after the execution of a Function Module. In the tab **Export** you can select the items you want to add to the output of the extraction type.

### Add Items to Output

Mark the checkbox in the output column to add an item to the output of the extraction type.

To use the filtering function, enter text in the headers of the columns **Name** and **Description**.

Tip

It is possible to use tables as output parameters, see [Add Tables to Output](#add-tables-to-output).

## Changing Parameters

Changing parameters represent parameters that can be used for input and output. In the tab **Changings** you can define the changing parameters.

## Table Parameters

Table parameters are parameters presented in a table structure consisting of multiple rows. Tables can be used for input and output. In the tab **Tables** you can define table parameters for importing and exporting data into and from an SAP Function Module or BAPI.

Tables represent a structure of multiple rows of the same data type.

To use the filtering function, enter text in the headers of the columns **Name** and **Description**.

Note

Only **5** tables are available for parallel exporting.

- Click **[]** to check the names and data types of the table fields .
- Activate the checkbox in the output column to add items to the output .
- Click **[]** to edit tables .

### Access Metadata of Tables

Click **[]** to display the metadata of a table. The metadata includes the name and the data type of all fields. After the function module was edited in SAP, refresh the metadata by clicking **Refresh metadata**

### Add Tables to Output

Mark the checkbox in the output column to add a table to the output.

### Edit Tables

You can assign tables elements (i.e. fields) similarly to single [scalar fields](#scalar-parameters):

1. Click **[]**. The window "Edit Table Contents" opens.
1. Click **[Add]** to add new set of parameters.
1. Enter values or runtime parameters.\
   When using runtime parameters, make sure the data type of the input matches the SAP data type.
1. Click **[Remove]** to delete a row.

## Exceptions

**Exceptions** refer to ABAP exceptions / errors messages of an SAP BAPI. If an exception occurs during runtime, Xtract Universal returns a corresponding error message.

By default, all exceptions result in errors when running the BAPI extraction type. To ignore exceptions during runtime, deselect the exceptions in the **Exceptions** tab.

This page shows how to use the BWCube extraction type.\
The BWCube extraction type can be used to extract MDX or BICS data directly from BW InfoProviders (e.g., Cubes) or from BW Queries. The BW Queries can be based on InfoProviders.

### Prerequisites

- A connection to an SAP system is available, see [SAP Connection](../sap-connection/).
- The SAP user has sufficient user rights, see [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#bw-cube-bw-query).
- To extract a BW Query, the attribute *Allow External Access to this Query* of the Query must be active in the BEx Query Designer or the BW Modeling Tool, see [Troubleshooting Article: Allow external access to BW Queries](https://support.theobald-software.com/helpdesk/KB/View/13800-allow-external-access-to-bw-queries).

Warning

**Missing Authorization.**\
To use the BWCube extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust [SAP Authority Objects: BW Query / BW Cube](../setup-in-sap/sap-authority-objects/#bw-cube-bw-query) accordingly.

### MDX versus BICS

The BWCube extraction type can extract data using the OLAP BAPI Interface (MDX) or the native BICs interface. The following table shows the differences between the two extractors:

| MDX | BICS | | --- | --- | | **Lookup syntax for BEx Queries:** `[tech. name of InfoPovider]/[tech. name of BEx Query]` | **Lookup syntax for BEx Queries:** `[tech. name of BEx Query]` Example: `0SD_C03_Q0018` | | **Lookup syntax for InfoProviders:** `$[tech. name of InfoProvoider]` | **Lookup syntax for InfoProviders:** `[tech. name of InfoProvider]` Example: `0SD_C03` | | **Wildcards in lookup:** The BEx-Query setting [Allow External Access to BW Queries](https://support.theobald-software.com/helpdesk/KB/View/13800-allow-external-access-to-bw-queries) is required. Example: `*0SD_C03_Q0018` instead of `0SD_C03/0SD_C03_Q0018` | **Wildcards in lookup:** not required and therefore not supported | | **Supported InfoProviders:** InfoCubes, Multiproviders, Composite Providers | **Supported InfoProviders:** InfoCubes, MuliProviders, Composite Providers, DSOs | | **Column names of Key figures:** EnterpriseID | **Column names of Key figures:** Technical name If techn. name is empty: name of the base measure. If name of the base measure is empty: EnterpriseID. **Tip:** In case of duplicate names, change the technical name in the BEx Query Designer. | | **Limit of Dimensions:** max. 50 dimensions | **Limit of Dimensions:** - |

### Create a BW Cube Extraction

1. In the main window of the Designer, click **[New]** to create a new extraction. The window "Create Extraction" opens.
1. Select an [SAP Connection](../sap-connection/) of type **RFC** from the drop-down menu **Source**.
1. Enter a unique name for your extraction.
1. Select the extraction type **BW Cube**.
1. Click **[OK]**. The main window of the extraction type opens automatically.

The majority of the functions of the extraction type can be accessed in the main window.

### Look Up a BW Cube or Query

1. In the main window of the extraction type, click **[]**. The window “Cube and Query Lookup” opens.

1. Select the *Extractor*, *Datasource Type* and *Extraction Settings* of the object .

   | Option | Description | | --- | --- | | *Extractor* | defines if data is extracted using the OLAP BAPI Interface (MDX) or the native BICS Interface. BICS can only be used in combination with the [NetWeaver RFC protocol](../sap-connection/settings/#rfc-libraries). | | *Datasource Type* | defines if the look up searches for a BEx Query or an InfoProvider. | | *Extraction Settings* | only available for the MDX extractor. Use **Only Structure** if your BWCube extraction was created in an older software version, see [extraction settings](settings/) for more information. |

1. In the search bar, enter the name of a Query or a BW Cube / InfoProvider . Use wildcards (\*), if needed.

1. Click **[]**. Search results are displayed.\
   Alternatively click **[Direct Load]** to skip the lookup and load the BW Cube / InfoProvider directly. **[Direct Load]** only works if the full name is entered correctly in the search bar.

1. Select a Query or BW Cube / InfoProvider and click **[OK]** to confirm.

The application now returns to the main window of the component.

Note

Click **[Refresh Metadata]** to renew metadata. This is necessary if a data source has been adjusted in SAP, another source system has been connected, or the source system has been updated.

Warning

**Invalid action**\
A BW Query does not apprear in the list.\
Switch on the attribute *Allow External Access to this Query* in the BEx Query Designer or the BW Modeling Tool. For additional details see the [Troubleshooting Article: Allow external access to BW Queries](https://support.theobald-software.com/helpdesk/KB/View/13800-allow-external-access-to-bw-queries).

### Define the BWCube Extraction Type

The BWCube extraction type offers the following options for Query and BW Cube extractions:

1. In the tree structure of the extraction type, select the measure (key figures) you want to extract

   Tip

   The tree structure represents the metadata of the Query (or InfoProvider).\
   The first directory contains all the measures (key figures) . The following directories correspond to dimensions and often contain additional dimension properties .

1. Within the key figures directory, click the arrow to display the available units. Select a unit, if needed.

1. In the following directories, select the dimensions and properties you want to extract .

1. Optional: Right-click on a dimension to add filters to the dimension, see [Dimension Filters](variables-and-filters/#set-dimension-filters).

1. Optional: If a BW Query has a defined variable, click **[Edit Variables]** to edit the variable or provide input values, see [Variables](variables-and-filters/#edit-variables).

1. Click **[Load live preview]** to display a live preview of the data. For every selected dimension or property, a key figure and a unit is displayed in the result.

1. Check the [Extraction Settings](settings/) and the [General Settings](general-settings/) before running the extraction.

1. Click **[OK]** to save the extraction type.

You can now run the extraction, see [Execute and Automate Extractions](../execute-and-automate/).

Runtime parameters are are placeholders for values that are passed at runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom). They can be created in context of [Query Variables](../variables-and-filters/#edit-variables) and [Dimension Filters](../variables-and-filters/#set-dimension-filters).

## Create Runtime Parameters

There are two types of runtime parameters:

- [Scalar parameters](#scalar-parameters) represent a single value.
- [List parameters](#list-parameters) represent multiple values.

### Scalar Parameters

Follow the steps below to create a scalar runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add Scalar]** to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.

   | Type | Description | | --- | --- | | *Text* | Can be used for any type of SAP selection field. | | *Number* | Can be used for numeric SAP selection fields. | | *Flag* | Can only be used for SAP selection fields THAT require an ‘X’ (true) or a blank ‘‘ (false) as input value. |

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

### List Parameters

Follow the steps below to create a list runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add List]** to define list parameters that contain multiple values separated by commas e.g., 1,10 or “1”, “10”. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

## Assign Runtime Parameters

Follow the steps below to assign the runtime parameters to variables or dimension filters.

1. In the main window of the extraction type, click **[Edit Variables]**. The window "Edit Selections" opens.

1. Add a filter to the variable, see [Variables](../variables-and-filters/#edit-variables).

1. Click the icon button next to the input field to switch between static values () and runtime parameters (). If no icon button is available, [create a runtime parameter](#create-runtime-parameters).

1. Select a runtime parameter from the dropdown-list.

1. Click **[OK]** to confirm the input.

1. In the main window of the extraction type, right-click the dimension you want to parameterize.

1. Click **Edit Filters**. The window "Edit Selections" opens.

1. Add a filter for the dimension, see [Dimension Filter](../variables-and-filters/#set-dimension-filters).

1. Click the icon button next to the input field to switch between static values () and runtime parameters (). If no icon button is available, [create a runtime parameter](#create-runtime-parameters).

1. Select a runtime parameter from the dropdown-list.

1. Click **[OK]** to confirm the input.

Pass values during runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom).

This page contains an overview of the settings in the window "General Settings".\
To open the general settings, click **General Settings** in the main window of the extraction type.

### Misc. Tab

The *Misc.* tab covers cache settings, column encryption and keywords of an extraction type.

#### Cache results

The *Cache results* option is only available in [pull destinations](../../destinations/), e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times. To decrease the SAP server load, you can select the **Cache results** option, this way the pull destination pulls the data from cache and not from the SAP.

Cache results increases the performance and limits the impact on the SAP system. If this behavior is undesirable (for example, because the data must be always 100% up-to-date), the cache option must be explicitly disabled.

#### Keywords

One or more keywords (tags) can be assigned to an extraction. Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the "Search Extractions" window. To open the "Search Extractions" window, click **[ Search]** in the main window of the Designer.

Tip

To add keywords to multiple extractions at once, select the extractions in the main window of the Designer.\
Right-click + **Add/Remove keywords** opens the window "Add/Remove Keywords To/From Multiple Extractions".

### Primary Key Tab

Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.

The depicted example shows the SAP object *MAKT* with its primary key inherited from SAP in the general settings of the Designer. In this example the primary key consists of *MANDT*, *MATNR* and *SPRAS*. The primary key is also taken over in the destination.

Note

A defined primary key field in a table is a prerequisite for merging data.

#### Generate Surrogate Key Column

If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs. The surrogate keys are hash values of type signed 8 byte integer, e.g., `#-3008591679982390000`. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.

### Security Tab

Restrict user access to the extraction. For more information, see [Restrict Designer Access](../../access-restrictions/restrict-designer-access/).

### Columns Order Tab

The "Columns Order" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns. Index 0 defines the first column in the result set, index 1 the second columns, etc.

| Option | Description | | --- | --- | | **Enabled** | When this option is active, the defined column order is applied when running the extraction. | | **Swap** | Swaps the index of 2 columns. All other columns keep their indexes. | | **Insert** | Inserts the selected column into the selected index. All other indexes are recalculated. | | **Reset default** | Restores the original column order. |

This page contains an overview of the extraction settings in the BW Cube extraction type.\
To open the extraction settings, click ****Extraction Settings**** in the main window of the extraction type.

### Extraction Settings

#### Package Size

The extracted data is split into packages of the defined size. The default value is 50000 lines.\
A package size between 20000 and 50000 is advisable for large amounts of data. 0 means no packaging. Not using packaging can lead to an RFC timeout for large data extractions.

Warning

**RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table**\
To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.

This option is not supported when using BICS mode.

#### Row Limit

Specifies the maximum number of extracted records. 0 extracts all data. You can use this option to perform tests with a small amount of data by entering a row limit of e.g., 1000.

This option is not supported when using BICS mode.

#### Member Display

This setting is only available for the BICS extractor.

- *Key*: Extracts the key of the dimension member.
- *Text* (default): Extracts the text of the dimension member.
- *Text and Key*: Adds an additional column for every dimension with the suffix `.KEY`. The original column contains the text of the dimension member, the column with the `.KEY` suffix contains the key of the dimension member.

#### Formatted Values

If this option is active, the values of key figures are formatted as defined in the Query Designer, e.g., scaling factor and number of decimal places.

#### Only Structure

Activate **Only Structure** if your BW Cube extraction was created in an old version of the BW Cube component. The method for the metadata retrieval has changed, which affects especially BEx Queries with two structures.\
This option can only be activated and deactivated in the [Look Up](../#look-up-a-bw-cube-or-query) window of the component.

### Automatic Slicing Dimension

This option sets a dimension for an automatic slicing. Slicing is the act of picking a subset of a cube by choosing a single value for one of its dimensions. Automatic slicing means that a loop is executed for each single value of the chosen slicing dimension (characteristic) to extract the result from BW.

**Automatic Slicing Dimension** allows the extraction of a large amount of data (millions of records) from BW.

### Dimension on Columns

The following extraction setting is only available for MDX extractions. It appears in the extraction settings window after retrieving the metadata of an MDX extraction.

**Dimension on Columns** allows selecting another dimension/structure than the measure (key figures) dimension on the column axis. This swaps the measures and the selected dimension: the measures are placed on rows, the selected dimension is placed on columns.

Note that:

- your BEx Query must contain a dimension structure.
- dimension filters on columns are not applied.
- the selected properties for the new column dimension are ignored.
- unit columns are only supported when *key figures* is selected for the columns.
- up to 1000 members will be loaded when confirming the extraction settings window. These members will be the column names.

Recommendation

We recommend only using structures on columns.

Example:

- Output for **Dimension on Columns** = *Key Figures*
- Output for **Dimension on Columns** = *Cal. Year/Quarter [0CALQUARTER]*

### Experimental

The following settings are only available for BICS extractions. They appear in the extraction settings window after retrieving the metadata of a BICS extraction.

#### Create BICS Compatibility Report

When encountering an error using BICS, click **[Create BICS Compatibility Report]** to run a number of tests to help us analyze why and where the error occured.\
The test results are automatically stored in a .zip file. If you have multiple different SAP systems, perform this test on each of them.\
Send the resulting .zip files to the [Theobald Support](https://support.theobald-software.com) team.

This page shows how to use variables and dimension filters to filter the data that is extracted with the BWCube extraction type. There are two options to apply filters:

- [Variables](#edit-variables) are usually defined in SAP to create filter options for BW Queries. They can be edited in the BWCube extraction type.
- [Dimension Filters](#set-dimension-filters) are defined in the BWCube extraction type to filter data from InfoProviders or BW Queries that do not have variables.

### Edit Variables

BW queries often have defined variables to create filter options. Depending on the type of BEx variable (single value, multiple value, interval or complex selection) input fields of the variables are enabled or disabled.

1. [Look up](../#look-up-a-bw-cube-or-query) a BEx Query with defined variables .
1. Click **[Edit Variables]** . The window "Edit variables for [name of the query]" opens.
1. Select a field that uses variables from the drop-down menu .
1. Click **[Single]**, **[Range]** or **[List]** to add a corresponding filter, see [Filter Options](#filter-options).
1. In the column **Sign** , select *Include* to add the filtered data to the output or select *Exclude* to remove the filtered data from the output.
1. Select an operator in the column **Option** , see [Filter Options](#filter-options).
1. In the column **Value**, enter values directly into the input fields **Low** and **High**, assign existing [runtime parameters](../edit-runtime-parameters/) or look up pre-defined values in SAP.
   - Static values: Enter values directly into the Low and High input fields.
   - Runtime parameters: Select an existing runtime parameter from the drop-down list.
   - Pre-defined values: If available, select pre-defined values from SAP:
1. Click **[OK]** to confirm your input.
1. Click **[Load live preview]** in the main window of the component to check the result of the filter. If runtime parameters are defined, you are prompted to populate the parameters with actual values.

### Set Dimension Filters

Each dimension of a BEx Query or an InfoCube offers the possibility to set a filter to execute the MDX statement in BW using the selected filter values. Follow the steps below to create dimension filters in the BWCube extraction type.

1. In the tree structure of the extraction type, right-click a dimension to open the context menu.
1. Click **[Edit Filter]**. The window "Edit Selections" opens.
1. Click **[Single]**, **[Range]** or **[List]** to add a corresponding filter, see [Filter Options](#filter-options).
1. In the column **Sign** , select *Include* to add the filtered data to the output or select *Exclude* to remove the filtered data from the output.
1. In the column **Option**, select an operator , see [Filter options](#filter-options).
1. In the column **Value**, enter values directly into the input fields **Low** and **High**, assign existing [runtime parameters](../edit-runtime-parameters/) or look up pre-defined values in SAP.
   - Static values: Enter values directly into the Low and High input fields.
   - Runtime parameters: Select an existing runtime parameter from the drop-down list.
   - Pre-defined values: If available, select pre-defined values from SAP.
1. Click **[OK]** to confirm your input.
1. Click **[Load live preview]** in the main window of the extraction type to check the result of the filter. If runtime parameters are defined, you are prompted to populate the parameters with actual values.

When filters are applied, the symbol is displayed in the treeview of the dimensions.

Warning

**Extraction fails - Error message: Argument cannot be null or empty**\
When a filter is set, a value cannot be empty for an extraction to run.\
Make sure to pass a value (# is accepted).

### Filter Options

The BW Cube extraction type offers the following filter options:

| Type | Operator | Description | | --- | --- | --- | | Single | | Compare data to a single specified value. | | | *(not) like pattern* | True if data values do (not) contain to the specified value. | | | *(not) equal to* | True if data is (not) equal to the specified value. | | | *at least* | True if data is greater than or equal to the specified value. | | | *more than* | True if data is greater than the specified value. | | | *at most* | True if data is less than or equal to the specified value. | | | *less than* | True if data is less than the specified value. | | Range | | Check if the data is (not) within a specified range of values. | | | *(not) between* | True if data values do (not) lie between the 2 specified values. | | List | | Check if the data is part of a specified list of values. | | | *element of* | True if data values are part of the list. |

This page shows how to use the DeltaQ extraction type.\
The DeltaQ extraction type can be used to extract delta data from SAP DataSources. This means that only recently added or changed data is extracted, instead of a full load. For more information on the delta process, see [Initialize a Delta Process](update-mode/#initialize-a-delta-process).

### Prerequisites

- A connection to an SAP system is available, see [SAP Connection](../sap-connection/).
- The SAP user has sufficient user rights, see [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#deltaq).
- Configure your SAP system to make DataSources accessible, see [SAP Customization for DeltaQ](../setup-in-sap/customization-for-deltaq/).
- To use DataSources in Xtract Universal, make sure that the DataSources are activated in SAP.

Warning

**Missing Authorization.**\
To use the DeltaQ extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust [SAP Authority Objects: DeltaQ](../setup-in-sap/sap-authority-objects/#deltaq) accordingly.

### SAP Transactions for Working with DeltaQ

The following SAP transaction can support you when working with DataSources:

- SBIW - SAP DataSources start page

- RSA3 - Extractor Checker

- RSA5 - Install DataSources and Hierarchies from Business Content

- RSA6 - Postprocess DataSources and Hierarchies

- SM59 - Configuration of RFC Connections

- SMQS - qRFC Monitor (QOUT Scheduler): Number of *Max.Conn.* should be at least 10

- SM37 - Background jobs

- SM58 - Transactional RFC

- SM50 - Process overview

- SMGW - Gateway Monitor

- RSA7 - Delta Queue Maintenance

- SMQ1 - qRFC Monitor (outbound queue)

- WE02 - IDoc list

- WE20 - Partner profiles

### Create a DeltaQ Extraction

1. In the main window of the Designer, click **[New]** to create a new extraction. The window "Create Extraction" opens.
1. Select an [SAP Connection](../sap-connection/) of type **RFC** from the drop-down menu **Source**.
1. Enter a unique name for your extraction.
1. Select the extraction type **DeltaQ**.
1. Click **[OK]**. The main window of the extraction type opens automatically.

The majority of the functions of the extraction type can be accessed in the main window.

### Look Up Extractors or Hierarchies

1. In the main window of the extraction type, click **[]**. The window “Look Up OLTP Source” opens.

1. In the field **Name**, enter the name of a DataSource . Use wildcards ( * ), if needed.

1. Click **[]**. Search results are displayed.

1. Select an extractor and click **[OK]** to confirm.

1. In the main window of the extraction type, click **[]**. The window “Look Up OLTP Source” opens

1. In the field **Name**, enter the name of a Hierarchy . Use wildcards ( * ), if needed. Alternatively, enter the description of a Hierarchy in the field **Description**.

1. Click **[]**. Search results are displayed.

1. Select an extractor of type *HIER* and click **[OK]** to confirm. All available Hierarchies are fetched and the window "Look Up DeltaQ Hierarchies" opens.

1. Select a Hierarchy and click **[OK]** to confirm.

The application now returns to the main window of the extraction type.

### Define the DeltaQ Extraction Type

The DeltaQ extraction type offers the following options for DataSource extractions:

1. Navigate to **Gateway** and click **[]** to look up an RFC destination. For more information, see [DeltaQ Customizing: Gateway](deltaq-customization/#gateway).

1. Navigate to **Logical Destination** and click **[]** to look up a logical RFC target system. For more information, see [DeltaQ Customizing: Logical Destination](deltaq-customization/#logical-destination).

1. Click **Customizing Check** to validate the DeltaQ Customizing on the SAP system. Make sure that all check marks are green. For more information, see [DeltaQ Customizing: Customizing Check](deltaq-customization/#customizing-check).

1. Select an [Update Mode](update-mode/), e.g., to initialize delta extractions.

1. Select the items you want to extract.

1. Optional: click the **Edit** option next to an item to [filter the data](selections/).

1. Click **[Run]** to testrun the extraction and validate your settings.

1. Click **[Activate]** to activate the extraction in SAP. After a successful activation, a corresponding status message opens:

   Note

   The activation is only required for the update modes *Delta*, *Full* or *Init*. Do not activate the extraction for the *Delta Update* mode.

1. Check the [Extraction Settings](settings/) and the [General Settings](general-settings/) before running the extraction.

1. Click **[OK]** to save the extraction type.

1. Navigate to **Gateway** and click **[]** to look up an RFC destination. For more information, see [DeltaQ Customizing: Gateway](deltaq-customization/#gateway).

1. Navigate to **Logical Destination** and click **[]** to look up a logical RFC target system. For more information, see [DeltaQ Customizing: Logical Destination](deltaq-customization/#logical-destination).

1. Click **Customizing Check** to validate the DeltaQ Customizing on the SAP system. Make sure that all check marks are green. For more information, see [DeltaQ Customizing: Customizing Check](deltaq-customization/#customizing-check).

1. Select an [Update Mode](update-mode/), e.g., to initialize delta extractions.

1. Select the items you want to extract.

1. Optional: click the **Edit** option next to an item to [filter the data](selections/).

1. Optional: click **Extraction Settings** to set the language and output format of the Hierarchy, see [extraction settings](settings/).

1. Check the [Extraction Settings](settings/) and the [General Settings](general-settings/) before running the extraction.

1. Click **[Run]** to testrun the extraction and validate your settings.

1. Click **[OK]** to save the extraction type.

Tip

Unlike attributes and transactions, Hierarchies do not have to be activated.

You can now run the extraction, see [Execute and Automate Extractions](../execute-and-automate/).

### Execute DeltaQ in Parallel

When extracting multiple DataSources in parallel, it is recommended to use separate RFC destinations, e.g., XTRACT01, XTRACT02, etc.\
Parallel execution of DataSources with an identical RFC destination is possible, but not recommended.

______________________________________________________________________

#### Related Links

- [DeltaQ Troubleshooting Guide](../../troubleshooting/#deltaq-troubleshooting).
- [SAP Help: Activate the BI Content DataSource](https://help.sap.com/saphelp_scm70/helpdata/ru/d8/8f5738f988d439e10000009b38f842/content.htm?no_cache=true)
- [Knowledge Base Article: Create Generic DataSource using Function Module and Timestamps](../../knowledge-base/create-generic-datasource-using-function-module-and-timestamps/)

This page shows how to reference the SAP RFC destination in the DeltaQ extraction type.

Using the DeltaQ extraction type requires customization in SAP, see [Customizing for DeltaQ](../../setup-in-sap/customization-for-deltaq/). After an RFC Destination is set up in SAP, the RFC destination and the RFC target system must be entered in the DeltaQ extraction type.

### Settings in the DeltaQ Extraction Type

#### Gateway

Click **[]** to look up an [RFC destination](../../setup-in-sap/customization-for-deltaq/) or enter the data of your RFC destination manually:

| Input Field | Description | | --- | --- | | Host | The name (or IP address) of your SAP system. Make sure that the Gateway host is the same as in your [SAP Connection](../../sap-connection/). | | Service | The gateway service is generally *sapgwNN*, where *NN* is the instance number of your SAP system, e.g., a number between *00* and *99*. *NN* must have the same value as the *System No* field in your [SAP Connection](../../sap-connection/) or the instance number in your SAP logon. | | Program ID | The name of the registered RFC server. Make sure that the registration of the Program ID and the host is whitelisted in the reginfo ACL on the SAP Gateway, see [SAP Blog: RFC Gateway Security](https://blogs.sap.com/2021/01/26/rfc-gateway-security-part-1-basic-understanding/). |

#### Logical Destination

Click **[]** to look up a logical RFC target system or enter the name of the RFC target system manually.

### Customizing Check

In the main window of the component click **Customizing Check** to validate the DeltaQ customizing on the SAP system. Make sure that all check marks are green.

Runtime parameters are are placeholders for values that are passed at runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom). They can be created in context of [Selections](../selections/).

## Create Runtime Parameters

There are two types of runtime parameters:

- [Scalar parameters](#scalar-parameters) represent a single value.
- [List parameters](#list-parameters) represent multiple values.

### Scalar Parameters

Follow the steps below to create a scalar runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add Scalar]** to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.

   | Type | Description | | --- | --- | | *Text* | Can be used for any type of SAP selection field. | | *Number* | Can be used for numeric SAP selection fields. | | *Flag* | Can only be used for SAP selection fields THAT require an ‘X’ (true) or a blank ‘‘ (false) as input value. |

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

### List Parameters

Follow the steps below to create a list runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add List]** to define list parameters that contain multiple values separated by commas e.g., 1,10 or “1”, “10”. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

## Assign Runtime Parameters

Follow the steps below to assign the runtime parameters to selections.

1. In the main window of the extraction type, click the **[Edit]** button next to the selection you want to parameterize. The window "Edit Selections" opens.
1. Add a filter to the selection, see [Edit Selections](../selections/#edit-selections).
1. Click the icon button next to the input field to switch between static values () and runtime parameters (). If no icon button is available, [create a runtime parameter](#create-runtime-parameters).
1. Select a runtime parameter from the dropdown-list.
1. Click **[OK]** to confirm the input.

Pass values during runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom).

This page contains an overview of the settings in the window "General Settings".\
To open the general settings, click **General Settings** in the main window of the extraction type.

### Misc. Tab

The *Misc.* tab covers cache settings, column encryption and keywords of an extraction type.

#### Cache results

The *Cache results* option is only available in [pull destinations](../../destinations/), e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times. To decrease the SAP server load, you can select the **Cache results** option, this way the pull destination pulls the data from cache and not from the SAP.

Cache results increases the performance and limits the impact on the SAP system. If this behavior is undesirable (for example, because the data must be always 100% up-to-date), the cache option must be explicitly disabled.

#### Keywords

One or more keywords (tags) can be assigned to an extraction. Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the "Search Extractions" window. To open the "Search Extractions" window, click **[ Search]** in the main window of the Designer.

Tip

To add keywords to multiple extractions at once, select the extractions in the main window of the Designer.\
Right-click + **Add/Remove keywords** opens the window "Add/Remove Keywords To/From Multiple Extractions".

### Primary Key Tab

Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.

The depicted example shows the SAP object *MAKT* with its primary key inherited from SAP in the general settings of the Designer. In this example the primary key consists of *MANDT*, *MATNR* and *SPRAS*. The primary key is also taken over in the destination.

Note

A defined primary key field in a table is a prerequisite for merging data.

#### Generate Surrogate Key Column

If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs. The surrogate keys are hash values of type signed 8 byte integer, e.g., `#-3008591679982390000`. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.

### Security Tab

Restrict user access to the extraction. For more information, see [Restrict Designer Access](../../access-restrictions/restrict-designer-access/).

### Columns Order Tab

The "Columns Order" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns. Index 0 defines the first column in the result set, index 1 the second columns, etc.

| Option | Description | | --- | --- | | **Enabled** | When this option is active, the defined column order is applied when running the extraction. | | **Swap** | Swaps the index of 2 columns. All other columns keep their indexes. | | **Insert** | Inserts the selected column into the selected index. All other indexes are recalculated. | | **Reset default** | Restores the original column order. |

This page shows how to filter the data that is extracted by the DeltaQ extraction type. Selections limit the result set of the DeltaQ extraction type to extract only records that match the selection.

### Edit Selections

Note

Do not define selections when using the update mode Delta Update, because the selections of the Init mode are automatically applied.

Follow the steps below to edit selection fields and filter data:

1. In the subsection *OLTP Fields*, click **Edit** next to the field you want to edit. The window “Edit selection” opens.

1. Click **[Single]**, **[Range]** or **[List]** to add a corresponding filter, see [Filter Options](#filter-options).

1. In the column **Sign** , select *Include* to add the filtered data to the output or select *Exclude* to remove the filtered data from the output.

1. In the column **Option** , select an operator, see [Filter Options](#filter-options).

1. In the column **Value**, enter values directly into the input fields **Low** and **High** or assign existing [runtime parameters](../edit-runtime-parameters/) to the selection fields .

   Note

   When runtime parameters are available, you can use the icon button next to the input field to switch between static values () and runtime parameters ().

1. Click **[OK]** to confirm your input.

1. Click **[Load live preview]** in the main window of the extraction type to check the result of your selection. If runtime parameters are defined, you are prompted to populate the parameters with actual values.

### Filter Options

The DeltaQ extraction type offers the following filter options:

| Type | Operator | Description | | --- | --- | --- | | Single | | Compare data to a single specified value. | | | *(not) like pattern* | True if data values do (not) contain to the specified value. | | | *(not) equal to* | True if data is (not) equal to the specified value. | | | *at least* | True if data is greater than or equal to the specified value. | | | *more than* | True if data is greater than the specified value. | | | *at most* | True if data is less than or equal to the specified value. | | | *less than* | True if data is less than the specified value. | | Range | | Check if the data is (not) within a specified range of values. | | | *(not) between* | True if data values do (not) lie between the 2 specified values. | | List | | Check if the data is part of a specified list of values. | | | *element of* | True if data values are part of the list. |

### Script Expressions for DeltaQ

Script expressions are usually used to determine a dynamic date based on the current date.

| Input | Description | | --- | --- | | `#{ DateTime.Now.ToString("yyyyMMdd") }#` | Current date in SAP format (yyyyMMdd) | | `#{ String.Concat(DateTime.Now.Year.ToString(), "0101") }#` | Current year concatenated with "0101" (yyyy0101) | | `#{ String.Concat(DateTime.Now.ToString("yyyy"), "0101") }#` | Current year concatenated with "0101" (yyyy0101) | | `#{ String.Concat(DateTime.Now.ToString("yyyyMMdd").Substring(0,4), "0101") }#` | Current year concatenated with "0101" (yyyy0101) |

For more information on script expression, see [Script Expressions](../../parameters/script-expressions/).

### Data Format

Use the following internal SAP representation for input:

- Date: The date 01.01.1999 has the internal representation 19990101 (YYYYMMDD).
- Year period: The year period 001.1999 has the internal representation 1999001 (YYYYPPP).
- Numbers: Numbers must contain the leading zeros, e.g., customer number 1000 has the internal representation 0000001000.

Warning

**Values accept only the internal SAP representation.**\
Input that does not use the internal SAP representation results in error messages. Use the internal SAP representation. Example:

```text
ERPConnect.ABAPProgramException: RfcInvoke failed(RFC_ABAP_MESSAGE): Enter date in the format \_.\_.\_

```

This page contains an overview of the extraction settings in the DeltaQ extraction type.\
To open the extraction settings, click ****Extraction Settings**** in the main window of the extraction type.

The DeltaQ settings consist of the following tabs:

- [*Base*](#base-settings)
- [*Hierarchy*](#hierarchy-settings)

## Base Settings

### Transfer Mode

The raw data packages can be sent by SAP via *tRFC* call or *Data-IDoc*. *tRFC* is is the default setting.\
Switch to *IDoc* to monitor the raw data packages in the transaction WE02 (IDoc-Monitoring) for debugging reasons.

### Misc. Settings

#### Automatic Synchronization

Option to prevent manual changes in the transactional system when switching from test environment to production environment.\
Example: To use DeltaQ extractions in the production environment, the data source has to be enabled in the production environment. If **Automatic Synchronization** is active, the activation is performed automatically and the timestamp of the data source is changed to be consistent with the settings of the SAP system.

Note

If the data source is modified in the SAP system, manually activate the data source in the DeltaQ component, even when **Automatic Synchronization** is active. Otherwise data load will fail. This behavior belongs to the SAP design, see [SAP Help: Replication of DataSources](https://help.sap.com/viewer/ccc9cdbdc6cd4eceaf1e5485b1bf8f4b/7.4.19/en-US/4a12eaff76df1b42e10000000a42189c.html).

#### Add Serialization Info to Output

Adds the columns *DataPackageID* and *RowCounter* to the output.\
Example: the following columns that are a composite key of the SAP records are included in the output:

- *RequestID*
- *DataPackageID*
- *RowCounter*

Note

Newer data has a higher PackageID. In the same package newer data has a higher RowCounter.

#### Accept Gaps in DataPackage ID

At the end of each extraction the DeltaQ component performs a consistency check. The extraction is considered consistent if all data packages arrive correctly. Example: When using a filter function in the user exit of an OLTP source, certain data packages are not sent. In this case the filter function is an inconsistency. If **Accept Gaps in DataPackage ID** is active, gaps in the package numbering are not considered inconsistencies. Only use this option when a filter function exists in the user exit.

#### Timeout (sec)

Enter a time period (in seconds). The timeout applies when an extraction finishes on the SAP side, but not all tRFC calls have been received.

## Hierarchy Settings

The following settings only apply to Hierarchy extractions.

### Extraction

#### Language

Enter the language of the Hierarchy, e.g., ‘E’ or ‘D’.

#### Hierarchy Name

Enter the name of the Hierarchy.

#### Hierarchy Class

Enter the class of the Hierarchy.

#### Representation

- *ParentChild*: The Hierarchy is represented in the SAP parent-child format, see [Output Formats: ParentChild](../../hierarchy/output-format/#parentchild-format). Example:
- *Natural*: The SAP parent-child Hierarchy is transformed into a regular hierarchy, see [Output Formats: Natural](../../hierarchy/output-format/#natural-format). Example:
- *ParentChildWithNodeNames*: The Hierarchy is represented in a reduced SAP parent-child format that only includes single nodes and their parent, see [Output Formats: ParentChildWithNodeNames](../../hierarchy/output-format/#parentchildwithnodenames-format). Example:

### Natural Representation

Note

The subsection *Natural Settings* is only active, when the **Representation** is set to *Natural*.

#### Level Count

Defines the maximum number of levels. The following example shows a Hierarchy with four levels.

#### Fill empty levels

Copies the bottom element of the Hierarchy until the last level. The following example depicts the previously shown Hierarchy with the activated *Repeat Leaves* option.

#### Description texts for levels

Sets the output field *LevelTextN* for each field *LevelN* containing the text based on the system language settings.

#### Leaves only

Returns only the leaves as data records.

## Maintenance

Click **[Maintenance]** to open a list of Init requests of the DataSource (SAP transaction RSA7).

Select an Init request and click **[]** to delete it. This is necessary when re-initializing a delta process.

The DeltaQ extraction type is primarily used for delta extractions. This means that only recently added or changed data is extracted, instead of a full load. The data that is extracted is defined by the **Update Mode** setting in the main window of the extraction type.

### Update Modes

The DeltaQ extraction type offers the following update modes:

| Mode | Parameter Value | Description | | --- | --- | --- | | Full | *Full* | Extracts all data that match the set selection criteria. | | Delta Update | *Delta* | Only extracts data added or changed on the SAP system since the last delta request. [Initialize a delta process](#initialize-a-delta-process) before running a delta update. To prevent errors, aborts and gaps in your data during a delta run, run the next extraction in the update mode **Repeat**. | | Delta Initialization | *Init* | Extracts all data as full load and initializes the delta process. When re-initializing a delta process, first delete any existing Inits using the **[Maintenance]** button in the [extraction settings](../settings/) menu. | | Repeat | *Repeat* | Repeats the last delta run and updates and any delta data accumulated since the last run. Deletes any data from the last (unsuccessful) delta update before running a repeat. You can run a repeat multiple times. | | Delta Init (without data) | *InitNoData* | Initializes the delta process without extracting any data from the SAP DataSources. The result of the *Delta Init* on the SAP side. When re-initializing a delta process, first delete any existing Inits using the **[Maintenance]** button in the [extraction settings](../settings/) menu. | | Non-cumulative init | *InitNoncumulative* | Relevant for DataSources like *2LIS_03_BX*. | | Activate (don't extract) | *Activate* | Activates a DataSource similar to the **[Activate]** button, but it is more practical when activating DataSources in a batch. No data is extracted. |

Tip

**Update mode** can be set dynamically at runtime using the parameters values to overwrite the extraction parameter *updateType*, see [Run an Extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

For more information about when to use which update mode, see [Initialize a Delta Process](#initialize-a-delta-process)

### Initialize a Delta Process

To start a delta process with the DeltaQ extraction type, the delta must be initialized first. The following delta process shows when to use which [Update Modes](#update-modes).

#### Step 1: Delta Initialization [C]

This mode requests all data that meets the selection criteria.

Set **Update Mode** to *Delta Initialization* and run the extraction.\
The initialization selections are copied to load the delta records. You can now use *Delta Update*.

Note

When re-initializing a delta process, first delete any existing Inits (initialization requests) by clicking **[Maintenance]** in the [extraction settings](../settings/).

#### Alternative Step 1: Delta Init (without data) [S]

This mode is similar to *Delta Initialization*, but no data is extracted from the SAP DataSource.

Set **Update Mode** to *Delta Init (without data)* and run the extraction.\
You can now use *Delta Update*.

Note

When re-initializing a delta process, first delete any existing Inits (initialization requests) by clicking **[Maintenance]** in the [extraction settings](../settings/).

#### Step 2: Delta Update [D]

*Delta Update* only extracts data added or changed on the SAP system since the last delta request.

Run the extraction with *Delta Initialization* or *Delta Init (without data)* once before setting the **Update Mode** to *Delta Update*.

Note

To prevent errors, aborts and gaps run the next extraction in the update mode *Repeat*.

#### Optional: Repeat [R]

This mode repeats the last delta run and updates all data accumulated since the last run. If the last run was unsuccessful, all data from the last delta update are deleted before a new run is started.\
A **Repeat** can be started several times.

Many DataSources have the field ROCANCEL. This field defines if records are added or overwritten depending on the delta process type of the DataSource. It defines how a record is updated in the delta process.\
In an ABR mode:

- *blank* returns an after image
- *'X'* returns a before image
- *'D'* deletes the record
- *'R'* returns a reverse image

#### Optional: Delta Queue - RSA7

Once delta is activated, you can view the queued datasets in the Delta queue in SAP transaction RSA7.\
If there is no new data to be transferred, a corresponding protocol message is displayed and the data pipeline is empty.

Note

Before initiating the next update, make sure that a delta update has been executed successfully. Running a new delta update removes the last one.

______________________________________________________________________

#### Related Links

- [SAP Help: Delta Transfer to BI](https://help.sap.com/doc/saphelp_nw70/7.0.31/en-US/37/4f3ca8b672a34082ab3085d3c22145/content.htm?no_cache=true)
- [SAP Help: Delta Process](https://help.sap.com/viewer/ccc9cdbdc6cd4eceaf1e5485b1bf8f4b/7.4.23/en-US/4f18f6aa3fca410ae10000000a42189d.html)

Xtract Universal allows you to extract data from SAP systems and load it to different destination environments. There are two types of destinations, depending on where the extraction process is started:

- **Pull Destinations**

  ______________________________________________________________________

  Extractions with pull destinations provide data on request. The extraction process is started by the destination environment. When a consumer requests the data, Xtract Universal translates the request into a query for the underlying SAP system, retrieves the data directly from the source system and delivers it to the consumer.

- **Push Destinations**

  ______________________________________________________________________

  Extractions with push destinations provide data proactively. The extraction process is started in Xtract Universal, e.g. via a [scheduled extraction](../execute-and-automate/call-via-scheduler/). An extraction with push destinations extracts data from the SAP source systems and loads them into the destination, where the data can be processed further.

### Manage Destinations

To open a list of all existing destinations, navigate to **Server > Destinations**.\
The window "Manage Destinations" opens. Here, you can edit, delete and add new destinations.

Note

The **http-csv** destination is a default destination and cannot be deleted.

### Databases / Data Warehouses

### Business Intelligence / Analytics / ETL

### Business Systems

### Cloud Storage

### Generic Destinations

This page shows how to set up and use the Microsoft Power BI destination. The Microsoft Power BI destination loads data to Power BI

The destination offers two ways to connect Power BI with Xtract Universal:

- Power BI custom connector
- Generic Power Query M-script using one of the following ways:
  - [Power BI Desktop](#power-bi-desktop)
  - [Power BI Service - Microsoft Fabric Dataflow Gen2](#power-bi-service-microsoft-fabric-dataflow-gen2)

With the Power Query M-script you can additionally directly integrate data into Dataflow Gen2 using [Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/data-factory/create-first-dataflow-gen2).

## Supported Power BI Environments

Xtract Universal supports the following environments:

- [Power BI Desktop](https://powerbi.microsoft.com/en-us/desktop/) via [Power BI Custom Connector](#power-bi-custom-connector)
- [Power BI service](https://docs.microsoft.com/en-us/power-bi/power-bi-overview#the-parts-of-power-bi) via [Power BI on-premises data gateway](../../../knowledge-base/connect-to-power-bi-service/)
- [Power BI Report Server](https://docs.microsoft.com/en-us/power-bi/report-server/get-started) via [Power Query M-script](#power-query-m-script)

| | Custom Connector | M-script | | --- | --- | --- | | Power BI Desktop | | | | Power BI Service | | | | Power BI Report Server | | |

## Video Tutorial

The following YouTube tutorial shows how to install the Power BI Custom Connector and how to use Xtract Universal with Power BI:

## Create a new Microsoft Power BI Destination

Follow the steps below to add a new Microsoft Power BI destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Microsoft Power BI* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination. To use the Microsoft Power BI destination, no further settings are necessary.

## Assign the Microsoft Power BI Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Microsoft Power BI destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

## Power BI Custom Connector

The Microsoft Power BI destination works in combination with a custom extension file for Power BI. This extension file *XtractUniversalExtension.pqx* is located in the *powerbi* folder of the Xtract Universal installation directory, e.g., `C:\Program Files\XtractUniversal\powerbi\`. The Power BI Custom Connector was developed following Microsoft's guidelines for [custom connectors in Power BI](https://docs.microsoft.com/en-US/power-bi/desktop-connector-extensibility).

### Install the Power BI Custom Connector

Follow the steps below to install the Power BI Custom Connector.

1. Open the *powerbi* folder located in the Xtract Univesal installation directory, e.g., `C:\Program Files\XtractUniversal`.

1. Run the *install-connector* PowerShell script.\
   This script copies the *XtractUniversalExtension.pqx* file to the `[Documents]\Power BI Desktop\Custom Connectors` folder. If the folder does not exist, the script creates the folder.

1. Run the *trust-connector* PowerShell script.\
   This script modifies the registry at `HKLM:\SOFTWARE\Policies\Microsoft\Power BI Desktop\`. The script adds or modifies the registry value *TrustedCertificateThumbprints* with the thumbprint of the *XtractUniversalExtension.pqx* file. This procedure follows Microsoft's recommendations for [trusting third-party connectors](https://docs.microsoft.com/en-us/power-bi/desktop-trusted-third-party-connectors).

1. Check if the directory `[Documents]\Power BI Desktop\Custom Connectors` exists.

1. Create this directory if it doesn't exist.

1. Open the *powerbi* folder located in the Xtract Univesal installation directory, e.g., `C:\Program Files\XtractUniversal\powerbi\`.

1. Copy the *XtractUniversalExtension.pqx* file from the *powerbi* folder to `[Documents]\Power BI Desktop\Custom Connectors`.

1. Restart Power BI.

1. In Power BI Desktop, navigate to **File > Options and settings > Options > Security**.

1. In the subsection [Data Extensions](https://docs.microsoft.com/en-us/power-bi/desktop-connector-extensibility#data-extension-security), activate the option **(Not Recommended) Allow any extension to load without validation or warning**.

1. Click **[OK]**.

1. Restart Power BI.

The Xtract Universal data source is now available within Power BI.

#### Power BI Settings

Adjust the following settings in Power BI:

1. In Power BI, navigate to **File > Options and settings > Options** to open the option menu.

1. Activate the option **(Not Recommended) Allow any extension to load without validation or warning** in the tab *Security*.

1. Optional: Increase the Power BI cache in the tab *Load Data* to prevent multiple calls from Power BI to Xtract Universal when extracting large data volumes.

   Note

   Multiple calls to Xtract Universal result in in multiple entries for the same extraction in the Xtract Universal extraction log.

### Connect Power BI with Xtract Universal

Follow the steps below to connect Power BI with Xtract Universal

1. In Power BI, select Xtract Universal from the *Get Data* menu.

1. Click the **[Connect]** button.

1. Enter the URL of the Xtract Universal [web server](../../designer/#connect-the-designer-to-a-server) `<Protocol>://<HOST or IP address>:<Port>/`.

   When prompted for *Anonymous*, *Basic* or *Windows* authentication, follow the steps as outlined in [Single Sign On and SAP authentication](#single-sign-on-and-sap-authentication).\
   The "Navigator" window lists all extractions that are [assigned to a Microsoft Power BI destination](#assign-the-microsoft-power-bi-destination-to-an-extraction) in Xtract Universal.

1. Select an extraction from the list. The preview data shows the actual SAP column headers and preview data (real data or dummy data, depending on the extraction type).

1. Click the **[Load]** button. This triggers an extraction in Xtract Universal and writes the extracted data to Power BI.

The data is now available for further processing.

### Parameterizing in Power BI Custom Connector

When using the Power BI Custom Connector, Xtract Universal [custom parameters](../../parameters/extraction-parameters/#custom) can be populated when setting up the connection in Power BI.

A list of an extraction's custom parameters is displayed under the *Custom* tab in the "Run Extraction" window. In the depicted example the list contains one entry *BUKRS_low*.

The list of Xtract Universal custom parameters is exposed in Power BI Desktop when creating a report based on the selected extraction. The exposed Xtract Universal parameters can be filled with values from within Power BI Desktop.

## Power Query M-script

Instead of the Power BI Custom Connector, you can also use the Custom Power Query M-script to connect Power BI to Xtract Universal. The Power BI Query M-script is located inside the Xtract Universal installation directory, e.g., `C:\Program Files\XtractUniversal\powerbi\loading_script.txt`.

Warning

**Use of the Custom Connector & Query M-script**\
Power Query M-script and Power BI Custom Connector do not belong together.\
Use either the Power Query M-script **or** the Power BI Custom Connector.

### Power BI Desktop

Note

Only use the extractions with the Power BI Connector destination assigned to them.

Follow the steps below to set up the Power Query M-script in Power BI to connect with Xtract Universal:

1. Create a new Power BI report using **Home > Get Data > Blank Query** as a data source.

1. Open the **[Advanced Editor]**.

1. Open the Xtract Universal *loading_script.txt* in any text editor.

1. Copy the content of *loading_script.txt* into the *Advanced Editor* in Power BI.

1. Change the values for **ExtractionName** and the **ServerURL** to match the names of your Xtract Universal extraction and [web server](../../designer/#connect-the-designer-to-a-server) .

1. Within the *Advanced Editor*, click **[Done]** to confirm the script.

1. Click **[Close & Apply]**.

1. In Power BI, navigate to **File > Options and settings > Options** to open the option menu.

1. Activate the option **(Not Recommended) Allow any extension to load without validation or warning** in the tab *Security*.

1. Optional: Increase the Power BI cache in the tab *Load Data* to prevent multiple calls from Power BI to Xtract Universal when extracting large data volumes.

   Note

   Multiple calls to Xtract Universal result in in multiple entries for the same extraction in the Xtract Universal extraction log.

### Parameterizing in Power Query M-Script

1. Open the Power Query M-script.

1. Navigate to *Parameters* and replace the values with actual values or with parameters defined in Power BI.

   ```text
       // Record containing run parameters with corresponding values, can be empty
       // Usage: <XU parameter name>= <value or Power BI parameter>
       // MUST NOT use "name" as a record field here
       Parameters = [ /*rows= "300", myparameter= SomePowerBIParameter*/ ],

   ```

### Power BI Service (Microsoft Fabric Dataflow Gen2)

#### Requirements

- Active [Power BI on-premises data gateway](../../../knowledge-base/connect-to-power-bi-service/)
- Power BI Destination within Xtract Universal
- Microsoft Fabric Subscription

#### Procedure

1. Navigate to **My Workspace > New > Data factoring > Dataflow Gen2**
1. Create a new Power BI report using **Home > Get Data > Blank Query** as a data source.
1. Open the **[Advanced Editor]**.
1. Open the Xtract Universal *loading_script.txt* in any text editor.
1. Copy the content of *loading_script.txt* into the *Advanced Editor*.
1. Select the appropriate data gateway from the dropdown-list.
1. Click **[Next]**.

## Single Sign On and SAP Authentication

When setting up the Xtract Universal data source in Power BI for the first time, you are prompted for one of the following authorization methods. Select an authorization method according to your landscape:

- *Anonymous*: Select this option if the Xtract Universal server settings don't require any authentication for running an extraction.
- *Basic*: Select this option if the *Require SAP Credentials to be explicitly supplied for execution* checkbox is marked in the [SAP Source Settings](../../sap-connection/settings/#authentication) in Xtract Universal. Enter your SAP credentials in the respective input fields.
- *Windows*: Select this option if you want to use [SSO](../../sap-connection/sso-with-logon-ticket/) or if you have restricted access to extractions in the Xtract Universal server settings. Enter \\\\ in the *user* field and your Windows password in the *Password* field.

Xtract Universal and the Power BI Connector destination support single sign on (SSO) to SAP. If SSO is set up correctly, the Windows credentials of the executing Power BI user are mapped to this user's SAP credentials. This leverages the user's SAP authorizations and Power BI will only show data that matches the user's SAP authorizations.

______________________________________________________________________

## Related Links

- [Connect Xtract Universal to Power BI Service](../../../knowledge-base/connect-to-power-bi-service/)
- [Microsoft Documentation: What is an on-premises data gateway?](https://docs.microsoft.com/en-us/power-bi/connect-data/service-gateway-onprem)
- [Microsoft Documentation: Use custom data connectors with the on-premises data gateway](https://docs.microsoft.com/en-us/power-bi/connect-data/service-gateway-custom-connectors)
- [Microsoft Documentation: Configure scheduled refresh](https://docs.microsoft.com/en-us/power-bi/connect-data/refresh-scheduled-refresh)
- [Microsoft Documentation: Parameters in Power BI Desktop](https://docs.microsoft.com/en-us/power-query/power-query-query-parameters)
- [Microsoft Fabric - Creating Dataflows](https://learn.microsoft.com/en-us/fabric/data-factory/create-first-dataflow-gen2)

This page shows how to set up and use the Alteryx destination. The Alteryx destination enables users to load SAP data directly from within Alteryx.

## Requirements

To use the Alteryx destination, the Xtract Universal Alteryx plugin for the Alteryx Designer must be installed.

The Xtract Universal setup installs the Xtract Universal Alteryx plugin if there is a valid Alteryx installation on the current system. If you install Alteryx after installing Xtract Universal, run the Xtract Universal setup again. The Xtract Universal setup creates the following entries and extensions in the installation directory of that Alteryx installation:

- `Alteryx\Settings\AdditionalPlugins\XtractUniversal.ini`
- `Alteryx\bin\RuntimeData\icons\categories\XtractUniversal.png`
- `Alteryx\bin\RuntimeData\DefaultSettings.xml`

Note

Run the Xtract Universal setup on every machine that needs the Xtract Universal Alteryx plugin. If there is more than one Alteryx installation on your system, the Xtract Universal setup only detects one installation. In this case, a manual installation is required.

When encountering issues during or after the installation of the plugin (e.g., the plugin is not showing in Alteryx), send the setup.log file located in `C:\Program Files\XtractUniversal\alteryx\setup.log` to the [Theobald Support](https://support.theobald-software.com).

## Create a new Alteryx Destination

Follow the steps below to add a new Alteryx destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Alteryx* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination. To use the Alteryx destination, no further settings are necessary.

## Assign the Alteryx Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Alteryx destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

## Use Xtract Universal in Alteryx

To use Xtract Universal extractions in the Alteryx Designer, add the the Xtract Universal tool to your Alteryx workflow. The specify the [*Connection*](#connection) settings and the [*Parameters*](#parameters) settings of the Xtract Universal tool.

### Connection

Connect to the Xtract Universal Server and select the extraction you want to to execute in Alteryx.

Note

Make sure the Xtract Universal server is running.

#### Server

Enter the name, IP or domain and port to access the server that runs Xtract Universal within the network. Format: `[Server]:[Port]`.

#### Extraction

Select an extraction from the drop-down menu.\
Only extractions with that have the Alteryx destination [assigned](#assign-the-alteryx-destination-to-an-extraction) to them are displayed.

#### Send SAP credentials

Activate the checkbox **Send SAP credentials**, if the option *Require SAP Credentials to be explicitly supplied for execution* is active in the [SAP source](../../sap-connection/settings/#authentication) settings in Xtract Universal.\
The setting *Send SAP credentials* can be useful in self service scenarios. When each extraction needs to be executed using an individual user's SAP credentials instead of the globally defined credentials.

#### Authenticate using current Windows user

Activate the checkbox **Authenticate using current Windows user** to use the Windows user that runs Alteryx for authentication.

### Parameters

In the tab *Parameters* tab, the Xtract Universal tool loads available [parameters](../../parameters/) for the specified extraction. The depicted examples show how to override custom parameters created in Xtract Universal.

In the following example, an extraction of SAP customers contains the [custom parameter](../../parameters/extraction-parameters/#custom) *city*. The parameter is available in the tab *Custom Defined Parameters* and can be overwritten with a static value. To override the parameter *city*, activate the checkbox **Override** and enter a new value, e.g., "Stuttgart".

The Xtract Universal tool can receive inputs, e.g., via the *Input Data* tool. The data input can be used to dynamically override parameters in Xtract Universal.\
In the following example, an extraction of SAP customers contains the [custom parameter](../../parameters/extraction-parameters/#custom) *city* The parameter is available in the tab *Custom Defined Parameters* and can be overwritten with a dynamic input value.

To override the parameter *city*, activate the checkbox **Override** and the checkbox **Map**. Select an item out of the drop-down list in the field **value**.

Tip

If the connection to the specified Xtract Universal server is not established and no errors are shown, the Xtract Unversal tool offers a tooltip in the following format: `[Extraction] @ [Server]`.

This page shows how to set up and use the Amazon S3 destination. The Amazon S3 destination loads data to the [Amazon S3](https://aws.amazon.com/s3/) cloud storage.\
For more information on Amazon S3, see [AWS Documentation: Getting Started with Amazon S3](https://aws.amazon.com/s3/getting-started/).

Tip

You can install Xtract Universal in an Amazon Elastic Compute (EC2) instance. Check [Amazon EC2: Getting Started with Amazon EC2](https://aws.amazon.com/ec2/getting-started/) to deploy an instance where you can install Xtract Universal in your AWS Account. Make sure the instance is deployed in the same region as your SAP solution to reduce latency and optimize performance.

## Requirements

- An Amazon Web Services (AWS) Account.
- [Access Keys](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html) (consisting of "access key ID" and "secret access key") of your AWS user or an IAM role attached to the EC2 instance that runs Xtract Universal, see [AWS Documentation: Using an IAM role to grant permissions to applications running on Amazon EC2 instances](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html). For more information on the IAM role, see [AWS Documentation: Security best practices in IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html).
- An S3 bucket, in which you can upload data.
- Sufficient permissions for list, read and write activities on S3. You must grant these rights in the user policy, but you can limit them to certain buckets. In the following example, the set permissions have been tested in a test environment:

Note

Xtract Universal uses so called [Multipart](https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html) upload for uploading data to S3. Data extracted from SAP is uploaded to S3 not as one big chunk of data but in smaller parts. These parts are buffered on the S3 side. If the extraction is successful, those parts are assembled by S3 into one file. While the extraction is still running this file is not visible on S3.

Recommendation

It's recommended you enable S3 versioning or perform data backups regularly, see [Amazon AWS: Getting Started - Backup & Restore with AWS](https://aws.amazon.com/backup-restore/getting-started/).

### Extraction Failed

In case the extraction fails, the already uploaded parts are deleted from S3. In case of an "uncontrolled" extraction failure, for example due to network issues, Xtract Universal will not be able to delete those uploaded parts from S3.

Therefore it is recommended to change the settings on S3 in a way that will trigger the automatic deletion of unused multiparts, e.g. after a day. You can find this setting by selecting a bucket and opening the "Management" tab. Select "Lifecycle" and "Add lifecycle rule" and create a rule for deleting unused multiparts.

## Create a new Amazon S3 Destination

Follow the steps below to add a new Amazon S3 destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Amazon S3* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

### API Endpoints

The endpoint defines which IP address is used to connect to an Amazon S3 instance.

#### Use AWS S3 Endpoint

When this option is active, Xtract Universal connects to Amazon S3 using a URL that consists of the bucket name and the Amazon S3 default URL: `https://[BucketName].s3.amazonaws.com/`

#### Use VPC Endpoint

When this option is active, Xtract Universal connects to Amazon S3 using a [Virtual Private Cloud (VPC) endpoint](https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html). This option requires Xtract Universal to run on an AWS VPC instance. Enter the VPC endpoint in the format `http://bucket.[endpoint]` in the field **VPC endpoint**. Example:\
*http://bucket.vpce-0123456789abcdefg-hijklmno.s3.us-east-1.vpce.amazonaws.com*.

### Authenthication

The authentication method defines how Xtract Universal authenticates itself against Amazon AWS.

#### Use IAM role

When this option is active, the credentials and permissions of the IAM role assigned to the [EC2 instance](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2.html), on which Xtract Universal is running are used for authentication. For more information on the IAM role, see [AWS Documentation: Security best practices in IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html).

#### Use user credentials

Preferable authentication method towards Amazon AWS. Determine the values for the [Access Keys (access key ID and secret key)](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html) via AWS Identity and Access Management ([IAM](https://console.aws.amazon.com/iam/home#/home)).

#### Connect

After entering *Access key ID* and *Secret key*, click **[Connect]** to connect to AWS. After successfully connecting, select bucket name and region.

### Bucket

#### Bucket name and Region

Select a bucket and a region of the bucket's location. The SAP data is extracted into the selected bucket.

Note

The drop-down menus list **all** available buckets and regions, make sure to select the correct combination of bucket & region. Validate the connectivity to the selected bucket by clicking **\[Test Connection)**.

#### Test Connection

Validates the right combination of bucket and region. Insures bucket's accessibility from Xtract Universal using the entered access keys.

### Server-side encryption

Choose how to encrypt data after uploading them to S3.

Note

The setting "Server-side encryption" does not relate to transport encryption between Xtract Universal and S3. By default, the channel for sending data to S3 is always encrypted.

#### None

Server-sided encryption of data not active. For more information, see [AWS Documentation: Protecting data with server-side encryption](https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html).

#### SSE-S3

Encrypts data using the by default available S3 user account encryption key, see [S3 Managed Encryption Keys](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingServerSideEncryption.html).

#### SSE-KMS / Key ID

Encryption using a custom encryption key created on AWS, see [AWS Key Management Services](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingKMSEncryption.html). The key can be created on the [AWS website](https://console.aws.amazon.com/iam/home#/encryptionKeys/.)

### Misc

All settings in *Misc* are optional.

#### Folder path

Creates a folder structure within the bucket. Script expressions are supported, see [Use Script Expressions as Dynamic Folder Paths](#use-script-expressions-as-dynamic-folder-paths).

- Create a single folder by entering a folder name without slashes: `[folder]`
- Create subfolders using the following syntax: `[folder]/[subfolder_1]/[subfolder_2]/[..]`

Note

The specified folder path applies to all extractions. To define a folder path for single extractions, use the **Folder** option in the [Destination Settings](#destination-settings).

#### File Owner

If you upload files as an AWS user of an Account A to an Account B, you can select the option "Bucket Owner". Without a declared owner, uploaded files cannot be opened directly.

### File Format

Select the required file format. You can choose between *CSV*, *JSON* and *Parquet*.

#### CVS Settings

The settings for file type *CSV* correspond to the settings of the *Flat File CSV* destination:

- [CSV Settings](../csv-flat-file/#csv-settings)
- [Convert / Encoding](../csv-flat-file/#convert-encoding)

#### JSON Settings

To use the JSON file format, no further settings are necessary.

#### Parquet Settings

The settings for file type *Parquet* correspond to the settings of the *Flat File Parquet* destination:

- [Compatibility Mode](../parquet/#compatibility-mode)

## Connection Retry

Connection retry is a built-in function of the Amazon S3 destination. The retry function is activated by default.

Connection retry is a functionality that prevents extractions from failing in case of transient connection interruptions to Amazon S3. Xtract Universal follows an exponential retry strategy. The selected exponential strategy results in seven retry attempts and an overall timespan of 140 seconds. If a connection is not established during the timespan of 140 seconds, the extraction fails.

The retry function is applied after receiving one of the following HTTP errors or exceptions:

- 503 Service Unavailable
- 504 Gateway Timeout
- WebExceptionStatus.ConnectionClosed
- WebExceptionStatus.ConnectFailure
- WebExceptionStatus.Timeout
- WebExceptionStatus.RequestCanceled
- WebExceptionStatus.SendFailure
- WebExceptionStatus.NameResolutionFailure

For more general information about retry strategies in an AWS S3 environment go to the official [AWS Help](https://docs.aws.amazon.com/general/latest/gr/api-retries.html).

## Assign the Amazon S3 Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Amazon S3 destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

Note

If the name of an object does not begin with a letter, it will be prefixed with an ‘x’, e.g. an object by the name `_namespace_tabname.csv` will be renamed `x_namespace_tabname.csv` when uploaded to the destination. This is to ensure that all uploaded objects are compatible with Azure Data Factory, Hadoop and Spark, which require object names to begin with a letter or give special meaning to objects whose names start with certain non-alphabetic characters.

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Folder

Writes the extracted data to a specific folder structure within the bucket. If the specified folder does not exist, it is created.

- Create a single folder by entering a folder name without slashes: `[folder]`
- Create subfolders using the following syntax: `[folder]/[subfolder_1]/[subfolder_2]/[..]`

Note

The specified folder path only applies to the selected extraction. To define a folder path for all extractions, use the **Folder path** option in the [Destination Details](#destination-details).

#### Use Script Expressions as Dynamic Folder Paths

Script expressions can be used to generate a dynamic folder path. This allows generating folder paths that are composed of an extraction's properties, e.g., extraction name, SAP source object. The described scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example: /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFields]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. | | `#{Extraction.Fields["[0D_NW_CODE]"].Selections[0].Value}#` | Only for BWCube extractions (MDX mode): returns the input value of a defined selection. | | `#{Extraction.Fields["[0D_NW_CHANN]"].RangeSelections[0].LowerValue}#` | Only for BWCube extractions (MDX mode): returns the lower input value of a defined selection range. | | `#{Extraction.Fields["[0D_NW_CHANN]"].RangeSelections[0].UpperValue}#` | Only for BWCube extractions (MDX mode): returns the upper input value of a defined selection range. | | `#{Extraction.Fields["0D_NW_CODE"].Selections[0].Value}#` | Only for BWCube extractions (BICS mode): returns the input value of a defined selection. | | `#{Extraction.Fields["0D_NW_CHANN"].RangeSelections[0].LowerValue}#` | Only for BWCube extractions (BICS mode): returns the lower input value of a defined selection range. | | `#{Extraction.Fields["0D_NW_CHANN"].RangeSelections[0].UpperValue}#` | Only for BWCube extractions (BICS mode): returns the upper input value of a defined selection range. |

### Compression

Compression is only available for the csv file format, see [Destination Details: File Format](#file-format).

#### GZip

The data is transferred compressed and stored as a gz file.

### File Splitting

Writes extraction data of a single extraction to multiple files. Each filename is appended by *\_part[nnn]*.

#### Max. file size

The value set in *Max. file size* determines the maximum size of each file.

Note

The option *Max. file size* does not apply to gzip files. The size of a gzipped file cannot be determined in advance.

### Empty Files

When this option is active, empty result sets create an empty file in the target environment. Deactivate this option to not create empty files.

______________________________________________________________________

## Related Links

- [Knowledge Base Article: Run Xtract Universal in a VM on AWS EC2](../../../knowledge-base/run-xu-in-aws/)
- [Amazon AWS: Getting Started - Backup & Restore with AWS](https://aws.amazon.com/backup-restore/getting-started/)
- [Amazon S3: Getting Started with Amazon S3](https://aws.amazon.com/s3/getting-started/)
- [Amazon EC2: Getting Started with Amazon EC2](https://aws.amazon.com/ec2/getting-started/)
- [Amazon Documentation: Amazon EC2 security groups for Linux instances](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html)
- [Amazon Documentation: Security best practices in IAM](https://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html)
- [Amazon Documentation: Overview of managing access](https://docs.aws.amazon.com/AmazonS3/latest/userguide/access-control-overview.html)

This page shows how to set up and use the Amazon Redshift destination. The Amazon Redshift destination loads data to the [Amazon Redshift database](https://aws.amazon.com/redshift/). For more information on Amazon Redshift, see [Getting Started with Amazon Redshift](https://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html).

## Requirements

- Run an instance with valid credentials.
- Your client computer has to be authorized to access the cluster.
- For establishing a connection to Amazon Redshift, a suitable database driver is required.
- Download and install the x64 Redshift ODBC driver version 2.x from the official [website](https://docs.aws.amazon.com/redshift/latest/mgmt/odbc20-install-config-win.html). If the driver is missing, the connection test fails.

Note

Prior to Xtract Universal version 2.102.0 you have to install the Mono.Security.dll assembly instead of above mentioned ODBC driver. You can download the complete Mono package from [the official project site](http://download.mono-project.com/archive/2.0/download/). Make sure to install the Mono.Security assembly, compiled on .NET 2.0. by copying the Mono.Security.dll file into your Xtract Universal installation directory.

## Create a new Amazon Redshift Destination

Follow the steps below to add a new Amazon Redshift destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Amazon Redshift* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

#### Endpoint

Enter the server address of the Amazon Redshift system. The server address uses the following format:\
`<cluster-name>.<random-id>.<region>.redshift.amazonaws.com`

You can find the server address in the properties of your cluster database.

#### Port

Enter the port number for the connection.

#### Username / Password

Enter the username and password of the database user.

#### Database

Enter the name of the database you want to write to.

#### Test Connection

Checks the database connection.

## Assign the Amazon Redshift Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Amazon Redshift destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Preparation

Defines the action on the target database before the data is inserted into the target table.

| Option | Description | | --- | --- | | *Drop & Create* | Remove table if available and create new table (default). | | *Truncate Or Create* | Empty table if available, otherwise create. | | *Create If Not Exists* | Create table if not available. | | *Prepare Merge* | Prepares the merge process and creates e.g. a temporary staging table, see [Merge Data](#merge-data). | | *None* | No action. | | *Custom SQL* | Here you can define your own script, see [Custom SQL Statements](#custom-sql-statements). |

To only create the table in the first step and not insert any data, you have two options:

1. Copy the SQL statement and execute it directly on the target data database.
1. Select the *None* option for **Row Processing** and execute the extraction.

Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.

### Row Processing

Defines how the data is inserted into the target table.

| Option | Description | | --- | --- | | *Insert* | Insert records (default). | | *Fill merge staging table* | Insert records into the staging table. | | *None* | No action. | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). | | *Merge (deprecated)* | This option is obsolete, see [Merge Data](#merge-data). Use the *Fill merge staging table* option. |

### Finalization

Defines the action on the target database after the data has been successfully inserted into the target table.

| Option | Description | | --- | --- | | *Finalize Merge* | Closes the merge process and deletes the temporary staging table, for example. | | *None* | No action (default). | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Transaction style

Note

The available options for Transaction Style vary depending on the destination.

#### One Transaction

*Preparation*, *Row Processing* and *Finalization* are all performed in a single transaction.

- Advantage: clean rollback of all changes.
- Disadvantage: possibly extensive locking during the entire extraction period.

Recommendation

Only use *One Transaction* in combination with DML commands, e.g., "truncate table" and "insert. Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command. Example: If a table is created in the preparation step, the opened "OneTransaction" is committed and a rollback in the next steps is not performed correctly.

#### Three Transactions

*Preparation*, *Row Processing* and *Finalization* are each executed in a separate transaction.

- Advantage: clean rollback of the individual sections, possibly shorter locking phases than with *One Transaction* (e.g. with DDL in *Preparation*, the entire DB is only locked during preparation and not for the entire extraction duration).
- Disadvantage: no rollback of previous step possible (error in *Row Processing* only rolls back changes from *Row Processing*, but not *Preparation*).

#### RowProcessingOnly

Only *Row Processing* is executed in a transaction. *Preparation* and *Finalization* without an explicit transaction (implicit commits).

- Advantage: DDL in 'Preparation *and* Finalization\* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)
- Disadvantage: no rollback of *Preparation/Finalization*.

#### No Transaction

No explicit transactions.

- Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.
- Disadvantage: no rollback

## Merge Data

The following example depicts the update of the existing data records in a database by running an extraction to merge data. In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP.

With a merge command, the updated value is written to the destination database table. The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.

Tip

Alternatively to merging, the data can be also updated by means of full load. The full load method is less efficient.

### Prerequisites

You need a table with existing SAP data, in which new data can be merged.\
Ideally, the table with existing data is created in the initial load with the corresponding **Preparation** option and filled with data with the **Row Processing** option *Insert*.

After the table is created, open SAP and change a field value in the SAP table that is used for the data merge.

Warning

**Faulty merge.**\
Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the [General Settings](../../table/general-settings/#primary-key-tab) of the extraction type to execute the merge command.

### Merge Command

The merge process is performed using a staging table and takes place in three steps:

1. A temporary table is created.
1. The data is inserted in the temporary table.
1. The temporary table is merged with the target table and then the temporary table is deleted.

Follow the steps below to set up the merge process in Xtract Universal:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions.
1. Click **[Destination]**. The window "Destination Settings" opens.
1. Make sure to assign Amazon Redshift destination to the extraction.
1. Apply the following destination settings:
1. Click **[OK]** and [run the extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

More information about the updated fields can be found in the SQL statement.\
It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update.\
Fields that do not appear in the SQL statement are not affected by changes.

## Custom SQL Statements

The Amazon Redshift destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the Amazon Redshift destination:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..

1. Click **[Destination]**, the window "Destination Settings" opens.

1. Make sure to assign the Amazon Redshift destination to the extraction.

1. Select the option *Custom SQL* from the drop-down list in one of the following sections:

   - [Preparation](#preparation)
   - [Row Processing](#row-processing)
   - [Finalization](#finalization)

1. Click **[Edit SQL]** . The window "Edit SQL" opens.

1. Enter your custom SQL statement and click **[OK]** to confirm your input.

### Use Templates

Existing SQL commands can be used as templates.\
You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:

- [Preparation](#preparation), e.g., in *Drop & Create* or *Create if Not Exists*
- [Row Processing](#row-processing), e.g., in *Insert* or *Merge*
- [Finalization](#finalization)

Follow the steps below to generate a Custom SQL command from a template:

1. In one of the staging steps, select the *Custom SQL* option from the drop-down list .
1. Click **[Edit SQL]** . The dialogue "Edit SQL" opens.
1. Navigate to the drop-down menu and select an existing command .
1. Click **[Generate Statement]**. A new statement is generated.
1. Click **[Copy]** to copy the statement to the clipboard.
1. Click **[OK]** to confirm your input.

Check out the [Microsoft SQL Server example](../microsoft-sql-server/#custom-sql-statements) for details on predefined expressions.

Note

The custom SQL code is used for SQL Server destinations. A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.

### Use Script Expressions

You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported:

| Input | Description | | --- | --- | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.TableName }#` | Name of the database table extracted data is written to. | | `#{Extraction.RowsCount }#` | Count of the extracted rows. | | `#{Extraction.RunState}#` | Status of the extraction (Running, FinishedNoErrors, FinishedErrors, Cancelled). | | `#{(int)Extraction.RunState}#` | Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors, 6 = Cancelled). | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. |

For more information, see [Script Expressions](../../parameters/script-expressions/).

```sql
#{
   iif
   (
      ExistsTable("MAKT"),
      "TRUNCATE TABLE \"MAKT\";",
      "
         CREATE TABLE \"MAKT\"(
            \"MATNR\" VARCHAR(18),
            \"SPRAS\" VARCHAR(2),
            \"MAKTX\" VARCHAR(40));
      "
   )
}#

```

### Create a Status Overview

The table "ExtractionStatistics" provides an overview and status of the executed Xtract Universal extractions. To create the "ExtractionStatistics" table, create an SQL table according to the following example:

```sql
CREATE TABLE [dbo].[ExtractionStatistics](
    [TableName] [nchar](50) NULL,
    [RowsCount] [int] NULL,
    [Timestamp] [nchar](50) NULL,
    [RunState] [nchar](50) NULL
) ON [PRIMARY]
GO

```

The *ExtractionStatistics* table is filled in the **Finalization** process step, using the following SQL statement:

```sql
INSERT INTO [ExtractionStatistics]
(
     [TableName], 
     [RowsCount], 
     [Timestamp],
     [RunState]
)
VALUES
(
     '#{Extraction.TableName}#', 
     '#{Extraction.RowsCount}#', 
     '#{Extraction.Timestamp}#',
     '#{Extraction.RunState}#'
);

```

This page shows how to set up and use the Microsoft Azure Storage destination. The Microsoft Azure Storage destination loads data to a cloud based Azure Storage.

## Video Tutorial

The following YouTube tutorial shows how to set up Xtract Universal with the Azure Storage destination:

## Requirements

The Azure Storage (Blob / Data Lake) destination supports the following Azure storage account types:

- General-purpose V2 (including Azure Data Lake Storage Gen2)
- General-purpose V1
- BlockBlobStorage
- BlobStorage

To use the Azure Storage (Blob / Data Lake) destination you need one of the above Azure storage accounts. For more information, see [Microsoft Documentation: Azure storage account overview](https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview).

## Create a new Microsoft Azure Storage Destination

Follow the steps below to add a new Microsoft Azure Storage destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Microsoft Azure Storage* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

### Connection Type

The subsection *Connection Type* offers the following methods for authenticating and authorizing access to Azure storage:

- **Access Key**

  ______________________________________________________________________

  This method of authentication authorizes access to the complete storage account. For more information, see [Microsoft Documentation: Manage storage account access keys](https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage).

- **Entra ID**

  ______________________________________________________________________

  This method of authentication uses OAuth 2.0 and [Microsoft Entra ID](https://learn.microsoft.com/en-us/rest/api/storageservices/authorize-with-azure-active-directory) (formerly Azure Active Directory), see [Authentication via Microsoft Entra ID for Azure Storage](../../../knowledge-base/authentication-via-entra-id-with-azure-storage/). Access rights can be granted on storage account or container level.

- **Shared Access Signature (Account)**

  ______________________________________________________________________

  This method of authentication uses Shared Access Signatures (SAS) to access the complete storage account. For more information, see [Microsoft Documentation: Grant limited access to Azure Storage resources using shared access signatures](https://learn.microsoft.com/en-us/azure/storage/common/storage-sas-overview?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json&bc=%2Fazure%2Fstorage%2Fblobs%2Fbreadcrumb%2Ftoc.json).

- **Shared Access Signature (Container)**

  ______________________________________________________________________

  This method of authentication uses Shared Access Signatures (SAS) to access a specific storage container. For more information, see [Microsoft Documentation: Create SAS tokens for storage containers](https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/create-sas-tokens?view=doc-intel-4.0.0#use-the-azure-portal)

For information on advantages and disadvantages of the different authentication methods, see [Microsoft Documentation: Choosing the right authentication method](https://learn.microsoft.com/en-us/azure/storage/common/storage-explorer-security).

### Access Key Parameters / SAS Parameters

The input fields in the subsection *Access key parameters* / *SAS parameters* vary depending on the selected [connection type](#connection-type).

#### Storage account

Enter your storage account name. Do not enter the full URL.

#### Access key

Enter the access key of the Azure Storage account. You can copy the storage account name and access key from the [Azure Portal](https://docs.microsoft.com/en-us/azure/storage/common/storage-account-keys-manage?toc=/azure/storage/blobs/toc.json#view-access-keys-and-connection-string).

#### Connect

Click **[Connect]** to establish a connection to the storage account. If the connection is successful, a "Connection successful" info window opens.

#### Storage account

Enter your storage account name.

#### Tenant ID

Enter the ID of the Azure AD tenant.

#### Client ID

Enter the ID of the registered app. You can copy the tenant ID and client ID in the [Azure portal](https://portal.azure.com/).

#### Connect

Follow the steps below to authenticate Xtract Universal against Microsoft:

1. Click **[Connect]**. The window "Azure OAuth 2.0" opens.
1. When prompted, sign in with your Microsoft credentials. Make sure that the user meets the following requirements:
   - The user has the 'Storage Blob Data Contributor' or 'Owner' role in Azure Storage.
   - The user does not use Multifactor Authentication (MFA) as extractions fail when the MFA of the user expires.
1. If the connection is successful, a "Connection successful" info window opens.

Warning

**The window "Entra ID" shows a blank screen.**\
If the window "Entra ID" shows a blank screen, the content is likely blocked by the Internet Explorer ESC (Enhanced Security Configuration) on Windows servers.\
To disable the Internet Explorer ESC, refer to the instructions in the [Microsoft Documentation: How to turn off Internet Explorer ESC on Windows servers](https://learn.microsoft.com/en-us/previous-versions/troubleshoot/browsers/security-privacy/enhanced-security-configuration-faq#how-to-turn-off-internet-explorer-esc-on-windows-servers).

#### Storage account

Enter your storage account name. Do not enter the full URL.

#### SAS token

Enter the SAS token at the Azure Storage container level. You can copy the SAS token from the Azure portal in the following menu: **Storage accounts > [account_name] > Security + networking > Shared access signature**. Note that the following permissions are required when using Shared Access Signature (SAS):

- Add
- Create
- Write
- Delete
- List

#### Connect

Click **[Connect]** to establish a connection to the storage account. If the connection is successful, you can select an existing container from the drop down list **Container**.

#### Storage account

Enter your storage account name. Do not enter the full URL.

#### Container

Enter the name of an existing Azure storage container.

#### SAS token

Enter the SAS token generated at the Azure Storage container level. You can copy the SAS token from the Azure portal in the following menu: **Storage accounts > [account_name] > Data storage > Containers > [account_name] > Generate SAS**. Note that the following permissions are required when using Shared Access Signature (SAS):

- Add
- Create
- Write
- Delete
- List

#### Connect

Click **[Connect]** to establish a connection to the storage account.

### Container

This subsection is activated after a connection to the storage account was successfully established.

- When using Access Key authentication, select a Blob container from the drop-down list.
- When using Entry ID authentication, enter the name of the Blob container manually.
- When using Shared Access Signature (Account) authentication, select a Blob container from the drop-down list.

#### Test connection

Click **[Test Connection]** to check if the storage container can be accessed.\
If the connection is successful, a "Connection to container \<*name of container*> successful" info window opens.

The *Azure Storage (Blob / Data Lake)* destination can now be used.

### Misc

Note

The settings in *Misc* can only be used in combination with a Blob container.

#### Folder path

Creates a folder structure within the container. Script expressions are supported, see [Use Script Expressions as Dynamic Folder Paths](#use-script-expressions-as-dynamic-folder-paths).

- Create a single folder by entering a folder name without slashes: `[folder]`
- Create subfolders using the following syntax: `[folder]/[subfolder_1]/[subfolder_2]/[..]`

Note

The specified folder path applies to all extractions. To define a folder path for single extractions, use the **Folder** option in the [Destination Settings](#destination-settings).

### File Format

Select the required file format. You can choose between *CSV*, *JSON* and *Parquet*.

#### CVS Settings

The settings for file type *CSV* correspond to the settings of the *Flat File CSV* destination:

- [CSV Settings](../csv-flat-file/#csv-settings)
- [Convert / Encoding](../csv-flat-file/#convert-encoding)

#### JSON Settings

To use the JSON file format, no further settings are necessary.

#### Parquet Settings

The settings for file type *Parquet* correspond to the settings of the *Flat File Parquet* destination:

- [Compatibility Mode](../parquet/#compatibility-mode)

## Connection Retry and Rollback

Connection retry is a built-in function of the Microsoft Azure Storage destination. The retry function is activated by default.

Connection retry is a functionality that prevents extractions from failing in case of transient connection interruptions to Microsoft Azure Storage. Xtract Universal follows an exponential retry strategy. The selected exponential strategy results in seven retry attempts and an overall timespan of 140 seconds. If a connection is not established during the timespan of 140 seconds, the extraction fails.

The retry function is applied after receiving one of the following HTTP errors or exceptions:

- 503 Service Unavailable
- 504 Gateway Timeout
- WebExceptionStatus.ConnectionClosed
- WebExceptionStatus.ConnectFailure
- WebExceptionStatus.Timeout
- WebExceptionStatus.RequestCanceled
- WebExceptionStatus.SendFailure
- WebExceptionStatus.NameResolutionFailure

Rollback covers scenarios where extractions do not fail due to connection failures to Azure but due to an error when connecting to SAP. In those cases Xtract Universal tries to remove any files from Azure storage that were created in the course of the extraction.

## Assign the Microsoft Azure Storage Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Microsoft Azure Storage destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

Note

If the name of an object does not begin with a letter, it will be prefixed with an ‘x’, e.g. an object by the name `_namespace_tabname.csv` will be renamed `x_namespace_tabname.csv` when uploaded to the destination. This is to ensure that all uploaded objects are compatible with Azure Data Factory, Hadoop and Spark, which require object names to begin with a letter or give special meaning to objects whose names start with certain non-alphabetic characters.

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Blob Type

#### Append Blob

Creates an [Append Blob](https://docs.microsoft.com/en-us/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs#about-append-blobs).

#### Block Blob

Creates a [Block Blob](https://docs.microsoft.com/en-us/rest/api/storageservices/understanding-block-blobs--append-blobs--and-page-blobs#about-block-blobs).

Note

For both file types an MD5 hash is created upon upload to Azure storage.

### Folder

Writes the extracted data to a specific folder structure within the container. If the specified folder does not exist, it is created.

- Create a single folder by entering a folder name without slashes: `[folder]`
- Create subfolders using the following syntax: `[folder]/[subfolder_1]/[subfolder_2]/[..]`

Note

The specified folder path only applies to the selected extraction. To define a folder path for all extractions, use the **Folder path** option in the [Destination Details](#destination-details).

#### Use Script Expressions as Dynamic Folder Paths

Script expressions can be used to generate a dynamic folder path. This allows generating folder paths that are composed of an extraction's properties, e.g., extraction name, SAP source object. The described scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example: /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFields]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. | | `#{Extraction.Fields["[0D_NW_CODE]"].Selections[0].Value}#` | Only for BWCube extractions (MDX mode): returns the input value of a defined selection. | | `#{Extraction.Fields["[0D_NW_CHANN]"].RangeSelections[0].LowerValue}#` | Only for BWCube extractions (MDX mode): returns the lower input value of a defined selection range. | | `#{Extraction.Fields["[0D_NW_CHANN]"].RangeSelections[0].UpperValue}#` | Only for BWCube extractions (MDX mode): returns the upper input value of a defined selection range. | | `#{Extraction.Fields["0D_NW_CODE"].Selections[0].Value}#` | Only for BWCube extractions (BICS mode): returns the input value of a defined selection. | | `#{Extraction.Fields["0D_NW_CHANN"].RangeSelections[0].LowerValue}#` | Only for BWCube extractions (BICS mode): returns the lower input value of a defined selection range. | | `#{Extraction.Fields["0D_NW_CHANN"].RangeSelections[0].UpperValue}#` | Only for BWCube extractions (BICS mode): returns the upper input value of a defined selection range. |

### Compression

Compression is only available for the csv file format, see [Destination Details: File Format](#file-format).

#### GZip

The data is transferred compressed and stored as a gz file.

### File Splitting

Writes extraction data of a single extraction to multiple files. Each filename is appended by *\_part[nnn]*.

#### Max. file size

The value set in *Max. file size* determines the maximum size of each file.

Note

The option *Max. file size* does not apply to gzip files. The size of a gzipped file cannot be determined in advance.

### Empty Files

When this option is active, empty result sets create an empty file in the target environment. Deactivate this option to not create empty files.

______________________________________________________________________

## Related Links

- [Knowledge Base Article: Authentication via Microsoft Entra ID for Azure Storage](../../../knowledge-base/authentication-via-entra-id-with-azure-storage/)
- [Integration via Azure Data Factory](../../execute-and-automate/call-via-etl/#integration-via-azure-data-factory)

This page shows how to set up and use the Microsoft Azure Synapse Analytics destination. The Microsoft Azure Synapse Analytics destination loads data to an Azure Synapse SQL Pool.

## Requirements

To use the Azure Synapse Analytics SQL Pool destination, you need:

- An [Azure Analytics SQL database](https://docs.microsoft.com/en-us/azure/azure-sql/database/single-database-create-quickstart?tabs=azure-portal).
- [Azure portal firewall rules](https://docs.microsoft.com/en-us/azure/azure-sql/database/secure-database-tutorial#create-firewall-rules) that grant access for the IP addresses Xtract Universal is running on.

## Create a new Microsoft Azure Synapse Analytics Destination

Follow the steps below to add a new Microsoft Azure Synapse Analytics destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Microsoft Azure Synapse Analytics* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

#### Server Name

Enter the name of the Azure Servers in the following format:\
`[servername].database.windows.net`

#### User Name

Enter the user name.

#### Password

Enter the password.

#### Database Name

Enter the name of the Azure Synapse SQl Pool.

#### Test Connection

Check the database connection.

## Assign the Microsoft Azure Synapse Analytics Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Microsoft Azure Synapse Analytics destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Preparation

Defines the action on the target database before the data is inserted into the target table.

| Option | Description | | --- | --- | | *Drop & Create* | Remove table if available and create new table (default). | | *Truncate Or Create* | Empty table if available, otherwise create. | | *Create If Not Exists* | Create table if not available. | | *Prepare Merge* | Prepares the merge process and creates e.g. a temporary staging table, see [Merge Data](#merge-data). | | *None* | No action. | | *Custom SQL* | Here you can define your own script, see [Custom SQL Statements](#custom-sql-statements). |

To only create the table in the first step and not insert any data, you have two options:

1. Copy the SQL statement and execute it directly on the target data database.
1. Select the *None* option for **Row Processing** and execute the extraction.

Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.

### Row Processing

Defines how the data is inserted into the target table.

| Option | Description | | --- | --- | | *Insert* | Insert records (default). | | *Fill merge staging table* | Insert records into the staging table. | | *None* | No action. | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Finalization

Defines the action on the target database after the data has been successfully inserted into the target table.

| Option | Description | | --- | --- | | *Finalize Merge* | Closes the merge process and deletes the temporary staging table, for example. | | *None* | No action (default). | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Debugging

Warning

**Performance decrease!**\
The performance decreases when bulk insert is disabled. Disable the bulk insert only when necessary, e.g., upon request of the support team.

By activating the checkbox **Disable bulk operations**, the default bulk insert is deactivated when writing to a database.

This option enables detailed error analysis, if certain data rows cannot be persisted on the database. Possible causes for the incorrect behavior are incorrect values with regard to the stored data type.

Debugging needs to be deactivated again, after the successful error analysis, otherwise the performance of the database write processes remains low.

Note

Bulk operations are not supported when using [Custom SQL statements](#custom-sql-statements), e.g., in *Row Processing*. Bulk operations lead to performance decrease. To increase performance when using [Custom SQL statements](#custom-sql-statements), it is recommended to perform the custom processing in the *Finalization* step.

### Transaction style

Note

The available options for Transaction Style vary depending on the destination.

#### RowProcessingOnly

Only *Row Processing* is executed in a transaction. *Preparation* and *Finalization* without an explicit transaction (implicit commits).

- Advantage: DDL in 'Preparation *and* Finalization\* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)
- Disadvantage: no rollback of *Preparation/Finalization*.

#### No Transaction

No explicit transactions.

- Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.
- Disadvantage: no rollback

## Merge Data

The following example depicts the update of the existing data records in a database by running an extraction to merge data. In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP.

With a merge command, the updated value is written to the destination database table. The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.

Tip

Alternatively to merging, the data can be also updated by means of full load. The full load method is less efficient.

### Prerequisites

You need a table with existing SAP data, in which new data can be merged.\
Ideally, the table with existing data is created in the initial load with the corresponding **Preparation** option and filled with data with the **Row Processing** option *Insert*.

After the table is created, open SAP and change a field value in the SAP table that is used for the data merge.

Warning

**Faulty merge.**\
Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the [General Settings](../../table/general-settings/#primary-key-tab) of the extraction type to execute the merge command.

### Merge Command

The merge process is performed using a staging table and takes place in three steps:

1. A temporary table is created.
1. The data is inserted in the temporary table.
1. The temporary table is merged with the target table and then the temporary table is deleted.

Follow the steps below to set up the merge process in Xtract Universal:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions.
1. Click **[Destination]**. The window "Destination Settings" opens.
1. Make sure to assign Microsoft Azure Synapse Analytics destination to the extraction.
1. Apply the following destination settings:
1. Click **[OK]** and [run the extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

More information about the updated fields can be found in the SQL statement.\
It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update.\
Fields that do not appear in the SQL statement are not affected by changes.

## Custom SQL Statements

The Microsoft Azure Synapse Analytics destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the Microsoft Azure Synapse Analytics destination:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..

1. Click **[Destination]**, the window "Destination Settings" opens.

1. Make sure to assign the Microsoft Azure Synapse Analytics destination to the extraction.

1. Select the option *Custom SQL* from the drop-down list in one of the following sections:

   - [Preparation](#preparation)
   - [Row Processing](#row-processing)
   - [Finalization](#finalization)

1. Click **[Edit SQL]** . The window "Edit SQL" opens.

1. Enter your custom SQL statement and click **[OK]** to confirm your input.

### Use Templates

Existing SQL commands can be used as templates.\
You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:

- [Preparation](#preparation), e.g., in *Drop & Create* or *Create if Not Exists*
- [Row Processing](#row-processing), e.g., in *Insert* or *Merge*
- [Finalization](#finalization)

Follow the steps below to generate a Custom SQL command from a template:

1. In one of the staging steps, select the *Custom SQL* option from the drop-down list .
1. Click **[Edit SQL]** . The dialogue "Edit SQL" opens.
1. Navigate to the drop-down menu and select an existing command .
1. Click **[Generate Statement]**. A new statement is generated.
1. Click **[Copy]** to copy the statement to the clipboard.
1. Click **[OK]** to confirm your input.

Check out the [Microsoft SQL Server example](../microsoft-sql-server/#custom-sql-statements) for details on predefined expressions.

Note

The custom SQL code is used for SQL Server destinations. A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.

### Use Script Expressions

You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported:

| Input | Description | | --- | --- | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.TableName }#` | Name of the database table extracted data is written to. | | `#{Extraction.RowsCount }#` | Count of the extracted rows. | | `#{Extraction.RunState}#` | Status of the extraction (Running, FinishedNoErrors, FinishedErrors, Cancelled). | | `#{(int)Extraction.RunState}#` | Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors, 6 = Cancelled). | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. |

For more information, see [Script Expressions](../../parameters/script-expressions/).

```sql
#{
   iif
   (
      ExistsTable("MAKT"),
      "TRUNCATE TABLE \"MAKT\";",
      "
         CREATE TABLE \"MAKT\"(
            \"MATNR\" VARCHAR(18),
            \"SPRAS\" VARCHAR(2),
            \"MAKTX\" VARCHAR(40));
      "
   )
}#

```

### Create a Status Overview

The table "ExtractionStatistics" provides an overview and status of the executed Xtract Universal extractions. To create the "ExtractionStatistics" table, create an SQL table according to the following example:

```sql
CREATE TABLE [dbo].[ExtractionStatistics](
    [TableName] [nchar](50) NULL,
    [RowsCount] [int] NULL,
    [Timestamp] [nchar](50) NULL,
    [RunState] [nchar](50) NULL
) ON [PRIMARY]
GO

```

The *ExtractionStatistics* table is filled in the **Finalization** process step, using the following SQL statement:

```sql
INSERT INTO [ExtractionStatistics]
(
     [TableName], 
     [RowsCount], 
     [Timestamp],
     [RunState]
)
VALUES
(
     '#{Extraction.TableName}#', 
     '#{Extraction.RowsCount}#', 
     '#{Extraction.Timestamp}#',
     '#{Extraction.RunState}#'
);

```

______________________________________________________________________

## Related Links

- [Microsoft Documentation: Microsoft Azure Synapse Analytics](https://docs.microsoft.com/en-us/azure/synapse-analytics/)
- [Integration via Azure Data Factory](../../execute-and-automate/call-via-etl/#integration-via-azure-data-factory)

This page shows how to set up and use the Flat File CSV destination. The Flat File CSV destination is a CSV (comma-separated values) flat file that can be written to a local directory or a network drive.

## Create a new Flat File CSV Destination

Follow the steps below to add a new Flat File CSV destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Flat File CSV* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

### File output path

#### File output path

Enter the folder path to save the destination flat files in. If the entered folder does not exist, a new folder is created.

Note

To write flat files to a network drive, you need to:

- Enter the **File output path** in [UNC format](https://docs.microsoft.com/en-us/dotnet/standard/io/file-path-formats#unc-paths) e.g., `\\Server2\Share\Folder1`.
- Grant write permission to the directory to the [Xtract Universal service](../../server/service-account/) or run the service under a user account that has write permission.

### CSV Settings

#### Column seperator

Defines how two columns in CSV are separated.

#### Row separator

Defines how two rows in CSV are separated.

#### Quote symbol

Defines which character is used to encase field data. A sequence of characters may be used as "Quote symbol". Quotation is applied in the following scenarios:

- The Column separator is part of the field data.
- The Quote symbol is part of the field data.
- The Row separator is part of the field data.
- The Escape character is part of the field data.

#### Escape character

When Escape character is part of the field data, the respective field containing this character is encased by the "Quote symbol". The default escape character is the backslash ''. The field may remain empty.

#### Column names in first row

Defines if the first row contains the column names. This option is set per default.

#### Row separator after last row

Defines if the last row contains a row separator. This option is set per default.

### Convert / Encoding

#### Decimal separator

Defines the decimal separator of decimal number for the output. Dot (.) is the default value.

#### Date format

Defines a customized date format (e.g. YYYY-MM-DD or MM/DD/YYYY) for converting valid SAP dates (YYYYMMDD). Default is YYYY-MM-DD.

#### Time format

Defines a customized time format (e.g. HH-MM-SS or HH:MM:SS) for converting valid SAP times (HHMMSS). Default is HH:MM:SS.

#### Text Encoding

Defines the text encoding.

## Assign the Flat File CSV Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Flat File CSV destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Existing files

If a flat file by the same name already exists in the target directory, the following actions can be taken:

| Option | Description | | --- | --- | | **Replace file** | The export process overwrites existing files. | | **Append results** | The export process appends new data to an already existing file, see [Column Mapping](#column-mapping). | | **Abort extraction** | The process is aborted, if the file already exists. |

### File Splitting

Writes extraction data of a single extraction to multiple files. Each filename is appended by *\_part[nnn]*.

#### Max. file size

The value set in *Max. file size* determines the maximum size of each file.

Note

The option *Max. file size* does not apply to gzip files. The size of a gzipped file cannot be determined in advance.

### Column Mapping

Activate **Column Mapping** when appending data to an existing file or entity that has different column names or a different number of columns.\
This can be the case when extracting data from two or more extractions into the same destination file, where the column names of the extraction and the destination file differ.

Note

The column names in the extraction and destination must be unique. If duplicated column names are found, an error message is displayed. The column names must be corrected, before column mapping can be used.

#### Requirements

When working with flat files, ensure that:

- the XU server and the Designer both have access to the destination file.
- the [output directory](#destination-details) and the [file name](#file-name) of the extraction match the destination file.
- the [Column Name Style](#column-name-style) of the extraction and destination file match.

#### Mapping

Follow the steps below to map data:

1. Select the option **Append results** in the subsection [Existing Files](#existing-files).

1. Activate the option **Column Mapping**.

1. Click **[Map]** to assign columns. The window "Column Mapping" opens.

   - *Destination Columns* displays the names of the columns that are available in the destination file or entity.
   - *Not Mapped* defines whether or not columns are mapped to the destination columns.
   - *Source Columns* defines which SAP column is mapped to a destination column.

1. Depending on the column names of the source and target file, follow the instructions below:

   1. If the column names of the extraction and the names of the destination columns match, click **[Auto map by name]**.
   1. If the column names do not match, assign columns manually by selecting the respective SAP column from the dropdown menu under *Source Columns*.
   1. If a column does not have a counterpart or is not supposed to be appended, activate the checkbox in the column *Not Mapped*.

1. Click **[OK]** to confirm your input.

When running the extraction the extracted data is added to the destination file or entity as specified in the column mapping.

Tip

In case an error message pops up, click **[Show more]** to see a description of what caused the error.

This page shows how to set up and use the HTTP CSV destination.\
The HTTP CSV destination is a generic CSV stream over HTTP. It is supported by many products, e.g., Layer2, INFONEA and KNIME.

## Create a new HTTP CSV Destination

Follow the steps below to add a new HTTP CSV destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *HTTP CSV* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

### CSV Settings

#### Column seperator

Defines how two columns in CSV are separated.

#### Row separator

Defines how two rows in CSV are separated.

#### Quote symbol

Defines which character is used to encase field data. A sequence of characters may be used as "Quote symbol". Quotation is applied in the following scenarios:

- The Column separator is part of the field data.
- The Quote symbol is part of the field data.
- The Row separator is part of the field data.
- The Escape character is part of the field data.

#### Escape character

When Escape character is part of the field data, the respective field containing this character is encased by the "Quote symbol". The default escape character is the backslash ''. The field may remain empty.

#### Column names in first row

Defines if the first row contains the column names. This option is set per default.

#### Row separator after last row

Defines if the last row contains a row separator. This option is set per default.

### Convert / Encoding

#### Decimal separator

Defines the decimal separator of decimal number for the output. Dot (.) is the default value.

#### Date format

Defines a customized date format (e.g. YYYY-MM-DD or MM/DD/YYYY) for converting valid SAP dates (YYYYMMDD). Default is YYYY-MM-DD.

#### Time format

Defines a customized time format (e.g. HH-MM-SS or HH:MM:SS) for converting valid SAP times (HHMMSS). Default is HH:MM:SS.

#### Text Encoding

Defines the text encoding.

## Assign the HTTP CSV Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your HTTP CSV destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

This page shows how to set up and use the Dataiku destination. The Dataiku destination enables users to load SAP data directly from within [Dataiku](https://www.dataiku.com/).

## Requirements

To use the Dataiku destination, the Xtract Universal Dataiku plugin must be installed. You can fetch the [xu-dataiku-plugin](https://github.com/theobald-software/xu-dataiku-plugin) from the gitHub repository.

For information on how to install plugins in Dataiku, see [Dataiku Documentation: Installing plugins](https://doc.dataiku.com/dss/latest/plugins/installing.html).

## Create a new Dataiku Destination

Follow the steps below to add a new Dataiku destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Dataiku* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination. To use the Dataiku destination, no further settings are necessary.

## Assign the Dataiku Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Dataiku destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

## Use Xtract Universal in Dataiku

To use Xtract Universal extractions in the Dataiku Designer, make sure that the [Xtract Universal Dataiku Plugin](https://github.com/theobald-software/xu-dataiku-plugin) is installed in Dataiku.

### Plugin Settings

The Xtract Universal plugin has a settings menu that contains settings for the [connection to an Xtract Universal server](#connection). To open the plugin settings, navigate to **Plugin > Installed > Xtract Universal > Settings > Xtract Universal Server**.

#### Definable inline

Allows users to define a connection to an Xtract Universal server on a dataset level. The connection can be defined when adding a new Xtract Universal dataset to a flow. If active, the drop-down menu to select an Xtract Universal server preset contains the option *Manually defined*.

#### Definable at project level

Allows users to define a connection to an Xtract Universal server on a project level. Connections that are created at the project level are available for all datasets within the project. The connection can be defined in the project settings. To open the project settings for Xtract Universal, navigate to **Settings > Plugin presets > Xtract Universal**.

#### ADD PRESET

Click **[+ADD PRESET]** to create a connection to an Xtract Universal server on the plugin level. Connections that are created at the plugin level are available in all projects and datasets.

#### Description

Add a description of the connection.

#### Xtract Universal Server

Enter the hostname or IP address of the server that runs Xtract Universal without the port. The port is specified in the **Customize Port** option.

#### Transport Layer Security (HTTPS)

Activate this option if [TLS is enabled in Xtract Universal](../../access-restrictions/restrict-server-access/#activate-tls-encryption). Optionally, enter a valid username and password for the authentication against the Xtract Universal server. Currently, only [Xtract Universal users](../../access-restrictions/user-management/#create-custom-users) are supported (no Active Directory users).

#### Customize Port

Specify the port that is used to communicate with the Xtract Universal server. Make sure the connection uses the same port that is defined in the Xtract Universal [web server settings](../../server/server-settings/#web-server). The default ports are 8065 without TLS and 8165 with TLS.

### Connection

There are multiple ways to connect to an Xtract Universal Server:

- Add a connection on the plugin level
- Add a connection on a [project level](#definable-at-project-level)
- Add a connection on a [dataset level](#definable-inline) (deactivated by default)

Follow the steps below to add a connection to an Xtract Universal server on the plugin level. Connections that are defined on the plugin level are available in all projects and datasets.

1. Open the plugin settings for the Xtract Universal server. The settings are located in **Plugin > Installed > Xtract Universal > Settings > Xtract Universal Server**.
1. Click **[+ADD PRESET]** to create a new connection.
1. When prompted, enter a name for the connection and click **[CREATE]**.
1. In the field **Xtract Universal server**, enter the host name or IP address of the Xtract Universal server.
1. If [TLS is enabled in Xtract Universal](../../access-restrictions/restrict-server-access/#activate-tls-encryption), activate the option **Use TLS for communication with the XU server** and provide valid credentials.
1. If a non-default port is used for the Xtract Universal [web server](../../server/server-settings/#web-server), click **Customize port** to specify the port.
1. Click **[SAVE]**.

The connection is now available in all projects and datasets.

### Add Datasets with Xtract Universal

Connect to the Xtract Universal Server and select the extraction you want to to execute in Dataiku.

Note

Make sure the Xtract Universal server is running.

1. Open a project in Dataiku.

1. Click **[+DATASET] > Xtract Universal**.

1. Click **Xtract Universal Extraction**.

1. In the drop-down menu **Xtract Universal server preset**, select an existing [connection to an Xtract Universal server](#connection).

1. In the drop-down menu **Extraction**, select an existing extraction. Only extractions with that have the Dataiku destination [assigned](#assign-the-dataiku-destination-to-an-extraction) to them are displayed.

   Tip

   If no extractions are available, check if the [connection settings](#plugin-settings) are correct and if there are extractions on the server that use the Dataiku destination.

1. Optional: click **[+ADD AN OBJECT]** to pass values to an Xtract Universal [extraction parameter](../../parameters/extraction-parameters/). The plugin fetches all available parameters from Xtract Universal.

   - Select a parameter from the drop down menu **Name**.
   - Enter a value for the parameter in the input field **Value**.

1. Click **[TEST & GET SCHEMA]**. A preview of the data is displayed.

1. Click **[CREATE]**. The data is loaded into the workflow.

Tip

In case of errors, navigate to **Administration > Maintenance** to check the logs. The logs contain plugin logs and stack traces, see [Dataiku Documentation: Diagnosing and debugging issues](https://doc.dataiku.com/dss/latest/troubleshooting/diagnosing.html).

______________________________________________________________________

## Related Links

- [Dataiku Documentation](https://doc.dataiku.com/dss/latest/)
- [Dataiku Knowledge Base](https://knowledge.dataiku.com/latest/)
- [Xtract Universal Server](../../server/)

This page shows how to set up and use the EXASolution destination. The EXASolution destination loads data to an EXASolution database.

## Requirements

As of Xtract Universal version 4.2.26.0 the Exasol ADO.Net driver ExaDataProvider is provided with the setup of Xtract Universal. There are no additional installations needed to use Exasol database destination.

## Create a new EXASolution Destination

Follow the steps below to add a new EXASolution destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *EXASolution* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

#### Connection string

Enter the name or IP of the DB2 server and the port number.

#### User Id / Password

Enter the user ID and password of the database user.

#### Schema

Enter the schema name of the database.

#### Test Connection

Checks the database connection.

## Assign the EXASolution Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your EXASolution destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Preparation

Defines the action on the target database before the data is inserted into the target table.

| Option | Description | | --- | --- | | *Drop & Create* | Remove table if available and create new table (default). | | *Truncate Or Create* | Empty table if available, otherwise create. | | *Create If Not Exists* | Create table if not available. | | *Prepare Merge* | Prepares the merge process and creates e.g. a temporary staging table, see [Merge Data](#merge-data). | | *None* | No action. | | *Custom SQL* | Here you can define your own script, see [Custom SQL Statements](#custom-sql-statements). |

To only create the table in the first step and not insert any data, you have two options:

1. Copy the SQL statement and execute it directly on the target data database.
1. Select the *None* option for **Row Processing** and execute the extraction.

Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.

### Row Processing

Defines how the data is inserted into the target table.

| Option | Description | | --- | --- | | *Insert* | Insert records (default). | | *Fill merge staging table* | Insert records into the staging table. | | *None* | No action. | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). | | *Merge (deprecated)* | This option is obsolete, see [Merge Data](#merge-data). Use the *Fill merge staging table* option. |

### Finalization

Defines the action on the target database after the data has been successfully inserted into the target table.

| Option | Description | | --- | --- | | *Finalize Merge* | Closes the merge process and deletes the temporary staging table, for example. | | *None* | No action (default). | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Debugging

Warning

**Performance decrease!**\
The performance decreases when bulk insert is disabled. Disable the bulk insert only when necessary, e.g., upon request of the support team.

By activating the checkbox **Disable bulk operations**, the default bulk insert is deactivated when writing to a database.

This option enables detailed error analysis, if certain data rows cannot be persisted on the database. Possible causes for the incorrect behavior are incorrect values with regard to the stored data type.

Debugging needs to be deactivated again, after the successful error analysis, otherwise the performance of the database write processes remains low.

Note

Bulk operations are not supported when using [Custom SQL statements](#custom-sql-statements), e.g., in *Row Processing*. Bulk operations lead to performance decrease. To increase performance when using [Custom SQL statements](#custom-sql-statements), it is recommended to perform the custom processing in the *Finalization* step.

### Transaction style

Note

The available options for Transaction Style vary depending on the destination.

#### One Transaction

*Preparation*, *Row Processing* and *Finalization* are all performed in a single transaction.

- Advantage: clean rollback of all changes.
- Disadvantage: possibly extensive locking during the entire extraction period.

Recommendation

Only use *One Transaction* in combination with DML commands, e.g., "truncate table" and "insert. Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command. Example: If a table is created in the preparation step, the opened "OneTransaction" is committed and a rollback in the next steps is not performed correctly.

#### Three Transactions

*Preparation*, *Row Processing* and *Finalization* are each executed in a separate transaction.

- Advantage: clean rollback of the individual sections, possibly shorter locking phases than with *One Transaction* (e.g. with DDL in *Preparation*, the entire DB is only locked during preparation and not for the entire extraction duration).
- Disadvantage: no rollback of previous step possible (error in *Row Processing* only rolls back changes from *Row Processing*, but not *Preparation*).

#### RowProcessingOnly

Only *Row Processing* is executed in a transaction. *Preparation* and *Finalization* without an explicit transaction (implicit commits).

- Advantage: DDL in 'Preparation *and* Finalization\* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)
- Disadvantage: no rollback of *Preparation/Finalization*.

#### No Transaction

No explicit transactions.

- Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.
- Disadvantage: no rollback

## Merge Data

The following example depicts the update of the existing data records in a database by running an extraction to merge data. In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP.

With a merge command, the updated value is written to the destination database table. The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.

Tip

Alternatively to merging, the data can be also updated by means of full load. The full load method is less efficient.

### Prerequisites

You need a table with existing SAP data, in which new data can be merged.\
Ideally, the table with existing data is created in the initial load with the corresponding **Preparation** option and filled with data with the **Row Processing** option *Insert*.

After the table is created, open SAP and change a field value in the SAP table that is used for the data merge.

Warning

**Faulty merge.**\
Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the [General Settings](../../table/general-settings/#primary-key-tab) of the extraction type to execute the merge command.

### Merge Command

The merge process is performed using a staging table and takes place in three steps:

1. A temporary table is created.
1. The data is inserted in the temporary table.
1. The temporary table is merged with the target table and then the temporary table is deleted.

Follow the steps below to set up the merge process in Xtract Universal:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions.
1. Click **[Destination]**. The window "Destination Settings" opens.
1. Make sure to assign EXASolution destination to the extraction.
1. Apply the following destination settings:
1. Click **[OK]** and [run the extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

More information about the updated fields can be found in the SQL statement.\
It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update.\
Fields that do not appear in the SQL statement are not affected by changes.

## Custom SQL Statements

The EXASolution destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the EXASolution destination:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..

1. Click **[Destination]**, the window "Destination Settings" opens.

1. Make sure to assign the EXASolution destination to the extraction.

1. Select the option *Custom SQL* from the drop-down list in one of the following sections:

   - [Preparation](#preparation)
   - [Row Processing](#row-processing)
   - [Finalization](#finalization)

1. Click **[Edit SQL]** . The window "Edit SQL" opens.

1. Enter your custom SQL statement and click **[OK]** to confirm your input.

### Use Templates

Existing SQL commands can be used as templates.\
You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:

- [Preparation](#preparation), e.g., in *Drop & Create* or *Create if Not Exists*
- [Row Processing](#row-processing), e.g., in *Insert* or *Merge*
- [Finalization](#finalization)

Follow the steps below to generate a Custom SQL command from a template:

1. In one of the staging steps, select the *Custom SQL* option from the drop-down list .
1. Click **[Edit SQL]** . The dialogue "Edit SQL" opens.
1. Navigate to the drop-down menu and select an existing command .
1. Click **[Generate Statement]**. A new statement is generated.
1. Click **[Copy]** to copy the statement to the clipboard.
1. Click **[OK]** to confirm your input.

Check out the [Microsoft SQL Server example](../microsoft-sql-server/#custom-sql-statements) for details on predefined expressions.

Note

The custom SQL code is used for SQL Server destinations. A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.

### Use Script Expressions

You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported:

| Input | Description | | --- | --- | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.TableName }#` | Name of the database table extracted data is written to. | | `#{Extraction.RowsCount }#` | Count of the extracted rows. | | `#{Extraction.RunState}#` | Status of the extraction (Running, FinishedNoErrors, FinishedErrors, Cancelled). | | `#{(int)Extraction.RunState}#` | Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors, 6 = Cancelled). | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. |

For more information, see [Script Expressions](../../parameters/script-expressions/).

```sql
#{
   iif
   (
      ExistsTable("MAKT"),
      "TRUNCATE TABLE \"MAKT\";",
      "
         CREATE TABLE \"MAKT\"(
            \"MATNR\" VARCHAR(18),
            \"SPRAS\" VARCHAR(2),
            \"MAKTX\" VARCHAR(40));
      "
   )
}#

```

### Create a Status Overview

The table "ExtractionStatistics" provides an overview and status of the executed Xtract Universal extractions. To create the "ExtractionStatistics" table, create an SQL table according to the following example:

```sql
CREATE TABLE [dbo].[ExtractionStatistics](
    [TableName] [nchar](50) NULL,
    [RowsCount] [int] NULL,
    [Timestamp] [nchar](50) NULL,
    [RunState] [nchar](50) NULL
) ON [PRIMARY]
GO

```

The *ExtractionStatistics* table is filled in the **Finalization** process step, using the following SQL statement:

```sql
INSERT INTO [ExtractionStatistics]
(
     [TableName], 
     [RowsCount], 
     [Timestamp],
     [RunState]
)
VALUES
(
     '#{Extraction.TableName}#', 
     '#{Extraction.RowsCount}#', 
     '#{Extraction.Timestamp}#',
     '#{Extraction.RunState}#'
);

```

This page shows how to set up and use the Google Cloud Storage destination. The Google Cloud Storage destination loads data to a Google Cloud Storage.

## About Google Cloud Storage

Google Cloud Platform (GCP) is a collection of cloud services provided by Google. Google Cloud Platform is available at [cloud.google.com](https://cloud.google.com).

Google Cloud Storage is one of the Google services used for storing data in the Google infrastructure. For more information see [Google Cloud Storage Documentation](https://cloud.google.com/storage/docs#docs).

### GCP console

The GCP console allows configuring of all resources and services. To get to the overview dashboard, navigate to the [Google Cloud Storage](https://cloud.google.com/storage) page and click **[Console]** or **[Go to console]**.

To access all settings and services use the navigation menu on the upper left side.

## Requirements

- A Google account
- A Google Cloud Platform (GCP) subscription (trial version offered)
- A project ("My First Project" is pre-defined)
- A Google Cloud Storage (GCS) bucket for data extractions

## Create a new Google Cloud Storage Destination

Follow the steps below to add a new Google Cloud Storage destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Google Cloud Storage* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

### GCS Settings

### Connection Type & Parameters

The subsection *Connection Type* offers two different methods for authenticating and authorizing access to an Azure Storage account:

- **User Login**

  ______________________________________________________________________

  Logs into Google Cloud Storage using the OAuth client ID authentication. To enable the OAuth 2.0 protocol, configure an OAuth flow with the required access permissions to Xtract Universal, see [Knowledge Base Article: Setting Up OAuth 2.0 for the Google Cloud Storage Destination](../../../knowledge-base/google-cloud-storage-oauth/).

- **Service Account**

  ______________________________________________________________________

  Logs into Google Cloud Storage using the credentials of a service account for authentication. The service account is identified by a RSA key pair. When creating the keys, the user receives a service account file from Google containing information about the account.

The following input fields vary depending on the selected authentication method.

#### Client ID

Enter the client ID created in the OAuth 2.0 setup. The client ID uses the following format:\
`<project-number>-<unique-identifier>.apps.googleusercontent.com`

Example: *123456789012-abcdefghijklmnopqrstuvwxyz123456.apps.googleusercontent.com*

#### Client Secret

Enter the client secret created in the OAuth 2.0 setup.

#### Connect

Processes the previously created OAuth flow to establish a connection with the storage account. Choose your Google account and grant access to Xtract Universal in all required windows. When a connection is successful, an "Authentication succeeded" message is displayed in the browser. In Xtract Universal a "Connection established" message is displayed in a separate window.

Warning

**This app isn't verified.**\
This error message occurs if you did not verify the app. Click **[Advanced]** and **[Go to Xtract Universal (unsafe)]** to continue.

#### Key file

Enter a path to the service account file, that is created together with the keys. Make sure that the Xtract Universal service has access to the file.

### Bucket

When using OAuth 2.0 authentication, the "Bucket" subsection can only be filled after a connection to the storage account has been established.

#### Project ID

The Project ID can be looked up in the GCP dashboard under *Project info*.

#### Bucket name

When using OAuth 2.0 authentication, click **[Get buckets]** to display available buckets.\
A bucket can be created in the navigation menu under **Storage > Browser**.

Choose a bucket name, location type and storage class or access control.

Under **Advanced Settings (optional)** you can select the desired encryption method applied to the bucket. Get more details on encryption on the official [Google Help Page](https://cloud.google.com/storage/docs/encryption).

### Encryption

#### Default

Applies the encryption method specified in your GCS bucket.\
Google encrypts all data that is stored on the Google servers by default. In addition you can use the Google Cloud Key Management Service (KMS) to create and apply keys to your buckets.\
The KMS can be enabled in the GCP console's navigation menu under **Security > Cryptographic Keys**.

#### Customer-supplied

If you check the *Customer-supplied* option, you need to provide a valid AES256 Crypto Key (256 bit in length).\
The Crypto key is not stored in the GCP and demands the additional effort to be able to to decrypt your data later.

#### Crypto key

When the encryption method *Customer Supplied* is active, enter the cryptographic key for the encryption.

### Misc

#### Folder path

Creates a folder structure within the bucket. Script expressions are supported, see [Use Script Expressions as Dynamic Folder Paths](#use-script-expressions-as-dynamic-folder-paths).

- Create a single folder by entering a folder name without slashes: `[folder]`
- Create subfolders using the following syntax: `[folder]/[subfolder_1]/[subfolder_2]/[..]`

Note

The specified folder path applies to all extractions. To define a folder path for single extractions, use the **Folder** option in the [Destination Settings](#destination-settings).

### File Format

Select the required file format. You can choose between *CSV*, *JSON* and *Parquet*.

#### CVS Settings

The settings for file type *CSV* correspond to the settings of the *Flat File CSV* destination:

- [CSV Settings](../csv-flat-file/#csv-settings)
- [Convert / Encoding](../csv-flat-file/#convert-encoding)

#### JSON Settings

To use the JSON file format, no further settings are necessary.

#### Parquet Settings

The settings for file type *Parquet* correspond to the settings of the *Flat File Parquet* destination:

- [Compatibility Mode](../parquet/#compatibility-mode)

## Connection Retry

Connection retry is a built-in function of the Google Cloud Storage destination. The retry function is activated by default.

Connection retry is a functionality that prevents extractions from failing in case of transient connection interruptions to Google Cloud Storage. Xtract Universal follows an exponential retry strategy. The selected exponential strategy results in eight retry attempts and an overall timespan of 140 seconds. If a connection is not established during the timespan of 140 seconds, the extraction fails.

The retry function is applied after receiving one of the following HTTP errors or exceptions:

- 503 Service Unavailable
- 504 Gateway Timeout
- WebExceptionStatus.ConnectionClosed
- WebExceptionStatus.ConnectFailure
- WebExceptionStatus.Timeout
- WebExceptionStatus.RequestCanceled
- WebExceptionStatus.SendFailure
- WebExceptionStatus.NameResolutionFailure

## Assign the Google Cloud Storage Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Google Cloud Storage destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

Note

If the name of an object does not begin with a letter, it will be prefixed with an ‘x’, e.g. an object by the name `_namespace_tabname.csv` will be renamed `x_namespace_tabname.csv` when uploaded to the destination. This is to ensure that all uploaded objects are compatible with Azure Data Factory, Hadoop and Spark, which require object names to begin with a letter or give special meaning to objects whose names start with certain non-alphabetic characters.

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Folder

Writes the extracted data to a specific folder structure within the bucket. If the specified folder does not exist, it is created.

- Create a single folder by entering a folder name without slashes: `[folder]`
- Create subfolders using the following syntax: `[folder]/[subfolder_1]/[subfolder_2]/[..]`

Note

The specified folder path only applies to the selected extraction. To define a folder path for all extractions, use the **Folder path** option in the [Destination Details](#destination-details).

#### Use Script Expressions as Dynamic Folder Paths

Script expressions can be used to generate a dynamic folder path. This allows generating folder paths that are composed of an extraction's properties, e.g., extraction name, SAP source object. The described scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example: /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFields]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. | | `#{Extraction.Fields["[0D_NW_CODE]"].Selections[0].Value}#` | Only for BWCube extractions (MDX mode): returns the input value of a defined selection. | | `#{Extraction.Fields["[0D_NW_CHANN]"].RangeSelections[0].LowerValue}#` | Only for BWCube extractions (MDX mode): returns the lower input value of a defined selection range. | | `#{Extraction.Fields["[0D_NW_CHANN]"].RangeSelections[0].UpperValue}#` | Only for BWCube extractions (MDX mode): returns the upper input value of a defined selection range. | | `#{Extraction.Fields["0D_NW_CODE"].Selections[0].Value}#` | Only for BWCube extractions (BICS mode): returns the input value of a defined selection. | | `#{Extraction.Fields["0D_NW_CHANN"].RangeSelections[0].LowerValue}#` | Only for BWCube extractions (BICS mode): returns the lower input value of a defined selection range. | | `#{Extraction.Fields["0D_NW_CHANN"].RangeSelections[0].UpperValue}#` | Only for BWCube extractions (BICS mode): returns the upper input value of a defined selection range. |

### Compression

Compression is only available for the csv file format, see [Destination Details: File Format](#file-format).

| Option | Description | | --- | --- | | **None** | The data is transferred uncompressed and stored as a csv file. | | **GZip** | The data is transferred compressed and stored as a gz file. |

### File Splitting

Writes extraction data of a single extraction to multiple files. Each filename is appended by *\_part[nnn]*.

#### Max. file size

The value set in *Max. file size* determines the maximum size of each file.

Note

The option *Max. file size* does not apply to gzip files. The size of a gzipped file cannot be determined in advance.

### Empty Files

When this option is active, empty result sets create an empty file in the target environment. Deactivate this option to not create empty files.

This page shows how to set up and use the Huawei Cloud OBS destination. The Huawei Cloud OBS destination loads data to a Huawei Cloud Object Storage Service (OBS).

Warning

**File Fragments in the Cloud Storage**\
Huawei Cloud OBS destination uses multipart upload. That means that data is uploaded in fragments that are merged into a single file at the end of the extraction. When an extraction fails due to connection issues, the request to cancel the multipart upload can fail.\
Delete the fragments manually, see [Huawei Cloud Support: Deleting Fragments Directly](https://support.huaweicloud.com/intl/en-us/obs_faq/obs_faq_0046.html#section1).

## Create a new Huawei Cloud OBS Destination

Follow the steps below to add a new Huawei Cloud OBS destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Huawei Cloud OBS* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

### Authentication

#### Access Key ID (AK)

Enter the access key of the Huawei Cloud OBS account. For more information on how to create access keys, see [Huawei Cloud Support: Managing Access Keys (AK and SK)](https://support.huaweicloud.com/intl/en-us/usermanual-iam/iam_02_0003.html).

#### Secret Access Key ID (SK)

Enter the secret access key of the Huawei Cloud OBS account. For more information on how to create access keys, see [Huawei Cloud Support: Managing Access Keys (AK and SK)](https://support.huaweicloud.com/intl/en-us/usermanual-iam/iam_02_0003.html).

#### Region

Select the region of the data storage.

#### Connect

Click **[Connect]** to establish a connection to the storage account. If the connection is successful, "Connected" is displayed next to the button.

### Bucket

This setting only becomes available after a connection to the storage account is established.\
Select a bucket. The SAP data is extracted into the selected bucket. Click **[]** to refresh the list of available buckets.

### Misc

#### Folder path

Creates a folder structure within the bucket. Script expressions are supported, see [Use Script Expressions as Dynamic Folder Paths](#use-script-expressions-as-dynamic-folder-paths).

- Create a single folder by entering a folder name without slashes: `[folder]`
- Create subfolders using the following syntax: `[folder]/[subfolder_1]/[subfolder_2]/[..]`

Note

The specified folder path applies to all extractions. To define a folder path for single extractions, use the **Folder** option in the [Destination Settings](#destination-settings).

### File Format

Select the required file format. You can choose between *CSV*, *JSON* and *Parquet*.

#### CVS Settings

The settings for file type *CSV* correspond to the settings of the *Flat File CSV* destination:

- [CSV Settings](../csv-flat-file/#csv-settings)
- [Convert / Encoding](../csv-flat-file/#convert-encoding)

#### JSON Settings

To use the JSON file format, no further settings are necessary.

#### Parquet Settings

The settings for file type *Parquet* correspond to the settings of the *Flat File Parquet* destination:

- [Compatibility Mode](../parquet/#compatibility-mode)

## Connection Retry and Rollback

Connection retry is a built-in function of the Huawei Cloud OBS destination. The retry function is activated by default.

Connection retry is a functionality that prevents extractions from failing in case of transient connection interruptions to Huawei Cloud OBS. Xtract Universal follows an exponential retry strategy. The selected exponential strategy results in seven retry attempts and an overall timespan of 140 seconds. If a connection is not established during the timespan of 140 seconds, the extraction fails.

The retry function is applied after receiving one of the following HTTP errors or exceptions:

- 503 Service Unavailable
- 504 Gateway Timeout
- WebExceptionStatus.ConnectionClosed
- WebExceptionStatus.ConnectFailure
- WebExceptionStatus.Timeout
- WebExceptionStatus.RequestCanceled
- WebExceptionStatus.SendFailure
- WebExceptionStatus.NameResolutionFailure

Rollback covers scenarios where extractions do not fail due to connection failures to Huawei but e.g. due to an error when connecting to SAP. In those cases Xtract Universal tries to remove any files from the Huawei Cloud storage that were created in the course of the extraction.

## Assign the Huawei Cloud OBS Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Huawei Cloud OBS destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Folder

Writes the extracted data to a specific folder structure within the bucket. If the specified folder does not exist, it is created.

- Create a single folder by entering a folder name without slashes: `[folder]`
- Create subfolders using the following syntax: `[folder]/[subfolder_1]/[subfolder_2]/[..]`

Note

The specified folder path only applies to the selected extraction. To define a folder path for all extractions, use the **Folder path** option in the [Destination Details](#destination-details).

#### Use Script Expressions as Dynamic Folder Paths

Script expressions can be used to generate a dynamic folder path. This allows generating folder paths that are composed of an extraction's properties, e.g., extraction name, SAP source object. The described scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example: /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFields]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. | | `#{Extraction.Fields["[0D_NW_CODE]"].Selections[0].Value}#` | Only for BWCube extractions (MDX mode): returns the input value of a defined selection. | | `#{Extraction.Fields["[0D_NW_CHANN]"].RangeSelections[0].LowerValue}#` | Only for BWCube extractions (MDX mode): returns the lower input value of a defined selection range. | | `#{Extraction.Fields["[0D_NW_CHANN]"].RangeSelections[0].UpperValue}#` | Only for BWCube extractions (MDX mode): returns the upper input value of a defined selection range. | | `#{Extraction.Fields["0D_NW_CODE"].Selections[0].Value}#` | Only for BWCube extractions (BICS mode): returns the input value of a defined selection. | | `#{Extraction.Fields["0D_NW_CHANN"].RangeSelections[0].LowerValue}#` | Only for BWCube extractions (BICS mode): returns the lower input value of a defined selection range. | | `#{Extraction.Fields["0D_NW_CHANN"].RangeSelections[0].UpperValue}#` | Only for BWCube extractions (BICS mode): returns the upper input value of a defined selection range. |

### Compression

Compression is only available for the csv file format, see [Destination Details: File Format](#file-format).

#### GZip

The data is transferred compressed and stored as a gz file.

### File Splitting

Writes extraction data of a single extraction to multiple files. Each filename is appended by *\_part[nnn]*.

#### Max. file size

The value set in *Max. file size* determines the maximum size of each file.

Note

The option *Max. file size* does not apply to gzip files. The size of a gzipped file cannot be determined in advance.

### Empty Files

When this option is active, empty result sets create an empty file in the target environment. Deactivate this option to not create empty files.

This page shows how to set up and use the IBM Db2 destination. The IBM Db2 destination loads data to an IBM Db2 destination.

## Requirements

The appropriate version (32bit for 32bit OS, 64bit for 64bit OS) of the ADO .NET driver must be installed.\
Select the *IBM Data Server Driver Package* and then the *IBM Data Server Driver Package (Windows AMD64 and Intel EM64T)* or *IBM Data Server Driver Package (Windows 32-bit AMD and Intel x86)*, see [IBM Data Server Client Packages - Version 11.5 GA](https://www.ibm.com/support/pages/node/387577).

If a fixed version is available, download the fixed version of the provider from the software vendor's website, see [Overview IBM Data Server Client Packages](https://www.ibm.com/support/pages/node/323035).

## Create a new IBM Db2 Destination

Follow the steps below to add a new IBM Db2 destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *IBM Db2* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

#### Provider

Select a .Net provider for DB2. To install a provider, refer to [requirements](#requirements).

#### Host Name / Port

Enter the name or IP of the DB2 server and the port number.

#### Username / Password

IBM Db2 authentication user name and password.

#### Database name

Enter the name of the IBM database.

#### Default schema

Enter the schema of the DB2 database.

#### Test Connection

Check the database connection.

## Assign the IBM Db2 Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your IBM Db2 destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Preparation

Defines the action on the target database before the data is inserted into the target table.

| Option | Description | | --- | --- | | *Drop & Create* | Remove table if available and create new table (default). | | *Truncate Or Create* | Empty table if available, otherwise create. | | *Create If Not Exists* | Create table if not available. | | *Prepare Merge* | Prepares the merge process and creates e.g. a temporary staging table, see [Merge Data](#merge-data). | | *None* | No action. | | *Custom SQL* | Here you can define your own script, see [Custom SQL Statements](#custom-sql-statements). |

To only create the table in the first step and not insert any data, you have two options:

1. Copy the SQL statement and execute it directly on the target data database.
1. Select the *None* option for **Row Processing** and execute the extraction.

Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.

### Row Processing

Defines how the data is inserted into the target table.

| Option | Description | | --- | --- | | *Insert* | Insert records (default). | | *Fill merge staging table* | Insert records into the staging table. | | *None* | No action. | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). | | *Merge (deprecated)* | This option is obsolete, see [Merge Data](#merge-data). Use the *Fill merge staging table* option. |

### Finalization

Defines the action on the target database after the data has been successfully inserted into the target table.

| Option | Description | | --- | --- | | *Finalize Merge* | Closes the merge process and deletes the temporary staging table, for example. | | *None* | No action (default). | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Transaction style

Note

The available options for Transaction Style vary depending on the destination.

#### One Transaction

*Preparation*, *Row Processing* and *Finalization* are all performed in a single transaction.

- Advantage: clean rollback of all changes.
- Disadvantage: possibly extensive locking during the entire extraction period.

Recommendation

Only use *One Transaction* in combination with DML commands, e.g., "truncate table" and "insert. Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command. Example: If a table is created in the preparation step, the opened "OneTransaction" is committed and a rollback in the next steps is not performed correctly.

#### Three Transactions

*Preparation*, *Row Processing* and *Finalization* are each executed in a separate transaction.

- Advantage: clean rollback of the individual sections, possibly shorter locking phases than with *One Transaction* (e.g. with DDL in *Preparation*, the entire DB is only locked during preparation and not for the entire extraction duration).
- Disadvantage: no rollback of previous step possible (error in *Row Processing* only rolls back changes from *Row Processing*, but not *Preparation*).

#### RowProcessingOnly

Only *Row Processing* is executed in a transaction. *Preparation* and *Finalization* without an explicit transaction (implicit commits).

- Advantage: DDL in 'Preparation *and* Finalization\* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)
- Disadvantage: no rollback of *Preparation/Finalization*.

#### No Transaction

No explicit transactions.

- Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.
- Disadvantage: no rollback

## Merge Data

The following example depicts the update of the existing data records in a database by running an extraction to merge data. In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP.

With a merge command, the updated value is written to the destination database table. The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.

Tip

Alternatively to merging, the data can be also updated by means of full load. The full load method is less efficient.

### Prerequisites

You need a table with existing SAP data, in which new data can be merged.\
Ideally, the table with existing data is created in the initial load with the corresponding **Preparation** option and filled with data with the **Row Processing** option *Insert*.

After the table is created, open SAP and change a field value in the SAP table that is used for the data merge.

Warning

**Faulty merge.**\
Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the [General Settings](../../table/general-settings/#primary-key-tab) of the extraction type to execute the merge command.

### Merge Command

The merge process is performed using a staging table and takes place in three steps:

1. A temporary table is created.
1. The data is inserted in the temporary table.
1. The temporary table is merged with the target table and then the temporary table is deleted.

Follow the steps below to set up the merge process in Xtract Universal:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions.
1. Click **[Destination]**. The window "Destination Settings" opens.
1. Make sure to assign IBM Db2 destination to the extraction.
1. Apply the following destination settings:
1. Click **[OK]** and [run the extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

More information about the updated fields can be found in the SQL statement.\
It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update.\
Fields that do not appear in the SQL statement are not affected by changes.

## Custom SQL Statements

The IBM Db2 destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the IBM Db2 destination:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..

1. Click **[Destination]**, the window "Destination Settings" opens.

1. Make sure to assign the IBM Db2 destination to the extraction.

1. Select the option *Custom SQL* from the drop-down list in one of the following sections:

   - [Preparation](#preparation)
   - [Row Processing](#row-processing)
   - [Finalization](#finalization)

1. Click **[Edit SQL]** . The window "Edit SQL" opens.

1. Enter your custom SQL statement and click **[OK]** to confirm your input.

### Use Templates

Existing SQL commands can be used as templates.\
You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:

- [Preparation](#preparation), e.g., in *Drop & Create* or *Create if Not Exists*
- [Row Processing](#row-processing), e.g., in *Insert* or *Merge*
- [Finalization](#finalization)

Follow the steps below to generate a Custom SQL command from a template:

1. In one of the staging steps, select the *Custom SQL* option from the drop-down list .
1. Click **[Edit SQL]** . The dialogue "Edit SQL" opens.
1. Navigate to the drop-down menu and select an existing command .
1. Click **[Generate Statement]**. A new statement is generated.
1. Click **[Copy]** to copy the statement to the clipboard.
1. Click **[OK]** to confirm your input.

Check out the [Microsoft SQL Server example](../microsoft-sql-server/#custom-sql-statements) for details on predefined expressions.

Note

The custom SQL code is used for SQL Server destinations. A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.

### Use Script Expressions

You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported:

| Input | Description | | --- | --- | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.TableName }#` | Name of the database table extracted data is written to. | | `#{Extraction.RowsCount }#` | Count of the extracted rows. | | `#{Extraction.RunState}#` | Status of the extraction (Running, FinishedNoErrors, FinishedErrors, Cancelled). | | `#{(int)Extraction.RunState}#` | Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors, 6 = Cancelled). | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. |

For more information, see [Script Expressions](../../parameters/script-expressions/).

```sql
#{
   iif
   (
      ExistsTable("MAKT"),
      "TRUNCATE TABLE \"MAKT\";",
      "
         CREATE TABLE \"MAKT\"(
            \"MATNR\" VARCHAR(18),
            \"SPRAS\" VARCHAR(2),
            \"MAKTX\" VARCHAR(40));
      "
   )
}#

```

### Create a Status Overview

The table "ExtractionStatistics" provides an overview and status of the executed Xtract Universal extractions. To create the "ExtractionStatistics" table, create an SQL table according to the following example:

```sql
CREATE TABLE [dbo].[ExtractionStatistics](
    [TableName] [nchar](50) NULL,
    [RowsCount] [int] NULL,
    [Timestamp] [nchar](50) NULL,
    [RunState] [nchar](50) NULL
) ON [PRIMARY]
GO

```

The *ExtractionStatistics* table is filled in the **Finalization** process step, using the following SQL statement:

```sql
INSERT INTO [ExtractionStatistics]
(
     [TableName], 
     [RowsCount], 
     [Timestamp],
     [RunState]
)
VALUES
(
     '#{Extraction.TableName}#', 
     '#{Extraction.RowsCount}#', 
     '#{Extraction.Timestamp}#',
     '#{Extraction.RunState}#'
);

```

This page shows how to set up and use the Flat File JSON destination. The Flat File JSON destination is a JSON flat file that can be written to a local directory or a network drive.

## Create a new Flat File JSON Destination

Follow the steps below to add a new Flat File JSON destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Flat File JSON* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

#### File output path

Enter the folder path to save the destination flat files in. If the entered folder does not exist, a new folder is created.

Note

To write flat files to a network drive, you need to:

- Enter the **File output path** in [UNC format](https://docs.microsoft.com/en-us/dotnet/standard/io/file-path-formats#unc-paths) e.g., `\\Server2\Share\Folder1`.
- Grant write permission to the directory to the [Xtract Universal service](../../server/service-account/) or run the service under a user account that has write permission.

## Assign the Flat File JSON Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Flat File JSON destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Existing files

If a flat file by the same name already exists in the target directory, the following actions can be taken:

| Option | Description | | --- | --- | | **Replace file** | The export process overwrites existing files. | | **Abort extraction** | The process is aborted, if the file already exists. |

### File Splitting

Writes extraction data of a single extraction to multiple files. Each filename is appended by *\_part[nnn]*.

#### Max. file size

The value set in *Max. file size* determines the maximum size of each file.

Note

The option *Max. file size* does not apply to gzip files. The size of a gzipped file cannot be determined in advance.

This page shows how to set up and use the HTTP JSON destination.\
The HTTP JSON destination is a generic JSON stream over HTTP.

## Create a new HTTP JSON Destination

Follow the steps below to add a new HTTP JSON destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *HTTP JSON* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination. To use the JSON destination, no further settings are necessary.

## Assign the HTTP JSON Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your HTTP JSON destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

This page shows how to set up and use the KNIME destination. The KNIME destination loads data to [KNIME](https://www.knime.com/).

## Create a new KNIME Destination

Follow the steps below to add a new KNIME destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *KNIME* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination. To use the KNIME destination, no further settings are necessary.

## Assign the KNIME Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your KNIME destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

## KNIME Integration via SAP Reader

### Requirements

The following software applications must be installed for integration into KNIME:

- Latest version of Xtract Universal, see [Download Trial Version](https://theobald-software.com/en/download-trial)
- Latest version of the [KNIME Analytics Platform](https://www.knime.com/downloads)

### Step by Step Guide

1. Create an extraction in Xtract Universal. Select KNIME as the destination for the extraction .
1. Start the *KNIME Analytics Platform*.
1. Install the extension [SAP Reader (Theobald Software)](https://hub.knime.com/knime/extensions/org.knime.features.sap.theobald/latest/org.knime.sap.theobald.node.v2.SAPTheobaldReader2NodeFactory).
1. Drag & Drop the Node / Source 'SAP Reader (Theobald Software)' onto the KNIME Canvas.
1. Open the SAP Reader Task 'Settings' and enter the URL address of the Xtract Universal Server, e.g. `http://localhost:8065/` .
1. Click **[Fetch Queries]** and select an extraction.
1. Confirm your input with **[OK]** .
1. Start the extraction via **[Execute]**.
1. Check the extracted SAP data via **[SAP Query Result]**.

______________________________________________________________________

## Related Links

- [KNIME SAP Reader (Theobald Software)](https://hub.knime.com/knime/extensions/org.knime.features.sap.theobald/latest/org.knime.sap.theobald.node.v2.SAPTheobaldReader2NodeFactory)
- [Youtube-Video: Webinar "SAP Data to Insights with KNIME"](https://www.youtube.com/watch?v=KQLLoDUoOEg)
- [Knowledge Base Article: Dynamic Runtime Parameter within KNIME Workflow](../../../knowledge-base/dynamic-runtime-paramater-within-KNIME-workflow/)

This page shows how to set up and use the Microsoft Fabric Mirroring destination. This destination enables users to continously map SAP data to a datalake using Microsoft Fabric's open mirroring feature.

## About Open Mirroring

Microsoft Fabric's Open Mirroring allows users to replicate existing data directly into Fabric's OneLake from a variety of external databases and other data sources. The data is written into an open mirroring landing zone, where it is processed by the mirroring replication engine of Microsoft Fabric. Mirroring delivers the data incrementally in a parquet format that is used to merge the data in OneLake.

For more information, see [Microsoft Documentation: Open mirroring in Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/database/mirrored-database/open-mirroring).

Note

Note that Microsoft Fabric Open Mirroring is still in preview. During preview, not all Fabric regions support Mirroring. For more information, see [Microsoft Documentation: Microsoft Fabric preview information](https://learn.microsoft.com/en-us/fabric/fundamentals/preview).

## Demonstration Video

The following video demonstrates how to set up the Microsoft Fabric Mirroring destination to synchronize SAP data with Microsoft Fabric.

## Workflow

Follow the workflow below to set up Open Mirroring with Xtract Universal:

1. Create a mirrored database in Microsoft Fabric.
1. [Create a Microsoft Fabric Mirroring destination](#create-a-new-microsoft-fabric-mirroring-destination) in Xtract Universal.
1. Create a [TableCDC extraction](../../table-cdc/) and assign the Microsoft Fabric Mirroring destination.
1. [Run the extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer) to initialize the delta mechanism and optionally load the fist data to Microsoft Fabric.
1. [Schedule the extraction](../../execute-and-automate/call-via-scheduler/) to run in an interval of your choice.

## Requirements

- An active and running Fabric capacity is required. A paused or deleted capacity affects Mirroring so that no data is replicated.

- The Landing Zone URL of a mirrored database in Microsoft OneLake is required, see [Microsoft Tutorial: Configure Microsoft Fabric open mirrored databases](https://learn.microsoft.com/en-us/fabric/database/mirrored-database/open-mirroring-tutorial).

- The Microsoft Fabric Mirroring destination is currently only supported by the [Table CDC](../../table-cdc/) extraction type. Table CDC requires the installation of the corresponding [custom function modules](../../setup-in-sap/custom-function-module-for-tablecdc/) in SAP. Also note that Table CDC is licensed and purchased separately from other extraction types.

- The Microsoft Fabric Mirroring destination uses [Microsoft Entra ID (formerly Azure Active Directory)](https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id) with OAuth 2.0 for authentication. Follow the steps below to set up the authentication with Microsoft:

  1. Register Microsoft Fabric OneLake as an app using the [Azure portal > App registrations](https://portal.azure.com/#view/Microsoft_AAD_RegisteredApps/ApplicationsListBlade) service. For more information, see [Microsoft Documentation: Register an application in Microsoft Entra ID](https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-register-app?tabs=certificate#configure-platform-settings).

  1. Make sure the registered app allows authentication from *Mobile and desktop applications*:

  1. Add the following API permissions to the registered app:

     - **Azure Storage > user impersonation**\
       This allows Xtract Universal to act on behalf of the signed-in user and access Azure Storage resources like OneLake.
     - **Microsoft Graph > User.Read**\
       This allows Xtract Universal to read the profile of the signed-in user.

     For more information, see [Microsoft Documentation: Add permissions to your web API](https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-configure-app-access-web-apis#add-permissions-to-access-your-web-api).

  1. When connecting Xtract Universal to Microsoft Fabric, make sure to provide the credentials of a Microsoft user that has one of the following user roles:

     - Storage Blob Data Contributor
     - Storage Blob Data Owner

     For more information, see [Microsoft Documentation: Assign an Azure role](https://learn.microsoft.com/en-us/azure/storage/blobs/assign-azure-role-data-access?tabs=portal#assign-an-azure-role).

Tip

You can also use the regular [Table](../../table/) extraction type to extract the initial full load of the Table.

## Create a new Microsoft Fabric Mirroring Destination

Follow the steps below to add a new Microsoft Fabric Mirroring destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Microsoft Fabric Mirroring* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

### Authentication

The Microsoft Fabric Mirroring destination uses [Microsoft Entra ID](https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id) for authentication. Register Microsoft OneLake as an application in Entra ID and configure OneLake to use the registered application.

#### Tenant ID

Enter the Directory (tenant) ID of the registered app.

#### Client ID

Enter the Application (client) ID of the registered app.

Tip

You can copy the tenant ID and client ID in the [Microsoft Entra admin center](https://entra.microsoft.com/#home) or the [Azure portal](https://portal.azure.com/), where the app is registered.

#### Authenticate using Entra ID

Follow the steps below to authenticate Xtract Universal against Microsoft:

1. Click **[Authenticate using Entra ID]**. The window "Entra ID" opens.
1. When prompted, sign in with your Microsoft credentials. Make sure that the Microsoft user does not use Multifactor Authentication (MFA) as extractions fail when the MFA of the user expires.
1. If the connection is successful, a "Connection successful" info window opens.

Warning

**The window "Entra ID" shows a blank screen.**\
If the window "Entra ID" shows a blank screen, the content is likely blocked by the Internet Explorer ESC (Enhanced Security Configuration) on Windows servers.\
To disable the Internet Explorer ESC, refer to the instructions in the [Microsoft Documentation: How to turn off Internet Explorer ESC on Windows servers](https://learn.microsoft.com/en-us/previous-versions/troubleshoot/browsers/security-privacy/enhanced-security-configuration-faq#how-to-turn-off-internet-explorer-esc-on-windows-servers).

### Landing Zone

#### URL

Enter the URL of the Microsoft Fabric Mirroring Landing Zone. The URL uses the following format:\
`https://onelake.dfs.fabric.microsoft.com/<workspace id>/<mirrored database id>/Files/LandingZone/`

You can copy the URL from the [Microsoft Fabric portal](https://app.fabric.microsoft.com/home). The Landing Zone URL is displayed in the details section on the Home screen of the mirrored database.

Example: `https://onelake.dfs.fabric.microsoft.com/12345678-aaaa-bbbb-cccc-123456789abc/12345678-dddd-ffff-gggg-123456789abc/Files/LandingZone`

## Connection Retry and Rollback

Connection retry is a built-in function of the Microsoft Fabric Mirroring destination. The retry function is activated by default.

Connection retry is a functionality that prevents extractions from failing in case of transient connection interruptions to Microsoft Fabric Mirroring. Xtract Universal follows an exponential retry strategy. The selected exponential strategy results in seven retry attempts and an overall timespan of 140 seconds. If a connection is not established during the timespan of 140 seconds, the extraction fails.

The retry function is applied after receiving one of the following HTTP errors or exceptions:

- 503 Service Unavailable
- 504 Gateway Timeout
- WebExceptionStatus.ConnectionClosed
- WebExceptionStatus.ConnectFailure
- WebExceptionStatus.Timeout
- WebExceptionStatus.RequestCanceled
- WebExceptionStatus.SendFailure
- WebExceptionStatus.NameResolutionFailure

## Assign the Microsoft Fabric Mirroring Destination to an Extraction

Extractions write data to their assigned destination. The Microsoft Fabric Mirroring destination only supports [Table CDC](../../table-cdc/) extractions. Follow the steps below to assign a destination to a Table CDC extraction:

1. In the main window of the Designer, select a Table CDC extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Microsoft Fabric Mirroring destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

Note

If an object's name does not begin with a letter, the prefix 'x' is automatically appended, e.g., `_namespace_tabname.csv` is renamed to `x_namespace_tabname.csv` upon upload.\
This ensures compatibility with Azure Data Factory, Hadoop, and Spark, which require names to start with a letter or handle certain symbols differently.

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

## Related Links

- [Microsoft Documentation: What is Mirroring in Fabric?](https://learn.microsoft.com/en-us/fabric/database/mirrored-database/overview)
- [Microsoft Documentation: Open mirroring in Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/database/mirrored-database/open-mirroring)
- [Microsoft Documentation: Tutorial Configure Microsoft Fabric open mirrored databases](https://learn.microsoft.com/en-us/fabric/database/mirrored-database/open-mirroring-tutorial)

This page shows how to set up and use the Microsoft Fabric (OneLake) destination. The Microsoft Fabric (OneLake) destination enables users to load SAP data to a Microsoft Fabric Lakehouse.

## Demonstration Video

The following video demonstrates how to set up the Microsoft Fabric (OneLake) destination and how to load SAP data into a Microsoft Fabric Lakehouse.

## Requirements

The Microsoft Fabric (OneLake) destination uses [Microsoft Entra ID (formerly Azure Active Directory)](https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id) with OAuth 2.0 for authentication. Follow the steps below to set up the authentication with Microsoft:

1. Register Microsoft Fabric OneLake as an app using the [Azure portal > App registrations](https://portal.azure.com/#view/Microsoft_AAD_RegisteredApps/ApplicationsListBlade) service. For more information, see [Microsoft Documentation: Register an application in Microsoft Entra ID](https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-register-app?tabs=certificate#configure-platform-settings).

1. Make sure the registered app allows authentication from *Mobile and desktop applications*:

1. Add the following API permissions to the registered app:

   - **Azure Storage > user impersonation**\
     This allows Xtract Universal to act on behalf of the signed-in user and access Azure Storage resources like OneLake.
   - **Microsoft Graph > User.Read**\
     This allows Xtract Universal to read the profile of the signed-in user.

   For more information, see [Microsoft Documentation: Add permissions to your web API](https://learn.microsoft.com/en-us/entra/identity-platform/quickstart-configure-app-access-web-apis#add-permissions-to-access-your-web-api).

1. When connecting Xtract Universal to Microsoft Fabric, make sure to provide the credentials of a Microsoft user that has one of the following user roles:

   - Storage Blob Data Contributor
   - Storage Blob Data Owner

   For more information, see [Microsoft Documentation: Assign an Azure role](https://learn.microsoft.com/en-us/azure/storage/blobs/assign-azure-role-data-access?tabs=portal#assign-an-azure-role).

## Create a new Microsoft Fabric (OneLake) Destination

Follow the steps below to add a new Microsoft Fabric (OneLake) destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Microsoft Fabric (OneLake)* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

### Authentication

The Microsoft Fabric (OneLake) destination uses [Microsoft Entra ID](https://www.microsoft.com/en-us/security/business/identity-access/microsoft-entra-id) for authentication. Register Microsoft OneLake as an application in Entra ID and configure OneLake to use the registered application.

#### Tenant ID

Enter the Directory (tenant) ID of the registered app.

#### Client ID

Enter the Application (client) ID of the registered app.

Tip

You can copy the tenant ID and client ID in the [Microsoft Entra admin center](https://entra.microsoft.com/#home) or the [Azure portal](https://portal.azure.com/), where the app is registered.

#### Authenticate using Entra ID

Follow the steps below to authenticate Xtract Universal against Microsoft:

1. Click **[Authenticate using Entra ID]**. The window "Entra ID" opens.
1. When prompted, sign in with your Microsoft credentials. Make sure that the Microsoft user does not use Multifactor Authentication (MFA) as extractions fail when the MFA of the user expires.
1. If the connection is successful, a "Connection successful" info window opens.

Warning

**The window "Entra ID" shows a blank screen.**\
If the window "Entra ID" shows a blank screen, the content is likely blocked by the Internet Explorer ESC (Enhanced Security Configuration) on Windows servers.\
To disable the Internet Explorer ESC, refer to the instructions in the [Microsoft Documentation: How to turn off Internet Explorer ESC on Windows servers](https://learn.microsoft.com/en-us/previous-versions/troubleshoot/browsers/security-privacy/enhanced-security-configuration-faq#how-to-turn-off-internet-explorer-esc-on-windows-servers).

### Files Folder

#### URL

Enter the URL of the Lakehouse, including the folder path in which the data is written. Example:\
`https://onelake.dfs.fabric.microsoft.com/my-workspace/my-lakehouse.Lakehouse/my-folder`

You can copy the URL in the [Microsoft Fabric portal](https://app.fabric.microsoft.com/home) using the properties of a OneLake folder:

### File Format

Select the required file format. You can choose between *CSV*, *JSON* and *Parquet*.

#### CVS Settings

The settings for file type *CSV* correspond to the settings of the *Flat File CSV* destination:

- [CSV Settings](../csv-flat-file/#csv-settings)
- [Convert / Encoding](../csv-flat-file/#convert-encoding)

#### JSON Settings

To use the JSON file format, no further settings are necessary.

#### Parquet Settings

The settings for file type *Parquet* correspond to the settings of the *Flat File Parquet* destination - [Compatibility Mode](../parquet/#compatibility-mode).

## Connection Retry and Rollback

Connection retry is a built-in function of the Microsoft Fabric (OneLake) destination. The retry function is activated by default.

Connection retry is a functionality that prevents extractions from failing in case of transient connection interruptions to Microsoft Fabric (OneLake). Xtract Universal follows an exponential retry strategy. The selected exponential strategy results in seven retry attempts and an overall timespan of 140 seconds. If a connection is not established during the timespan of 140 seconds, the extraction fails.

The retry function is applied after receiving one of the following HTTP errors or exceptions:

- 503 Service Unavailable
- 504 Gateway Timeout
- WebExceptionStatus.ConnectionClosed
- WebExceptionStatus.ConnectFailure
- WebExceptionStatus.Timeout
- WebExceptionStatus.RequestCanceled
- WebExceptionStatus.SendFailure
- WebExceptionStatus.NameResolutionFailure

Rollback applies to situations where extractions fail, not because of connection issues, but due to an error when connecting to SAP. In these cases, Xtract Universal attempts to delete any files created in the Lakehouse during the extraction process.

## Assign the Microsoft Fabric (OneLake) Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Microsoft Fabric (OneLake) destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

Note

If an object's name does not begin with a letter, the prefix 'x' is automatically appended, e.g., `_namespace_tabname.csv` is renamed to `x_namespace_tabname.csv` upon upload.\
This ensures compatibility with Azure Data Factory, Hadoop, and Spark, which require names to start with a letter or handle certain symbols differently.

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Empty Files

When this option is active, empty result sets create an empty file in the target environment. Deactivate this option to not create empty files.

______________________________________________________________________

## Related Links

- [Mirosoft Tutorial: Create a Fabric workspace](https://learn.microsoft.com/en-us/fabric/data-engineering/tutorial-lakehouse-get-started)
- [Mirosoft Tutorial: Create a lakehouse, ingest sample data, and build a report](https://learn.microsoft.com/en-us/fabric/data-engineering/tutorial-build-lakehouse)

This page shows how to set up and use the Microsoft SQL Server destination. The Microsoft SQL Server destination loads data to a Microsoft SQL Server Database or a Microsoft Azure SQL Database destination.

## Requirements

No driver installation is required since the ADO .NET driver for SQL Server is delivered and installed as a part of the .NET framework.

## Create a new Microsoft SQL Server Destination

Follow the steps below to add a new Microsoft SQL Server destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Microsoft SQL Server* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

#### Server Name

Specifies the host address of the SQL Server. Please note the following syntax:

| Syntax | Example | | --- | --- | | [ServerName] | `dbtest` | | [ServerName],[Port] | `dbtest,1433` | | [ServerName].[Domain],[Port] | `dbtest.theobald.software,1433` |

It is only necessary to specify the port if it has been edited outside the SQL standard.

#### Require TLS encryption

Client-side enforcement for using [TLS encrpytion](https://docs.microsoft.com/en-us/azure/sql-database/sql-database-connect-query#tls-considerations-for-sql-database-connectivity). Adds the following parameters to the connection string:

- Encrypt = On
- TrustServerCertificate = Off

For more information, see [Microsoft Documentation: Enable Encrypted Connections to the Database Engine](https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/enable-encrypted-connections-to-the-database-engine?view=sql-server-2017)

#### Windows Authentication

Uses the service account that runs the Xtract Universal service for authentication against the SQL Server.

Note

To successfully connect to the database using Windows authentication, make sure to [run the XU service under a Windows AD user](../../server/service-account/) with access to the database.

#### Impersonate authenticated caller

Uses the Windows AD user that triggers the extraction for authentication against the SQL Server using [Kerberos authentication.](https://blogs.msdn.microsoft.com/sqlupdates/2014/12/05/sql-server-kerberos-and-spn-quick-reference/) To use this functionality a similar configuration as for [Kerberos Single Sign On against SAP](../../../knowledge-base/sso-with-kerberos-snc/) is required.

#### User Name

The user name for the SQL Server authentication.

#### Password

The password for the SQL Server authentication

#### Database Name

Defines the name of the SQL Server database.

#### Test Connection

Checks the database connection.

## Assign the Microsoft SQL Server Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Microsoft SQL Server destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Preparation

Defines the action on the target database before the data is inserted into the target table.

| Option | Description | | --- | --- | | *Drop & Create* | Remove table if available and create new table (default). | | *Truncate Or Create* | Empty table if available, otherwise create. | | *Create If Not Exists* | Create table if not available. | | *Prepare Merge* | Prepares the merge process and creates e.g. a temporary staging table, see [Merge Data](#merge-data). | | *None* | No action. | | *Custom SQL* | Here you can define your own script, see [Custom SQL Statements](#custom-sql-statements). |

To only create the table in the first step and not insert any data, you have two options:

1. Copy the SQL statement and execute it directly on the target data database.
1. Select the *None* option for **Row Processing** and execute the extraction.

Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.

### Row Processing

Defines how the data is inserted into the target table.

| Option | Description | | --- | --- | | *Insert* | Insert records (default). | | *Fill merge staging table* | Insert records into the staging table. | | *None* | No action. | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). | | *Merge (deprecated)* | This option is obsolete, see [Merge Data](#merge-data). Use the *Fill merge staging table* option. |

### Finalization

Defines the action on the target database after the data has been successfully inserted into the target table.

| Option | Description | | --- | --- | | *Finalize Merge* | Closes the merge process and deletes the temporary staging table, for example. | | *None* | No action (default). | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Debugging

Warning

**Performance decrease!**\
The performance decreases when bulk insert is disabled. Disable the bulk insert only when necessary, e.g., upon request of the support team.

By activating the checkbox **Disable bulk operations**, the default bulk insert is deactivated when writing to a database.

This option enables detailed error analysis, if certain data rows cannot be persisted on the database. Possible causes for the incorrect behavior are incorrect values with regard to the stored data type.

Debugging needs to be deactivated again, after the successful error analysis, otherwise the performance of the database write processes remains low.

Note

Bulk operations are not supported when using [Custom SQL statements](#custom-sql-statements), e.g., in *Row Processing*. Bulk operations lead to performance decrease. To increase performance when using [Custom SQL statements](#custom-sql-statements), it is recommended to perform the custom processing in the *Finalization* step.

### Transaction style

Note

The available options for Transaction Style vary depending on the destination.

#### One Transaction

*Preparation*, *Row Processing* and *Finalization* are all performed in a single transaction.

- Advantage: clean rollback of all changes.
- Disadvantage: possibly extensive locking during the entire extraction period.

Recommendation

Only use *One Transaction* in combination with DML commands, e.g., "truncate table" and "insert. Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command. Example: If a table is created in the preparation step, the opened "OneTransaction" is committed and a rollback in the next steps is not performed correctly.

#### Three Transactions

*Preparation*, *Row Processing* and *Finalization* are each executed in a separate transaction.

- Advantage: clean rollback of the individual sections, possibly shorter locking phases than with *One Transaction* (e.g. with DDL in *Preparation*, the entire DB is only locked during preparation and not for the entire extraction duration).
- Disadvantage: no rollback of previous step possible (error in *Row Processing* only rolls back changes from *Row Processing*, but not *Preparation*).

#### RowProcessingOnly

Only *Row Processing* is executed in a transaction. *Preparation* and *Finalization* without an explicit transaction (implicit commits).

- Advantage: DDL in 'Preparation *and* Finalization\* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)
- Disadvantage: no rollback of *Preparation/Finalization*.

#### No Transaction

No explicit transactions.

- Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.
- Disadvantage: no rollback

## Merge Data

The following example depicts the update of the existing data records in a database by running an extraction to merge data. In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP.

With a merge command, the updated value is written to the destination database table. The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.

Tip

Alternatively to merging, the data can be also updated by means of full load. The full load method is less efficient.

### Prerequisites

You need a table with existing SAP data, in which new data can be merged.\
Ideally, the table with existing data is created in the initial load with the corresponding **Preparation** option and filled with data with the **Row Processing** option *Insert*.

After the table is created, open SAP and change a field value in the SAP table that is used for the data merge.

Warning

**Faulty merge.**\
Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the [General Settings](../../table/general-settings/#primary-key-tab) of the extraction type to execute the merge command.

### Merge Command

The merge process is performed using a staging table and takes place in three steps:

1. A temporary table is created.
1. The data is inserted in the temporary table.
1. The temporary table is merged with the target table and then the temporary table is deleted.

Follow the steps below to set up the merge process in Xtract Universal:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions.
1. Click **[Destination]**. The window "Destination Settings" opens.
1. Make sure to assign Microsoft SQL Server destination to the extraction.
1. Apply the following destination settings:
1. Click **[OK]** and [run the extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

More information about the updated fields can be found in the SQL statement.\
It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update.\
Fields that do not appear in the SQL statement are not affected by changes.

## Custom SQL Statements

The Microsoft SQL Server destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the Microsoft SQL Server destination:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..

1. Click **[Destination]**, the window "Destination Settings" opens.

1. Make sure to assign the Microsoft SQL Server destination to the extraction.

1. Select the option *Custom SQL* from the drop-down list in one of the following sections:

   - [Preparation](#preparation)
   - [Row Processing](#row-processing)
   - [Finalization](#finalization)

1. Click **[Edit SQL]** . The window "Edit SQL" opens.

1. Enter your custom SQL statement and click **[OK]** to confirm your input.

### Use Templates

Existing SQL commands can be used as templates.\
You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:

- [Preparation](#preparation), e.g., in *Drop & Create* or *Create if Not Exists*
- [Row Processing](#row-processing), e.g., in *Insert* or *Merge*
- [Finalization](#finalization)

Follow the steps below to generate a Custom SQL command from a template:

1. In one of the staging steps, select the *Custom SQL* option from the drop-down list .
1. Click **[Edit SQL]** . The dialogue "Edit SQL" opens.
1. Navigate to the drop-down menu and select an existing command .
1. Click **[Generate Statement]**. A new statement is generated.
1. Click **[Copy]** to copy the statement to the clipboard.
1. Click **[OK]** to confirm your input.

Check out the [Microsoft SQL Server example](./#custom-sql-statements) for details on predefined expressions.

Note

The custom SQL code is used for SQL Server destinations. A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.

### Use Script Expressions

You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported:

| Input | Description | | --- | --- | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.TableName }#` | Name of the database table extracted data is written to. | | `#{Extraction.RowsCount }#` | Count of the extracted rows. | | `#{Extraction.RunState}#` | Status of the extraction (Running, FinishedNoErrors, FinishedErrors, Cancelled). | | `#{(int)Extraction.RunState}#` | Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors, 6 = Cancelled). | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. |

For more information, see [Script Expressions](../../parameters/script-expressions/).

```sql
#{
   iif
   (
      ExistsTable("MAKT"),
      "TRUNCATE TABLE \"MAKT\";",
      "
         CREATE TABLE \"MAKT\"(
            \"MATNR\" VARCHAR(18),
            \"SPRAS\" VARCHAR(2),
            \"MAKTX\" VARCHAR(40));
      "
   )
}#

```

### Create a Status Overview

The table "ExtractionStatistics" provides an overview and status of the executed Xtract Universal extractions. To create the "ExtractionStatistics" table, create an SQL table according to the following example:

```sql
CREATE TABLE [dbo].[ExtractionStatistics](
    [TableName] [nchar](50) NULL,
    [RowsCount] [int] NULL,
    [Timestamp] [nchar](50) NULL,
    [RunState] [nchar](50) NULL
) ON [PRIMARY]
GO

```

The *ExtractionStatistics* table is filled in the **Finalization** process step, using the following SQL statement:

```sql
INSERT INTO [ExtractionStatistics]
(
     [TableName], 
     [RowsCount], 
     [Timestamp],
     [RunState]
)
VALUES
(
     '#{Extraction.TableName}#', 
     '#{Extraction.RowsCount}#', 
     '#{Extraction.Timestamp}#',
     '#{Extraction.RunState}#'
);

```

### Custom SQL Example

In the depicted example, the table *KNA1* is extended by a column with the current timestamp of type *DATETIME*. The new column is filled dynamically using a .NET-based function.

Note

The data types that can be used in the SQL statement depend on the SQL Server database version.

1. In the staging step **Preparation**, select the option *Custom SQL* from the drop-down list and click **[Edit SQL]**. The window "Edit SQL" opens.

1. In the drop-down menu, select the option *Drop & Create* and click **[Generate Statement]** to [use the template](#use-templates) for *Drop & Create*.

1. Add the following line in the generated statement:

   ```sql
   [Extraction_Date] NATIONAL CHARACTER VARYING(23)

   ```

1. Click **[OK]** to confirm your input.

1. In the staging step **Row Processing**, select the option *Insert*.\
   At this point, no data is written from the SAP source system, but `NULL` values are written to the newly created *Extraction_Date* column.

1. In the staging step **Finalization**, the `NULL` values can be filled by a custom SQL statement. Select the option *Custom SQL* from the drop-down list and click **[Edit SQL]**. The window "Edit SQL" opens.

1. Paste the following SQl statement into the editor:

   ```sql
   UPDATE [dbo].[KNA1] 
   SET [Extraction_Date] = '#{Extraction.Timestamp}#' 
   WHERE [Extraction_Date] IS NULL;

   ```

   The `NULL` values are filled with the [current date of the extraction](#use-script-expressions) and written to the SQL target table using the T-SQL command `UPDATE`.

1. Click **[OK]** to confirm your input and [run the extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

Check the existence of the extended column *Extraction_Date* in the SQL Server View of table *KNA1*.

______________________________________________________________________

## Related Links

- [Knowledge Base Article: Post-Processing Column Name Style](../../../knowledge-base/adjust-column-name-style/)
- [Knowledge Base Article: Collation Settings for MSSQL Server Destination](../../../knowledge-base/collation-sql-server/)
- [Integration via Azure Data Factory](../../execute-and-automate/call-via-etl/#integration-via-azure-data-factory)

This page shows how to set up and use the MySQL destination. The MySQL destination loads data to a MySQL database.

## Requirements

As of Xtract Universal version 4.2.26.0 the MySQL ADO.Net driver is provided with the setup of Xtract Universal. There are no additional installations needed to use MySQL database destination.

## Create a new MySQL Destination

Follow the steps below to add a new MySQL destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *MySQL* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

#### Server Name

Enter the name of the server the MySQL database is hosted on.

#### Server Port

Enter the port of the server the MySQL database is hosted on.

#### User Name

Enter the MySQL database user ID.

#### Password

Enter the password of the database user.

#### Database

Enter the name of the MySQL database.

#### Test Connection

Check the database connection.

## Assign the MySQL Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your MySQL destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Preparation

Defines the action on the target database before the data is inserted into the target table.

| Option | Description | | --- | --- | | *Drop & Create* | Remove table if available and create new table (default). | | *Truncate Or Create* | Empty table if available, otherwise create. | | *Create If Not Exists* | Create table if not available. | | *Prepare Merge* | Prepares the merge process and creates e.g. a temporary staging table, see [Merge Data](#merge-data). | | *None* | No action. | | *Custom SQL* | Here you can define your own script, see [Custom SQL Statements](#custom-sql-statements). |

To only create the table in the first step and not insert any data, you have two options:

1. Copy the SQL statement and execute it directly on the target data database.
1. Select the *None* option for **Row Processing** and execute the extraction.

Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.

### Row Processing

Defines how the data is inserted into the target table.

| Option | Description | | --- | --- | | *Insert* | Insert records (default). | | *Fill merge staging table* | Insert records into the staging table. | | *None* | No action. | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). | | *Merge (deprecated)* | This option is obsolete, see [Merge Data](#merge-data). Use the *Fill merge staging table* option. |

### Finalization

Defines the action on the target database after the data has been successfully inserted into the target table.

| Option | Description | | --- | --- | | *Finalize Merge* | Closes the merge process and deletes the temporary staging table, for example. | | *None* | No action (default). | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Transaction style

Note

The available options for Transaction Style vary depending on the destination.

#### One Transaction

*Preparation*, *Row Processing* and *Finalization* are all performed in a single transaction.

- Advantage: clean rollback of all changes.
- Disadvantage: possibly extensive locking during the entire extraction period.

Recommendation

Only use *One Transaction* in combination with DML commands, e.g., "truncate table" and "insert. Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command. Example: If a table is created in the preparation step, the opened "OneTransaction" is committed and a rollback in the next steps is not performed correctly.

#### Three Transactions

*Preparation*, *Row Processing* and *Finalization* are each executed in a separate transaction.

- Advantage: clean rollback of the individual sections, possibly shorter locking phases than with *One Transaction* (e.g. with DDL in *Preparation*, the entire DB is only locked during preparation and not for the entire extraction duration).
- Disadvantage: no rollback of previous step possible (error in *Row Processing* only rolls back changes from *Row Processing*, but not *Preparation*).

#### RowProcessingOnly

Only *Row Processing* is executed in a transaction. *Preparation* and *Finalization* without an explicit transaction (implicit commits).

- Advantage: DDL in 'Preparation *and* Finalization\* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)
- Disadvantage: no rollback of *Preparation/Finalization*.

#### No Transaction

No explicit transactions.

- Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.
- Disadvantage: no rollback

## Merge Data

The following example depicts the update of the existing data records in a database by running an extraction to merge data. In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP.

With a merge command, the updated value is written to the destination database table. The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.

Tip

Alternatively to merging, the data can be also updated by means of full load. The full load method is less efficient.

### Prerequisites

You need a table with existing SAP data, in which new data can be merged.\
Ideally, the table with existing data is created in the initial load with the corresponding **Preparation** option and filled with data with the **Row Processing** option *Insert*.

After the table is created, open SAP and change a field value in the SAP table that is used for the data merge.

Warning

**Faulty merge.**\
Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the [General Settings](../../table/general-settings/#primary-key-tab) of the extraction type to execute the merge command.

### Merge Command

The merge process is performed using a staging table and takes place in three steps:

1. A temporary table is created.
1. The data is inserted in the temporary table.
1. The temporary table is merged with the target table and then the temporary table is deleted.

Follow the steps below to set up the merge process in Xtract Universal:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions.
1. Click **[Destination]**. The window "Destination Settings" opens.
1. Make sure to assign MySQL destination to the extraction.
1. Apply the following destination settings:
1. Click **[OK]** and [run the extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

More information about the updated fields can be found in the SQL statement.\
It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update.\
Fields that do not appear in the SQL statement are not affected by changes.

## Custom SQL Statements

The MySQL destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the MySQL destination:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..

1. Click **[Destination]**, the window "Destination Settings" opens.

1. Make sure to assign the MySQL destination to the extraction.

1. Select the option *Custom SQL* from the drop-down list in one of the following sections:

   - [Preparation](#preparation)
   - [Row Processing](#row-processing)
   - [Finalization](#finalization)

1. Click **[Edit SQL]** . The window "Edit SQL" opens.

1. Enter your custom SQL statement and click **[OK]** to confirm your input.

### Use Templates

Existing SQL commands can be used as templates.\
You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:

- [Preparation](#preparation), e.g., in *Drop & Create* or *Create if Not Exists*
- [Row Processing](#row-processing), e.g., in *Insert* or *Merge*
- [Finalization](#finalization)

Follow the steps below to generate a Custom SQL command from a template:

1. In one of the staging steps, select the *Custom SQL* option from the drop-down list .
1. Click **[Edit SQL]** . The dialogue "Edit SQL" opens.
1. Navigate to the drop-down menu and select an existing command .
1. Click **[Generate Statement]**. A new statement is generated.
1. Click **[Copy]** to copy the statement to the clipboard.
1. Click **[OK]** to confirm your input.

Check out the [Microsoft SQL Server example](../microsoft-sql-server/#custom-sql-statements) for details on predefined expressions.

Note

The custom SQL code is used for SQL Server destinations. A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.

### Use Script Expressions

You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported:

| Input | Description | | --- | --- | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.TableName }#` | Name of the database table extracted data is written to. | | `#{Extraction.RowsCount }#` | Count of the extracted rows. | | `#{Extraction.RunState}#` | Status of the extraction (Running, FinishedNoErrors, FinishedErrors, Cancelled). | | `#{(int)Extraction.RunState}#` | Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors, 6 = Cancelled). | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. |

For more information, see [Script Expressions](../../parameters/script-expressions/).

```sql
#{
   iif
   (
      ExistsTable("MAKT"),
      "TRUNCATE TABLE \"MAKT\";",
      "
         CREATE TABLE \"MAKT\"(
            \"MATNR\" VARCHAR(18),
            \"SPRAS\" VARCHAR(2),
            \"MAKTX\" VARCHAR(40));
      "
   )
}#

```

### Create a Status Overview

The table "ExtractionStatistics" provides an overview and status of the executed Xtract Universal extractions. To create the "ExtractionStatistics" table, create an SQL table according to the following example:

```sql
CREATE TABLE [dbo].[ExtractionStatistics](
    [TableName] [nchar](50) NULL,
    [RowsCount] [int] NULL,
    [Timestamp] [nchar](50) NULL,
    [RunState] [nchar](50) NULL
) ON [PRIMARY]
GO

```

The *ExtractionStatistics* table is filled in the **Finalization** process step, using the following SQL statement:

```sql
INSERT INTO [ExtractionStatistics]
(
     [TableName], 
     [RowsCount], 
     [Timestamp],
     [RunState]
)
VALUES
(
     '#{Extraction.TableName}#', 
     '#{Extraction.RowsCount}#', 
     '#{Extraction.Timestamp}#',
     '#{Extraction.RunState}#'
);

```

This page shows how to set up and use the Oracle destination. The Oracle destination loads data to an Oracle database.

## Requirements

As of Xtract Universal version 4.2.34.0 the Oracle data provider is included in to the setup of Xtract Universal. There are no additional installations needed to use Oracle database destination.

## Create a new Oracle Destination

Follow the steps below to add a new Oracle destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Oracle* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination. The Oracle destination supports 3 types of connections:

| Connection Type | Description | | --- | --- | | [**Default**](#default) | Connect to an Oracle database (on-premises). | | [**Wallet**](#wallet) | Connect to an Oracle Database using a connection from a wallet. Use this option when [mTLS (Mutual TLS)](https://docs.oracle.com/en/cloud/paas/autonomous-database/adbsa/connect-introduction.html#GUID-9A472E49-3B2B-4D9F-9DC2-D3E6E4454285) authentication is required. | | [**Connect Descriptor**](#connect-descriptor) | Connect to an Oracle database (on-premises and cloud) via TLS. |

The input fields vary depending on the selected authentication method.

### Default

#### Host

Enter the name of the Oracle server.

#### Port

Enter the Oracle server connection port (Default: 1521).

#### SID / Service name

Enter the unique name (SID) or the alias (service name) of the Oracle database.

#### Username

Enter the user name.

#### Password

Enter the password.

#### Test Connection

Check the database connection.

### Wallet

#### TNS Name

Enter the TNS name of the connection as it is stored in the `tnsnames.ora` file in your wallet.\
For more information, see [Oracle Documentation: Download Client Credentials (Wallets)](https://docs.oracle.com/en-us/iaas/autonomous-database/doc/download-client-credentials.html).

#### Username

Enter the user name.

#### Password

Enter the password.

#### Wallet location

Enter the path to your wallet, e.g., `C:\Oracle\Wallet`.\
Note that the wallet location must be accessible for the user that runs the Xtract Universal service.

#### Test Connection

Check the database connection.

### Connect Descriptor

#### Connect Descriptor

Enter a connect descriptor (connection string), see [Oracle Documentation: View TNS Names and Connection Strings for an Autonomous Database Instance](https://docs.oracle.com/en/cloud/paas/autonomous-database/serverless/adbsb/connect-download-wallet.html#GUID-BE884A1B-034D-4CD6-9B71-83A4CCFDE9FB). A Connect Descriptor uses the following format:

```text
(DESCRIPTION =
(ADDRESS = (PROTOCOL = TCP)
(HOST = [oracle host name])(PORT = [port number]))
(CONNECT_DATA =
(SERVER = DEDICATED)
(SERVICE_NAME = [oracle service name])))

```

#### Username

Enter the user name.

#### Password

Enter the password.

#### Test Connection

Check the database connection.

## Assign the Oracle Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Oracle destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Preparation

Defines the action on the target database before the data is inserted into the target table.

| Option | Description | | --- | --- | | *Drop & Create* | Remove table if available and create new table (default). | | *Truncate Or Create* | Empty table if available, otherwise create. | | *Create If Not Exists* | Create table if not available. | | *Prepare Merge* | Prepares the merge process and creates e.g. a temporary staging table, see [Merge Data](#merge-data). | | *None* | No action. | | *Custom SQL* | Here you can define your own script, see [Custom SQL Statements](#custom-sql-statements). |

To only create the table in the first step and not insert any data, you have two options:

1. Copy the SQL statement and execute it directly on the target data database.
1. Select the *None* option for **Row Processing** and execute the extraction.

Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.

### Row Processing

Defines how the data is inserted into the target table.

| Option | Description | | --- | --- | | *Insert* | Insert records (default). | | *Fill merge staging table* | Insert records into the staging table. | | *None* | No action. | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). | | *Merge (deprecated)* | This option is obsolete, see [Merge Data](#merge-data). Use the *Fill merge staging table* option. |

### Finalization

Defines the action on the target database after the data has been successfully inserted into the target table.

| Option | Description | | --- | --- | | *Finalize Merge* | Closes the merge process and deletes the temporary staging table, for example. | | *None* | No action (default). | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Debugging

Warning

**Performance decrease!**\
The performance decreases when bulk insert is disabled. Disable the bulk insert only when necessary, e.g., upon request of the support team.

By activating the checkbox **Disable bulk operations**, the default bulk insert is deactivated when writing to a database.

This option enables detailed error analysis, if certain data rows cannot be persisted on the database. Possible causes for the incorrect behavior are incorrect values with regard to the stored data type.

Debugging needs to be deactivated again, after the successful error analysis, otherwise the performance of the database write processes remains low.

Note

Bulk operations are not supported when using [Custom SQL statements](#custom-sql-statements), e.g., in *Row Processing*. Bulk operations lead to performance decrease. To increase performance when using [Custom SQL statements](#custom-sql-statements), it is recommended to perform the custom processing in the *Finalization* step.

### Transaction style

Note

The available options for Transaction Style vary depending on the destination.

#### One Transaction

*Preparation*, *Row Processing* and *Finalization* are all performed in a single transaction.

- Advantage: clean rollback of all changes.
- Disadvantage: possibly extensive locking during the entire extraction period.

Recommendation

Only use *One Transaction* in combination with DML commands, e.g., "truncate table" and "insert. Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command. Example: If a table is created in the preparation step, the opened "OneTransaction" is committed and a rollback in the next steps is not performed correctly.

#### Three Transactions

*Preparation*, *Row Processing* and *Finalization* are each executed in a separate transaction.

- Advantage: clean rollback of the individual sections, possibly shorter locking phases than with *One Transaction* (e.g. with DDL in *Preparation*, the entire DB is only locked during preparation and not for the entire extraction duration).
- Disadvantage: no rollback of previous step possible (error in *Row Processing* only rolls back changes from *Row Processing*, but not *Preparation*).

#### RowProcessingOnly

Only *Row Processing* is executed in a transaction. *Preparation* and *Finalization* without an explicit transaction (implicit commits).

- Advantage: DDL in 'Preparation *and* Finalization\* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)
- Disadvantage: no rollback of *Preparation/Finalization*.

#### No Transaction

No explicit transactions.

- Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.
- Disadvantage: no rollback

## Merge Data

The following example depicts the update of the existing data records in a database by running an extraction to merge data. In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP.

With a merge command, the updated value is written to the destination database table. The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.

Tip

Alternatively to merging, the data can be also updated by means of full load. The full load method is less efficient.

### Prerequisites

You need a table with existing SAP data, in which new data can be merged.\
Ideally, the table with existing data is created in the initial load with the corresponding **Preparation** option and filled with data with the **Row Processing** option *Insert*.

After the table is created, open SAP and change a field value in the SAP table that is used for the data merge.

Warning

**Faulty merge.**\
Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the [General Settings](../../table/general-settings/#primary-key-tab) of the extraction type to execute the merge command.

### Merge Command

The merge process is performed using a staging table and takes place in three steps:

1. A temporary table is created.
1. The data is inserted in the temporary table.
1. The temporary table is merged with the target table and then the temporary table is deleted.

Follow the steps below to set up the merge process in Xtract Universal:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions.
1. Click **[Destination]**. The window "Destination Settings" opens.
1. Make sure to assign Oracle destination to the extraction.
1. Apply the following destination settings:
1. Click **[OK]** and [run the extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

More information about the updated fields can be found in the SQL statement.\
It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update.\
Fields that do not appear in the SQL statement are not affected by changes.

## Custom SQL Statements

The Oracle destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the Oracle destination:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..

1. Click **[Destination]**, the window "Destination Settings" opens.

1. Make sure to assign the Oracle destination to the extraction.

1. Select the option *Custom SQL* from the drop-down list in one of the following sections:

   - [Preparation](#preparation)
   - [Row Processing](#row-processing)
   - [Finalization](#finalization)

1. Click **[Edit SQL]** . The window "Edit SQL" opens.

1. Enter your custom SQL statement and click **[OK]** to confirm your input.

### Use Templates

Existing SQL commands can be used as templates.\
You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:

- [Preparation](#preparation), e.g., in *Drop & Create* or *Create if Not Exists*
- [Row Processing](#row-processing), e.g., in *Insert* or *Merge*
- [Finalization](#finalization)

Follow the steps below to generate a Custom SQL command from a template:

1. In one of the staging steps, select the *Custom SQL* option from the drop-down list .
1. Click **[Edit SQL]** . The dialogue "Edit SQL" opens.
1. Navigate to the drop-down menu and select an existing command .
1. Click **[Generate Statement]**. A new statement is generated.
1. Click **[Copy]** to copy the statement to the clipboard.
1. Click **[OK]** to confirm your input.

Check out the [Microsoft SQL Server example](../microsoft-sql-server/#custom-sql-statements) for details on predefined expressions.

Note

The custom SQL code is used for SQL Server destinations. A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.

### Use Script Expressions

You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported:

| Input | Description | | --- | --- | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.TableName }#` | Name of the database table extracted data is written to. | | `#{Extraction.RowsCount }#` | Count of the extracted rows. | | `#{Extraction.RunState}#` | Status of the extraction (Running, FinishedNoErrors, FinishedErrors, Cancelled). | | `#{(int)Extraction.RunState}#` | Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors, 6 = Cancelled). | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. |

For more information, see [Script Expressions](../../parameters/script-expressions/).

```sql
#{
   iif
   (
      ExistsTable("MAKT"),
      "TRUNCATE TABLE \"MAKT\";",
      "
         CREATE TABLE \"MAKT\"(
            \"MATNR\" VARCHAR(18),
            \"SPRAS\" VARCHAR(2),
            \"MAKTX\" VARCHAR(40));
      "
   )
}#

```

### Create a Status Overview

The table "ExtractionStatistics" provides an overview and status of the executed Xtract Universal extractions. To create the "ExtractionStatistics" table, create an SQL table according to the following example:

```sql
CREATE TABLE [dbo].[ExtractionStatistics](
    [TableName] [nchar](50) NULL,
    [RowsCount] [int] NULL,
    [Timestamp] [nchar](50) NULL,
    [RunState] [nchar](50) NULL
) ON [PRIMARY]
GO

```

The *ExtractionStatistics* table is filled in the **Finalization** process step, using the following SQL statement:

```sql
INSERT INTO [ExtractionStatistics]
(
     [TableName], 
     [RowsCount], 
     [Timestamp],
     [RunState]
)
VALUES
(
     '#{Extraction.TableName}#', 
     '#{Extraction.RowsCount}#', 
     '#{Extraction.Timestamp}#',
     '#{Extraction.RunState}#'
);

```

This page shows how to set up and use the Flat File Parquet destination. The Flat File Parquet destination loads data to a Parquet Database destination.

## Create a new Flat File Parquet Destination

Follow the steps below to add a new Flat File Parquet destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Flat File Parquet* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

#### Output directory

Enter the folder path to save the destination flat files in. If the entered folder does not exist, a new folder is created.

Note

To write flat files to a network drive, you need to:

- Enter the **File output path** in [UNC format](https://docs.microsoft.com/en-us/dotnet/standard/io/file-path-formats#unc-paths) e.g., `\\Server2\Share\Folder1`.
- Grant write permission to the directory to the [Xtract Universal service](../../server/service-account/) or run the service under a user account that has write permission.

#### Compatibility mode

The following compatibility modes are available:

- *Pure*: standard parquet format

- *Spark*: ensures compatibility with Apache Spark by using different data types than the standard parquet format:

  | SAP | Pure / BigQuery | Spark | | --- | --- | --- | | INT1 | UINT_8 | INT16 | | TIMS | TIME_MILLIS | UTF8 |

- *BigQuery*: ensures compatibility with with Google BigQuery by formatting the columns names, see [BigQuery Documentation: Column Names](https://cloud.google.com/bigquery/docs/schemas?hl=en#column_names).

## Assign the Flat File Parquet Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Flat File Parquet destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Existing files

If a flat file by the same name already exists in the target directory, the following actions can be taken:

| Option | Description | | --- | --- | | **Replace file** | The export process overwrites existing files. | | **Abort extraction** | The process is aborted, if the file already exists. |

This page shows how to set up and use the PostgreSQL destination. The PostgreSQL destination loads data to an EXASolution database.

## Requirements

Data is pushed into the PostgreSQL DB system through the [Npgsql data provider version 8.1](https://www.npgsql.org/index.html). The data provider is provided with the setup of Xtract Universal.

Xtract Universal supports all [PostgreSQL](https://www.npgsql.org/doc/compatibility.html#postgresql) versions compatible with Npgsql. For more information, see [PostgreSQL: Versioning](https://www.postgresql.org/support/versioning/).

### TLS Encryption with PostgreSQL

Requirements for using TLS encryption with PostgreSQL:

- Xtract Universal and the Npgsql.dll driver must be up-to-date and must support TLS with new PostgreSQL versions. If necessary, install the newest version of Xtract Universal.
- the [certificate](../../access-restrictions/install-x.509-certificate/) for the authentication must be valid.
- the Subject Alternative Name of the certificate must be used as the PostgreSQL host, see [**Private endpoint**](#destination-details).
- the certification authority (CA) that signed the certificate and the certificate itself must be trustworthy, see [PostgreSQL Documentation: Secure TCP/IP Connections with SSL](https://www.postgresql.org/docs/11/ssl-tcp.html).

## Create a new PostgreSQL Destination

Follow the steps below to add a new PostgreSQL destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *PostgreSQL* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

#### Server Name

Name or IP of the database server.

#### TLS Mode

The following TLS modes can be selected for the connection:

| Mode | Description | | --- | --- | | Prefer | Default Value that uses TLS encryption, if supported by the server. If this is not the case, unencrypted connections are used. | | Require | Xtract Universal forces an encrypted TLS connection to the PostgreSQL server - unencrypted connections are not established. | | Disable | **Caution!** An unencrypted, insecure connection is established to the PostgreSQL server. |

Make sure that the Certification authority (CA) that signed the certificate or the certificate itself is trusted by the client. For more information, see [Secure TCP/IP Connections with SSL](https://www.postgresql.org/docs/11/ssl-tcp.html).

#### Private endpoint

This field is optional. You can enter an alternative hostname under which a connection is established.

Example: The PostgreSQL database is hosted on a cloud and access to the database's cloud domain name is restricted by company policy. In this case the database can be accessed through a private endpoint. Enter the private endpoint in this field. The PostgreSQL cloud domain name must be entered in the field **Server Name** for certificate validation.

#### Port

Port of the database server. Port 5432 is selected by default.

#### Windows Authentication

Uses the service account, under which the XU service is running, for authentication against the PostgreSQL server, see [PostgreSQL Documentation: Client authentication](https://www.postgresql.org/docs/11/client-authentication.html).

Note

To successfully connect to the database using Windows authentication, make sure to [run the XU service under a Windows AD user](../../server/service-account/) with access to the database.

#### Username

Enter the name of the database user.

#### Password

Enter the password of the database user.

#### Database

Enter the name of the database.

#### Test Connection

Test the database connection.

Warning

**The remote certificate is invalid according to the validation procedure**\
When using TLS encryption, this error message can have multiple causes e.g. invalid or untrustworthy certificates. See [TLS Encryption with PostgreSQL](#tls-encryption-with-postgresql) for information.

## Assign the PostgreSQL Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your PostgreSQL destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Preparation

Defines the action on the target database before the data is inserted into the target table.

| Option | Description | | --- | --- | | *Drop & Create* | Remove table if available and create new table (default). | | *Truncate Or Create* | Empty table if available, otherwise create. | | *Create If Not Exists* | Create table if not available. | | *Prepare Merge* | Prepares the merge process and creates e.g. a temporary staging table, see [Merge Data](#merge-data). | | *None* | No action. | | *Custom SQL* | Here you can define your own script, see [Custom SQL Statements](#custom-sql-statements). |

To only create the table in the first step and not insert any data, you have two options:

1. Copy the SQL statement and execute it directly on the target data database.
1. Select the *None* option for **Row Processing** and execute the extraction.

Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.

### Row Processing

Defines how the data is inserted into the target table.

| Option | Description | | --- | --- | | *Insert* | Insert records (default). | | *Fill merge staging table* | Insert records into the staging table. | | *None* | No action. | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Finalization

Defines the action on the target database after the data has been successfully inserted into the target table.

| Option | Description | | --- | --- | | *Finalize Merge* | Closes the merge process and deletes the temporary staging table, for example. | | *None* | No action (default). | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Debugging

Warning

**Performance decrease!**\
The performance decreases when bulk insert is disabled. Disable the bulk insert only when necessary, e.g., upon request of the support team.

By activating the checkbox **Disable bulk operations**, the default bulk insert is deactivated when writing to a database.

This option enables detailed error analysis, if certain data rows cannot be persisted on the database. Possible causes for the incorrect behavior are incorrect values with regard to the stored data type.

Debugging needs to be deactivated again, after the successful error analysis, otherwise the performance of the database write processes remains low.

Note

Bulk operations are not supported when using [Custom SQL statements](#custom-sql-statements), e.g., in *Row Processing*. Bulk operations lead to performance decrease. To increase performance when using [Custom SQL statements](#custom-sql-statements), it is recommended to perform the custom processing in the *Finalization* step.

### Transaction style

Note

The available options for Transaction Style vary depending on the destination.

#### One Transaction

*Preparation*, *Row Processing* and *Finalization* are all performed in a single transaction.

- Advantage: clean rollback of all changes.
- Disadvantage: possibly extensive locking during the entire extraction period.

Recommendation

Only use *One Transaction* in combination with DML commands, e.g., "truncate table" and "insert. Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command. Example: If a table is created in the preparation step, the opened "OneTransaction" is committed and a rollback in the next steps is not performed correctly.

#### Three Transactions

*Preparation*, *Row Processing* and *Finalization* are each executed in a separate transaction.

- Advantage: clean rollback of the individual sections, possibly shorter locking phases than with *One Transaction* (e.g. with DDL in *Preparation*, the entire DB is only locked during preparation and not for the entire extraction duration).
- Disadvantage: no rollback of previous step possible (error in *Row Processing* only rolls back changes from *Row Processing*, but not *Preparation*).

#### RowProcessingOnly

Only *Row Processing* is executed in a transaction. *Preparation* and *Finalization* without an explicit transaction (implicit commits).

- Advantage: DDL in 'Preparation *and* Finalization\* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)
- Disadvantage: no rollback of *Preparation/Finalization*.

#### No Transaction

No explicit transactions.

- Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.
- Disadvantage: no rollback

## Merge Data

The following example depicts the update of the existing data records in a database by running an extraction to merge data. In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP.

With a merge command, the updated value is written to the destination database table. The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.

Tip

Alternatively to merging, the data can be also updated by means of full load. The full load method is less efficient.

### Prerequisites

You need a table with existing SAP data, in which new data can be merged.\
Ideally, the table with existing data is created in the initial load with the corresponding **Preparation** option and filled with data with the **Row Processing** option *Insert*.

After the table is created, open SAP and change a field value in the SAP table that is used for the data merge.

Warning

**Faulty merge.**\
Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the [General Settings](../../table/general-settings/#primary-key-tab) of the extraction type to execute the merge command.

### Merge Command

The merge process is performed using a staging table and takes place in three steps:

1. A temporary table is created.
1. The data is inserted in the temporary table.
1. The temporary table is merged with the target table and then the temporary table is deleted.

Follow the steps below to set up the merge process in Xtract Universal:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions.
1. Click **[Destination]**. The window "Destination Settings" opens.
1. Make sure to assign PostgreSQL destination to the extraction.
1. Apply the following destination settings:
1. Click **[OK]** and [run the extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

More information about the updated fields can be found in the SQL statement.\
It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update.\
Fields that do not appear in the SQL statement are not affected by changes.

## Custom SQL Statements

The PostgreSQL destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the PostgreSQL destination:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..

1. Click **[Destination]**, the window "Destination Settings" opens.

1. Make sure to assign the PostgreSQL destination to the extraction.

1. Select the option *Custom SQL* from the drop-down list in one of the following sections:

   - [Preparation](#preparation)
   - [Row Processing](#row-processing)
   - [Finalization](#finalization)

1. Click **[Edit SQL]** . The window "Edit SQL" opens.

1. Enter your custom SQL statement and click **[OK]** to confirm your input.

### Use Templates

Existing SQL commands can be used as templates.\
You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:

- [Preparation](#preparation), e.g., in *Drop & Create* or *Create if Not Exists*
- [Row Processing](#row-processing), e.g., in *Insert* or *Merge*
- [Finalization](#finalization)

Follow the steps below to generate a Custom SQL command from a template:

1. In one of the staging steps, select the *Custom SQL* option from the drop-down list .
1. Click **[Edit SQL]** . The dialogue "Edit SQL" opens.
1. Navigate to the drop-down menu and select an existing command .
1. Click **[Generate Statement]**. A new statement is generated.
1. Click **[Copy]** to copy the statement to the clipboard.
1. Click **[OK]** to confirm your input.

Check out the [Microsoft SQL Server example](../microsoft-sql-server/#custom-sql-statements) for details on predefined expressions.

Note

The custom SQL code is used for SQL Server destinations. A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.

### Use Script Expressions

You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported:

| Input | Description | | --- | --- | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.TableName }#` | Name of the database table extracted data is written to. | | `#{Extraction.RowsCount }#` | Count of the extracted rows. | | `#{Extraction.RunState}#` | Status of the extraction (Running, FinishedNoErrors, FinishedErrors, Cancelled). | | `#{(int)Extraction.RunState}#` | Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors, 6 = Cancelled). | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. |

For more information, see [Script Expressions](../../parameters/script-expressions/).

```sql
#{
   iif
   (
      ExistsTable("MAKT"),
      "TRUNCATE TABLE \"MAKT\";",
      "
         CREATE TABLE \"MAKT\"(
            \"MATNR\" VARCHAR(18),
            \"SPRAS\" VARCHAR(2),
            \"MAKTX\" VARCHAR(40));
      "
   )
}#

```

### Create a Status Overview

The table "ExtractionStatistics" provides an overview and status of the executed Xtract Universal extractions. To create the "ExtractionStatistics" table, create an SQL table according to the following example:

```sql
CREATE TABLE [dbo].[ExtractionStatistics](
    [TableName] [nchar](50) NULL,
    [RowsCount] [int] NULL,
    [Timestamp] [nchar](50) NULL,
    [RunState] [nchar](50) NULL
) ON [PRIMARY]
GO

```

The *ExtractionStatistics* table is filled in the **Finalization** process step, using the following SQL statement:

```sql
INSERT INTO [ExtractionStatistics]
(
     [TableName], 
     [RowsCount], 
     [Timestamp],
     [RunState]
)
VALUES
(
     '#{Extraction.TableName}#', 
     '#{Extraction.RowsCount}#', 
     '#{Extraction.Timestamp}#',
     '#{Extraction.RunState}#'
);

```

This page shows how to set up and use the QlikSense & QlikView destination. The QlikSense & QlikView destination loads data to QlikSense or QlikView.

## About

The QlikSense & QlikView destination [generates a data load script](#generate-a-data-load-script), that needs to be inserted into the data load editor of your Qlik application. Depending on whether you run a QlikSense or QlikView application, Xtract Univeral creates a different data load script.

Running the Qlik application triggers the respective Xtract Universal extraction via the data load script. Xtract Universal sends the extracted SAP data through an HTTP-CSV stream directly to your Qlik application.

## Create a new QlikSense & QlikView Destination

Follow the steps below to add a new QlikSense & QlikView destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *QlikSense & QlikView* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination. To use the QlikSense & QlikView destination, no further settings are necessary.

## Assign the QlikSense & QlikView Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your QlikSense & QlikView destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

## Generate a Data Load Script

Follow the steps below to generate a data load script in Xtract Universal. The data load script is required to run extractions in your Qlik application.

1. In the main window of the Designer, select an extraction from the list of extractions.
1. Make sure the extraction has the QlikSense & QlikView destination [assigned](#assign-the-qliksense-qlikview-destination-to-an-extraction) to it.
1. Click **[Run]**. The window "Run Extraction" opens.
1. Optional (when using QlikSense): In the *Parameters* section, mark the check boxes for the parameters that you want to add as SET variables in the QlikSense script.
1. Click **[Generate Qlik Script]** to generate a data load script. The window "Script" opens.
1. Select the *QlikView Script* or *QlikSense Script* tab.
1. Click **[Copy to Clipboard]** to copy the script.

When using QlikView, paste the copied script into the QlikView data load editor. For QlikSense, see [Run Extractions from QlikSense](#run-extractions-from-qliksense).

Note

The "SET methods" cannot be edited in the "Script" window. Edit the SET methods in the Qlik data load editor.

### Run Extractions from QlikSense

Before copying the Qlik script [generated by Xtract Universal](#generate-a-data-load-script) to QlikSense, perform the following steps in QlikSense:

1. Create a new data connection of type REST.

   Note

   In QlikSense the default value for the *Timeout* is 30 seconds. Increase the timeout to a sufficiently high value if the time till the first data package arrives from SAP is higher than 30 seconds. The maximum input value is 10.000 seconds.

1. Enter the URL of the Xtract Universal Server and port into the URL text field. In the depicted example, the Xtract Universal server runs on `http://localhost:8065/`.

1. Enter *Xtract_Universal* into the name text field.

1. Activate the security option **Allow response headers**. This option ensures that error messages from Xtract Universal are passed to QlikSense.

1. Paste the QlikSense script from Xtract Universal into the QlikSense Data load editor.

Warning

**Response headers are denied by the current connection. Please edit your connection in order to enable response headers loading.**\
When this error message is displayed in the REST connection, activate the option "Allow response headers" in the *Security* settings of the connector.

### About the QlikSense Data Load Script

Xtract Universal creates a QlikSense script with the following properties:

- The script uses QlikSense [interpretation functions](https://help.qlik.com/en-US/sense/June2020/Subsystems/Hub/Content/Sense_Hub/Scripting/InterpretationFunctions/interpretation-functions.htm) *Num#*, *Text*, *Date* and *Time*. For fields, where an adequate data type can't be determined, no interpretation function is used.
- The field description and the SAP origin of the field are assigned as tags to all fields.
- All date fields with `$date` are explicitly tagged. This function assures that fields containing a [date before January 1, 1980](https://help.qlik.com/en-US/sense/April2020/Subsystems/Hub/Content/Sense_Hub/Scripting/date-time-interpretation.htm) are recognized as date fields in QlikSense.
- The usage of Xtract Universal [Extraction Parameters](../../parameters/extraction-parameters/) is supported. To make parameters available in the QlikSense script, activate the parameters in the ["Run Extraction" window](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer) of Xtract Universal.

Note

Do not change the assigned value of variables *xuOriginDateFormat* and *xuOriginTimeFormat*. The chosen format enables Xtract Universal to send the data of date and time fields to QlikSense. Changing the format stops the QlikSense script from running.

This page shows how to set up and use the Salesforce destination. The Salesforce destination loads data to Salesforce.

## Requirements

#### Salesforce Edition

This destination requires one of the following Salesforce editions:

- *Enterprise*
- *Unlimited*
- *Performance Edition* The Salesforce edition has to include the Integration via web service API feature.

#### Permissions

To run an extraction the API-Enabled permission is needed. If the object has to be created the user also needs following permissions:

- *Modify All Data*
- *Customize Application*
- *Manage Profiles and Permission Sets*

## Create a new Salesforce Destination

Follow the steps below to add a new Salesforce destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Salesforce* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

#### Username

Enter your Salesforce username.

#### Password

Enter the corresponding password.

#### Security Token

Enter the Security Token that was generated by Salesforce and is used to access API functions.

#### Reset Security Token

Opens a link to the website where you can reset your current Security Token. To reset your security token on Salesforce, at the top navigation bar go to **your name > Setup > Personal Setup > My Personal Information > Reset My Security Token**.

#### Environment

Select **Production** to write data directly to your productive environment or select **Test** to write data to a test environment.

#### Test Connection

Check the database connection.

## Assign the Salesforce Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Salesforce destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Preparation

| Option | Description | | --- | --- | | *None* | No action. | | *Delete & Create* | Deletes the object with the specified name and creates a new one. | | *Create If Not Exists* | Creates a new object if no object with the specified name could be found. |

### Row Processing

| Option | Description | | --- | --- | | *None* | No action. | | *Insert* | Inserts all records into the specified object. | | *Merge* | Inserts all records into the specified object and updates already existing entries. |

### Concurrency Mode

#### Parallel

Process batches in parallel mode. This is the default value.

#### Serial

Process batches in serial mode. Processing in parallel can cause database contention. When this is severe, the job may fail. If you experience this issue, submit the job with serial concurrency mode. This guarantees that batches are processed one at a time. Note that using this option may significantly increase the processing time for a job.

This page shows how to set up and use the SAP HANA destination. The SAP HANA destination loads data to an SAP HANA database or to SAP Datasphere. When loading data to SAP Datasphere, SAP Datasphere's underlying HANA database is used as a destination.

## Requirements

To establish a connection to an SAP HANA database or SAP Datasphere, the *SAP HANA Data Provider for Microsoft ADO.NET* version **2.17.22** is required. The data provider is part of the SAP HANA Client setup.

[Download SAP HANA Client Setup version 2.17.22](../../../assets/files/xu/SAP_HANA_CLIENT_IMDB_CLIENT20_017_22.zip)

## Create a new SAP HANA Destination

Follow the steps below to add a new SAP HANA destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *SAP HANA* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination. The destination details can be defined for an SAP HANA database or for an SAP Datasphere connection.

For information on SAP Datasphere connection parameters, see [SAP Help: Create a Database User](https://help.sap.com/docs/SAP_DATASPHERE/be5967d099974c69b77f4549425ca4c0/798e3fd6707940c3bd2219b2d1ebaac2.html?locale=en-US) and [SAP Help: Obtaining Your OPEN SQL Schema Connection Information](https://help.sap.com/docs/SAP_DATASPHERE/be5967d099974c69b77f4549425ca4c0/b78ad208f8c4494489aabf97284679b6.html?locale=en-US#obtaining-%5B%E2%80%A6%5Dnformation).

#### Server Name

Enter the address of the server (including the port number). Note that there are different port numbers for SAP HANA and SAP Datasphere.

#### User Name

Enter the SAP HANA/SAP Datasphere user name.

#### Password

Enter the user password.

#### Database

Enter the name of the database.

#### Schema

Enter the name of the database schema.

#### Use encryption

Activates connection encryption. This option is required when connecting to SAP Datasphere.

#### Test Connection

Check the database connection.

## Assign the SAP HANA Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your SAP HANA destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Preparation

Defines the action on the target database before the data is inserted into the target table.

| Option | Description | | --- | --- | | *Drop & Create* | Remove table if available and create new table (default). | | *Truncate Or Create* | Empty table if available, otherwise create. | | *Create If Not Exists* | Create table if not available. | | *Prepare Merge* | Prepares the merge process and creates e.g. a temporary staging table, see [Merge Data](#merge-data). | | *None* | No action. | | *Custom SQL* | Here you can define your own script, see [Custom SQL Statements](#custom-sql-statements). |

To only create the table in the first step and not insert any data, you have two options:

1. Copy the SQL statement and execute it directly on the target data database.
1. Select the *None* option for **Row Processing** and execute the extraction.

Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.

### Row Processing

Defines how the data is inserted into the target table.

| Option | Description | | --- | --- | | *Insert* | Insert records (default). | | *Fill merge staging table* | Insert records into the staging table. | | *None* | No action. | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). | | *Upsert (deprecated)* | This option is obsolete, see [Merge Data](#merge-data). Use the *Fill merge staging table* option. |

### Finalization

Defines the action on the target database after the data has been successfully inserted into the target table.

| Option | Description | | --- | --- | | *Finalize Merge* | Closes the merge process and deletes the temporary staging table, for example. | | *None* | No action (default). | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Debugging

Warning

**Performance decrease!**\
The performance decreases when bulk insert is disabled. Disable the bulk insert only when necessary, e.g., upon request of the support team.

By activating the checkbox **Disable bulk operations**, the default bulk insert is deactivated when writing to a database.

This option enables detailed error analysis, if certain data rows cannot be persisted on the database. Possible causes for the incorrect behavior are incorrect values with regard to the stored data type.

Debugging needs to be deactivated again, after the successful error analysis, otherwise the performance of the database write processes remains low.

Note

Bulk operations are not supported when using [Custom SQL statements](#custom-sql-statements), e.g., in *Row Processing*. Bulk operations lead to performance decrease. To increase performance when using [Custom SQL statements](#custom-sql-statements), it is recommended to perform the custom processing in the *Finalization* step.

### Transaction style

Note

The available options for Transaction Style vary depending on the destination.

#### One Transaction

*Preparation*, *Row Processing* and *Finalization* are all performed in a single transaction.

- Advantage: clean rollback of all changes.
- Disadvantage: possibly extensive locking during the entire extraction period.

Recommendation

Only use *One Transaction* in combination with DML commands, e.g., "truncate table" and "insert. Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command. Example: If a table is created in the preparation step, the opened "OneTransaction" is committed and a rollback in the next steps is not performed correctly.

#### Three Transactions

*Preparation*, *Row Processing* and *Finalization* are each executed in a separate transaction.

- Advantage: clean rollback of the individual sections, possibly shorter locking phases than with *One Transaction* (e.g. with DDL in *Preparation*, the entire DB is only locked during preparation and not for the entire extraction duration).
- Disadvantage: no rollback of previous step possible (error in *Row Processing* only rolls back changes from *Row Processing*, but not *Preparation*).

#### RowProcessingOnly

Only *Row Processing* is executed in a transaction. *Preparation* and *Finalization* without an explicit transaction (implicit commits).

- Advantage: DDL in 'Preparation *and* Finalization\* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)
- Disadvantage: no rollback of *Preparation/Finalization*.

#### No Transaction

No explicit transactions.

- Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.
- Disadvantage: no rollback

## Merge Data

The following example depicts the update of the existing data records in a database by running an extraction to merge data. In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP.

With a merge command, the updated value is written to the destination database table. The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.

Tip

Alternatively to merging, the data can be also updated by means of full load. The full load method is less efficient.

### Prerequisites

You need a table with existing SAP data, in which new data can be merged.\
Ideally, the table with existing data is created in the initial load with the corresponding **Preparation** option and filled with data with the **Row Processing** option *Insert*.

After the table is created, open SAP and change a field value in the SAP table that is used for the data merge.

Warning

**Faulty merge.**\
Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the [General Settings](../../table/general-settings/#primary-key-tab) of the extraction type to execute the merge command.

### Merge Command

The merge process is performed using a staging table and takes place in three steps:

1. A temporary table is created.
1. The data is inserted in the temporary table.
1. The temporary table is merged with the target table and then the temporary table is deleted.

Follow the steps below to set up the merge process in Xtract Universal:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions.
1. Click **[Destination]**. The window "Destination Settings" opens.
1. Make sure to assign SAP HANA destination to the extraction.
1. Apply the following destination settings:
1. Click **[OK]** and [run the extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

More information about the updated fields can be found in the SQL statement.\
It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update.\
Fields that do not appear in the SQL statement are not affected by changes.

## Custom SQL Statements

The SAP HANA destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the SAP HANA destination:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..

1. Click **[Destination]**, the window "Destination Settings" opens.

1. Make sure to assign the SAP HANA destination to the extraction.

1. Select the option *Custom SQL* from the drop-down list in one of the following sections:

   - [Preparation](#preparation)
   - [Row Processing](#row-processing)
   - [Finalization](#finalization)

1. Click **[Edit SQL]** . The window "Edit SQL" opens.

1. Enter your custom SQL statement and click **[OK]** to confirm your input.

### Use Templates

Existing SQL commands can be used as templates.\
You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:

- [Preparation](#preparation), e.g., in *Drop & Create* or *Create if Not Exists*
- [Row Processing](#row-processing), e.g., in *Insert* or *Merge*
- [Finalization](#finalization)

Follow the steps below to generate a Custom SQL command from a template:

1. In one of the staging steps, select the *Custom SQL* option from the drop-down list .
1. Click **[Edit SQL]** . The dialogue "Edit SQL" opens.
1. Navigate to the drop-down menu and select an existing command .
1. Click **[Generate Statement]**. A new statement is generated.
1. Click **[Copy]** to copy the statement to the clipboard.
1. Click **[OK]** to confirm your input.

Check out the [Microsoft SQL Server example](../microsoft-sql-server/#custom-sql-statements) for details on predefined expressions.

Note

The custom SQL code is used for SQL Server destinations. A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.

### Use Script Expressions

You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported:

| Input | Description | | --- | --- | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.TableName }#` | Name of the database table extracted data is written to. | | `#{Extraction.RowsCount }#` | Count of the extracted rows. | | `#{Extraction.RunState}#` | Status of the extraction (Running, FinishedNoErrors, FinishedErrors, Cancelled). | | `#{(int)Extraction.RunState}#` | Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors, 6 = Cancelled). | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. |

For more information, see [Script Expressions](../../parameters/script-expressions/).

```sql
#{
   iif
   (
      ExistsTable("MAKT"),
      "TRUNCATE TABLE \"MAKT\";",
      "
         CREATE TABLE \"MAKT\"(
            \"MATNR\" VARCHAR(18),
            \"SPRAS\" VARCHAR(2),
            \"MAKTX\" VARCHAR(40));
      "
   )
}#

```

### Create a Status Overview

The table "ExtractionStatistics" provides an overview and status of the executed Xtract Universal extractions. To create the "ExtractionStatistics" table, create an SQL table according to the following example:

```sql
CREATE TABLE [dbo].[ExtractionStatistics](
    [TableName] [nchar](50) NULL,
    [RowsCount] [int] NULL,
    [Timestamp] [nchar](50) NULL,
    [RunState] [nchar](50) NULL
) ON [PRIMARY]
GO

```

The *ExtractionStatistics* table is filled in the **Finalization** process step, using the following SQL statement:

```sql
INSERT INTO [ExtractionStatistics]
(
     [TableName], 
     [RowsCount], 
     [Timestamp],
     [RunState]
)
VALUES
(
     '#{Extraction.TableName}#', 
     '#{Extraction.RowsCount}#', 
     '#{Extraction.Timestamp}#',
     '#{Extraction.RunState}#'
);

```

______________________________________________________________________

## Related Links

- [SAP Help: SAP HANA Data Provider](https://help.sap.com/viewer/0eec0d68141541d1b07893a39944924e/2.0.00/en-US/469dee9e6d611014af70d4e9a9cd6b0a.html)
- [SAP HANA Client Setup](https://blogs.sap.com/2017/12/14/sap-hana-2.0-client-installation-and-update-by-the-sap-hana-academy/)
- [Integrating Data and Managing Spaces in SAP Datasphere](https://help.sap.com/docs/SAP_DATASPHERE/be5967d099974c69b77f4549425ca4c0/798e3fd6707940c3bd2219b2d1ebaac2.html)
- [Connect to Your Open SQL Schema](https://help.sap.com/docs/SAP_DATASPHERE/be5967d099974c69b77f4549425ca4c0/b78ad208f8c4494489aabf97284679b6.html#obtaining-your-open-sql-schema-connection-information)
- [Obtaining Your OPEN SQL Schema Connection Information](https://help.sap.com/docs/SAP_DATASPHERE/be5967d099974c69b77f4549425ca4c0/b78ad208f8c4494489aabf97284679b6.html#obtaining-your-open-sql-schema-connection-information)

This page shows how to set up and use the Microsoft Power BI Report Server destination. The Microsoft Power BI Report Server destination loads data in Power BI Report Server (SSRS).

## Requirements

The Power BI Report Server destination requires the following components:

- Visual Studio 2017 or higher
- the [Microsoft Reporting Services Projects](https://marketplace.visualstudio.com/items?itemName=ProBITools.MicrosoftReportProjectsforVisualStudio) plugin (version 2.6.11 or higher) for Visual Studio.
- Power BI Report Server (January 2020 or later)
- Xtract Universal (version 4.29 or higher)
- To use [Transport Layer Security](../../access-restrictions/restrict-server-access/#activate-tls-encryption), it is necessary to modify the Registry of the machine that runs the SSRS server according to the [Microsoft Documentation: Configure Strong cryptography](https://docs.microsoft.com/en-us/mem/configmgr/core/plan-design/security/enable-tls-1-2-client#configure-for-strong-cryptography).

Note

The Power BI Report Builder is not supported.

## Installation

To use the Power BI Report Server destination, install the [Microsoft Reporting Services Projects](https://marketplace.visualstudio.com/items?itemName=ProBITools.MicrosoftReportProjectsforVisualStudio) plugin in Visual Studio. After the installation is complete, close Visual Studio.

Microsoft Power BI Report Server (SQL Server Reporting Services) supports a wide variety of data sources out of the box. To add Xtract Universal to the list of data sources, install the [Xtract Universal Report Server Plugin](#installation-of-the-xtract-universal-report-server-plugin) for Visual Studio **and** the Report Server. The plugin must be installed on both environments to consume the data extracted by Xtract Universal, see graphic below:

To complete the installation close all Visual Studio windows.

Note

Make sure to install the latest version of the *Microsoft Reporting Services Projects* plugin and Xtract Universal.

### Installation of the Xtract Universal Report Server Plugin

The Xtract Universal Report Server Plugin can be installed as part of the [Xtract Universal Setup](../../setup/installation/).\
To install the Xtract Universal Report Server Plugin on multiple environments without installing the Xtract Universal Designer, follow the steps below:

1. Make sure the *Microsoft Reporting Services Projects* plugin for Visual Studio is installed and active.
1. Close Visual Studio.
1. [Install Xtract Universal](../../setup/installation/) on the environment on which the license runs.
1. Download the [XtractUniversalReportServerPluginSetup.exe](../../../assets/files/xu/XtractUniversalReportServerPluginSetup.exe).
1. Run the *XtractUniversalReportServerPluginSetup.exe* on any environment that uses Visual Studio to design reports or where the Report Server runs. This installs the Xtract Universal Report Server Plugin on all compatible versions of Visual Studio and/or Report Server found on the environment.
1. After the installation on the Report Server is complete, restart the Report Server for the changes to take effect. You can restart the server in the Report Server Configuration Manager by clicking **[stop]** and then **[start]**.

Note

If the *Reporting Services Projects* plugin for Visual Studio is updated, the Xtract Universal Report Server Plugin is not available anymore. The Report Server Plugin must be reinstalled.

After installation of Xtract Universal Report Server Plugin the following entries and extensions are available in the Visual Studio installation directory `C:\Program Files (x86)\Microsoft Visual Studio\2019\Enterprise\Common7\IDE\CommonExtensions\Microsoft\SSRS\`:

- XUDataExtension2020.05.dll
- Theobald.Common.dll
- Theobald.Distillery.Common.dll
- Theobald.Net.dll
- RSReportDesigner.config

## Create a new Microsoft Power BI Report Server Destination

Follow the steps below to add a new Microsoft Power BI Report Server destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Microsoft Power BI Report Server* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination. To use the Microsoft Power BI Report Server destination, no further settings are necessary.

## Assign the Microsoft Power BI Report Server Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Microsoft Power BI Report Server destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

## SSRS in Visual Studio

The following section contains step-by-step instructions on how to access SAP data in the SSRS environment via Xtract Universal.

#### Prerequisites in Xtract Universal

To make SAP data available in the SSRS environment, [create an extraction](../../../getting-started/#create-an-extraction) in the Xtract Universal Designer. Make sure the Microsoft Power BI Report Server destination is [assigned](#assign-the-microsoft-power-bi-report-server-destination-to-an-extraction) to the extraction.

### Add an Extraction as a Data Source in Visual Studio

Follow the steps below to create an Xtract Universal data source in Visual Studio:

1. Create a new "Report Server Project" in Visual Studio.
1. Right-click the *Shared Data Sources* folder in the *Solution Explorer* and select **Add New Data Source**. The window "Shared Data Source Properties" opens.
1. In the tab *General*, assign a name for the new data source .
1. Select the type *Xtract Universal* from the drop-down list .\
   If *Xtract Universal* is not available or displayed in all caps, make sure that the latest Xtract Universal version and *Microsoft Reporting Services Projects* plugin are both installed.
1. Enter a *connection string* to the [Xtract Universal Web Server](../../designer/#connect-the-designer-to-a-server) in the format `Url=http://[host]:[port]/` e.g, `Url=http://localhost:8065/`. Note that the *connection string* is case sensitive.
1. Optional (only required if [access restriction to the Xtract Universal web server](../../access-restrictions/restrict-server-access/) is configured or if the SAP source requires SAP credentials): Switch to the *Credentials* tab and enter your [username and password](../../access-restrictions/user-management/) for Xtract Universal.
1. Confirm your input with **[OK]**.

If Xtract Universal is not available in the drop-down list even though the *Microsoft Reporting Services Projects* plugin (version 2.6.11 or higher) and the latest version of Xtract Universal are both installed, send the log file located in `C:\Program Files\XtractUniversal\ssrs\log.txt` to [Theobald Support](https://support.theobald-software.com).

Tip

The *connection string* is part of the URL string displayed in the ["Run Extraction" window](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer) of the Designer. Copy the URL string up to '?' and paste it as the *connection string* into Visual Studio.

Note

When **https Restricted...** or **https Unrestricted** is activated in the [XU web server settings](../../access-restrictions/restrict-server-access/#activate-tls-encryption), make sure to modify the Registry of the machine that runs the SSRS server according to the [Microsoft Documentation: Configure Strong cryptography](https://docs.microsoft.com/en-us/mem/configmgr/core/plan-design/security/enable-tls-1-2-client#configure-for-strong-cryptography).

### Create a Report using an Xtract Universal Data Source

The following steps guide you through the creation of a Report in Visual Studio using an Xtract Universal data source:

1. [Add a Power BI Report Server extraction as a data source in Visual Studio](#add-an-extraction-as-a-data-source-in-visual-studio).

1. Right-click the *Reports* folder in the *Solution Explorer* and select **Add New Report**. The "Report Wizard" opens.

1. In the wizard, select a data source and click **[Next]**.

1. Click **[Query Builder...]**. The window "Query Designer" opens.

1. Select an Xtract Universal extraction from the drop-down menu .

   Note

   If no extractions are available, make sure the *connection string* in the data source is correct and an extraction with an *SQL Server Reporting Server* destination exists in Xtract Universal.

   If you don't see the editor shown below, click **Edit as Text**.

1. Optional: Change [runtime parameters](../../parameters/extraction-parameters/) of the extraction . Parameter changes are visible in the Query string . Invalid input is marked with a red circle that displays an error message when hovering over it.

1. Click **[Preview]** to run the extraction in preview mode.

1. Confirm your input with **[OK]**. The Query Builder closes.

1. Make sure the Query string from the Query Builder is displayed in the Report Wizard before clicking **[Next]**.

1. Complete the Report Wizard according to your preferences.

After creating the report, you can access the Query Builder by right-clicking the *Dataset* in *Report Data* and selecting **Query...**.

Tip

Passing the (Windows) user that runs a report on the report server or in Visual Studio is supported. For this, you need to set up the [web server authentication](../../server/server-settings/#web-server) in Xtract Universal. Single Sign On in SAP is also supported, see [Single Sign On](../../sap-connection/#single-sign-on-sso).

### Parameterization

Xtract Universal Designer uses [runtime parameters](../../parameters/extraction-parameters/) for parameterization.\
Runtime parameters are accessible in the Query Builder. They can have one of the following **Behaviors**:

| Behaviour | Description | | --- | --- | | *Default* | Uses the value specified in Xtract Universal Designer. | | *Constant* | Enter a constant value to be used during runtime. | | *Parameterized* | Enter the name of a dynamic query parameter to use as a runtime parameter. The value of that parameter can either come from an input field, from the user or from a computed parameter using a formula. |

Note

Every runtime parameter with *Parameterized Behaviour* allows only one query parameter name as its input. If you want to use multiple inputs for a runtime parameter, you can use VS Report Designer tools to combine multiple parameters into a single *Computed Query Parameter*, see [Use Computed Query Parameters for SSRS with Xtract Universal](../../../knowledge-base/xu-ssrs-parameterizing-in-vs/).

Use VS query parameters as input for Xtract runtime parameters.

1. To create a new query parameter right-click the data set in the *Report Data* section and select **Dataset Properties**. The window "Dataset Properties" opens.
1. Switch to the tab *Parameters* and press **[Add]**.
1. Enter a *Parameter Name* and a *Parameter Value* or use the **[f(x)]** button to use formulas and/or combine multiple input values.
1. Switch to the *Query* tab and press **[Query Designer...]**. The window "Query Designer" opens.
1. Select *Parameterized* as the **Behaviour** of the runtime parameter you want to dynamize.
1. Enter the name of the new query parameter under **Value**.
1. Confirm your input with **[OK]**.

If a query parameter is NULL, that parameter is not passed at runtime and thus will be ignored.

Note

Depending on the extraction type, some runtime parameters are mandatory, e.g., most custom parameters.

1. Right-click the input field of the parameter you want to be optional and select *Parameter Properties*. The window "Report Parameter Properties" opens.
1. In the *General* tab, activate the checkbox **Allow null value**.
1. Confirm your input with **[OK]**. A checkbox **NULL** appears next to the input field.
1. If the checkbox **NULL** is activated, the parameter will be ignored at runtime.

Tip

You can also use a computed query parameter to create a value of NULL. Create a formula that returns *Nothing* as the value.

______________________________________________________________________

## Related Links

- [Microsoft Documentation: Report Design Tips](https://docs.microsoft.com/en-us/sql/reporting-services/report-design/report-design-tips-report-builder-and-ssrs?view=sql-server-ver15)
- [Microsoft Documentation: Reporting Services Tutorials (SSRS)](https://docs.microsoft.com/en-us/sql/reporting-services/reporting-services-tutorials-ssrs?view=sql-server-ver15)
- [Microsoft Documentation: Add a Query Parameter to Create a Report Parameter](https://docs.microsoft.com/en-us/sql/reporting-services/tutorial-add-a-parameter-to-your-report-report-builder?view=sql-server-ver15#Query)
- [Knowledge Base Article: Use Computed Query Parameters for SSRS with Xtract Universal](../../../knowledge-base/xu-ssrs-parameterizing-in-vs/)
- [Integration via Azure Data Factory](../../execute-and-automate/call-via-etl/#integration-via-azure-data-factory)

This page shows how to set up and use the Microsoft SharePoint destination. The Microsoft SharePoint destination loads data into a custom list on a SharePoint server.

## Requirements

To extract data into a SharePoint Custom list, you need either your own SharePoint server or access to a SharePoint Online system as part of Office365.

If your SharePoint server isn't configured for remote access already, go to **Central Administration -> Application Management -> Configure alternate access mappings** and add an appropriate mapping for the zone "Internet".

## Create a new Microsoft SharePoint Destination

Follow the steps below to add a new Microsoft SharePoint destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Microsoft SharePoint* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

#### SharePoint on-premises or SharePoint Online

Select the type of SharePoint environment to write data to.

#### Site URL

Enter the URL of your SharePoint server. To write data to a specific site on the server, include the sub-directories in the URL. The URL uses the following format:

- SharePoint Online: `https://<your-tenant>.sharepoint.com/sites/<your-site-name>`
- SharePoint on-premises: `http(s)://<your-server-name or domain>/<site-path>`

Note

Make sure you only enter the base path, omit page information a browser might show you in the address line like `_layouts/15/start.aspx#/` or similar.

#### User

Enter your SharePoint user name.

#### Password

Enter the password for your SharePoint user account.

#### Test connection

Check the database connection.

## Assign the Microsoft SharePoint Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Microsoft SharePoint destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Mode

| Option | Description | | --- | --- | | *Drop, Create & Insert* | Creates a new list on the SharePoint system, deleting any previous list with the same name. | | *Create If Not Exists & Merge* | Merges data with an existing list or creates a new list, if none with the specified name is found. | | *Truncate & Insert* | Creates a new list on the SharePoint system, deleting any previous list with the same name. | | *Merge only*\* | Merges rows, without deleting rows or the list itself. |

#### Max. threads

Sets the number of threads for communication with the SharePoint server. It is recommended to use the default value *2*.

Note

Increasing the thread number may increase the upload speed, depending on the server and network setup. Setting the value to *1* slows down the speed significantly, which may help with a heavy load of the SharePoint server or when connection problems occur.

This page shows how to set up and use the Snowflake destination. The Snowflake destination loads data into a Snowflake environment.

## Requirements

The connection to the Snowflake target environment is made via the ODBC driver for Windows 64-bit architectures. No additional installations are required to use the Snowflake destination.

- Download and install the [SnowflakeDSIIDriver](https://sfc-repo.snowflakecomputing.com/odbc/win64/latest/index.html) on the machines that run the Xtract Universal server and the Xtract Universal Designer.
- To connect through a proxy server, configure the following environment variables: `http_proxy`, `https_proxy`, `no_proxy`.\
  For more information, see [Snowflake: ODBC Configuration and Connection Parameters](https://docs.snowflake.com/en/user-guide/odbc-parameters.html#using-environment-variables).
- The ODBC default port (443) for HTTPS is enabled and allows outbound traffic from the network to Snowflake's services.
- The Snowflake account used to upload data to Snowflake needs the corresponding access and role privileges, see [Snowflake Documentation: Overview of Access Control - Roles](https://docs.snowflake.com/en/user-guide/security-access-control-overview#roles). The following privileges are required:
  - PUT command
  - COPY command
  - TABLE

The Snowflake destination only supports [Snowflake managed data stages](https://docs.snowflake.com/en/user-guide/data-load-local-file-system). To write SAP data to external stages in Snowflake, refer to the [Azure Storage](../azure-storage/), [AWS S3](../amazon-aws-s3/) or [Google Cloud](../google-cloud-storage/) destination.

## Create a new Snowflake Destination

Follow the steps below to add a new Snowflake destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Snowflake* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

### General

#### Output directory

Enter an existing local directory in which the extracted data is written as a csv file.\
Process during an extraction:

- A csv file is created in the output directory.
- Once the file has reached a certain size, it is zipped (gzip), see [File Splitting](#file-splitting).
- The zipped file is uploaded via PUT command to the Snowflake staging area.
- The data is then copied via COPY command to the target table in Snowflake.

This process (gzip + PUT command) repeats itself until the extraction is finished. While running an extraction, the csv files in the local directory and the staging area are deleted.

#### Account Name

Enter the Snowflake authentication account.\
The account name can be derived from the connection URL.

- URL for organization account identifier:\
  `https://[organization]-[account_name].snowflakecomputing.com/console#/`
- URL for region account identifier (legacy):\
  `https://[account_name].[region].[cloud].snowflakecomputing.com/console#/`

#### Database

Enter the name of the Snowflake database.

#### Schema

Enter the schema of the Snowflake database.

#### Role

Enter a Snowflake user role. If no user role is entered, the default user role is used for the connection.

### Account Identifier

#### Organization (preferred)

Enter the name of your organization. Authentication via the account name in your organization is the preferred way of identifying youself against Snowflake, see [Snowflake Documentation: Account Name in Your Organization](https://docs.snowflake.com/en/user-guide/admin-account-identifier.html#format-1-preferred-account-name-in-your-organization)

#### Region (legacy)

Select the region of the Snowflake environment from the drop-down-menu. The selected region must match the information in the assigned account.

Note

Regions with the suffix *(legacy)* are deprecated. Select the *(legacy)* option if you connect to Snowflake using an old *Cloud Region ID*. For more information on the current *Cloud Region IDs*, see [Snowflake Documentation: Supported Cloud Regions](https://docs.snowflake.com/en/user-guide/intro-regions.html).

### Authentication

#### Username

Enter the Snowflake authentication user name.

#### Basic

If this option is active, basic authentication is used for authentication. Enter the Snowflake authentication password of the user in the field **Password**.

Warning

**Deprecation of single-factor password sign-ins.**\
Snowflake announced the deprecation of single-factor password sign-ins to enforce more secure authentication. This affects the basic authentication option in Xtract Universal. To comply with the new authentication policy, we recommend using Key Pair authentication. Snowflake will enforce the new authentication policy around June to October 2025. For more information, refer to the [Snowflake Documentation: Deprecation of single-factor password sign-ins](https://docs.snowflake.com/en/user-guide/security-mfa-rollout).

#### Key Pair

If this option is active, key pairs are used for authentication, see [Snowflake Documentation: Key Pair Authentication & Key Pair Rotation](https://docs.snowflake.com/en/user-guide/key-pair-auth). Select the path to your private key in the field **Private Key Path**. Both encrypted and non-encrypted private keys are supported. If you use encrypted private key for authentication, enter the password that is used by snowflake to decrypt it in the field **Key password**.

### Stages

Click **[Test Connection]** to fetch all stages and warehouses from Snowflake. The stages and warehouses then become available for selection.

#### Stage name

Select a Snowflake Stage. Note that only Snowflake managed data stages are displayed.

#### Warehouse

Select a Snowflake Data Warehouse.

## Assign the Snowflake Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Snowflake destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### File Splitting

Writes extraction data of a single extraction to multiple files. Each filename is appended by *\_part[nnn]*.

#### Max. file size

The value set in *Max. file size* determines the maximum size of each file.

Note

The option *Max. file size* does not apply to gzip files. The size of a gzipped file cannot be determined in advance.

### Preparation

Defines the action on the target database before the data is inserted into the target table.

| Option | Description | | --- | --- | | *None* | No action. | | *Drop & Create* | Remove table if available and create new table (default). | | *Truncate Or Create* | Empty table if available, otherwise create. | | *Truncate* | Empty table if available. | | *Create If Not Exists* | Create table if not available. | | *Custom SQL* | Here you can define your own script, see [Custom SQL Statements](#custom-sql-statements). |

To only create the table in the first step and not insert any data, you have two options:

1. Copy the SQL statement and execute it directly on the target data database.
1. Select the *None* option for **Row Processing** and execute the extraction.

Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.

### Row Processing

Defines how the data is inserted into the target table.

| Option | Description | | --- | --- | | *None* | No action. | | *Copy file to table* | Insert records (default). | | *Merge File to table* | Insert records into the staging table. | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Finalization

Defines the action on the target database after the data has been successfully inserted into the target table.

| Option | Description | | --- | --- | | *None* | No action (default). | | *Custom SQL* | Define your own script, see [Custom SQL Statements](#custom-sql-statements). |

### Transaction style

Note

The available options for Transaction Style vary depending on the destination.

#### One Transaction

*Preparation*, *Row Processing* and *Finalization* are all performed in a single transaction.

- Advantage: clean rollback of all changes.
- Disadvantage: possibly extensive locking during the entire extraction period.

Recommendation

Only use *One Transaction* in combination with DML commands, e.g., "truncate table" and "insert. Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command. Example: If a table is created in the preparation step, the opened "OneTransaction" is committed and a rollback in the next steps is not performed correctly. For more information, see [Snowflake Documentation: DDL Statements](https://docs.snowflake.com/en/sql-reference/transactions#label-transactions-ddl).

#### RowProcessingOnly

Only *Row Processing* is executed in a transaction. *Preparation* and *Finalization* without an explicit transaction (implicit commits).

- Advantage: DDL in 'Preparation *and* Finalization\* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)
- Disadvantage: no rollback of *Preparation/Finalization*.

#### No Transaction

No explicit transactions.

- Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.
- Disadvantage: no rollback

### Empty Values

Warning

**NULL result in a non-nullable column.**\
By default empty strings are converted to NULL values when uploading data to Snowflake. To deactivate the conversion, disable **Replace empty values with SQL NULL**.

This option controls the Snowflake file format parameter EMPTY_FIELD_AS_NULL. When **Replace empty values with SQL NULL** is active, empty strings are converted to NULL values when uploading data to Snowflake.

### Append Extraction Timestamp

When **Add timestamp column to result set** is active, an additional column *ExtractionTimestamp* is added to the output of the extraction. The timestamp uses the format `yyyy-MM-dd'T'HH:mm:ss.SSS'Z'` (UTC).

## Merge Data

The following example depicts the update of the existing data records in a database by running an extraction to merge data. In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP.

With a merge command, the updated value is written to the destination database table. The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.

Tip

Alternatively to merging, the data can be also updated by means of full load. The full load method is less efficient.

### Prerequisites

You need a table with existing SAP data, in which new data can be merged.\
Ideally, the table with existing data is created in the initial load with the corresponding **Preparation** option and filled with data with the **Row Processing** option *Insert*.

After the table is created, open SAP and change a field value in the SAP table that is used for the data merge.

Warning

**Faulty merge.**\
Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the [General Settings](../../table/general-settings/#primary-key-tab) of the extraction type to execute the merge command.

### Merge Command

The merge process is performed using a staging table and takes place in three steps:

1. A temporary table is created.
1. The data is inserted in the temporary table.
1. The temporary table is merged with the target table and then the temporary table is deleted.

Follow the steps below to set up the merge process in Xtract Universal:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions.
1. Click **[Destination]**. The window "Destination Settings" opens.
1. Make sure to assign Snowflake destination to the extraction.
1. Apply the following destination settings:
1. Click **[OK]** and [run the extraction](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

More information about the updated fields can be found in the SQL statement.\
It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update.\
Fields that do not appear in the SQL statement are not affected by changes.

## Custom SQL Statements

The Snowflake destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the Snowflake destination:

1. In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..

1. Click **[Destination]**, the window "Destination Settings" opens.

1. Make sure to assign the Snowflake destination to the extraction.

1. Select the option *Custom SQL* from the drop-down list in one of the following sections:

   - [Preparation](#preparation)
   - [Row Processing](#row-processing)
   - [Finalization](#finalization)

1. Click **[Edit SQL]** . The window "Edit SQL" opens.

1. Enter your custom SQL statement and click **[OK]** to confirm your input.

### Use Templates

Existing SQL commands can be used as templates.\
You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:

- [Preparation](#preparation), e.g., in *Drop & Create* or *Create if Not Exists*
- [Row Processing](#row-processing), e.g., in *Insert* or *Merge*
- [Finalization](#finalization)

Follow the steps below to generate a Custom SQL command from a template:

1. In one of the staging steps, select the *Custom SQL* option from the drop-down list .
1. Click **[Edit SQL]** . The dialogue "Edit SQL" opens.
1. Navigate to the drop-down menu and select an existing command .
1. Click **[Generate Statement]**. A new statement is generated.
1. Click **[Copy]** to copy the statement to the clipboard.
1. Click **[OK]** to confirm your input.

Check out the [Microsoft SQL Server example](../microsoft-sql-server/#custom-sql-statements) for details on predefined expressions.

Note

The custom SQL code is used for SQL Server destinations. A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.

### Use Script Expressions

You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported:

| Input | Description | | --- | --- | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.TableName }#` | Name of the database table extracted data is written to. | | `#{Extraction.RowsCount }#` | Count of the extracted rows. | | `#{Extraction.RunState}#` | Status of the extraction (Running, FinishedNoErrors, FinishedErrors, Cancelled). | | `#{(int)Extraction.RunState}#` | Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors, 6 = Cancelled). | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. |

For more information, see [Script Expressions](../../parameters/script-expressions/).

```sql
#{
   iif
   (
      ExistsTable("MAKT"),
      "TRUNCATE TABLE \"MAKT\";",
      "
         CREATE TABLE \"MAKT\"(
            \"MATNR\" VARCHAR(18),
            \"SPRAS\" VARCHAR(2),
            \"MAKTX\" VARCHAR(40));
      "
   )
}#

```

### Create a Status Overview

The table "ExtractionStatistics" provides an overview and status of the executed Xtract Universal extractions. To create the "ExtractionStatistics" table, create an SQL table according to the following example:

```sql
CREATE TABLE [dbo].[ExtractionStatistics](
    [TableName] [nchar](50) NULL,
    [RowsCount] [int] NULL,
    [Timestamp] [nchar](50) NULL,
    [RunState] [nchar](50) NULL
) ON [PRIMARY]
GO

```

The *ExtractionStatistics* table is filled in the **Finalization** process step, using the following SQL statement:

```sql
INSERT INTO [ExtractionStatistics]
(
     [TableName], 
     [RowsCount], 
     [Timestamp],
     [RunState]
)
VALUES
(
     '#{Extraction.TableName}#', 
     '#{Extraction.RowsCount}#', 
     '#{Extraction.Timestamp}#',
     '#{Extraction.RunState}#'
);

```

### Custom SQL Example

In the depicted example, the DataSource *0FI_AP_4* is extended by one column that is filled with the user-defined runtime parameter *RUNTIMEPARAMETER*. The filling of the new column is implemented dynamically in the **Finalization** section of the destination settings.

1. In Snowflake, deselect the **Error on Column Count Mismatch** option in the *XTRACT_UNIVERSAL* File Format.

1. Open the extraction and click **Edit runtime parameter** to create the runtime parameter *RUNTIMEPARAMETER* .

1. Choose a field in the sections *Fields* and click **Edit** to add a selection criterion that uses the runtime parameter .

1. Close the extraction and open the [Destination Settings](#assign-the-snowflake-destination-to-an-extraction) of the extraction.

1. Make sure to assign the Snowflake destination to the extraction.

1. In the staging step **Preparation**, select the option *Custom SQL* from the drop-down list and click **[Edit SQL]**. The window "Edit SQL" opens.

1. In the drop-down menu, select the option *Drop & Create* and click **[Generate Statement]** to [use the template](#use-templates) for *Drop & Create*.

1. Add the following line in the generated statement:

   ```sql
   "RUNTIMEPARAMETER" VARCHAR(4),

   ```

1. Click **[OK]** to confirm your input.

1. In the staging step **Row Processing**, select the option *Copy file to table*.\
   At this point, no data from the SAP source system but `NULL` values are written to the newly created column *RUNTIMEPARAMETER*.

1. In the staging step **Finalization**, the `NULL` values can be filled by a custom SQL statement. Select the option *Custom SQL* from the drop-down list and click **[Edit SQL]**. The window "Edit SQL" opens.

1. Paste the following SQl statement into the editor:

   ```sql
   UPDATE "0FI_AP_4"
   SET RUNTIMEPARAMETER= '@RUNTIMEPARAMETER'
   WHERE RUNTIMEPARAMETER IS NULL;

   ```

   The `NULL` values are filled with the runtime parameter *RUNTIMEPARAMETER* and written into the SQL target table using the T-SQL command `UPDATE`.

1. Click **[OK]** to confirm your input and close the destination settings.

1. Open the "Run Extraction" window of the Designer and enter a value for the runtime parameter , before running the extraction .

Check the existence of the new column *RUNTIMEPARAMETER* in the Snowflake Console of the table *0FI_AP_4*.

______________________________________________________________________

## Related Links

- [Requirements: .NET-Framework](../../setup/requirements/#other-applications-and-frameworks)
- [Requirements: 64-bit Architecture](../../setup/requirements/#hardware)
- [Snowflake Documentation: Installing and Configuring the ODBC Driver for Windows](https://docs.snowflake.com/en/user-guide/odbc-windows.html)
- [Snowflake Documentation: Snowflake Identifiers](https://docs.snowflake.com/en/sql-reference/identifiers-syntax.html#double-quoted-identifiers)
- [Knowledge Base Article: Integrate SAP Data into a Snowflake Data Warehouse](../../../knowledge-base/integrate_sap_data_into_a_Snowflake_data_warehouse/)
- [Knowledge Base Article: SAP Integration with Matillion Data Loader](../../../knowledge-base/create-a-custom-cennector-in-matillion-data-loader/)
- [Extraction Parameters](../../parameters/extraction-parameters/#custom)

This page shows how to set up and use the Tableau destination. The Tableau destination loads data into Tableau Analytics Platform.

The Tableau destination enables you to save data extracted from SAP as Hyper files. It is also possible to upload the Hyper file to Tableau Server or Tableau Online.

## Requirements

- Tableau Export Library
- Visual C++ 2015 Runtime

[Download the Tableau Export Library and Visual C++ 2015 Runtime](https://s3.eu-central-1.amazonaws.com/cdn-files.theobald-software.com/download/XtractUniversal/tableau.zip)

If no Visual C++ 2015 Runtime is installed on your machine, run the vc_redist.x64.exe to install the Visual C++ 2015 Runtime. Copy the `tableau` folder into your Xtract Universal directory so that the following folder structure is created: `C:\Program Files\XtractUniversal\tableau\hyper`.

## Create a new Tableau Destination

Follow the steps below to add a new Tableau destination to Xtract Universal:

1. In the main window of the Designer, navigate to **Server > Manage Destinations**. The window “Manage Destinations” opens.
1. Click **[Add]** to create a new destination. The window "Destination Details" opens.
1. Enter a **Name** for the destination.
1. Select the destination type *Tableau* from the drop-down menu. A list of connection details opens.
1. Fill out the [destination details](#destination-details) to connect to the destination.
1. Click **[OK]** to confirm your input.

The destination can now be assigned to extractions.

### Destination Details

The destination details define the connection to the destination.

### Output directory

Enter the path to a folder on the Xtract Universal Server where the generated Tableau files are stored.

Note

Make sure that the directory exists.

### Tableau server

#### Upload to Tableau Server

Option to upload the extracted file (as a data source) to Tableau Server or Tableau Cloud.

#### Delete local file after upload

Option to remove the local file after a successful upload.

#### Host

Enter the IP address or domain name of the remote server, starting with `http://` or `https://`. Examples:

- `https://my-site.tableau.com`
- `https://my-site.us-west-2.tableau.com`
- `http://192.168.1.1`
- `https://us-west-2b.online.tableau.com` (for Tableau Cloud)

#### PAT name

Enter the name of your [Personal Access Token (PAT)](https://help.tableau.com/current/api/rest_api/en-us/REST/rest_api_concepts_auth.htm#make-a-sign-in-request-with-a-personal-access-token).\
To extract data to Tableau Server the site role "Server Administrator" is required.

#### PAT secret

Enter a valid token secret that corresponds to your token name.

#### Site (for Tableau Cloud)

Enter the name of a Tableau Cloud site. This field is mandatory for connecting to the Tableau Cloud.

#### [Test Connection]

Check the database connection.

### Tableau server settings

#### Site

A drop-down list of the sites stored on the connected on-prem server. Selecting a site is mandatory for uploading files.

#### Project

Select the project in which the extracted data is published.

## Assign the Tableau Destination to an Extraction

Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:

1. In the main window of the Designer, select an extraction.
1. Click **[Destination]**. The window “Destination Settings” opens.
1. In the “Destination Settings” window, select your Tableau destination from the dropdown list.
1. Optional: edit the [destination settings](#destination-settings).
1. Click **[OK]** to confirm your input.

When running the extraction, the extracted SAP data is now written to the destination.

### Destination Settings

The destination settings only affect the extraction that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click **[Destination]**. The window "Destination Settings" opens.

### File Name

Determines the name of the target table. The following options are available:

| Option | Description | | --- | --- | | **Same as name of SAP object** | Copy the name of the SAP object. | | **Same as name of extraction** | Adopt the name of the extraction. | | **Fully qualified extraction name** | Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use the fully qualified extraction name when the same extraction name is used in multiple [extraction groups](../../organize-extractions/). | | **Custom** | Define a name of your choice. |

#### Append timestamp

Add the timestamp in the UTC format (\_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction

### Column Name Style

Defines the style of the column name. Following options are available:

| Option | Description | | --- | --- | | *Code* | The SAP technical column name is used as column name in the destination e.g., MAKTX. | | *PrefixedCode* | The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX | | *CodeAndText* | The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). | | *TextAndCode* | The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)\_MAKTX. | | *Text* | The SAP description is used as column name in the destination e.g., Material Description (Short Text). |

### Convert dates

Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01). Target data uses a real date data-type and not the string data-type to store dates.

#### Year 0

Converts the SAP date 00000000 to the entered value.

#### Year 9999

Converts the SAP date 9999XXXX to the entered value.

#### Invalid values

If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value. NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.

### Existing files

If a flat file by the same name already exists in the target directory, the following actions can be taken:

| Option | Description | | --- | --- | | **Replace file** | The export process overwrites existing files. | | **Append results** | The export process appends new data to an already existing file, see [Column Mapping](#column-mapping). | | **Abort extraction** | The process is aborted, if the file already exists. |

### Column Mapping

Activate **Column Mapping** when appending data to an existing file or entity that has different column names or a different number of columns.\
This can be the case when extracting data from two or more extractions into the same destination file, where the column names of the extraction and the destination file differ.

Note

The column names in the extraction and destination must be unique. If duplicated column names are found, an error message is displayed. The column names must be corrected, before column mapping can be used.

#### Requirements

When working with flat files, ensure that:

- the XU server and the Designer both have access to the destination file.
- the [output directory](#destination-details) and the [file name](#file-name) of the extraction match the destination file.
- the [Column Name Style](#column-name-style) of the extraction and destination file match.

#### Mapping

Follow the steps below to map data:

1. Select the option **Append results** in the subsection [Existing Files](#existing-files).

1. Activate the option **Column Mapping**.

1. Click **[Map]** to assign columns. The window "Column Mapping" opens.

   - *Destination Columns* displays the names of the columns that are available in the destination file or entity.
   - *Not Mapped* defines whether or not columns are mapped to the destination columns.
   - *Source Columns* defines which SAP column is mapped to a destination column.

1. Depending on the column names of the source and target file, follow the instructions below:

   1. If the column names of the extraction and the names of the destination columns match, click **[Auto map by name]**.
   1. If the column names do not match, assign columns manually by selecting the respective SAP column from the dropdown menu under *Source Columns*.
   1. If a column does not have a counterpart or is not supposed to be appended, activate the checkbox in the column *Not Mapped*.

1. Click **[OK]** to confirm your input.

When running the extraction the extracted data is added to the destination file or entity as specified in the column mapping.

Tip

In case an error message pops up, click **[Show more]** to see a description of what caused the error.

Note

Columns that are not mapped are filled with `NULL` values.

______________________________________________________________________

## Related Links

- [Webinar: Visualize your SAP data in Tableau](https://www.youtube.com/watch?v=X6T3NfVDhJE)
- [Knowledge Base Article: Link a BEx query with a Hierarchy in Tableau](../../../knowledge-base/link-bex-query-with-hierarchy/)

This page shows how to run extractions automatically and manually. Examples on how to call extractions:

- [Commandline](call-via-commandline/)
- [Webservice](../../web-api/)
- [Scheduler](call-via-scheduler/)
- [ETL-Tools](call-via-etl/)

### About the Execution of Extractions

Extractions are triggered by an HTTP request and executed on the [Xtract Universal server](../server/).

The configuration of [source](../sap-connection/), [destination](../destinations/) and [extraction type](../introduction/#extraction-types) defines how the data transfer is performed.\
This configuration can contain dynamic elements, like [extraction parameters](../parameters/extraction-parameters/) and [script expressions](../parameters/script-expressions/). Depending on the destination, the execution of an extraction can be triggered either interactively or unattended.

#### Interactive Extractions

Extractions are typically triggered interactively when a user requires new or updated data from SAP, and no additional data storage system (like a data warehouse) is present. In these scenarios, execution of an extraction is typically triggered by one of Xtract Universal's plugins (like [Alteryx](../destinations/alteryx/), [Power BI Connector](../destinations/Power-BI-Connector/) or [Power BI Report Server](../destinations/server-report-services/)) or directly by the target environment ([QlikSense & QlikView](../destinations/qliksense-qlikview/)).

#### Unattended Extractions

When an additional data storage system (database, cloud storage, flat files) is present, extractions are typically triggered as part of an ELT-process (Extract, Load, Transform), which is run unattended at regular intervals by a scheduler or other orchestration software. In these scenarios, execution of an extractions is typically triggered by running the [XU command line tool](call-via-commandline/) from the orchestration software.

For advanced scenarios or environments that do not support commandline programs, the [HTTP Webservices](../../web-api/) can be used for triggering and monitoring executions.

Note

Xtract Universal does not have its own scheduler. You can use third party schedulers.

### Run parallel Extractions

The amount of possible parallel extractions depends on the hardware resources of the Windows server.

Every triggered extraction is executed in a separate process of the operating system. Reliability and throughput of the network connection, available RAM and disk throughput (for logging and caching) are all crucial factors for the parallelization. Other factors are the performance of the SAP source system the destination.

Note

Xtract Universal scales corresponding to the available hardware resources of the runtime environment.

### Automate the Creation of Extractions

As of Xtract Universal Version 4.26.1, the command line tool xu-config.exe is available in the installation directory of Xtract Universal, e.g. `C:\Program Files\XtractUniversal\xu-config.exe`. The tool creates extractions, sources and destinations outside of the Xtract Universal Designer.

For more information, see [Knowledge Base Article: Create Extractions via Commandline](../../knowledge-base/config-command-line-tool/).

______________________________________________________________________

#### Related Links

- [Knowledge Base Article: Call Extractions via Script](../../knowledge-base/call-extraction-via-script/)
- [Knowledge Base Article: Execute & Schedule all Extractions using an SSIS Package](../../knowledge-base/execute_all_defined_xu_extractions/)

Extractions can be run via the following command line tools:

| Command Line Tool | Operating System | Directory | | --- | --- | --- | | `xu.exe` | Windows | The command line tool is available in the default directory `C:\Program Files\XtractUniversal\xu.exe`. | | `xu.elf` | Unix-, Linux environment | [Download-Link](https://cdn-files.theobald-software.com/download/XtractUniversal/xu.elf.tar.gz) for the Linux version of the commandline tool. |

Note

Both command line tools do not differ in functionality and can be copied and run on any computer.\
Make sure that the host of the Xtract Universal server is accessible in the network.

### Call Extractions

The command line tool connects to an Xtract Universal server (service) and starts an extraction with the following runtime parameters:

| Runtime Parameters | Description | Syntax | | --- | --- | --- | | -h | A short documentation of the command line tool | `C:\Program Files\XtractUniversal>xu.exe -h` | | -n | Name of the extraction | `C:\Program Files\XtractUniversal\xu.exe -n <name>` | | -s | The name or IP address of the machine on which the Xtract Universal service runs. The default is localhost. You can find the current value in the "Run" window of the Designer. | `xu.exe -n <name> -s <host>` | | -p | The port on which the Xtract Universal service runs. The default is 8065. You can find the current value in the "Run" window of the Designer. | `xu.exe -n <name> -s <host> -p <port>` | | -o | Parameters to be set when running the extraction. Multiple parameters can be set. | `xu.exe -n <name> -s <host> -p <port> -o "param1=<wert1>" -o "param2=<wert2>"` | | -e | Use [Transport Layer Security (TLS)](https://docs.microsoft.com/en-us/windows/win32/secauthn/transport-layer-security-protocol) (1.2 or higher). | `xu.exe -n <name> -s <host> -p <port> -e` | | -a | Cancel all running instances of the extraction. | `xu.exe -a` | | -c | Clear the result cache and options of the extraction. This only works with [Pull Destinations](../../destinations/). | `xu.exe -c` |

#### Examples

To run an extraction on the Xtract Universal server, call the command line tool as follows:

```console
    xu.exe -n MaterialText
    xu.exe -n MaterialText -s 10.0.0.42 -p 80 -o "rows=1000"
    xu.exe -n MaterialText -s xusrv.corp.local -p 443 -o "rows=1000" -o "SPRAS=D" -e
    xu.exe -n MaterialText -a
    xu.exe "http://localhost:8065/start/MaterialText/&rows=1000"

```

### Return Codes

When an operation is completed successfully, the program returns *0*. In case of an error, the program returns one of the following status codes:

| Return Code | Description | | --- | --- | | 404 | Extraction does not exist | | 1001 | An undefined error occurred | | 1002 | File could not be found | | 1013 | Invalid input data | | 1014 | Invalid number of arguments | | 1015 | Name of the parameter is unknown | | 1016 | Invalid argument | | 1040 | Timeout error: Waiting period for HTTP-answer of the command line tool is exceeded | | 1053 | The URL of the extraction is incorrect | | 1087 | Invalid parameter |

### Standard Output and Standard Error Output

The output depends on the destination type of the extraction. If the extraction call was successful, the output is *0*.

#### Standard Output of Pull Destinations

When using [Pull Destinations](../../destinations/) like HTTP-CSV, HTTP-JSON, etc., extracted data is written into the standard output (sdtout).

#### Standard Output of Push Destinations

When using [Push Destinations](../../destinations/) an [Extraction Log](../../logs/) in CSV format is written into the standard output (stdout).

#### Standard Error Output

Logs and error notifications are written into the standard error output (stderr).

### Basic Authentication via Commandline

The command line tool supports extractions with basic authentication.

When scheduling extractions by executing the command line tool, you can pass credentials for basic authentication as arguments. While the name of the Xtract Universal Custom User can be passed directly, the Custom User Password needs to be stored in a Base 64 encoded file that is accessible by the command line tool. The file format can be chosen freely, e.g. .txt, .json, .xml.

1. Create a password file with the following command:

   ```text
   xu.exe -f <path to the location and name of the file>

   ```

   Example: `xu.exe -f "C:\temp\[name of the password file]"`.\
   The Windows user needs access rights to the file location. You don't have to create this documents in advance.

1. Enter a valid password as requested by the cmd screen. It must consist of at least 8 characters.

1. Pass a user name and the path to the password file as arguments in the command line tool. Example:

   ```text
   xu.exe -s todd.theobald.local -p 8165 -e -n MSEG -u Alice -b "C:\temp\password_custom_user"

   ```

1. Optional: When scheduling an extraction, make sure that the user of the task has access rights to the password file.

For more information use the command `xu.exe -h`.

______________________________________________________________________

#### Related Links

- [Knowledge Base Article: Call via Script](../../../knowledge-base/call-extraction-via-script/)
- [Knowledge Base Article: Create Extractions via Commandline](../../../knowledge-base/config-command-line-tool/)
- [Knowledge Base Article: Execute & Schedule all Extractions using an SSIS Package](../../../knowledge-base/execute_all_defined_xu_extractions/)

This page offers a list of resources about using Xtract Universal with ETL tools.\
All ETL tools offer the following options for running extractions:

- [Call via command line](../call-via-commandline/)
- [Call via webservices](../../../web-api/)

### Integration via Plug-In

- [Xtract Universal Alteryx Plugin](../../destinations/alteryx/#requirements)
- [KNIME Integration via SAP Reader (Theobald Software)](../../destinations/knime/#knime-integration-via-sap-reader)
- [Dynamic Runtime Parameter within KNIME Workflow](../../../knowledge-base/dynamic-runtime-paramater-within-KNIME-workflow/)
- [Xtract Universal Power BI Custom Connector](../../destinations/Power-BI-Connector/)
- [Power BI Configure scheduled refresh](https://docs.microsoft.com/en-us/power-bi/connect-data/refresh-scheduled-refresh)

### Integration via Webservice

- [GCS Integration with Google Scheduler](https://cloud.google.com/scheduler/docs/creating)
- [Schedule AWS Lambda Functions](https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/RunLambdaSchedule.html)
- [Generate Qlik data load script](../../destinations/qliksense-qlikview/#generate-a-data-load-script)
- [Azure Data Factory (ADF) Integration using Webservices](../../../knowledge-base/adf-integration-using-webservices/)
- [SAP Integration with Matillion Data Loader and Xtract Universal](../../../knowledge-base/create-a-custom-cennector-in-matillion-data-loader/)

### Integration via Command Line

- [Azure Data Factory (ADF) Integration using Commandline](../../../knowledge-base/adf-integration-using-command-line/)

### Integration via Azure Data Factory

- [Azure Data Factory (ADF) Integration using Webservices](../../../knowledge-base/adf-integration-using-webservices/)
- [Azure Data Factory (ADF) Integration using Commandline](../../../knowledge-base/adf-integration-using-command-line/)
- [Call Dynamic Extractions with Variables in Azure Data Factory](../../../knowledge-base/call-dynamic-extractions-with-variables-in-adf/)
- [Run an ADF pipeline when an SAP extraction file is successfully uploaded to Azure storage](../../../knowledge-base/run-an-ADF-pipeline-when-sap-extraction-file-is-successfully-uploaded-to-Azure-storage/)

This page shows how to use the command line tool *xu.exe* to schedule extractions with third party scheduling tools. Windows Task Scheduler and the SQL Server Agent are used as examples for setting up extraction schedules.

### Call via Windows Task Scheduler

1. Open the Windows Task Scheduler by typing *Taskschd.msc* into the command line.

1. Create a new task by clicking **Actions > Create Task ...** .

1. Enter a name for the task and an optional description.

1. In the tab *Triggers* click **[New...]** to add a time option.

1. Set a start date and time and confirm the entry with **[OK]** .

1. State the start program in *Program / script* in the tab *Actions*. Add the parameters of the extraction in *Add arguments (optional)*. Example:

   ```text
   "C:\Program Files\XtractUniversal\xu.exe" -s todd.theobald.local -p 8065 -n SAPPlants

   ```

1. Click **[OK]** to confirm the input.

1. Check the summary and finish the setup.

The extraction is now scheduled and can be run by right-clicking the task and selecting the *Run* option.

Tip

Multiple extractions can be assigned to a single task. Edit the task and switch to the *Actions* tab. Create a new action as described above.

### Call via SQL Server Agent

Note

You must have all necessary authorization for creating and executing jobs with the SQL Server Agent.

1. Open the SQL-Server-Management-Studio (SSMS) to connect to an SQL-Server.

1. Create a new job via **SQL Server Agent > New > Job...**.

1. Navigate to *Select a page > Steps* and click **[New]**.

1. Enter a **Step name**, **Type**, **Run as** and **Command** , e.g., *xu*, *Operating system (CmdExec)*, *SQL-Server-Agent Service Account*. Example:

   ```text
   "C:\Program Files\XtractUniversal\xu.exe" -s localhost -p 8065 -n customers

   ```

1. Click *Select a page > Advanced* to set further options. Examples:

   - **On success action**, **On failure action**
   - **Retry attemps**, **Retry interval**
   - **Output file**

1. Click **[OK]** to confirm your input.

1. Find the new job in **SQL Server Agent > Jobs**.

1. Right-click the job and select **Start Job at Step...** to execute the job.

1. The successful execution of the SQL Server Agent job is displayed in SSMS.

______________________________________________________________________

#### Related Links

- [Microsoft Documentation: SQL Server Agent](https://docs.microsoft.com/en-us/sql/ssms/agent/sql-server-agent?view=sql-server-ver15)
- [Microsoft Documentation: Start Task Scheduler](http://technet.microsoft.com/en-us/library/cc721931.aspx)

Xtract Universal Designer offers a test run option for extractions.\
You can define runtime parameters and other options to run an extraction directly from the Xtract Universal Designer.

Note

When running extractions in the Designer, the executing user is always the Windows account that runs the Designer, not the login user. To run extractions under a different user, start the Xtract Universal Designer application as a different user (`Shift` + ).

## Run Extractions in the Designer

1. In the main window of the Designer, select an extraction and click **[Run]** . The window "Run Extraction" opens.
1. If needed, define [extraction parameters](../../parameters/extraction-parameters/):
   - Select the checkbox of the parameter you want to override . The parameter is automatically added to the extraction URL and the xu-exe string.
   - Enter a value for the parameter.
1. Click **[Run]** to execute the extraction.

The status in the subsection [General Info](#general-info) indicates if the extraction finished successfully and if the extracted data was written to the destination.

## Run Extraction Window

The "Run extraction" window consists the following subsections:

- [General Info](#general-info)
- [Runtime parameters](#runtime-parameters)
- [URL and commandline](#url-and-commandline)
- [Logs and Output](#logs-and-output)
- [Buttons](#buttons)

Tip

You can open the "Run extraction" window by right-clicking an extraction or use the main menu bar to open the "Run Extraction" window.

### General Info

| Info Object | Description | | --- | --- | | Extraction name | Name of the extraction | | Source | Information about the source settings chosen for that extraction (Name, Host, Client, User Name, Instance No., Language) | | Destination | Name of the destination (Name, Type, Pull destination info) | | Execution start | Start date and time stamp of extraction run | | Time elapsed | Elapsed time of the extraction run | | Rows extracted | Number of extracted rows | | Status | Extraction status | | Duration | Extraction duration |

### Runtime Parameters

The three tabs "Extraction", "Source" and "Custom" contain extraction parameter to dynamize extractions. When editing extraction parameters, the [URL and commandline](#url-and-commandline) of the extraction also change.

For more information, see [Extraction Parameters](../../parameters/extraction-parameters/).

### URL and Commandline

The strings displayed in **URL** and **xu.exe** are generated and update automatically when changing [extraction parameters](../../parameters/extraction-parameters/). Use the strings to run the extraction outside of the Xtract Universal Designer.

#### URL

The extraction URL can be used in different integration scenarios and use cases. Examples:

- Use the extraction URL when it is not possible to use the command line tool xu.exe, e.g. in cloud based environments.
- Use the extraction URL when the extraction is set to a [Pull Destination](../../destinations/).
- Use the extraction URL to run the extraction in a web browser, e.g., for testing purposes.

Copy the URL with `Ctrl`+`C` or use the **[]** button on the right.

Tip

You can run an extraction in the browser without opening the "Run Extraction" window by right-clicking an extraction and selecting the option **Run in browser**.

#### xu.exe

This command allows running an extraction with the command line tool [xu.exe](../call-via-commandline/), that is installed with Xtract Universal. The tool is located in the installation directory of Xtract Universal, e.g., `C:\Program Files\XtractUniversal\xu.exe`.

Copy the expression with `Ctrl`+`C` or use the **[]** button on the right.

It is recommended to use the command line tool with [Push Destinations](../../destinations/). It can be called from a [Windows script](../../../knowledge-base/call-extraction-via-script/) or any [scheduler](../call-via-scheduler/), that can invoke Windows commandline calls.\
In the most simple case, the Windows task scheduler can be used for calling and scheduling extractions using xu.exe.

By default the following parameters are generated for any extraction:

- "-s" (Server for extraction)
- "-p" (Listening port of the Xtract Universal Server)
- "-n" (Name of the extraction)

### Logs and Output

When an extraction is executed, information about the extraction is displayed in the following section.

#### Log

In the *Log* tab the extraction log is displayed in real time. Activate the checkbox **Auto scroll to the end** to automatically scroll down to the last protocol.

#### xu.exe

In the *xu.exe* tab the log of the command line tool is displayed.

#### Output

In the *Output* tab the results of the extraction are displayed. This option is only available for the following destinations:

- HTTP-CSV
- HTTP-JSON
- Alteryx
- Microsoft SQL Server
- Power BI Report Server (SQL Server Reporting Services)
- QlikSense&QlikView

**Display rows from line**\
The extraction results are displayed to a maximum amount of 500 rows. The number of rows to be displayed can be changed using the **Display rows from line** boxes.

**[Display]**\
To filter the results, enter filter values above the columns of the results and click **[Display]**.

**[Clear search]**\
To undo any data filtering, click **[Clear search]**, followed by **[Display]**.

### Buttons

**[Run]**\
Runs the extraction.

**[Abort]**\
Aborts the extraction.

**[Close]**\
Closes the "Run Extraction" window.

Note

If the option *Request SAP credentials from caller when running extractions* is active in the extraction's [source settings](../../sap-connection/settings/#authentication), you are prompted to enter your SAP credentials when running an extraction. For this option, extractions must be called via [HTTPS - unrestricted](../../server/server-settings/#web-server).

This page shows how to use the BW Hierarchy extraction type. The BW Hierarchy extraction type can be used to extract Hierarchies and InfoObjects from SAP BW systems.

Note

To extract hierarchies from a BW system, use the BW Hierarchy extraction type. To extract hierarchies from an ERP system like ECC or S4, use the [ODP](../odp/) extraction type.

### Prerequisites

- A connection to an SAP system is available, see [SAP Connection](../sap-connection/).
- The SAP user has sufficient user rights, see [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#bw-hierarchy).

Warning

**Missing Authorization.**\
To use the BW Hierarchy extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust [SAP Authority Objects](/xtract-universal/documentation/setup-in-sap/sap-authority-objects#bw-hierarchy) accordingly.

### Create a BW Hierarchy Extraction

1. In the main window of the Designer, click **[New]** to create a new extraction. The window "Create Extraction" opens.
1. Select an [SAP Connection](../sap-connection/) of type **RFC** from the drop-down menu **Source**.
1. Enter a unique name for your extraction.
1. Select the extraction type **BW Hierarchy**.
1. Click **[OK]**. The main window of the extraction type opens automatically.

The majority of the functions of the extraction type can be accessed in the main window.

### Look Up a Hierarchy

1. In the main window of the component, click **[]**. The window “Hierarchy Lookup” opens.
1. Enter the name of a Hierarchy in the field **Hierarchy Name** or the name of an InfoObject in the field **InfoObject** . Use wildcards (\*) if needed.
1. Click **[]**. Search results are displayed.
1. Select a Hierarchy and click **[Select]**.

The application returns to the main window of the extraction type.

### Define the BW Hierarchy Extraction Type

The BW Hierarchy extraction type offers the following options for Hierarchy extractions:

1. Define the output format of the Hierarchy in the [extraction settings](settings/#extraction-settings) . The BW Hierarchy extraction type supports the following output formats:

   - [ParentChild Format](output-format/#parentchild-format)
   - [Natural Format](output-format/#natural-format)
   - [ParentChildWithNodeNames Format](output-format/#parentchildwithnodenames-format)

1. Click **[Load live preview]** to display a live preview of the first 100 records.

1. Check the [General Settings](general-settings/) before running the extraction..

1. Optional: The default value for **Date To** is *99991231*. To change the value, override the *dateTo* [extraction parameter](../parameters/extraction-parameters/) when calling the extraction.

1. Click **[OK]** to save the extraction type.

You can now run the extraction, see [Execute and Automate Extractions](../execute-and-automate/).

______________________________________________________________________

#### Related Links

- [Extraction Settings](settings/)
- [SAP Help: Uploading Hierarchies from Flat Files](https://help.sap.com/saphelp_scm700_ehp02/helpdata/en/fa/e92637c2cbf357e10000009b38f936/frameset.htm)

This page contains an overview of the settings in the window "General Settings".\
To open the general settings, click **General Settings** in the main window of the extraction type.

### Misc. Tab

The *Misc.* tab covers cache settings, column encryption and keywords of an extraction type.

#### Cache results

The *Cache results* option is only available in [pull destinations](../../destinations/), e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times. To decrease the SAP server load, you can select the **Cache results** option, this way the pull destination pulls the data from cache and not from the SAP.

Cache results increases the performance and limits the impact on the SAP system. If this behavior is undesirable (for example, because the data must be always 100% up-to-date), the cache option must be explicitly disabled.

#### Keywords

One or more keywords (tags) can be assigned to an extraction. Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the "Search Extractions" window. To open the "Search Extractions" window, click **[ Search]** in the main window of the Designer.

Tip

To add keywords to multiple extractions at once, select the extractions in the main window of the Designer.\
Right-click + **Add/Remove keywords** opens the window "Add/Remove Keywords To/From Multiple Extractions".

### Primary Key Tab

Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.

The depicted example shows the SAP object *MAKT* with its primary key inherited from SAP in the general settings of the Designer. In this example the primary key consists of *MANDT*, *MATNR* and *SPRAS*. The primary key is also taken over in the destination.

Note

A defined primary key field in a table is a prerequisite for merging data.

#### Generate Surrogate Key Column

If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs. The surrogate keys are hash values of type signed 8 byte integer, e.g., `#-3008591679982390000`. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.

### Security Tab

Restrict user access to the extraction. For more information, see [Restrict Designer Access](../../access-restrictions/restrict-designer-access/).

### Columns Order Tab

The "Columns Order" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns. Index 0 defines the first column in the result set, index 1 the second columns, etc.

| Option | Description | | --- | --- | | **Enabled** | When this option is active, the defined column order is applied when running the extraction. | | **Swap** | Swaps the index of 2 columns. All other columns keep their indexes. | | **Insert** | Inserts the selected column into the selected index. All other indexes are recalculated. | | **Reset default** | Restores the original column order. |

This page shows the which output formats are supported by the BW Hierarchy extraction type. The output format can be selected in the [extraction settings](../settings/) of the BW Hierarchy extraction type.

### ParentChild Format

The default output of the BW Hierarchy extraction type contains the following columns:

| Column | Description | | --- | --- | | **NodeID** | Unique node key. | | **ParentNodeID** | Key for parent node. | | **FirstChildNodeID** | Key for first child node. | | **NextNodeID** | Key for next node in the same hierarchical level. | | **InfoObjectName** | Name of InfoObject behind the corresponding node. | | **NodeName** | The node’s (technical) name. | | **NodeText** | The description text of the node. This column is only created when [**Fetch description texts**](../settings/#fetch-description-texts) is active. | | **DateFrom** | Date from which the node is valid. | | **DateTo** | Date the node is valid to. | | **Link** | If the value in *Link* is greater than 0, the node is a link node. The ID of the original node that was linked from is displayed. | | **Row** | Number of the row. The row number can be used as an ID or sort criterion when processing the extracted data. |

Example:

### Natural Format

| Column | Description | | --- | --- | | **LevelN** | Technical name of the node of the nth level. The number of levels is set in [**Level Count**](../settings/#level-count). The level count starts at level 0. | | **LevelTextN** | The description text of the nth level's node. This column is only created when [**Description texts for levels**](../settings/#description-texts-for-levels) is active. | | **InfoObjectName** | Name of InfoObject behind the node of the highest level. | | **NodeName** | Technical name of the node of the highest level. | | **NodeText** | The description text of the node of the highest level. This column is only created when [**Fetch description texts**](../settings/#fetch-description-texts) is active. | | **DateFrom** | Date from which the node is valid | | **DateTo** | Date the node is valid to. | | **Link** | If the value in *Link* is greater than 0, the node is a link node. The ID of the original node that was linked from is displayed. | | **Row** | Number of the row. The row number can be used as an ID or sort criterion when processing the extracted data. |

Example:

### ParentChildWithNodeNames Format

| Column | Description | | --- | --- | | **NodeID** | Unique node key. | | **NodeName** | The node’s (technical) name. | | **NodeText** | The description text of the node. This column is only created when [**Fetch description texts**](../settings/#fetch-description-texts) is active. | | **ParentNodeID** | Key for parent node. | | **ParentNodeName** | Name of the parent node. | | **InfoObjectName** | Name of InfoObject behind the corresponding node. | | **DateFrom** | Date from which the node is valid. | | **DateTo** | Date the node is valid to. | | **Link** | If the value in *Link* is greater than 0, the node is a link node. The ID of the original node that was linked from is displayed. | | **Row** | Number of the row. The row number can be used as an ID or sort criterion when processing the extracted data. |

Example:

This page contains an overview of the extraction settings in the BW Hierarchy extraction type.\
To open the extraction settings, click ****Extraction Settings**** in the main window of the extraction type.

### Extraction Settings

#### Representation

- *ParentChild*: The Hierarchy is represented in the SAP parent-child format, see [Output Formats: ParentChild](../output-format/#parentchild-format). Example:
- *Natural*: The SAP parent-child Hierarchy is transformed into a regular hierarchy, see [Output Formats: Natural](../output-format/#natural-format). Example:
- *ParentChildWithNodeNames*: The Hierarchy is represented in a reduced SAP parent-child format that only includes single nodes and their parent, see [Output Formats: ParentChildWithNodeNames](../output-format/#parentchildwithnodenames-format). Example:

#### Remove Leading Zeros

If this option is active, all leading zeros in the column *NodeName* of the leaves are removed. *NodeName* can then be used in a JOIN-condition with the corresponding Dimension-Key of a BW Cube extraction.\
The conversion works for compound InfoObjects, too. Example: 0CO_AREA (1000) and 0COSTCENTER (0000003100) become 1000/3100.

#### Fetch description texts

Sets the node text in the column *NodeText*.\
All texts of InfoObjects that have language dependent texts will be retrieved in the language that the SAP source connection uses. If no texts exist, the result does not contain any texts for entries of that InfoObject.

### Natural Settings

Note

The subsection *Natural Settings* is only active, when the **Representation** is set to *Natural*.

#### Level Count

Defines the maximum number of levels. The following example shows a Hierarchy with four levels.

#### Fill empty levels

Copies the bottom element of the Hierarchy until the last level. The following example depicts the previously shown Hierarchy with the activated *Repeat Leaves* option.

#### Description texts for levels

Sets the output field *LevelTextN* for each field *LevelN* containing the text based on the system language settings.

#### Leaves only

Returns only the leaves as data records.

### Debug

#### Enable Debug Logging

Adds more detailed logs for the BW Hierarchy extraction type to the extraction logs. Activate **Enable Debug Logging** only when necessary, e.g., upon request of the support team.

______________________________________________________________________

#### Related Links

- [SAP Help: About SAP BW Hierarchies](https://help.sap.com/saphelp_scm41/helpdata/en/90/fd36709c6411d5b4000050dadfb23f/content.htm?no_cache=true)

This page shows how to use the OData extraction type.\
The OData extraction type can be used to extract data provided by SAP OData services.

Note

OData extraction type ist still in **beta** phase. This means, the OData extraction type is subject to change. Extractions that use the OData extraction type can stop working with future updates and might require manual changes.

### About OData Services

The OData extraction type in Xtract Universal consumes the data provided by SAP OData services. The OData (Open Data) protocol defines a set of rules to create, edit and consume resources through RESTful interfaces (HTTP requests). Furthermore, the OData protocol describes the data and the data model, meaning the data has its respective metadata, including the following items:

- Column name
- Description
- Length
- Data type

OData services can be build in SAP on-premise systems using the [SAP Gateway Service Builder](https://help.sap.com/docs/SAP_NETWEAVER_AS_ABAP_751_IP/68bf513362174d54b58cddec28794093/cddd22512c312314e10000000a44176d.html). On SAP cloud systems, the OData services are accessible via [Communication Arrangements](https://learning.sap.com/learning-journeys/implement-sap-s-4hana-cloud-public-edition-for-sourcing-and-procurement/setting-up-communication-management_a913171c-c96d-47a9-81ec-dc9ee8754320).

The OData extraction type supports [OData V2](https://www.odata.org/documentation/odata-version-2-0/overview/) and [OData V4](https://www.odata.org/documentation/).

Tip

For information on how to create OData services that provide ODP based data, refer to the [Knowledge Base Article: Create OData Services using the SAP Gateway Builder](../../knowledge-base/create-odata-services-using-the-sap-gateway-builder/).

### Prerequisites

- Enable OData services in SAP, see [SAP Help: How to Enable OData Services in SAP S/4HANA](https://help.sap.com/docs/advanced-financial-closing/administration/how-to-enable-odata-services-in-sap-s4hana). For more information on how to build OData services, see [SAP Learning: Building OData Services with SAP Gateway](https://learning.sap.com/learning-journeys/building-odata-services-with-sap-gateway).

- Create an SAP connection of [type OData](../sap-connection/settings/#source-type-odata).

- Enable the fllowing Communication Scenarios:

  - General communication integration:
    - SAP_COM_0A06 (Communication System Read Integration)
    - SAP_COM_0A07 (Communication Arrangement Read Integration)
    - SAP_COM_0A48 (Communication Management Integration)
  - Metadata integration:
    - SAP_COM_0181 (OData Metadata Integration)
  - To extract CDS data:
    - SAP_COM_0531 (SAP Datasphere - ABAP CDS Extraction - OData Integration)
    - SAP_COM_0532 (SAP Datasphere - ABAP CDS Extraction - WebSocket Integration)

- Expose SAP OData services to 3rd party systems using Communication Arrangements, see [SAP Learning: Setting up Communication Management](https://learning.sap.com/learning-journeys/implement-sap-s-4hana-cloud-public-edition-for-sourcing-and-procurement/setting-up-communication-management_a913171c-c96d-47a9-81ec-dc9ee8754320). For more information on predefined OData services in the SAPS/4HANA Public Cloud, see [OData V4 List](https://api.sap.com/products/SAPS4HANACloud/apis/ODATAV4) and [OData V2 List](https://api.sap.com/products/SAPS4HANACloud/apis/ODATA).

- Create an SAP connection of [type OData](../sap-connection/settings/#source-type-odata).

### Create an OData Extraction

1. In the main window of the Designer, click **[New]** to create a new extraction. The window "Create Extraction" opens.
1. Select an [SAP Connection](../sap-connection/) of type **Odata** from the drop-down menu **Source**.
1. Enter a unique name for your extraction.
1. Select the extraction type **OData**. The **OData** extraction type is only available if the SAP source connection uses the [OData protocol](../sap-connection/settings/#source-type-odata).
1. Click **[OK]**. The main window of the extraction type opens automatically.

The majority of the functions of the extraction type can be accessed in the main window.

### Look up OData Services

1. In the main window of the extraction type, click **[]**. The window “Service Selection” opens.
1. In the field **Search pattern**, enter the technical service name of an OData V2 service or the service ID of an OData V4 service . Use wildcards (\*), if needed.
1. Click **[Search]**. Search results are displayed.
1. Click **[]** to select a service . The SAP objects that are available via the service are displayed in the right screen of the menu.
1. Select a service entity and click **[OK]** to confirm.

The application now returns to the main window of the extraction type.

### Define the OData Extraction Type

The OData extraction type offers the following options for data extractions:

1. In the section *Service Fields*, select the items to extract.
1. Click **[Load Preview]** to display a live preview of the first 100 records.
1. Optional: edit selections to apply (dynamic) filters. For more information, see [Edit Selections](selections/#edit-selections).
1. Check the [Extraction Settings](settings/) and the [General Settings](general-settings/) before running the extraction.
1. Click **[OK]** to save the extraction type.

You can now run the extraction, see [Execute and Automate Extractions](../execute-and-automate/).

Note

The OData data types **DateTime** and **DateTimeOffset** (original format: *yyyy-mm-ddThh:mm\[:ss[.fffffff]\]*) are truncated to the following format: *yyyyMMdd*.

Runtime parameters are are placeholders for values that are passed at runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom). They can be created in context of [Selections](../selections/).

## Create Runtime Parameters

The OData extraction type supports the use of [scalar parameters](#scalar-parameters) that represent single values.

### Scalar Parameters

Follow the steps below to create a scalar runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add Scalar]** to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.

   | Type | Description | | --- | --- | | *Text* | Can be used for any type of SAP selection field. | | *Number* | Can be used for numeric SAP selection fields. | | *Flag* | Can only be used for SAP selection fields THAT require an ‘X’ (true) or a blank ‘‘ (false) as input value. |

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

## Assign Runtime Parameters

Follow the steps below to assign the runtime parameters to selections.

1. In the main window of the extraction type, click the **[Edit]** button next to the selection you want to parameterize. The window "Edit Selections" opens.
1. Add a filter to the selection, see [Selections and Filters](../selections/#edit-selections).
1. Click the icon button next to the input field to switch between static values () and runtime parameters (). If no icon button is available, [create a runtime parameter](#create-runtime-parameters).
1. Select a runtime parameter from the dropdown-list.
1. Click **[OK]** to confirm the input.

Pass values during runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom).

This page contains an overview of the settings in the window "General Settings".\
To open the general settings, click **General Settings** in the main window of the extraction type.

### Misc. Tab

The *Misc.* tab covers cache settings, column encryption and keywords of an extraction type.

#### Cache results

The *Cache results* option is only available in [pull destinations](../../destinations/), e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times. To decrease the SAP server load, you can select the **Cache results** option, this way the pull destination pulls the data from cache and not from the SAP.

Cache results increases the performance and limits the impact on the SAP system. If this behavior is undesirable (for example, because the data must be always 100% up-to-date), the cache option must be explicitly disabled.

#### Keywords

One or more keywords (tags) can be assigned to an extraction. Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the "Search Extractions" window. To open the "Search Extractions" window, click **[ Search]** in the main window of the Designer.

Tip

To add keywords to multiple extractions at once, select the extractions in the main window of the Designer.\
Right-click + **Add/Remove keywords** opens the window "Add/Remove Keywords To/From Multiple Extractions".

### Primary Key Tab

Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.

The depicted example shows the SAP object *MAKT* with its primary key inherited from SAP in the general settings of the Designer. In this example the primary key consists of *MANDT*, *MATNR* and *SPRAS*. The primary key is also taken over in the destination.

Note

A defined primary key field in a table is a prerequisite for merging data.

#### Generate Surrogate Key Column

If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs. The surrogate keys are hash values of type signed 8 byte integer, e.g., `#-3008591679982390000`. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.

### Security Tab

Restrict user access to the extraction. For more information, see [Restrict Designer Access](../../access-restrictions/restrict-designer-access/).

### Columns Order Tab

The "Columns Order" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns. Index 0 defines the first column in the result set, index 1 the second columns, etc.

| Option | Description | | --- | --- | | **Enabled** | When this option is active, the defined column order is applied when running the extraction. | | **Swap** | Swaps the index of 2 columns. All other columns keep their indexes. | | **Insert** | Inserts the selected column into the selected index. All other indexes are recalculated. | | **Reset default** | Restores the original column order. |

This page shows how to filter the data that is extracted by the OData extraction type. Selections limit the result set of the OData extraction type to extract only records that match the selection.

### Edit Selections

Follow the steps below to edit selection fields and filter data:

1. In the subsection *Selection Screen*, click **[Edit]** next to the field you want to edit. The window “Edit selection” opens.

1. Click **[Single]**, **[Range]** or **[List]** to add a corresponding filter, see [Filter Options](#filter-options).

1. In the column **Sign** , select *Include* to add the filtered data to the output or select *Exclude* to remove the filtered data from the output.

1. In the column **Option** , select an operator, see [Filter Options](#filter-options).

1. In the column **Value**, enter values directly into the input fields **Low** and **High** or assign existing [runtime parameters](../edit-runtime-parameters/) to the selection fields .

   Note

   When runtime parameters are available, you can use the icon button next to the input field to switch between static values () and runtime parameters ().

1. Click **[OK]** to confirm your input.

1. Click **[Load live preview]** in the main window of the extraction type to check the result of your selection. If runtime parameters are defined, you are prompted to populate the parameters with actual values.

Note that edited selection fields overwrite the selection fields in the variant.

Tip

If you use multiple selection parameters, it is more efficient to create a variant in SAP.

The number of defined filters is displayed in square brackets next to the **[Edit]** button.

### Filter Options

The OData extraction type offers the following filter options:

| Type | Operator | Description | | --- | --- | --- | | Single | | Compare data to a single specified value. | | | *(not) like pattern* | True if data values do (not) contain to the specified value. | | | *(not) equal to* | True if data is (not) equal to the specified value. | | | *at least* | True if data is greater than or equal to the specified value. | | | *more than* | True if data is greater than the specified value. | | | *at most* | True if data is less than or equal to the specified value. | | | *less than* | True if data is less than the specified value. | | Range | | Check if the data is (not) within a specified range of values. | | | *(not) between* | True if data values do (not) lie between the 2 specified values. | | List | | Check if the data is part of a specified list of values. | | | *element of* | True if data values are part of the list. |

### Data Format

Use the following internal SAP representation for input:

- Date: The date 01.01.1999 has the internal representation 19990101 (YYYYMMDD).
- Year period: The year period 001.1999 has the internal representation 1999001 (YYYYPPP).
- Numbers: Numbers must contain the leading zeros, e.g., customer number 1000 has the internal representation 0000001000.

Warning

**Values accept only the internal SAP representation.**\
Input that does not use the internal SAP representation results in error messages. Use the internal SAP representation. Example:

```text
ERPConnect.ABAPProgramException: RfcInvoke failed(RFC_ABAP_MESSAGE): Enter date in the format \_.\_.\_

```

This page contains an overview of the extraction settings in the OData extraction type.\
To open the extraction settings, click ****Extraction Settings**** in the main window of the extraction type.

### Extraction Settings

#### Package Size

The extracted data is split into packages of the defined size. The default value is 50000 lines.\
A package size between 20000 and 50000 is advisable for large amounts of data. 0 means no packaging. Not using packaging can lead to an RFC timeout for large data extractions.

Warning

**RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table**\
To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.

#### Row Limit

Specifies the maximum number of extracted records. 0 extracts all data. You can use this option to perform tests with a small amount of data by entering a row limit of e.g., 1000.

This page shows how to use the ODP extraction type.\
The ODP extraction type can be used to extract data via the SAP Operational Data Provisioning (ODP) framework.

### About ODP

Operational data provisioning (ODP) is a framework in SAP ABAP applications for transferring data between systems. ODP provides a technical infrastructure for data extraction and replication from different SAP (ABAP) Systems, e.g.:

- ECC
- S/4 HANA
- BW
- BW/4 HANA

The ODP extraction type acts as a subscriber (consumer) and [subscribes](subscriptions/) to a data provider. ODP supports mechanisms to load data incrementally from data providers. For SAP BW/4HANA, ODP is the central infrastructure for data extraction and replication from SAP (ABAP) applications to an SAP BW/4HANA Data Warehouse.

The ODP extraction type provides data transfer from the following providers (also called [Provider Context](provider-context/)):

| Provider Context | SAP Source Objects | | --- | --- | | ABAP Core Data Services \[[ABAP_CDS](provider-context/#abap-cds-views)\] | - CDS Views | | SAP NetWeaver Business Warehouse or BW4/HANA \[[BW](provider-context/#bw-infoproviders)\] | **BW/4HANA:** - DSO / aDSO - CompositeProvider - InfoObjects - Query as InfoProvider **BW systems:** - CompositeProvider - InfoCubes - Semantically partitioned objects - HybridProviders - MultiProviders - InfoSets | | SAP HANA Information Views \[[HANA](provider-context/#hana-views)\] | - Analysis Views - Calculation Views - Associated Attribute Views | | DataSources/Extractors \[[SAPI](provider-context/#extractors)\] | - DataSources and Extractors | | SAP LT Queue Alias \[[SLT~your_queue_alias](provider-context/#slt-server)\] | - SAP Tables - Cluster tables - Pool tables |

Depending on the connected SAP source system there are differences in available provider contexts.\
For more information on SAP ODP, see [SAP Wiki: Operational Data Provisioning (ODP) and Delta Queue (ODQ)](https://wiki.scn.sap.com/wiki/pages/viewpage.action?pageId=449284646).

### Prerequisites

- A connection to an SAP system is available, see [SAP Connection](../sap-connection/).
- The SAP user has sufficient user rights, see [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#odp).
- Implement the following SAP notes to use ODP:
  - [1931427 - ODP Data Replication API 2.0](https://launchpad.support.sap.com/#/notes/1931427)
  - [2232584 - Release of SAP extractors for ODP replication (ODP SAPI)](https://launchpad.support.sap.com/#/notes/2232584)
  - [1560241 - Release of DataSources for ODP data replication API](https://launchpad.support.sap.com/#/notes/1560241)
  - [2196500 - ODP Package size cannot be reduced below 50 MB](https://launchpad.support.sap.com/#/notes/2196500/D)
  - [2191995 - ODQ Package Size cannot be reduced below 50 MByte](https://launchpad.support.sap.com/#/notes/2191995/D)
- DataSources have to be activated in SAP, see [SAP Help: Activating DataSources in the SAP OLTP System](https://help.sap.com/docs/SLH_advanced_compliance_reporting_service/7a60944343e543a1ab99e9b2904dab09/e5d447257a95416190d29638a64a5dfa.html).
- Before creating ODP extractions, test the ODP source in SAP using the ABAP report RODPS_REPL_TEST to rule out and troubleshoot ODP problems in the ODP source. For more information, see [SAP Wiki: Replication test with RODPS_REPL_TEST](https://wiki.scn.sap.com/wiki/display/BI/Replication+test+with+RODPS_REPL_TEST).

Note

The ODP API 1.0 has limitations compared to ODP API 2.0, e.g., ODP API 1.0 does not support the extraction of Hierarchy DataSources. For more information, see [SAP Wiki: Limitation of ODP API 1.0](https://wiki.scn.sap.com/wiki/display/BI/Limitation+of+ODP+API+1.0).

Warning

**Missing Authorization.**\
To use the ODP extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#odp) accordingly.

### Create an ODP Extraction

1. In the main window of the Designer, click **[New]** to create a new extraction. The window "Create Extraction" opens.
1. Select an [SAP Connection](../sap-connection/) of type **RFC** from the drop-down menu **Source**.
1. Enter a unique name for your extraction.
1. Select the extraction type **ODP**.
1. Click **[OK]**. The main window of the extraction type opens automatically.

The majority of the functions of the extraction type can be accessed in the main window.

### Look up Data Objects

1. In the main window of the extraction type, click **[]**. The window “Operational Data Provider Lookup” opens.
1. In the field **Name**, enter the name of an extractor . Use wildcards (\*), if needed.
1. Select a *Context* . Depending on the connected SAP source system there are differences in available [Provider Contexts](provider-context/).
1. Click **[]**. Search results are displayed.
1. Select an extractor and click **[OK]** to confirm.

The application now returns to the main window of the extraction type.

Note

To find DataSources, they have to be activated in SAP.

### Define the ODP Extraction Type

The ODP extraction type offers the following options for data extractions:

1. In the section *Fields*, select the items you want to extract.

   Note

   TS_SEQUENCE_NUMBER is a technical primary key that can be added to the output. When working with identical data sets, the data set with the highest sequence number is the most current data set.

1. Optional: edit selections to apply (dynamic) filters. For more information, see [Edit Selections](selections/#edit-selections).

   Note

   If your data source is a Hierarchy, see [Hierarchy Segments](provider-context/#segments-to-extract) for filter options.

1. Select an [Update Mode](update-mode/), e.g., to initialize delta extractions.

1. Click **[Load live preview]** to display a live preview of the first 100 records.

1. Check the [Extraction Settings](settings/) and the [General Settings](general-settings/) before running the extraction.

1. Click **[OK]** to save the extraction type.

You can now run the extraction, see [Execute and Automate Extractions](../execute-and-automate/).

______________________________________________________________________

#### Related Links

- [Youtube Tutorial: SAP ODP incremental load to SQL server](https://www.youtube.com/watch?v=-7pEm2VVPRg)

Runtime parameters are are placeholders for values that are passed at runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom). They can be created in context of [Selections](../selections/).

## Create Runtime Parameters

There are two types of runtime parameters:

- [Scalar parameters](#scalar-parameters) represent a single value.
- [List parameters](#list-parameters) represent multiple values.

### Scalar Parameters

Follow the steps below to create a scalar runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add Scalar]** to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.

   | Type | Description | | --- | --- | | *Text* | Can be used for any type of SAP selection field. | | *Number* | Can be used for numeric SAP selection fields. | | *Flag* | Can only be used for SAP selection fields THAT require an ‘X’ (true) or a blank ‘‘ (false) as input value. |

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

### List Parameters

Follow the steps below to create a list runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add List]** to define list parameters that contain multiple values separated by commas e.g., 1,10 or “1”, “10”. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

## Assign Runtime Parameters

Follow the steps below to assign the runtime parameters to selections.

1. In the main window of the extraction type, click the **[Edit]** button next to the selection you want to parameterize. The window "Edit Selections" opens.
1. Add a filter to the selection, see [Selections and Filters](../selections/#edit-selections).
1. Click the icon button next to the input field to switch between static values () and runtime parameters (). If no icon button is available, [create a runtime parameter](#create-runtime-parameters).
1. Select a runtime parameter from the dropdown-list.
1. Click **[OK]** to confirm the input.

Pass values during runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom).

This page contains an overview of the settings in the window "General Settings".\
To open the general settings, click **General Settings** in the main window of the extraction type.

### Misc. Tab

The *Misc.* tab covers cache settings, column encryption and keywords of an extraction type.

#### Cache results

The *Cache results* option is only available in [pull destinations](../../destinations/), e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times. To decrease the SAP server load, you can select the **Cache results** option, this way the pull destination pulls the data from cache and not from the SAP.

Cache results increases the performance and limits the impact on the SAP system. If this behavior is undesirable (for example, because the data must be always 100% up-to-date), the cache option must be explicitly disabled.

#### Keywords

One or more keywords (tags) can be assigned to an extraction. Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the "Search Extractions" window. To open the "Search Extractions" window, click **[ Search]** in the main window of the Designer.

Tip

To add keywords to multiple extractions at once, select the extractions in the main window of the Designer.\
Right-click + **Add/Remove keywords** opens the window "Add/Remove Keywords To/From Multiple Extractions".

### Primary Key Tab

Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.

The depicted example shows the SAP object *MAKT* with its primary key inherited from SAP in the general settings of the Designer. In this example the primary key consists of *MANDT*, *MATNR* and *SPRAS*. The primary key is also taken over in the destination.

Note

A defined primary key field in a table is a prerequisite for merging data.

#### Generate Surrogate Key Column

If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs. The surrogate keys are hash values of type signed 8 byte integer, e.g., `#-3008591679982390000`. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.

### Security Tab

Restrict user access to the extraction. For more information, see [Restrict Designer Access](../../access-restrictions/restrict-designer-access/).

### Columns Order Tab

The "Columns Order" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns. Index 0 defines the first column in the result set, index 1 the second columns, etc.

| Option | Description | | --- | --- | | **Enabled** | When this option is active, the defined column order is applied when running the extraction. | | **Swap** | Swaps the index of 2 columns. All other columns keep their indexes. | | **Insert** | Inserts the selected column into the selected index. All other indexes are recalculated. | | **Reset default** | Restores the original column order. |

This page shows how to use the Provider Contexts of the ODP extraction type. The ODP extraction type supports the following Provider Contexts:

| Provider Context | SAP Source Objects | | --- | --- | | ABAP Core Data Services \[[ABAP_CDS](#abap-cds-views)\] | - CDS Views | | SAP NetWeaver Business Warehouse or BW4/HANA \[[BW](#bw-infoproviders)\] | **BW/4HANA:** - DSO / aDSO - CompositeProvider - InfoObjects - Query as InfoProvider **BW systems:** - CompositeProvider - InfoCubes - Semantically partitioned objects - HybridProviders - MultiProviders - InfoSets | | SAP HANA Information Views \[[HANA](#hana-views)\] | - Analysis Views - Calculation Views - Associated Attribute Views | | DataSources/Extractors \[[SAPI](#extractors)\] | - DataSources and Extractors | | SAP LT Queue Alias \[[SLT~your_queue_alias](#slt-server)\] | - SAP Tables - Cluster tables - Pool tables |

## ABAP CDS Views

According to the Core Data Service (CDS) concept, data models based on CDS serve as central definitions that can be used in many different domains, such as transactional and analytical applications.

CDS is defined using an SQL-based data definition language (DDL). DLL leverages the standard SQL with several additional concepts, such as associations, which define the relationships between CDS views, and annotations that direct the domain-specific use of CDS artifacts.

### Available CDS

There are two types of CDS available:

- HANA CDS (defined in XS engine)
- ABAP CDS

For each ABAP CDS entity defined in the DDL source code, an SQL view is generated in the ABAP Dictionary. The same way as the views created with Dictionary tools (transaction SE11), ABAP CDS entities can be accessed in ABAP using Open SQL statements.

CDS is important for SAP application development. For example, S/4HANA uses CDS to provide both core transactional functionality as well as analytical content for reporting. In BW/4 HANA, ABAP based CDS views can be used for data extraction.

Two possible use cases for CDS:

- Direct access to a CDS view from BW/4 HANA
- Delta extraction from a CDS view in S/4 HANA to BW

CDS Views support Full and Delta Extraction.

### Replace BW Extractors with CDS Views in S/4HANA

In majority of cases traditional BW extractors can be used to extract data from an S/4HANA systems. Due to simplifications in the S/4HANA system, several extractors are now deprecated or can no longer be used in a usual manner. To use the deprecated or altered extractors, SAP may deliver extraction relevant (via the analytical annotations) CDS Views.

### Use ABAP CDS Views

Note

To find an ABAP CDS view it must have the following Annotation: `@Analytics.dataExtraction.Enabled: true`. If the source SAP system is not on a HANA DB, an additional Annotation is needed: `@Analytics.dataCategory: #CUBE/#FACT/#DIMENSION`.

Things that need to be considered when using BW InfoProviders:

1. When looking up ABAP CDS Views in the ODP extraction type, make sure to select the correct context.
1. When the delta load is available for the source object in the SAP source system, the **Delta Update** option is available in the ODP extraction type.
1. Click **[Load live preview]** to preview the data without running an extraction.

## BW InfoProviders

The Operational Data Provisioning (ODP) framework allows you to extract data from the InfoProviders in your source BW and BW/4 HANA systems. Available InfoProvider types are dependent on your source BW and BW/4 HANA system.

### Available InfoProviders

- **Available InfoProviders in SAP BW∕4HANA**

  ______________________________________________________________________

  - CompositeProvider - object type HCPR (full extraction)
  - DataStore objects (with delta extraction)
    - InfoObjects
    - Master data
    - Texts
    - Hierarchies
  - Query as InfoProvider

- **Additional InfoProviders in BW Systems**

  ______________________________________________________________________

  - CompositeProvider - object type HCPR (full extraction)
  - InfoCubes (with delta extraction)
  - Semantically partitioned objects
  - HybridProviders
  - MultiProviders
  - InfoSets

### Use BW InfoProviders

Things that need to be considered when using BW InfoProviders:

1. When looking up BW InfoProviders in the ODP extraction type, make sure to select the correct context.
1. When the delta load is available for the source object in the SAP source system, the **Delta Update** option is available in the ODP extraction type.
1. Click **[Load live preview]** to preview the data without running an extraction.

## Extractors

The ODP extraction type can be used to extract data from Business Content DataSource (Extractors). The majority of DataSources, including generic (custom) DataSources, can be released for Operational Data Provisioning.

The ODP extraction type does not change the implementation of application extractors. All features and capabilities remain unchanged.

An extractor (in ERP or S/4 HANA) is an encapsulated business object, representing multiple source tables already in the source system

### Available Extractors

- Transactional data
- Master data
- Text data
- Hierarchy data

There are standard delta extraction methods available for master data and transaction data.

### Use Extractors

Things that need to be considered when using Extractors:

- DataSources have to be activated in SAP, see [SAP Help: Set Up and Activate DataSources](https://help.sap.com/viewer/7a60944343e543a1ab99e9b2904dab09/CLOUD/en-US/e5d447257a95416190d29638a64a5dfa.html).
- When looking up DataSources or Extractors, make sure to select the correct context.
- If delta load is available for the source object in the SAP source system, the **Delta Update** option is available in the ODP extraction type.
- If the DataSource is a Hierarchy, there are additional settings, see [Hierarchies](#hierarchies).

### Hierarchies

If the selected source object is of type Hierarchy, the window "Select Hierarchy" opens.

1. Select a Hierarchy from the list of Hierarchies in the "Select Hierarchy" window.
1. Confirm your selection with **[OK]**.\
   The name of the selected Hierarchy is displayed under *Selected Hierarchy* .
1. Select which segments to extract .

#### Segment(s) to extract

Hierarchies are divided into segments by the API. Choose which segments of the Hierarchy to extract . The selected data is displayed in the *Fields* section.

| Option | Description | | --- | --- | | **Merges** | All segments contain the field Node ID. Using the Node ID *Merges* automatically combines all 3 segments. | | **Elements** | The segment *Elements* contains information about the elements of the Hierarchy e.g., name, parent, child, etc. | | **Texts** | The segment *Texts* contains the description texts of the *Elements*. The language of the descriptions depends on the language settings of the SAP connection. | | **Intervals** | The segment *Intervals* contains additional information if an element is an interval. In ODP no TO and FROM columns are displayed, see [SAP Note 3090500](https://launchpad.support.sap.com/#/notes/3090500). |

Note

Hierarchies can be passed as runtime parameters at runtime.

## HANA Views

Operational Data Provisioning (ODP) can be used to connect SAP HANA database of an SAP ABAP source system. The connection is provided via RFC.

### Available HANA Views:

- Analysis Views
- Calculation Views
- Associated Attribute Views

### Use HANA Views

Things that need to be considered when using HANA Views:

1. When looking up HANA Views in the ODP extraction type, make sure to select the correct context.
1. When the delta load is available for the source object in the SAP source system, the **Delta Update** option is available in the ODP extraction type.
1. Click **[Load live preview]** to preview the data without running an extraction.

## SLT Server

The Operational Data Provisioning (ODP) framework allows you to extract tables and simple views from SAP HANA systems via an SAP Landscape Transformation Replication Server (SLT). The SLT server is a trigger-based CDC solution that can replicate SAP tables and views and make them available as delta extracts.

### Available Tables

- Regular SAP tables
- Cluster tables
- Pool tables

### Requirements

The SLT server context requires an SAP Landscape Transformation Replication Server (SLT) that is set up for ODP, see [SAP Help: Transferring Data from SLT Using Operational Data Provisioning](https://help.sap.com/docs/SAP_NETWEAVER_750/ccc9cdbdc6cd4eceaf1e5485b1bf8f4b/6ca2eb9870c049159de25831d3269f3f.html?locale=en-US).

The following requirements apply to the SLT server:

- Minimum release version of the SLT server:
  - Add-On DMIS 2011 SP05
  - SAP NetWeaver 7.3 SPS10, 7.31 SPS09 or 7.4 SPS04 (ODP infrastructure)
- Add-On DMIS_2011 SP03/SP04 or higher or 2010 SP08/SP09 is installed in the SAP source system.
- The following SAP Notes are published in the SAP source system:
  - SAP Note 1863476
  - SAP Note 1817467 when using Add-On DMIS 2011 SP05

### Use SLT Server

Consider the following when using an SLT server:

1. When looking up data via an SLT server, make sure to select the correct context.
1. A live preview of the data is not available in the SLT server context.

For more information on SLT servers, see [SAP Help: Transferring Data from SLT Using Operational Data Provisioning](https://help.sap.com/docs/SAP_NETWEAVER_750/ccc9cdbdc6cd4eceaf1e5485b1bf8f4b/6ca2eb9870c049159de25831d3269f3f.html?locale=en-US) or download the [SLT-Performance-Guide (Nov 2022)](../../../assets/files/SLT-Performance-Guide_Nov_2022.pdf).

This page shows how to filter the data that is extracted by the ODP extraction type. Selections limit the result set of the ODP extraction type to extract only records that match the selection.

### Edit Selections

Follow the steps below to edit selection fields and filter data:

1. In the subsection *Fields*, click **Edit** next to the field you want to edit. The window “Edit selection” opens.

1. Click **[Single]**, **[Range]** or **[List]** to add a corresponding filter, see [Filter Options](#filter-options).

1. In the column **Sign** , select *Include* to add the filtered data to the output or select *Exclude* to remove the filtered data from the output.

1. In the column **Option** , select an operator, see [Filter Options](#filter-options).

1. In the column **Value**, enter values directly into the input fields **Low** and **High** or assign existing [runtime parameters](../edit-runtime-parameters/) to the selection fields .

   Note

   When runtime parameters are available, you can use the icon button next to the input field to switch between static values () and runtime parameters ().

1. Click **[OK]** to confirm your input.

1. Click **[Load live preview]** in the main window of the extraction type to check the result of your selection. If runtime parameters are defined, you are prompted to populate the parameters with actual values.

The number of defined filters is displayed in square brackets next to the **Edit** option.

### Filter Options

The ODP extraction type offers the following filter options:

| Type | Operator | Description | | --- | --- | --- | | Single | | Compare data to a single specified value. | | | *(not) like pattern* | True if data values do (not) contain to the specified value. Not not all ODP contexts and data sources support this option. | | | *(not) equal to* | True if data is (not) equal to the specified value. | | | *at least* | True if data is greater than or equal to the specified value. | | | *more than* | True if data is greater than the specified value. | | | *at most* | True if data is less than or equal to the specified value. | | | *less than* | True if data is less than the specified value. | | Range | | Check if the data is (not) within a specified range of values. | | | *(not) between* | True if data values do (not) lie between the 2 specified values. | | List | | Check if the data is part of a specified list of values. | | | *element of* | True if data values are part of the list. |

### Data Format

Use the following internal SAP representation for input:

- Date: The date 01.01.1999 has the internal representation 19990101 (YYYYMMDD).
- Year period: The year period 001.1999 has the internal representation 1999001 (YYYYPPP).
- Numbers: Numbers must contain the leading zeros, e.g., customer number 1000 has the internal representation 0000001000.

Warning

**Values accept only the internal SAP representation.**\
Input that does not use the internal SAP representation results in error messages. Use the internal SAP representation. Example:

```text
ERPConnect.ABAPProgramException: RfcInvoke failed(RFC_ABAP_MESSAGE): Enter date in the format \_.\_.\_

```

This page contains an overview of the extraction settings in the ODP extraction type.\
To open the extraction settings, click ****Extraction Settings**** in the main window of the extraction type.

### Extraction Settings

#### Package Size

The extracted data is split into packages of the defined size. The default package size is 14400000 bytes. When the package size is set to 0, SAP uses the default ODP packaging size.

Make sure that your SAP version includes the necessary notes, see [Prerequisites](../#prerequisites).

#### Adjust Currency Decimals

The default number of decimal places for a currency in the SAP database is 2 decimals. Currencies that do not have decimals are also stored in this format, e.g. JPY, VND, KRW, etc.

Example:

| Currency | Actual Amount | Amount stored in SAP database | | --- | --- | --- | | JPY | 100 | 1.00 | | KRW | 10000 | 100.00 |

When extracting currencies with no decimals, the amount stored in SAP is returned e.g., 100 JPY are extracted as 1.00. To correct the decimal placement of the extracted data, activate **Adjust Currency Decimals**. If **Adjust Currency Decimals** is active, currencies without decimals are multiplied by a factor that balances out the decimals.

**Adjust Currency Decimals** also requires the extraction of the corresponding CURRENCY field that can be used as a reference for the multiplication factor. Use the **[Load live preview]** function to find the correct currency field/s.

- If the currency field is part of the table, add it to the output.
- If the currency field is in another table, join the tables.
- If the reference is not part of a table, **Adjust Currency Decimals** cannot be used.

Note

The multiplication factor used in *Adjust Currency Decimals* is determined by the SAP currency table TCURX. To access the table, the following SAP Authority objects must be set in SAP: *S_TABU_NAM ACTVT=03; TABLE=TCURX*.

This page contains a description of the "Show active subscriptions" menu in the ODP extraction type. To open the menu, click **Show active subscriptions** in the main window of the extraction type.

### Show Active Subscriptions

The ODP extraction type acts as a subscriber (consumer) to [data providers](../provider-context/) to extract data from the data provider. The window "Delta Subscriptions for product" displays details about subscribers.

| Column Name | Description | | --- | --- | | Name | Technical name of all subscriptions of a specific Theobald Software Xtract product (e.g., Xtract Universal). | | Process | Technical name of a subscription | | R. (number of requests) | Number of executed delta requests | | Last request | Timestamp of the last delta request |

To delete a subscription, click **[]** on the right side of the window.

Tip

The information displayed in the "Delta Subscriptions for product" window can also be viewed in SAP transaction *ODQMON*.

The ODP extraction type can be used for delta extractions. This means that only recently added or changed data is extracted, instead of a full load. The data that is extracted is defined by the **Update Mode** setting in the main window of the extraction type.

### Update Modes

#### Full update

Extracts all data (full mode).

#### Delta update

Note

The **Delta update** option is ready for input only if the ODP provider in the SAP source system supports delta updates.

Runs a delta initialization, if no delta initialization is available for the selected subscriber. Runs a delta update, if there is a delta initialization for the selected subscriber. A delta update only extracts data that was added or changed on the SAP system since the last delta request.

- **Extract data**\
  Allows extracting data when running a delta initialization. Leaving this checkbox unchecked runs a delta initialization without extracting data.
- **Auto-sync subscription**\
  Allows deletion of the existing subscription and creates a new subscription, if required. Each extraction has an internal ID, which is part of the subscriber. If you change the filter of an extraction after the delta initialization, the *Auto-sync subscription* option automatically deletes the existing subscription and creates a new one. A subscription is deleted, if the error message "Illegal change in selection parameters" returns from the SAP system. To delete subscriptions manually, see [Subscriptions](../subscriptions/).

#### Delta recovery

Re-runs the last delta update.

#### Direct read (without ODQ)

Directly reads all available data, bypassing the ODQ (Operational Delta Queue). Direct read is the only update mode that supports data aggregation functions (*Maximum*, *Minimum* and *Sum*).

### Video Tutorial / Demo Case

The following YouTube tutorial shows how to the ODP update mode *Delta Update* can be used to load data increments to an SQL database destination.

This page shows how to use the ODP(OData) extraction type.\
The ODP(OData) extraction type can be used to extract ODP-based data via OData services.

Note

As the use of the RFC modules of the ODP Data Replication API is prohibited by SAP ([SAP Note 3255746](https://me.sap.com/notesLatestChanges/0003255746/E/diff)), the ODP(OData) extraction type is the recommended tool for extracting ODP data.

### About ODP via OData

Open Data Protocol (OData) is a web protocol for querying and updating data. OData can be used to access the [Open Data Provisioning (ODP)](../odp/#about-odp) framework and extract ODP data, e.g., Hierarchies, Tables, Views, DataSources, etc.

To extract ODP data via OData, a corresponding OData service is required. The data model of the service specifies, which data sets are accessible and how the data is structured. Once the service is registered in the SAP Gateway system, Xtract Universal can consume the service.

For more information, see [SAP Help: ODP-Based Data Extraction via OData](https://help.sap.com/doc/saphelp_nw75/7.5.5/en-US/11/853413cf124dde91925284133c007d/frameset.htm).

Tip

For information on how to create OData services that provide ODP based data, refer to the [Knowledge Base Article: Create OData Services using the SAP Gateway Builder](../../knowledge-base/create-odata-services-using-the-sap-gateway-builder/).

### Prerequisites

- OData services for ODP extractions are available in the SAP system, see [SAP Help: Generating a Service for Extracting ODP Data via OData](https://help.sap.com/doc/saphelp_nw75/7.5.5/en-US/69/b481859ef34bab9cc7d449e6fff7b6/frameset.htm).
- To extract a DataSource, the DataSource is activated in SAP, see [SAP Help: Set Up and Activate DataSources](https://help.sap.com/docs/SLH_advanced_compliance_reporting_service/7a60944343e543a1ab99e9b2904dab09/e5d447257a95416190d29638a64a5dfa.html).
- Before creating ODP (OData) extractions, test the ODP source in SAP using the ABAP report RODPS_REPL_TEST to rule out and troubleshoot ODP problems in the ODP source. For more information, see [SAP Wiki: Replication test with RODPS_REPL_TEST](https://help.sap.com/docs/SUPPORT_CONTENT/bwdabc/3361385256.html).

#### Supported Features

- All available [ODP contexts](../odp/provider-context/) of an SAP system are accessible.
- Delta extractions (with initial full load) if the ODP data source supports deltas, see [Subscriptions](subscriptions/).
- Data packaging for large amounts of data, see [Package Size](settings/#package-size).
- Selections to filter the data before extraction, see [Selections](selections/).

#### Limitations

Delta subscriptions are limited to one SAP user per service. The ODP framework has to be exposed in multiple different services for a single user to have multiple delta subscriptions.

### Create a ODP(OData) Extraction

1. In the main window of the Designer, click **[New]** to create a new extraction. The window "Create Extraction" opens.
1. Select an [SAP Connection](../sap-connection/) of type **RFC** from the drop-down menu **Source**.
1. Enter a unique name for your extraction.
1. Select the extraction type **ODP(OData)**. The **ODP(OData)** extraction type is only available if the SAP source connection uses the [OData protocol](../sap-connection/settings/#source-type-odata).
1. Click **[OK]**. The main window of the extraction type opens automatically.

The majority of the functions of the extraction type can be accessed in the main window.

### Look Up OData Services

1. In the main window of the extraction type, click **[]**. The window “Service Selection” opens.

1. In the field **Search pattern** , enter one of the following characteristics:

   - name of an OData service
   - description of an OData service
   - name of the entity inside the OData service

   Use wildcards (\*), if needed.

1. Click **[Search]** . Search results are displayed.

1. Select an OData service and click **[OK]** to confirm.

The application now returns to the main window of the extraction type.

Note

To find DataSources, they have to be activated in SAP.

### Define the ODP(OData) Extraction Type

The ODP(OData) extraction type offers the following options for data extractions:

1. In the section *Fields*, select the items you want to extract.
1. Optional: Edit a selection you want to change or dynamize. For more information, see [Edit Selections](selections/#edit-selections).
1. Optional: Activate the option **Delta / Change tracking** to initialize delta extractions. For more information, see [Subscriptions](subscriptions/).
1. Click **[Load live preview]** to display a live preview of the first 100 records.
1. Check the [Extraction Settings](settings/) and the [General Settings](general-settings/) before running the extraction.
1. Click **[OK]** to save the extraction type.

You can now run the extraction, see [Execute and Automate Extractions](../execute-and-automate/).

Runtime parameters are are placeholders for values that are passed at runtime. They can be created in context of [Selections](../selections/).

## Create Runtime Parameters

There are two types of runtime parameters:

- [Scalar parameters](#scalar-parameters) represent a single value.
- [List parameters](#list-parameters) represent multiple values.

### Scalar Parameters

Follow the steps below to create a scalar runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add Scalar]** to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.

   | Type | Description | | --- | --- | | *Text* | Can be used for any type of SAP selection field. | | *Number* | Can be used for numeric SAP selection fields. | | *Flag* | Can only be used for SAP selection fields THAT require an ‘X’ (true) or a blank ‘‘ (false) as input value. |

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

### List Parameters

Follow the steps below to create a list runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add List]** to define list parameters that contain multiple values separated by commas e.g., 1,10 or “1”, “10”. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

## Assign Runtime Parameters

Follow the steps below to assign the runtime parameters to selections.

1. In the main window of the component, click the **[Edit]** button next to the selection you want to parameterize. The window "Edit Selections" opens.
1. Select a filter option, see [Edit Selections](../selections/#edit-selections).
1. Click the icon button next to the input field to switch between static values () and runtime parameters (). If no icon button is available, [create a runtime parameter](#create-runtime-parameters).
1. Select a runtime parameter from the dropdown-list.
1. Click **[OK]** to confirm the input.

Pass values during runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom).

This page contains an overview of the settings in the window "General Settings".\
To open the general settings, click **General Settings** in the main window of the extraction type.

### Misc. Tab

The *Misc.* tab covers cache settings, column encryption and keywords of an extraction type.

#### Cache results

The *Cache results* option is only available in [pull destinations](../../destinations/), e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times. To decrease the SAP server load, you can select the **Cache results** option, this way the pull destination pulls the data from cache and not from the SAP.

Cache results increases the performance and limits the impact on the SAP system. If this behavior is undesirable (for example, because the data must be always 100% up-to-date), the cache option must be explicitly disabled.

#### Keywords

One or more keywords (tags) can be assigned to an extraction. Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the "Search Extractions" window. To open the "Search Extractions" window, click **[ Search]** in the main window of the Designer.

Tip

To add keywords to multiple extractions at once, select the extractions in the main window of the Designer.\
Right-click + **Add/Remove keywords** opens the window "Add/Remove Keywords To/From Multiple Extractions".

### Primary Key Tab

Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.

The depicted example shows the SAP object *MAKT* with its primary key inherited from SAP in the general settings of the Designer. In this example the primary key consists of *MANDT*, *MATNR* and *SPRAS*. The primary key is also taken over in the destination.

Note

A defined primary key field in a table is a prerequisite for merging data.

#### Generate Surrogate Key Column

If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs. The surrogate keys are hash values of type signed 8 byte integer, e.g., `#-3008591679982390000`. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.

### Security Tab

Restrict user access to the extraction. For more information, see [Restrict Designer Access](../../access-restrictions/restrict-designer-access/).

### Columns Order Tab

The "Columns Order" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns. Index 0 defines the first column in the result set, index 1 the second columns, etc.

| Option | Description | | --- | --- | | **Enabled** | When this option is active, the defined column order is applied when running the extraction. | | **Swap** | Swaps the index of 2 columns. All other columns keep their indexes. | | **Insert** | Inserts the selected column into the selected index. All other indexes are recalculated. | | **Reset default** | Restores the original column order. |

This page shows how to filter the data that is extracted by the ODP(OData) extraction type. Selections limit the result set of the ODP(OData) extraction type to extract only records that match the selection.

### Edit Selections

Follow the steps below to edit selection fields and filter data:

1. In the subsection Fields, click **Edit** next to the field you want to edit. The window “Edit selection” opens.

1. Select one of the following filter types:

   - **Single** : only extract data that equals a single specified value.
   - **Interval**: only extract data that lies within a specified range of values.

1. Enter values directly into the input fields or assign existing [runtime parameters](../edit-runtime-parameters/) to the selection.

   Note

   When runtime parameters are available, you can use the icon button next to the input field to switch between static values () and runtime parameters ().

1. Click **[OK]** to confirm your input.

1. Click **[Load live preview]** in the main window of the component to check the result of your selection. If runtime parameters are defined, you are prompted to populate the parameters with actual values.

When a filter is defined, the filter statement is displayed next to the **Edit** option.

### Data Format

Use the following internal SAP representation for input:

- Date: The date 01.01.1999 has the internal representation 19990101 (YYYYMMDD).
- Year period: The year period 001.1999 has the internal representation 1999001 (YYYYPPP).
- Numbers: Numbers must contain the leading zeros, e.g., customer number 1000 has the internal representation 0000001000.

Warning

**Values accept only the internal SAP representation.**\
Input that does not use the internal SAP representation results in error messages. Use the internal SAP representation. Example:

```text
ERPConnect.ABAPProgramException: RfcInvoke failed(RFC_ABAP_MESSAGE): Enter date in the format \_.\_.\_

```

This page contains an overview of the extraction settings in the ODP(OData) extraction type.\
To open the extraction settings, click ****Extraction Settings**** in the main window of the extraction type.

### Extraction Settings

#### Package Size

The extracted data is split into packages of the defined size. The default value is 50000 lines.\
A package size between 20000 and 50000 is advisable for large amounts of data. 0 means no packaging. Not using packaging can lead to an RFC timeout for large data extractions.

Warning

**RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table**\
To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.

The ODP(OData) extraction type acts as a subscriber to data providers to extract delta data from the data provider.

### Subscribe to a Data provider

To initialize delta extractions, activate the checkbox **Delta / Change tracking** and run the extraction once. The first run extracts all the data that matches the set selection criteria. After the initialization, the ODP(OData) extraction type only extracts data added or changed on the SAP system since the last run.

Tip

Use SAP transaction *ODQMON* to display information about active subscribers in SAP.

#### Limitations

Delta subscriptions are limited to one SAP user per service. The ODP framework has to be exposed in multiple different services for a single user to have multiple delta subscriptions.

### Terminate Subscriptions

To terminate a subscription in Xtract Universal, click **[Terminate subscription]** and deactivate the checkbox **Delta / Change tracking** .

This page shows how to use the OHS extraction type.\
The OHS extraction type can be used to extract data from Open Hub Service (OHS) destinations.

### Prerequisites

- A connection to an SAP system is available, see [SAP Connection](../sap-connection/).
- The SAP user has sufficient user rights, see [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#table).
- Configure your SAP BW system to make data sources accessible, see [Customization for OHS in BW](../setup-in-sap/customization-for-ohs-in-bw/).

Warning

**Missing Authorization.**\
To use the OHS extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#ohs) accordingly.

### Create an OHS Extraction

1. In the main window of the Designer, click **[New]** to create a new extraction. The window "Create Extraction" opens.
1. Select an [SAP Connection](../sap-connection/) of type **RFC** from the drop-down menu **Source**.
1. Enter a unique name for your extraction.
1. Select the extraction type **OHS**.
1. Click **[OK]**. The main window of the extraction type opens automatically.

The majority of the functions of the extraction type can be accessed in the main window.

### Look up an OHS Destination

1. In the main window of the extraction type, select an **Extraction type** in the main window of the component.

   - Select **Table** if you use BW4Hana2.0 and make sure that the open hub destination type in SAP is set to *database table*.
   - Select **Third party tool (legacy)** if you use BW4Hana2.0 and make sure that the open hub destination type in SAP is set to *third party tool*.

1. Click **[ Lookup]**. The window “OHS Lookup” opens.

1. In the field **OHS Destination**, enter the name of an OHS destination . Use wildcards (\*) if needed.

1. Click **[]**. Search results are displayed .

1. Select an OHS destination and click **[OK]** to confirm.

The application now returns to the main window of the extraction type.

### Define the OHS Extraction Type

The OHS extraction type offers the following options for OHS extractions:

1. If **Process Chain** is empty, enter an SAP process chain assigned to your OHS destination, see [SAP Help: Display/Maintenance of Process Chain Attributes](https://help.sap.com/docs/SAP_NETWEAVER_701/6da591e86c4b1014b43de329b9ffb859/4a2cf30c6ed91c62e10000000a42189c.html?locale=en-US).

   Note

   If **Process Chain** is left empty, the extraction fails.

1. Optional: Use the **Timeout** setting to set a maximum time period to wait for a notification from the BW system. If the time limit is reached, the extraction fails.

1. Check the [Extraction Settings](settings/) and the [General Settings](general-settings/) before running the extraction.

1. Click **[OK]** to save the extraction type.

You can now run the extraction, see [Execute and Automate Extractions](../execute-and-automate/).

This page contains an overview of the settings in the window "General Settings".\
To open the general settings, click **General Settings** in the main window of the extraction type.

### Misc. Tab

The *Misc.* tab covers cache settings, column encryption and keywords of an extraction type.

#### Cache results

The *Cache results* option is only available in [pull destinations](../../destinations/), e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times. To decrease the SAP server load, you can select the **Cache results** option, this way the pull destination pulls the data from cache and not from the SAP.

Cache results increases the performance and limits the impact on the SAP system. If this behavior is undesirable (for example, because the data must be always 100% up-to-date), the cache option must be explicitly disabled.

#### Keywords

One or more keywords (tags) can be assigned to an extraction. Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the "Search Extractions" window. To open the "Search Extractions" window, click **[ Search]** in the main window of the Designer.

Tip

To add keywords to multiple extractions at once, select the extractions in the main window of the Designer.\
Right-click + **Add/Remove keywords** opens the window "Add/Remove Keywords To/From Multiple Extractions".

### Primary Key Tab

Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.

The depicted example shows the SAP object *MAKT* with its primary key inherited from SAP in the general settings of the Designer. In this example the primary key consists of *MANDT*, *MATNR* and *SPRAS*. The primary key is also taken over in the destination.

Note

A defined primary key field in a table is a prerequisite for merging data.

#### Generate Surrogate Key Column

If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs. The surrogate keys are hash values of type signed 8 byte integer, e.g., `#-3008591679982390000`. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.

### Security Tab

Restrict user access to the extraction. For more information, see [Restrict Designer Access](../../access-restrictions/restrict-designer-access/).

### Columns Order Tab

The "Columns Order" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns. Index 0 defines the first column in the result set, index 1 the second columns, etc.

| Option | Description | | --- | --- | | **Enabled** | When this option is active, the defined column order is applied when running the extraction. | | **Swap** | Swaps the index of 2 columns. All other columns keep their indexes. | | **Insert** | Inserts the selected column into the selected index. All other indexes are recalculated. | | **Reset default** | Restores the original column order. |

This page contains an overview of the extraction settings in the OHS extraction type.\
To open the extraction settings, click ****Extraction Settings**** in the main window of the extraction type.

The OHS settings consist of two tabs:

- [*Table*](#table-settings)
- [*Third party tool (legacy)*](#third-party-tool-settings)

The settings correspond to the selected extraction type. Set either *Table* or *Third party tool (legacy)* settings.

Warning

**Could not load list of available function modules because permission for table ENLFDIR is missing**\
This warning appears if a technical SAP user does not have authorization rights to access the SAP table *ENLFDIR*. Confirm the warning as the user can **still** adjust the extraction settings.

## Table Settings

### Extraction Settings

#### Package Size

The extracted data is split into packages of the defined size. The default value is 50000 lines.\
A package size between 20000 and 50000 is advisable for large amounts of data. 0 means no packaging. Not using packaging can lead to an RFC timeout for large data extractions.

Warning

**RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table**\
To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.

#### Row Limit

Specifies the maximum number of extracted records. 0 extracts all data. You can use this option to perform tests with a small amount of data by entering a row limit of e.g., 1000.

### Function Module

Specifies the name of the function module used for data extraction. This field is filled automatically depending on what function modules are installed on your SAP system. Custom function modules are supported.

The following function modules can be used to extract tables:

- RFC_READ_TABLE (TAB512)
- /BODS/RFC_READ_TABLE (TAB2048)
- /SAPDS/RFC_READ_TABLE (TAB2048)
- /BODS/RFC_READ_TABLE2
- /SAPDS/RFC_READ_TABLE2
- Z_THEO_READ_TABLE
- [/THEO/READ_TABLE](/xtract-universal/documentation/setup-in-sap/custom-function-module-for-table-extraction) (recommended)

Warning

**Duplicates in the target environment.**\
The SAP standard modules for table extraction do not have pointers for table fields. In larger tables this may cause low performance and duplicates in the target environment. Use the function module [THEO/READ_TABLE](../../setup-in-sap/custom-function-module-for-table-extraction/#installation-of-theoread_table) from Theobald Software to ensure smooth extractions.

Note the necessary [SAP Authority Objects](../../setup-in-sap/sap-authority-objects/#table):

```text
S_TABU_NAM ACTVT=03; TABLE=ENLFDIR

```

### Extract Data in Background Job

If *Background job timeout (seconds)* checkbox is activated, the table extraction is executed as a background job in SAP. This setting is optional and is supported in combination with function module THEO/READ_TABLE or Z_THEO_READ_TABLE version 2.0. Activate the setting *Background job timeout (seconds)* for long-running extractions with a large amounts of data that may run into a timeout error (“Time limit exceeded”), when using the foreground mode.

Tip

The extraction jobs can be found in the SAP JobLog (**SM37**) under the JobName *theo_read_table*.

Warning

**Shared Memory ran out of memory!**\
If this error message pops up when running an extraction in the background, adjust the size of the Shared Memory. SAP recommends a Shared Memory size of 800MB~1.5GB for a production/test system or 2GB~4GB for S/4 systems, see [SAP Support: How to solve SYSTEM_NO_SHM_MEMORY runtime error](https://ga.support.sap.com/dtp/viewer/#/tree/1080/actions/12107).

### Advanced Settings

#### Background Job Timeout (seconds)

Sets a timeout period for extractions that run in background mode.\
The default value is 180 seconds. The maximum timeout value is 3600 seconds.

This option can be used if the data transfer to a destination takes a lot of time, e.g., when bulk-inserts are deactivated for database destinations.

Note

The background job timeout setting only takes effect if the extractions run in background mode using [/THEO/READ_TABLE](../../setup-in-sap/custom-function-module-for-table-extraction/).

#### Adjust Currency Decimals

The default number of decimal places for a currency in the SAP database is 2 decimals. Currencies that do not have decimals are also stored in this format, e.g. JPY, VND, KRW, etc.

Example:

| Currency | Actual Amount | Amount stored in SAP database | | --- | --- | --- | | JPY | 100 | 1.00 | | KRW | 10000 | 100.00 |

When extracting currencies with no decimals, the amount stored in SAP is returned e.g., 100 JPY are extracted as 1.00. To correct the decimal placement of the extracted data, activate **Adjust Currency Decimals**. If **Adjust Currency Decimals** is active, currencies without decimals are multiplied by a factor that balances out the decimals.

**Adjust Currency Decimals** also requires the extraction of the corresponding CURRENCY field that can be used as a reference for the multiplication factor. Use the **[Load live preview]** function to find the correct currency field/s.

- If the currency field is part of the table, add it to the output.
- If the currency field is in another table, join the tables.
- If the reference is not part of a table, **Adjust Currency Decimals** cannot be used.

Note

The multiplication factor used in *Adjust Currency Decimals* is determined by the SAP currency table TCURX. To access the table, the following SAP Authority objects must be set in SAP: *S_TABU_NAM ACTVT=03; TABLE=TCURX*.

## Third Party Tool Settings

#### Gateway host

Enter the data of your gateway host. It is often the same host as the SAP application server.

#### Gateway service

Enter the data of your gateway service (*sapgwXX*, where XX is the system number).

#### Program ID

Enter the program ID of the SAP RFC destination.

Xtract Universal supports multiple options to dynamize extractions:

- [Extraction parameters](extraction-parameters/)
- [Script expressions](script-expressions/)
- [SQL parameters](sql-parameters/)

While script expressions compute values, extraction parameters and by extension SQL parameters require users to pass actual values when running an extraction. Extraction parameters affect the extraction settings, destination settings, the SAP connection settings, and the user-defined (custom) runtime parameters of an extraction. This includes the runtime parameters that are used in SQl commands (SQL parameters).

### About Custom Runtime Parameters

User-defined runtime parameters can be used to filter SAP data before writing the data to the destination. They are part of the [extraction parameters](extraction-parameters/#custom) and can be used in SQL statements, see [SQL parameters](sql-parameters/). There are two types of custom runtime parameters:

- scalar parameters that represent a single value.
- list parameters that represent multiple values separated by a comma, e.g., 1,10 or “1”, “10”.

Most extraction types offer an *Edit runtime parameters* menu that allows users to create custom runtime parameters.

Once runtime parameters are available, a switch is added to all input fields that support runtime parameters. The switch allows users to switch between a static input value () and an existing runtime parameter (), e.g., *Parameter0*.

The following table shows what extraction types and settings support custom runtime parameters:

| Extraction Type | Settings that Support Custom Runtime Parameters | | --- | --- | | BAPI | [Import parameters](../bapi/input-and-output/#import-parameters), [Table parameters](../bapi/input-and-output/#table-parameters) | | BWCube | [BEx variables](../bwcube/variables-and-filters/#edit-variables), [dimension filters](../bwcube/variables-and-filters/#set-dimension-filters) | | DeltaQ | [Selections](../deltaq/selections/#edit-selections) | | ODP | [Selections](../odp/selections/#edit-selections) | | Query | [Selections](../query/variants-and-selections/#edit-selections) | | Report | [Selections](../report/variants-and-selections/#edit-selections) | | Table | [WHERE clause](../table/where-clause/), [HAVING clause](../table/having-clause/) | | Table CDC | [WHERE clause](../table-cdc/where-clause/) |

The Xtract Universal Designer can run extractions by passing parameters that define how data is extracted from the source.

The following categories of extraction parameters are available:

- [Extraction](#extraction) parameters affect the extraction and destination settings.
- [Source](#source) parameters affect the SAP connection settings.
- [Custom](#custom) parameters correspond to the user-defined runtime parameters or [SQL parameters](../sql-parameters/) of an extraction.

The parameters can be accessed in the "Run Extraction" window.\
To open the "Run Extraction" window, select an extraction from the [list of extractions](../../designer/#list-of-extractions) and click **[Run]**. For more information, see [Run Extractions](../../execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

### Extraction

The number of available parameters depends on the extraction type and destination.\
Example: *decimalSeparator* is a parameter specific to CSV destinations.

| Parameter | Description | Information | | --- | --- | --- | | clearBuffer | Clears/Keeps the result buffer | Default value is *false* | | preview | Enables/Disables the preview mode | Default value is *false* | | source | Selects the SAP source system from which data is extracted (e.g., SAP_DEV and SAP_PROD). Applies only if more than one SAP system is used. | - | | destination | Selects the destination to which extraction is written (e.g., db_1 and db_2). Applies only if more than one destination is used. | - | | rows | Sets the maximum number of rows to be extracted | Available for [Table](../../table/settings/#row-limit) | | whereClause | Sets a WHERE clause. SAP system variables are applicable. | Available for [Table](../../table/where-clause/) | | packageSize | Sets the package size | Available for [Table](../../table/settings/#package-size) | | updateMode | Sets the update mode to use for the run: Full, Delta, Recovery | Available for [ODP](../../odp/update-mode/) | | subscriptionSuffix | Suffix to use multiple inits on a single SAP system | Available for [ODP](../../odp/) | | extractDataOnDeltaInit | Extracts data if request is *delta init* | Available for [ODP](../../odp/update-mode/) | | hierarchyName | The name of the hierarchy to extract | Available for ODP, DeltaQ & Hierarchy | | representation | The representation / output format of the hierarchy to extract: "ParentChild", "Natural" or "ParentChildWithNodeNames" | Available for [Hierarchy](../../hierarchy/output-format/) | | dateTo | The valid-to-date of the hierarchy to extract in the format YYYYMMDD | Available for [Hierarchy](../../hierarchy/) | | variant | Name of a variant | Available for [Report](../../report/variants-and-selections/) & [SAP Query](../../query/variants-and-selections/) | | batchJobName | Name of the Batch Job | Available for [Report](../../report/settings/) | | gatwewayHost | Gateway Host | Available for [DeltaQ](../../deltaq/deltaq-customization/) | | gatewayService | Gateway Service | Available for [DeltaQ](../../deltaq/deltaq-customization/) | | programID | Program ID | Available for [DeltaQ](../../deltaq/deltaq-customization/) | | logicalDestination | Logical Destination | Available for [DeltaQ](../../deltaq/deltaq-customization/) | | requestID | Request ID (for Repair Request only) | Available for [DeltaQ](../../deltaq/deltaq-customization/) | | updateMode | Sets the update mode to use for the run: Init, Full, Delta, Activate, InitNoDelta, InitNoncumulative, Repeat | Available for [DeltaQ](../../deltaq/update-mode/) | | updateType (deprecated) | Sets the update mode to use for the run: F (Full), C (Delta Init), S (Init no data), D (Delta Update), R (Repeat), A (Activate) | Available for [DeltaQ](../../deltaq/update-mode/) | | decimalSeparator | Sets a symbol between integer and fractional part | Available for CSV destinations | | columnSeparator | Sets a symbol which indicates the start of a new column | Available for CSV destinations |

#### Example

1. Select the checkbox of the parameter you want to override.
1. Enter the value and confirm by pressing enter.
   - Extraction URL before changing the parameter:\
     `http://sherri.theobald.local:8065/start/KNA1/`
   - Extraction URL after editing the parameter **source** (name of the SAP source system):\
     `http://sherri.theobald.local:8065/start/KNA1/?source=SAP_PROD`

### Source

The connection settings to an SAP source can be changed dynamically via the URL and the [command-line tool xu.exe](../../execute-and-automate/call-via-commandline/). In the *Source* tab you can override the values that are defined in [SAP source details](../../sap-connection/settings/).

| Parameter name | Parameter description | | --- | --- | | lang | Changes the logon language of the SAP source system | | logonTicket | Changes the ticket issuer of the [SAP logon ticket](../../sap-connection/sso-with-logon-ticket/) |

Note

The parameter **logonTicket** can only be set if SAP logon ticket is selected as the authentication method in [SAP source details](../../sap-connection/sso-with-logon-ticket/#configure-sso-with-logon-ticket-in-the-sap-source).

#### Example

1. Select the checkbox of the parameter you want to override.
1. Enter the value and confirm by pressing enter.
   - Extraction URL before changing the parameter:\
     `http://sherri.theobald.local:8065/start/KNA1/`
   - Extraction URL after editing the parameter **lang** (language setting for the SAP source system):\
     `http://sherri.theobald.local:8065/start/KNA1/&lang=EN`

### Custom

The tab *Custom* is only active when [user-defined runtime parameters](../#about-custom-runtime-parameters) or [SQL parameters](../sql-parameters/) are available.\
Check the checkbox and enter a new value to set the user defined parameter.

#### Example

1. Select the checkbox of the parameter you want to override.
1. Enter the value and confirm by pressing enter.
   - Extraction URL before changing the parameter:\
     `http://sherri.theobald.local:8065/start/KNA1/`
   - Extraction URL after editing the parameter **myParameter** (name of a runtime parameter):\
     `http://sherri.theobald.local:8065/start/KNA1/&myParameter=EN`

______________________________________________________________________

#### Related Links:

- [Web API](../../../web-api/)
- [Script Expressions](../script-expressions/)

Script expressions offer a way of adding dynamic parameters to Xtract Universal. Script expressions are resolved at extraction runtime. The output of a script expression is a string. This string can be used as input for further .NET string operations.

Script expressions can be used in the following scenarios:

- as dynamic folder paths in cloud storage destinations.
- as dynamic file names in database destinations, cloud storage destinations and flat-file destinations.
- as custom SQL commands in database destinations.
- as selection parameters for [Table](../../table/where-clause/#script-expressions) or [DeltaQ](../../deltaq/selections/#script-expressions-for-deltaq) extractions.

### Syntax of Script Expressions

Script expressions use the C# syntax. They must begin and end with a hash symbol (#). The formula starts and ends with curly brackets ({}), e.g., `#{ Extraction.TableName }#`

Note

Expressions that are specific to Xtract Universal are case sensitive. Make sure to use the exact syntax as documented below.

#### IF-Statements

An IF-statement (ternary operator) is supported and uses the following syntax:

```c#
iif([bool condition], [string trueResult], [string falseResult])

```

**Examples:**

| Input | Output | Description | | --- | --- | --- | | `#{ iif(DateTime.Now.Month==7, "July","Unknown")}#` | July | In month 7 the output is "July", all else is "Unknown". | | `#{Extraction.ExtractionName}##{ iif(string.IsNullOrEmpty(Extraction.Context), string.Empty, "/" + Extraction.Context)}#` | | \*Extraction.Context\* returns a result only with ODP extractions. With all other extraction types the result is empty. - If the extraction name is 'SAP_1' and the extraction type is 'Table', the resulting file path would be `SAP_1/[filename]`. - If the extraction name is 'SAP_2' and the extraction type is 'ODP' and a SAP DataSource (extraction context: SAPI) is being extracted, the resulting file path would be `SAP_2/SAPI/[filename]`. |

### Script Expressions based on .NET

Xtract Universal script expressions support the following .NET objects, properties and methods from the .NET System Namespace of Xtract Universal's current [.NET framework](../../setup/requirements/#other-applications-and-frameworks):

*Object*, *Boolean*, *Char*, *String*, *SByte*, *Byte*, *Int16*, *UInt16*, *Int32*, *UInt32*, *Int64*, *UInt64*, *Single*, *Double*, *Decimal*, *DateTime*, *TimeSpan*, *Guid*, *Math*, *Convert*.

Note

The most common usage scenario is using the methods and properties of the .NET *DateTime* and *String* classes. For further information of supported [.NET classes and their properties and methods](https://docs.microsoft.com/en-us/dotnet/api/system?redirectedfrom=MSDN&view=netframework-4.7.2) including [DateTime](https://docs.microsoft.com/en-us/dotnet/api/system.datetime?view=net-5.0) and [String](https://docs.microsoft.com/en-us/dotnet/api/system.string?view=netframework-4.7.2) see the Microsoft online documentation.

#### Supported Keywords

The following key words are supported:

*true*, *false*, *null*.

### List of available Script Expressions

#### Use Script Expressions as Dynamic File Names

Script expressions can be used to generate a dynamic file name. This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].LowerValue}#` | Lower value of the range selection. | | `#{Extraction.Fields["FISCPER"].RangeSelections[0].UpperValue}#` | Upper value of the range selection. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFiels]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. |

#### Use Script Expressions as Dynamic Folder Paths

Script expressions can be used to generate a dynamic folder path. This allows generating folder paths that are composed of an extraction's properties, e.g., extraction name, SAP source object. The described scenario supports script expressions based on .NET and the following XU-specific custom script expressions:

| Input | Description | | --- | --- | | `#{Source.Name}#` | Name of the extraction's SAP source. | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.Type}#` | Extraction type (*Table*, *ODP*, *BAPI*, etc.). | | `#{Extraction.SapObjectName}#` | Name of the SAP object the extraction is extracting data from. | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. | | `#{Extraction.SapObjectName.TrimStart("/".ToCharArray())}#` | Removes the first slash '/' of an SAP object. Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. | | `#{Extraction.SapObjectName.Replace('/', '_')}#` | Replaces all slashes '/' of an SAP object. Example: /BIO/TMATERIAL to \_BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. | | `#{Extraction.Context}#` | Only for ODP extractions: returns the context of the ODP object (*SAPI*, *ABAP_CDS*, etc). | | `#{Extraction.Fields["[NameSelectionFields]"].Selections[0].Value}#` | Only for ODP extractions: returns the input value of a defined selection / filter. | | `#{Odp.UpdateMode}#` | Only for ODP extractions: returns the update mode (*Delta*, *Full*, *Repeat*) of the extraction. | | `#{TableExtraction.WhereClause}#` | Only for Table extractions: returns the WHERE clause of the extraction. | | `#{Extraction.Fields["[0D_NW_CODE]"].Selections[0].Value}#` | Only for BWCube extractions (MDX mode): returns the input value of a defined selection. | | `#{Extraction.Fields["[0D_NW_CHANN]"].RangeSelections[0].LowerValue}#` | Only for BWCube extractions (MDX mode): returns the lower input value of a defined selection range. | | `#{Extraction.Fields["[0D_NW_CHANN]"].RangeSelections[0].UpperValue}#` | Only for BWCube extractions (MDX mode): returns the upper input value of a defined selection range. | | `#{Extraction.Fields["0D_NW_CODE"].Selections[0].Value}#` | Only for BWCube extractions (BICS mode): returns the input value of a defined selection. | | `#{Extraction.Fields["0D_NW_CHANN"].RangeSelections[0].LowerValue}#` | Only for BWCube extractions (BICS mode): returns the lower input value of a defined selection range. | | `#{Extraction.Fields["0D_NW_CHANN"].RangeSelections[0].UpperValue}#` | Only for BWCube extractions (BICS mode): returns the upper input value of a defined selection range. |

#### Use Script Expressions in Custom SQL Statements

You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported:

| Input | Description | | --- | --- | | `#{Extraction.ExtractionName}#` | Name of the extraction. | | `#{Extraction.FullyQualifiedExtractionName}#` | Name of the extraction. If the extraction is part of an [extraction group](../../organize-extractions/), the name of the extraction group is included in the extraction name. This option avoids conflicts, when the extraction names are not unique. | | `#{Extraction.TableName }#` | Name of the database table extracted data is written to. | | `#{Extraction.RowsCount }#` | Count of the extracted rows. | | `#{Extraction.RunState}#` | Status of the extraction (Running, FinishedNoErrors, FinishedErrors, Cancelled). | | `#{(int)Extraction.RunState}#` | Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors, 6 = Cancelled). | | `#{Extraction.Timestamp}#` | Timestamp of the extraction. |

#### Use Script Expressions as Selection Parameters in Table and DeltaQ

Script expressions are usually used to determine a dynamic date based on the current date. When using script expressions in a [WHERE Clause](../../table/where-clause/#script-expressions), the value must be entered in single quotation marks.

| Input | Description | | --- | --- | | `#{ DateTime.Now.ToString("yyyyMMdd") }#` | Current date in SAP format (yyyyMMdd) | | `#{ String.Concat(DateTime.Now.Year.ToString(), "0101") }#` | Current year concatenated with "0101" (yyyy0101) | | `#{ String.Concat(DateTime.Now.ToString("yyyy"), "0101") }#` | Current year concatenated with "0101" (yyyy0101) | | `#{ String.Concat(DateTime.Now.ToString("yyyyMMdd").Substring(0,4), "0101") }#` | Current year concatenated with "0101" (yyyy0101) |

In Xtract Universal you can define custom runtime parameters that can be set dynamically when calling extractions. When using an SQL destination, these parameters are available for SQL commands.

A typical use case is the dynamization of WHERE clauses in the Table extraction type. The following table extraction has a custom parameter *WNAME* in the *WHERE-Clause*:

### Custom SQL Statement

In the window [Destination Settings](../../destinations/microsoft-sql-server/#destination-settings) you can use a custom SQL statement for the three database process steps and / or edit the SQL statement according to your requirements.

1. In the main window select an extraction with a custom parameter in the WHERE-Clause .
1. Click **[Destination]** . The window "Destination Settings" opens.
1. Select the option *Custom SQL* from the drop-down-menus in the following sections:
   - Preparation
   - Finalization
1. Click **[Edit SQL]**. The window "Edit SQL" opens.
1. Define an SQL statement and click **[OK]** to confirm your input .

### Custom SQL Example for Custom Parameters

In the following example the SAP table *KNA1* is expanded by adding the column *Custom_Parameter* of the type *NATIONAL CHARACTER VARYING(10)*. The column is filled dynamically by runtime parameters.

In the section **Row Processing** the column values from SAP are written into the previously created SQL target table. This SQL statement is therefore used as the default *Insert* statement. When rows are processed, only `NULL` values are written into the *Custom_Parameter* column.

In the section **Finalization** these `NULL` values are replaced using the SQL statements of the runtime parameter *WNAME* and the T-SQL command `UPDATE`.

Note

The data types that can be used in SQL statements depend on your SQL database version.

1. In the window "Destination Settings", select the option *Custom SQL* in the section **Preparation**. Click **Edit SQL**.

1. Select *Drop & Create* from the drop-down-menu and click **[Generate Statement]** .

1. Add the following line to the generated statement:

   ```sql
   [Custom_Parameter] NATIONAL CHARACTER VARYING(10)

   ```

1. Click **[OK]** to confirm your input.

1. In the window "Destination Settings", select the option *Custom SQL* in the section **Finalization**. Click **Edit SQL**.

1. Select *Insert* from the drop-down-menu and add the following SQL statement :

   ```sql
   UPDATE [dbo].[KNA1] 
   SET [Custom_Parameter] = @WNAME 
   WHERE [Custom_Parameter] IS NULL; 

   ```

1. Click **[OK]** to confirm your input .

### Set the Custom Parameter WNAME

1. Select the checkbox next to the parameter name to overwrite the parameter *WNAME*.
1. Enter the new value *US* and confirm your input by pressing enter.
1. Click **[Run]** to run the extraction.

### Result in SSMS

Check the result of the column *Custom_Parameter* in the SQL Server View of the *KNA1* table.

______________________________________________________________________

#### Related Links

- [Post-Processing Column Name Style](../../../knowledge-base/adjust-column-name-style/)

This page shows how to use the Query extraction type.\
The Query extraction type can be used to extract data via SAP queries.

### About SAP Queries

SAP Queries are used to access data sets in SAP, see [SAP Help - Working with Queries](https://help.sap.com/viewer/b1c834a22d05483b8a75710743b5ff26/7.51.6/en-US/0e05493bbccf41a79caed7099c82bd48.html) for more information. The SAP queries that can be used with Query extraction type are created by the SAP transactions SQ02 and SQ01. To use a BW Query as a data source, see [BW InfoCubes and BExQueries](../bwcube/).

### Prerequisites

- A connection to an SAP system is available, see [SAP Connection](../sap-connection/).
- The SAP user has sufficient user rights, see [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#query).

Warning

**Missing Authorization.**\
To use the Query extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust [SAP Authority Objects: SAP Query](../setup-in-sap/sap-authority-objects/#query) accordingly.

### Create a Query Extraction

1. In the main window of the Designer, click **[New]** to create a new extraction. The window "Create Extraction" opens.
1. Select an [SAP Connection](../sap-connection/) of type **RFC** from the drop-down menu **Source**.
1. Enter a unique name for your extraction.
1. Select the extraction type **Query**.
1. Click **[OK]**. The main window of the extraction type opens automatically.

The majority of the functions of the extraction type can be accessed in the main window.

### Look up an SAP Query

1. In the main window of the extraction type, click **[]**. The window "Query Lookup" opens.
1. Enter the name of an SAP query in the field **Query Name** or the name of a user group in the field **User group** . Use wildcards (\*) if needed.
1. Select the query work area that contains the query object . For more information, see [SAP Help: Query Areas](https://help.sap.com/doc/saphelp_nw74/7.4.16/en-us/4e/3bdad0b8503b0fe10000000a42189e/frameset.htm).
1. Click **[]**. Search results are displayed.
1. Select a query and click **[OK]**.

The application returns to the main window of the extraction type.

### Define the Query Extraction Type

The Query extraction type offers the following options for query extractions:

1. If the SAP query has variants, select a variant from the drop-down-list *Variant*. For more information, see [Choose a Variant](variants-and-selections/#choose-a-variant).
1. In the section *Selection Screen*, edit a selection criterion you want to change or dynamize . For more information, see [Edit Selections](variants-and-selections/#edit-selections).
1. Check the [Extraction Settings](settings/) and the [General Settings](general-settings/) before running the extraction.
1. Click **[OK]** to save the extraction type.

You can now run the extraction, see [Execute and Automate Extractions](../execute-and-automate/).

Runtime parameters are are placeholders for values that are passed at runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom). They can be created in context of [Selections](../variants-and-selections/#edit-selections).

## Create Runtime Parameters

There are two types of runtime parameters:

- [Scalar parameters](#scalar-parameters) represent a single value.
- [List parameters](#list-parameters) represent multiple values.

### Scalar Parameters

Follow the steps below to create a scalar runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add Scalar]** to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.

   | Type | Description | | --- | --- | | *Text* | Can be used for any type of SAP selection field. | | *Number* | Can be used for numeric SAP selection fields. | | *Flag* | Can only be used for SAP selection fields THAT require an ‘X’ (true) or a blank ‘‘ (false) as input value. |

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

### List Parameters

Follow the steps below to create a list runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add List]** to define list parameters that contain multiple values separated by commas e.g., 1,10 or “1”, “10”. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

## Assign Runtime Parameters

Follow the steps below to assign the runtime parameters to selections.

1. In the main window of the extraction type, click the **[Edit]** button next to the selection you want to parameterize. The window "Edit Selections" opens.
1. Add a filter to the selection, see [Edit Selections](../variants-and-selections/#edit-selections).
1. Click the icon button next to the input field to switch between static values () and runtime parameters (). If no icon button is available, [create a runtime parameter](#create-runtime-parameters).
1. Select a runtime parameter from the dropdown-list.
1. Click **[OK]** to confirm the input.

Pass values during runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom).

This page contains an overview of the settings in the window "General Settings".\
To open the general settings, click **General Settings** in the main window of the extraction type.

### Misc. Tab

The *Misc.* tab covers cache settings, column encryption and keywords of an extraction type.

#### Cache results

The *Cache results* option is only available in [pull destinations](../../destinations/), e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times. To decrease the SAP server load, you can select the **Cache results** option, this way the pull destination pulls the data from cache and not from the SAP.

Cache results increases the performance and limits the impact on the SAP system. If this behavior is undesirable (for example, because the data must be always 100% up-to-date), the cache option must be explicitly disabled.

#### Keywords

One or more keywords (tags) can be assigned to an extraction. Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the "Search Extractions" window. To open the "Search Extractions" window, click **[ Search]** in the main window of the Designer.

Tip

To add keywords to multiple extractions at once, select the extractions in the main window of the Designer.\
Right-click + **Add/Remove keywords** opens the window "Add/Remove Keywords To/From Multiple Extractions".

### Primary Key Tab

Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.

The depicted example shows the SAP object *MAKT* with its primary key inherited from SAP in the general settings of the Designer. In this example the primary key consists of *MANDT*, *MATNR* and *SPRAS*. The primary key is also taken over in the destination.

Note

A defined primary key field in a table is a prerequisite for merging data.

#### Generate Surrogate Key Column

If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs. The surrogate keys are hash values of type signed 8 byte integer, e.g., `#-3008591679982390000`. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.

### Security Tab

Restrict user access to the extraction. For more information, see [Restrict Designer Access](../../access-restrictions/restrict-designer-access/).

### Columns Order Tab

The "Columns Order" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns. Index 0 defines the first column in the result set, index 1 the second columns, etc.

| Option | Description | | --- | --- | | **Enabled** | When this option is active, the defined column order is applied when running the extraction. | | **Swap** | Swaps the index of 2 columns. All other columns keep their indexes. | | **Insert** | Inserts the selected column into the selected index. All other indexes are recalculated. | | **Reset default** | Restores the original column order. |

This page contains an overview of the extraction settings in the Query extraction type.\
To open the extraction settings, click ****Extraction Settings**** in the main window of the extraction type.

### Extraction Settings

#### Max Rows

Specifies the maximum number of extracted records. 0 extracts all data. You can use this option to perform tests with a small amount of data by entering a row limit of e.g., 1000.

#### Treat 'No Data Selected' as Error

If this option is active, an error message is displayed when there is no data to be extracted.

Most queries allow entering selections before query execution. Selections limit the result set of the query to extract only records that match the selection.\
A selection variant can be created in SAP, see [SAP Help: Query Variants](https://help.sap.com/docs/SAP_NETWEAVER_750/40d2cb3a4f9249d58e9bbc95f4dbaff8/4e535406a32c4f49e10000000a42189e.html?locale=en-US). The purpose of a variant is to minimize the necessity to enter selections when running a query.

Note

Manual selections and variants can be combined. Manual selections overwrite any selections in the variant.

### Choose a Variant

Choose a variant from the drop-down-list *Variant*.\
When creating a new variant in SAP after creating the extraction, click **[]** to load the new variant.

The selections of the variant are **not** displayed in the *Selection Screen* section of the window. To see the definition of a variant, open the variant in SAP.

Tip

You can define the variant at runtime by using a corresponding parameter in the extraction URL, see [Extraction Parameters](../../parameters/extraction-parameters/).

### Edit Selections

The *Selection Screen* in the main window of the component corresponds to the input screen in SAP.

Note

Some selection fields only have a technical name and no description. To understand which field corresponds to a field in SAP, open the input screen in SAP. Click on a selection field and press the function key `F1` to display the technical name of the selection field.

Follow the steps below to edit selection fields and filter data:

1. In the subsection *Selection Screen*, click **[Edit]** next to the field you want to edit. The window “Edit selection” opens.

1. Click **[Single]**, **[Range]** or **[List]** to add a corresponding filter, see [Filter Options](#filter-options).

1. In the column **Sign** , select *Include* to add the filtered data to the output or select *Exclude* to remove the filtered data from the output.

1. In the column **Option** , select an operator, see [Filter Options](#filter-options).

1. In the column **Value**, enter values directly into the input fields **Low** and **High** or assign existing [runtime parameters](../edit-runtime-parameters/) to the selection fields .

   Note

   When runtime parameters are available, you can use the icon button next to the input field to switch between static values () and runtime parameters ().

1. Click **[OK]** to confirm your input.

Note that edited selection fields overwrite the selection fields in the variant.

Tip

If you use multiple selection parameters, it is more efficient to create a variant in SAP.

### Filter Options

The Query extraction type offers the following filter options:

| Type | Operator | Description | | --- | --- | --- | | Single | | Compare data to a single specified value. | | | *(not) like pattern* | True if data values do (not) contain to the specified value. | | | *(not) equal to* | True if data is (not) equal to the specified value. | | | *at least* | True if data is greater than or equal to the specified value. | | | *more than* | True if data is greater than the specified value. | | | *at most* | True if data is less than or equal to the specified value. | | | *less than* | True if data is less than the specified value. | | Range | | Check if the data is (not) within a specified range of values. | | | *(not) between* | True if data values do (not) lie between the 2 specified values. | | List | | Check if the data is part of a specified list of values. | | | *element of* | True if data values are part of the list. |

### Data Format

Use the following internal SAP representation for input:

- Date: The date 01.01.1999 has the internal representation 19990101 (YYYYMMDD).
- Year period: The year period 001.1999 has the internal representation 1999001 (YYYYPPP).
- Numbers: Numbers must contain the leading zeros, e.g., customer number 1000 has the internal representation 0000001000.

Warning

**Values accept only the internal SAP representation.**\
Input that does not use the internal SAP representation results in error messages. Use the internal SAP representation. Example:

```text
ERPConnect.ABAPProgramException: RfcInvoke failed(RFC_ABAP_MESSAGE): Enter date in the format \_.\_.\_

```

This page shows how to use the Report extraction type.\
The Report extraction type can be used to extract data from most standard and custom ABAP reports and SAP transactions. A report extraction is possible if the report returns a table-like structure in SAP.

### About Reports

ABAP report programs extract and present huge amounts of data for SAP business applications. SAP offers predefined standard reports that cover the basic needs of customers. They can be executed via transaction codes.

Tip

Use transaction code SAP1 to get a list of all reports for all modules.

### Custom Reports

The extraction of custom reports (Z reports) is possible if the report returns a table-like structure in SAP. Issues specific to Z reports are not included in the scope of support provided by Theobald Software.

### Prerequisites

- A connection to an SAP system is available, see [SAP Connection](../sap-connection/).
- The SAP user has sufficient user rights, see [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#report).
- The custom function module `Z_XTRACT_IS_REMOTE_REPORT` is installed in your SAP system, see [Function Module for Reports](../setup-in-sap/custom-function-module-for-reports/#installation-of-z_xtract_is_remote_report). As of `Z_XTRACT_IS_REMOTE_REPORT` version 1.2 access to reports must be explicitly granted, see [Knowledge BAse Article: Authorize Access to Specific Reports](../../knowledge-base/authorize-access-to-specific-reports/).
- The report must return a table-like structure in SAP.

Warning

**Missing Authorization.**\
To use the Report extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust [SAP Authority Objects: Report](../setup-in-sap/sap-authority-objects/#report) accordingly.

### General Workflow

The following graphic shows the general workflow of using the Report extraction type:

### Create a Report Extraction

1. In the main window of the Designer, click **[New]** to create a new extraction. The window "Create Extraction" opens.
1. Select an [SAP Connection](../sap-connection/) of type **RFC** from the drop-down menu **Source**.
1. Enter a unique name for your extraction.
1. Select the extraction type **Report**.
1. Click **[OK]**. The main window of the extraction type opens automatically.

The majority of the functions of the extraction type can be accessed in the main window.

### Look up a Report or Transaction

1. In the main window of the extraction type, click **[]**. The window “Report Lookup” opens.

1. In the field **Report Name**, enter the name of a report to extract . Use wildcards (*), if needed. Alternatively, select* *TCODE*\* to look up SAP Transaction codes.

   Tip

   In certain cases reports cannot be determined based on the TCODE. You can check the report name of a TCODE using the SAP GUI menu **System > Status...**.

1. Click **[]** . Search results are displayed.

1. Select a report and click **[OK]** to confirm.

The application now returns to the main window of the extraction type.

### Define the Report Extraction Type

The Report extraction type offers the following options for report extractions:

1. If the report has variants, select a variant from the drop-down-list *Variant*. For more information, see [Choose a Variant](variants-and-selections/#choose-a-variant).
1. Optional: In the section *Selection Screen*, edit a selection criterion you want to change or dynamize . For more information, see [Edit Selections](variants-and-selections/#edit-selections).
1. Click **[Load live preview]** to display a live preview of the first 100 records.
1. Optional: If your report has varying column widths, activate **Dynamic column widths and offsets**. The column widths and offsets are then adjusted dynamically at report runtime.
1. Click **[Automatically detect columns]** to execute the report based on the selected variant or selections and detect columns automatically.
1. Check if the automatically detected columns are accurate.\
   When automatic column detection is not possible, the column names, widths and offsets must be set manually, see [Define Columns manually](report-columns-define/#define-columns-manually).
1. Optional: Define row settings to remove or parse certain rows, see [Define Rows](report-rows-define/).
1. Check the [Extraction Settings](settings/) and the [General Settings](general-settings/) before running the extraction.
1. Click **[OK]** to save the extraction type.

You can now run the extraction, see [Execute and Automate Extractions](../execute-and-automate/).

### Example Extraction

The depicted example shows how to set up a simple report extraction:

1. Look up report RLT10010 (Evaluation of Movements per Storage Type).
1. Select variant *VAR01*.
1. Load a live preview.
1. Automatically detect columns.
1. Remove the header (skip the first 8 rows).

______________________________________________________________________

#### Related Links

- [SAP Wiki: Types of ABAP Reports](https://wiki.scn.sap.com/wiki/display/ABAP/Types+of+Reports)
- [Knowledge Base Article: Authorize Access to Specific Reports](../../knowledge-base/authorize-access-to-specific-reports/)

Runtime parameters are are placeholders for values that are passed at runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom). They can be created in context of [Selections](../variants-and-selections/#edit-selections).

## Create Runtime Parameters

There are two types of runtime parameters:

- [Scalar parameters](#scalar-parameters) represent a single value.
- [List parameters](#list-parameters) represent multiple values.

### Scalar Parameters

Follow the steps below to create a scalar runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add Scalar]** to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.

   | Type | Description | | --- | --- | | *Text* | Can be used for any type of SAP selection field. | | *Number* | Can be used for numeric SAP selection fields. | | *Flag* | Can only be used for SAP selection fields THAT require an ‘X’ (true) or a blank ‘‘ (false) as input value. |

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

### List Parameters

Follow the steps below to create a list runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add List]** to define list parameters that contain multiple values separated by commas e.g., 1,10 or “1”, “10”. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

## Assign Runtime Parameters

Follow the steps below to assign the runtime parameters to selections.

1. In the main window of the extraction type, click the **[Edit]** button next to the selection you want to parameterize. The window "Edit Selections" opens.
1. Add a filter to the selection, see [Edit Selections](../variants-and-selections/#edit-selections).
1. Click the icon button next to the input field to switch between static values () and runtime parameters (). If no icon button is available, [create a runtime parameter](#create-runtime-parameters).
1. Select a runtime parameter from the dropdown-list.
1. Click **[OK]** to confirm the input.

Pass values during runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom).

This page contains an overview of the settings in the window "General Settings".\
To open the general settings, click **General Settings** in the main window of the extraction type.

### Misc. Tab

The *Misc.* tab covers cache settings, column encryption and keywords of an extraction type.

#### Cache results

The *Cache results* option is only available in [pull destinations](../../destinations/), e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times. To decrease the SAP server load, you can select the **Cache results** option, this way the pull destination pulls the data from cache and not from the SAP.

Cache results increases the performance and limits the impact on the SAP system. If this behavior is undesirable (for example, because the data must be always 100% up-to-date), the cache option must be explicitly disabled.

#### Keywords

One or more keywords (tags) can be assigned to an extraction. Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the "Search Extractions" window. To open the "Search Extractions" window, click **[ Search]** in the main window of the Designer.

Tip

To add keywords to multiple extractions at once, select the extractions in the main window of the Designer.\
Right-click + **Add/Remove keywords** opens the window "Add/Remove Keywords To/From Multiple Extractions".

### Primary Key Tab

Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.

The depicted example shows the SAP object *MAKT* with its primary key inherited from SAP in the general settings of the Designer. In this example the primary key consists of *MANDT*, *MATNR* and *SPRAS*. The primary key is also taken over in the destination.

Note

A defined primary key field in a table is a prerequisite for merging data.

#### Generate Surrogate Key Column

If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs. The surrogate keys are hash values of type signed 8 byte integer, e.g., `#-3008591679982390000`. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.

### Security Tab

Restrict user access to the extraction. For more information, see [Restrict Designer Access](../../access-restrictions/restrict-designer-access/).

### Columns Order Tab

The "Columns Order" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns. Index 0 defines the first column in the result set, index 1 the second columns, etc.

| Option | Description | | --- | --- | | **Enabled** | When this option is active, the defined column order is applied when running the extraction. | | **Swap** | Swaps the index of 2 columns. All other columns keep their indexes. | | **Insert** | Inserts the selected column into the selected index. All other indexes are recalculated. | | **Reset default** | Restores the original column order. |

This page shows how to define the columns in the result set of a report extraction.

A report column is defined by its name, offset and length. Per default, all columns are of data type *string*. To identify a columns, the report needs to be executed. Columns can then be identified based on the output.

Tip

At this stage, use a selection or variant that returns only a few records. This can be adapted later on.

- Certain classical ABAP reports are developed to use the pipe symbol '|' as a delimiter for the output columns. In this case the Report extraction type can automatically identify the columns. Automatic column detection also works for most ALV reports. Example:
- For reports where the output is not separated by the pipe symbol, the columns must be identified manually. Example:

For information on how to parse rows as columns, see [Parse Header Row as New Column](../report-rows-define/#parse-header-row-as-new-column).

### Define Columns Automatically

#### Automatically detect columns

Click **[Automatically detect columns]** to execute the report based on the selected variant or selections. If they can be identified automatically, the column name, width and offset are displayed in the *Columns* section in the main window of the extraction type.

#### Dynamic column width and offset

If this option is active, the column width and offset is adjusted dynamically at report runtime. This can be required for reports that have varying column widths depending on the report's selection criteria.

### Define Columns Manually

When automatic column detection is not possible, the column names, widths and offsets must be set manually.

1. Make sure no columns are defined yet. If columns are defines, click **[]** in the *Columns* section in the main window of the extraction type to delete the columns.
1. Click **[Load live preview]**. The report is executed based on the selected report variant or selections. The output of the report is displayed in the *Load Preview* section.
1. To define the beginning (offset) of a column, press and hold down the left mouse button in the *Load Preview* section.
1. To define the length of a column, move the mouse pointer to the right while still holding down the left mouse button.
1. Let go of the cursor. The report column is highlighted with a green background and an entry is added to the *Columns* section.
1. To change the column name, offset and width, click in the respective fields in the *Columns* section and enter a new value.
1. Repeat steps 3 to 6 until all columns are defined.

Note

Once a column is set and highlighted with a green background, its width and offset cannot be changed via the graphics editor. Change it using the *Columns* section.

______________________________________________________________________

#### Related Links

- [Knowledge Base Article: Parse Reports in Xtract Universal](../../../knowledge-base/parse-reports/).

This page shows how to define the rows in the result set of a report extraction.\
The row settings are located in the tab *Skip and Parse Rows*.

### Row Settings

The Report extraction type offers the following options to remove rows from the result set of a report extraction:

| Option | Description | | --- | --- | | [Skip rows from top](#skip-rows-from-top) | Removes the first n number of rows from the top of the report. | | [Skip rows from button](#skip-rows-from-bottom) | Removes the last n number of rows from the button of the report. | | [Skip rows by pattern](#skip-rows-from-bottom) | Uses a pattern or regular expression to remove rows that contain the pattern. | | [Skip rows by keyword](#add-row-to-skip) | Uses a keyword to remove all rows that contain the keyword. |

For more information on how to remove and parse rows to merge groups of data sets into a single data set, see [Knowledge Base Article: Parse Reports in Xtract Universal](../../../knowledge-base/parse-reports/).

#### Skip rows from top

Enter the number of rows you want to skip at the beginning of the report. Some reports display meta information in the header section of the report, before the actual report body. This setting allows skipping the meta information.

#### Skip rows from bottom

Similar to *skip rows from top*. Enter the number of rows you want to skip in the footer section of the report.

#### Report rows per data row

Use this setting for ABAP reports that return two or more "physical" rows to display a single "semantic" data row. Enter the number of physical rows that represent a single data row.\
Example: Report RIEQUI20

#### Report width

Use this setting in combination with **Report rows per data row**. **Report width** defines the length of each physical row. The maximum width of extracted reports is limited to 1024 characters per row.

#### Header pattern

Enter a search pattern to detect the table header, e.g., *Created on*.\
The Report extraction type scans the report output for this pattern and uses the row that contains this pattern as the report header. Any duplicates of the header row are removed from the result set.

Rows that contain the header pattern are displayed in a blue font in the preview section.

Tip

Alternatively, right-click the row that you want to use as a header and select **Select as header** from the context menu.

This setting is usually not required if the report's columns can be [detected automatically](../report-columns-define/#define-columns-automatically) and *Dynamic column widths and offsets* is active in the main window of the extraction type.

#### Skip rows by pattern

Enter a search pattern. All report rows that contain the pattern are removed from the result set.\
The skip row setting can be used to skip header rows that are repeated in the output body of reports, see also [Skip rows by keyword](#skip-rows-by-keyword).\
The live preview in the Report extraction type does not include the **Row Skip Pattern** option, because the rows are only removed after the report data is extracted from SAP.

- Regular expressions are supported.
- Multiple row skip patterns can be entered, separated by the pipe symbol "`|`", e.g., `2020|2021|-|Sum` removes all rows containing ‘2020’, ‘2021’, ‘-‘ and ‘Sum’.
- To process special symbols, add `\`before the symbol, e.g., `\*` removes rows that contain the sum symbol * .
- Only works with database destinations, e.g., Microsoft Azure Storage, Mircosoft SQl Server, Snowflake, etc.

Note

The setting *Skip rows by pattern* is usually not required if the report columns can be [detected automatically](../report-columns-define/#define-columns-automatically) and *Dynamic column widths and offsets* is checked in the Report window.

### Skip rows by keyword

Define rows that are removed from the result set of the report extraction. This option can be used to skip header rows that are repeated in the output body of reports.

#### Add Row to Skip

Click **[Add row to skip]** to define a row that is removed from the result set of the report extraction.\
Enter a unique keyword from the row you want to remove.\
Example: `**` removes all rows that contain the characters `**`.

Rows that are excluded from the result set are displayed in a gray font in the preview section.

Tip

Alternatively, right-click the row you want to remove and select **Ignore/unignore row** from the context menu.

### Parse Header Row as new Column

Define rows that are added to the result set as columns.\
This option can be used for reports that contain groups with multiple headers.

#### Add row to parse as column

Click **[Add row to parse as column]** to add a new column to the result set.\
Enter the following column properties manually or use the [parse helper](#parse-helper) to define the column properties:

| Property | Description | | --- | --- | | *Keyword* | Enter a unique keyword from the row to include it as a column in the result set. The report is scanned for rows that contain the string. The keyword search is case sensitive. | | *Name* | Enter a name for the new column. | | *Offset* | Define the beginning (offset) of the content that is written into the new column. *Example:* Enter 0 if the content for the new column is located at the start of the row. Enter 25 if the content starts 25 characters into the row. | | *Width* | Define the width of the new column. |

Tip

Alternatively, to add row to parse as column, right-click a header row you want to use as a column and select **Parse header row as new column** from the context menu.

Rows that are parsed as a column are displayed in a red font in the preview section.\
For more information, see [Knowledge Base Article: Parse Reports in Xtract Universal](../../../knowledge-base/parse-reports/).

#### Parse Helper

The parse helper is a feature that helps you define properties for new columns:

1. In the subsection *Parse Header Row as new Column*, click **[Add row to parse as column]** to add a new column to the result set.

1. Click **[]** . The window "Parse Helper" opens and a new preview of the report data is fetched. This process may take several seconds to complete.

1. Under **Search keyword** enter a unique keyword from the row you want to parse as a column. Matching rows are displayed in the preview section of the window.

   Note

   The keyword search is case sensitive.

1. Make sure only header rows are displayed in the preview section. If the preview includes regular rows, edit the keyword until only header rows remain.

1. Under **New Column Name** enter a name for the new column.

1. Mark the content of the column in the preview section by pressing and dragging the mouse pointer over the length of the content.

1. Click **[OK]** to save the column properties.

The new column is added at the beginning of the report *after* running the extraction. Therefore, the new column is not included in the preview section of the extraction type.

Rows that are parsed as a column are displayed in a red font in the preview section.

______________________________________________________________________

#### Related Links

- [Knowledge Base Article: Parse Reports in Xtract Universal](../../../knowledge-base/parse-reports/).

This page contains an overview of the extraction settings in the Report extraction type.\
To open the extraction settings, click ****Extraction Settings**** in the main window of the extraction type.

### Batch Processing

#### Use Background Mode

If you choose this option, the ABAP report is executed as a batch job in SAP. Use this option for long running reports in SAP that would run into an RFC timeout when called in dialog mode. Certain reports that throw an error message when running in dialog mode can be extracted when run in background mode.

#### Background Job Timeout

Enter a time period (in seconds). The Report extraction type polls the status of the batch job in SAP for the specified time period. If the SAP batch job is not finished by the specified time period, the extraction aborts.

#### Background Job Name

The name of the background job under which the report is run in SAP.

#### Spool Destination

Enter the name of the spool destination (printer). If the ABAP report produces an output, a spool destination is generated. The spool destination defines where the output would go if printed, e.g., LP01 or LOCL. You can display all spool requests in SAP using the SAP transaction SP01.

### Function Module

The Report component requires installation of the custom function module `Z_XTRACT_IS_REMOTE_REPORT` in your SAP system, see [Install Report Custom Function Module](../../setup-in-sap/custom-function-module-for-reports/). If you manually created the function module in your SAP system and gave it a different name, enter that name in this field. The default is `Z_XTRACT_IS_REMOTE_REPORT`.

______________________________________________________________________

#### Related Links

- [Microsoft: Regular Expressions in the Microsoft Online Help](http://msdn.microsoft.com/en-us/library/az24scfc.aspx)
- [Knowledge Base Article: Parse Reports in Xtract Universal](../../../knowledge-base/parse-reports/)
- [Install Report Custom Function Module](../../setup-in-sap/custom-function-module-for-reports/)

Most reports allow entering selections before report execution. Selections limit the result set of the report to extract only records that match the selection.

In SAP a [selection variant](https://help.sap.com/docs/btp/ABAP/3353524716.html) can be created in the input screen of an ABAP report. The purpose of a variant is to save selection settings on your input screen and to minimize the necessity to enter selections when running a report.

Note

Manual selections and variants can be combined. Manual selections overwrite any selections in the variant.

### Choose a Variant

Choose a variant from the drop-down-list *Variant*.\
When creating a new variant in SAP after creating the extraction, click **[]** to load the new variant.

The selections of the variant are **not** displayed in the *Selection Screen* section of the window. To see the definition of a variant, open the variant in SAP.

Tip

You can define the variant at runtime by using a corresponding parameter in the extraction URL, see [Extraction Parameters](../../parameters/extraction-parameters/).

### Edit Selections

The *Selection Screen* in the main window of the component corresponds to the input screen in SAP.

Note

Some selection fields only have a technical name and no description. To understand which field corresponds to a field in SAP, open the input screen in SAP. Click on a selection field and press the function key `F1` to display the technical name of the selection field.

Follow the steps below to edit selection fields and filter data:

1. In the subsection *Selection Screen*, click **[Edit]** next to the field you want to edit. The window “Edit selection” opens.

1. Click **[Single]**, **[Range]** or **[List]** to add a corresponding filter, see [Filter Options](#filter-options).

1. In the column **Sign** , select *Include* to add the filtered data to the output or select *Exclude* to remove the filtered data from the output.

1. In the column **Option** , select an operator, see [Filter Options](#filter-options).

1. In the column **Value**, enter values directly into the input fields **Low** and **High** or assign existing [runtime parameters](../edit-runtime-parameters/) to the selection fields .

   Note

   When runtime parameters are available, you can use the icon button next to the input field to switch between static values () and runtime parameters ().

1. Click **[OK]** to confirm your input.

1. Click **[Load live preview]** in the main window of the extraction type to check the result of your selection. If runtime parameters are defined, you are prompted to populate the parameters with actual values.

Note that edited selection fields overwrite the selection fields in the variant.

Tip

If you use multiple selection parameters, it is more efficient to create a variant in SAP.

### Filter Options

The Report extraction type offers the following filter options:

| Type | Operator | Description | | --- | --- | --- | | Single | | Compare data to a single specified value. | | | *(not) like pattern* | True if data values do (not) contain to the specified value. | | | *(not) equal to* | True if data is (not) equal to the specified value. | | | *at least* | True if data is greater than or equal to the specified value. | | | *more than* | True if data is greater than the specified value. | | | *at most* | True if data is less than or equal to the specified value. | | | *less than* | True if data is less than the specified value. | | Range | | Check if the data is (not) within a specified range of values. | | | *(not) between* | True if data values do (not) lie between the 2 specified values. | | List | | Check if the data is part of a specified list of values. | | | *element of* | True if data values are part of the list. |

### Data Format

Use the following internal SAP representation for input:

- Date: The date 01.01.1999 has the internal representation 19990101 (YYYYMMDD).
- Year period: The year period 001.1999 has the internal representation 1999001 (YYYYPPP).
- Numbers: Numbers must contain the leading zeros, e.g., customer number 1000 has the internal representation 0000001000.

Warning

**Values accept only the internal SAP representation.**\
Input that does not use the internal SAP representation results in error messages. Use the internal SAP representation. Example:

```text
ERPConnect.ABAPProgramException: RfcInvoke failed(RFC_ABAP_MESSAGE): Enter date in the format \_.\_.\_

```

______________________________________________________________________

#### Related Links

- [SAP Help: Report variants in SAP](https://help.sap.com/docs/btp/ABAP/3353524716.html)

This page shows how to connect to SAP.\
An SAP connection is required to use any Xtract Universal extraction type.

Warning

**Missing Authorization.**\
To establish a connection to SAP, the access to general authority objects must be available. Adjust the [SAP Authority Objects](/xtract-universal/documentation/setup-in-sap/sap-authority-objects#general-authorization-objects) accordingly.

**Supported Connection Methods**

- Connection to a single application server via [RFC protocol](settings/#source-type-rfc)
- Connection to a message server (Load Balancing) via [RFC protocol](settings/#source-type-rfc)
- Connection to a single application server or [public or private cloud instance via RFC over WebSocket](../../knowledge-base/access-data-in-the-sap-public-cloud/)
- Connection to SAP on-premise systems and SAP cloud systems via [OData protocol](settings/#source-type-odata)

**Supported Authentication Methods**

The RFC protocol supports the following authentication methods:

- Plain login using SAP username and password (system or dialogue user)
- [Secure Network Communication (SNC)](snc-authentication/) using username and password via basic authentication
- [SSO with Logon-Ticket](sso-with-logon-ticket/) using username and password via basic authentication

### Connect to SAP via RFC Protocol

Follow the steps below to create a source that connects to SAP via RFC protocol:

1. In the main window of the Designer, navigate to the menu bar and select **Server > Manage Sources**. The window "Manage Sources" opens.

1. Click **[Add]** to add a new SAP connection or click **[]** to edit an existing connection. The window "Change Source" opens.

1. Enter a name for the SAP connection in the field **Name**.

1. Select **RFC** as the source type.

1. In the *General* tab, select a connection method and enter the [system details](settings/#general) of your SAP system.

   Tip

   Input values for the SAP connection can be found in the Properties of the SAP Logon Pad, alternatively request the input values from the SAP Basis team.

1. In the *Authentication* tab, select one of the following authentication methods:

   - *Plain* uses the SAP username and password.
   - [*Secure Network communication (SNC)*](snc-authentication/) uses an encrypted connection between Xtract Universal and SAP with an SAP username and password.
   - [*SAP Logon Ticket*](sso-with-logon-ticket/) uses SAP Logon-Tickets in place of user credentials. This connection is not encrypted.

1. In the *RFC Options* tab, select an [RFC library](settings/#rfc-libraries) for the SAP connection. The default is the NetWeaver RFC library.

1. Optional: In the *Access Control* tab, you can restrict read and write access to the SAP source, see [Access Management](../access-restrictions/restrict-designer-access/#restrict-access-to-the-designer).

1. Click **[Test designer connection]** to validate the connection between the Xtract Universal Designer and the SAP system.

1. Click **[Test server connection]** to validate the connection between the Xtract Universal Server and the SAP system.

1. Click **[OK]** to save the SAP source.

For more information on the input options, see [Connection Settings](settings/#source-type-rfc).

### Connect to SAP via OData Protocol

Follow the steps below to create a source that connects to SAP via OData protocol:

1. In the main window of the Designer, navigate to the menu bar and select **Server > Manage Sources**. The window "Manage Sources" opens.
1. Click **[Add]** to add a new SAP connection or click **[]** to edit an existing connection. The window "Change Source" opens.
1. Enter a name for the SAP connection in the field **Name**.
1. Select **OData** as the source type.
1. Enter the [base URL](settings/#base-url) of your SAP system.
1. Enter login [credentials](settings/#username) for your SAP system.
1. Click **[Test Connect]** to validate the connection between the Xtract Universal Designer and the SAP system.
1. Optional: In the *Access Control* tab, you can restrict read and write access to the SAP source, see [Access Management](../access-restrictions/restrict-designer-access/#restrict-access-to-the-designer).
1. Click **[OK]** to save the SAP source.

For more information on the input options, see [Connection Settings](settings/#source-type-odata).

### Assign an SAP Source to Extractions

An SAP source is assigned when [creating an extraction](../../getting-started/#create-an-extraction).\
Follow the steps below to change the SAP source of an existing extraction:

1. Select an extraction from the list of extractions in the main window of the Designer.
1. Click **[Source]**. The window “Change Source” opens.
1. Select an SAP source from the dropdown list.
1. Click **[OK]** to confirm your input.

### Single-Sign-On (SSO)

BI client tools such as Power BI, Power Pivot, Alteryx, etc. can start extractions in Xtract Universal. Xtract Universal loads the extracted data directly into the tools. In this use case, it is often required that the extraction is executed with the SAP credentials of the (Windows AD) user, whose BI client triggered the extraction. This means that the SAP authorizations of the user apply. This is especially important when extracting BW/BEx queries.

The Windows credentials of the user are forwarded to SAP using Xtract Universal. On the way to SAP or on the SAP side, the Windows user and its SAP credentials are mapped.

#### Supported SSO Scenarios

Xtract Universal supports the following procedures for Single Sign-On (SSO):

- [Secure Network Communication (SNC) with Client Certificates](../../knowledge-base/sso-with-client-certificates/)
- [Secure Network Communication (SNC) with PSE and External ID](../../knowledge-base/sso-with-external-id/)
- [Secure Network Communication (SNC) with SAP’s Kerberos Wrapper Library (deprecated)](../../knowledge-base/sso-with-kerberos-snc/)
- [SAP Logon Ticket](../../knowledge-base/sso-with-logon-ticket/)

The authentication method can be selected in the SAP source connection settings.

### Connect via Router

If you access the SAP source system (Application server or Message server) via an SAP router, set the router string before the host name. For more information on SAP routers, see [SAP Help: SAP-Router](https://help.sap.com/viewer/6d9a59096c4b1014b507f15bed51571f/7.01.22/en-US/486b41efb74c07bee10000000a42189d.html).

Example:\
If the application server is "hamlet" and the router string is `/H/lear.theobald-software.com/H/`, set the host property to `/H/lear.theobald-software.com/H/hamlet`.

______________________________________________________________________

#### Related Links

- [Connection Settings](settings/)

This page contains an overview of the SAP connection settings in the window "Change Source".\
To open the server settings, navigate to **[Server] > [Manager Sources]** in the main window of the Designer and click **[]**.

## Source Type RFC

The source type **RFC** enables users to access SAP data using the RFC (Remote Function Call) protocol. The RFC connectivity supports connections to single application servers, message servers (Load Balancing) and public or private cloud instances (WebSocket).

For more information, see [SAP Help: RFC](https://help.sap.com/doc/saphelp_gbt10/1.0/en-US/48/88068ad9134076e10000000a42189d/frameset.htm).

### General

Certain input fields for the SAP connection vary depending on the selected connection method.

#### Host

Host name or IP address of the application server (Property Host).

#### Instance No.

A two-digit number between 00 and 99 (Property SystemNumber).

#### Client

A three-digit number of the SAP client between 000 and 999, e.g., 800.

#### Language

The logon language for the SAP system, e.g., EN for English or DE for German.

#### Message Server

Name or IP address of the message server (Property MessageServer).

#### System ID

Three-digit System ID (Property SID e.g., MSS).

#### Logon group

Property LogonGroup, usually *PUBLIC*.

#### Client

A three-digit number of the SAP client between 000 and 999, e.g., 800.

#### Language

The logon language for the SAP system, e.g., EN for English or DE for German.

#### Host

Name or IP address of the SAP (cloud) system.

#### Port

Port of the SAP (cloud) system, e.g., 443.

#### Library

Path to the SAP Cryptographic Library (download available in the SAP Service Marketplace).

#### Client PSE

Path to the client .pse file, see [Knowledge Base Article: Create a Client PSE to connect to SAP Cloud Systems](../../../knowledge-base/create-personal-security-environment/)

#### Client

A three-digit number of the SAP client between 000 and 999, e.g., 800.

#### Language

The logon language for the SAP system, e.g., EN for English or DE for German.

### Authentication

Certain input fields for the SAP connection vary depending on the selected authentication method.

#### User

SAP username.

#### Password

Password of the SAP user.

#### User name is alias

If this option is active, the name entered in the field **User** is used as the internet user alias, e.g., the communication user in the SAP Public Cloud. Activate this option when connecting to an SAP cloud system using the WebSocket connection method, see [Knowledge Base Article: Access Data in the SAP Public Cloud](../../../knowledge-base/access-data-in-the-sap-public-cloud/).

#### Request SAP credentials from caller when running extractions

If this option is active, SAP credentials entered in the **User** and **Password** fields are ignored. Provide SAP credentials via basic authentication when running an extraction. Caching the result of extractions is inactive.

#### User

SAP username.

#### Password

Password of the SAP user.

#### User name is alias

Activate this option when connecting to an SAP cloud system using the WebSocket connection method. When this option is active, the name entered in the field **User** is used as the internet user alias.

#### SNC library

Path to the SNC library, e.g., `C:\Program Files\SAP\FrontEnd\SecureLogin\sapcrypto.dll`

#### SNC Partner Name

The SAP Partner Name configured for the SAP application server, e.g., `p:SAPserviceERP/Alice@THEOBALD.LOCAL`.

#### Use static SAP credentials / Windows service account

This option activates SNC without SSO. If available, the SAP credentials in the fields **User** and **Password** are used for authentication. The Windows Active Directory user that opens the connection is the service account under which the Xtract Universal Windows service runs.

#### Request SAP credentials from caller

This option activates SNC with user and password. If this option is active, SAP credentials entered in the fields **User** and **Password** are ignored.\
Provide SAP credentials via basic authentication when running an extraction.

#### SSO - Log in as caller via External ID

This option activates SSO with External ID. SSO with External ID uses a Personal Security Environment (PSE) to create a trust relationship between the SAP application server and the service account that runs Xtract Universal. This allows Xtract Universal to impersonate any SAP user. For more information, see [Knowledge Base Article: SSO with External ID](../../../knowledge-base/sso-with-external-id/).

#### SSO - Impersonate caller via Kerberos

This option activates Kerberos SSO. The Windows Active Directory user is used for authentication. For this scenario “HTTPS - Restricted to AD users with Designer read access” must be selected and configured in the [Server Settings](../../server/server-settings/#web-server). For more information, see [Knowledge Base Article: SSO with Kerberos SNC](../../../knowledge-base/sso-with-kerberos-snc/).

#### SSO - Enroll certificate on behalf of caller

This option activates Certificate SSO. The Certificate SSO authentication uses Certificate Enrollment (Enroll-On-Behalf-Of) via Active Directory Certificate Services for the Windows Active Directory user that calls the extraction. For this scenario “HTTPS - Restricted to AD users with Designer read access” must be configured in the [Server Settings](../../server/server-settings/#web-server). For more information, see [Knowledge Base Article: SSO with Client Certificates](../../../knowledge-base/sso-with-client-certificates/).

#### Ticket issuer URL

URL of an Application Server Java (AS Java) that is configured to issue logon tickets. For more information, see [SAP Help: Configuring the AS Java to Issue Logon Tickets](https://help.sap.com/doc/saphelp_nw75/7.5.5/EN-US/4a/412251343f2ab1e10000000a42189c/frameset.htm).

#### Impersonate caller when running extractions (Kerberos SSO)

Activate this option to open the connection in the Windows Active Directory user context of the caller. Otherwise the connection is opened in the context of the service account running the Xtract Universal Windows service. For more information, see [Knowledge Base Article: SSO with Logon-Ticket](../../../knowledge-base/sso-with-logon-ticket/).

### RFC Options

#### RFC Libraries

Select an RFC library. The following RFC libraries are supported:

- NetWeaver RFC library (sapnwrfc.dll)
- Classic RFC library (librfc32.dll)

The RFC API (Remote Function Call) allows to establish an RFC connection to an SAP system from an external system that communicates as Client or Server with the SAP system. For more information on SAP libraries, see [SAP Help: RFC Libraries](https://help.sap.com/saphelp_nwpi71/helpdata/de/45/18e96cd26321a1e10000000a1553f6/frameset.htm).

SAP does not [support librfc32.dll](https://blogs.sap.com/2012/08/15/support-for-classic-rfc-library-ends-march-2016/) anymore.

Note

For certain older SAP releases, e.g., R/3 4.6C, it is necessary to enter the user name in upper case when using the NetWeaver RFC library.

Note

When using the NetWeaver RFC library with the [DeltaQ](../../deltaq/) extraction type or the [OHS](../../ohs/) extraction type, the RFC destination in SAP transaction SM59 must be set to *Unicode*. We recommend using the not supported librfc32.dll for certain extraction types, e.g., DeltaQ as it runs more stable than the NetWeaver RFC library.

#### Trace Directory

You can log debug information and save it locally.\
Enter a path to a local directory in the field **Trace directory** to save the debug information. For more information, see [Troubleshooting: Trace Directory](https://support.theobald-software.com/helpdesk/KB/View/14455-how-to-activate-tracing-for-xtract-products).

Clear the **Trace Directory** field when it is not needed.

Warning

**Increase of used hard drive memory.**\
A big amount of information is collected when debug logging is activated. This can decrease the capacity of your hard drives dramatically. Activate the debug logging only when necessary e.g., upon request of the support team.

#### Use SAPGUI

There are SAP Reports and BAPIs that require an installed SAP GUI even when they are called remotely. Activate this option only if necessary.

Warning

**'sapgui' start failed.**\
Sometimes SAP opens a pop-up window that requires input when running extractions. To deactivate pop-up windows, open the SAP GUI Logon pad and navigate to **Options... > Security Settings**. Click the **[Open Security Configuration]** button and select *Allow* as the **Default Action**. Apply the changes and close the SAP GUI Logon pad.

## Source Type OData

The source type **OData** enables users to access data from SAP on-premises and cloud systems using the OData (Open Data) protocol. OData defines a set of rules to create, edit and consume resources through RESTful interfaces (HTTP messages exchanging).

For more information, see [SAP Help: OData Services](https://help.sap.com/doc/77979cd206da4b7f9bd264b390d373fc/CLOUD/en-US/OData_Services.pdf).

#### Base URL

The base URL (protocol, hostname and port) of your SAP system. Examples:

- SAP on-premises systems: `http://sap-erp-as05.example.com:50000`
- SAP cloud systems: `https://my123456.s4hana.cloud.sap`

The URL can be requested from the SAP Basis team.

#### Username

When connecting to an SAP on-premises system, enter the username of an SAP dialog user. When connecting to an SAP private or public cloud system, enter the username of an [SAP Communication User](https://learning.sap.com/learning-journeys/implement-sap-s-4hana-cloud-public-edition-for-sourcing-and-procurement/setting-up-communication-management_a913171c-c96d-47a9-81ec-dc9ee8754320).

#### Password

Password of the SAP user.

#### Test Connect

Click **[Test Connect]** to test the OData connection.\
The radio buttons **OData V2** and **OData V4** indicate which OData protocol versions are enabled.\
For more information about the OData protocol version, refer to [OData V2](https://www.odata.org/documentation/odata-version-2-0/) and [OData V4](https://www.odata.org/documentation/).

## Access Control

Access control can be performed at the source level. This access control overrides the settings at the server level. For more information, see [Access Management](../../access-restrictions/restrict-designer-access/#restrict-access-to-the-designer).

This page shows how to encrypt communication between Xtract Universal and the SAP system via [Secure Network Communication (SNC)](https://help.sap.com/doc/saphelp_nw73ehp1/7.31.19/en-US/e6/56f466e99a11d1a5b00000e835363f/content.htm?no_cache=true).

### Prerequisites

- SNC must be configured in your SAP system. For more information about SNC configuration in SAP, see [SAP Help: Configuring the Application Server](http://help.sap.com/saphelp_nw73/helpdata/en/44/0e2e0cc7330d19e10000000a114a6b/frameset.htm).
- Check the SAP profile parameter *snc/gssapi_lib* in SAP (transaction RZ10) to determine, which library is used for encryption in your SAP system. Your SAP Basis has to import and configure the same library on the application server and on the machine that runs Xtract Universal, e.g., `sapcrypto.dll`.
- The Xtract Universal server must be set up to use the HTTPS protocol, see [Server Settings](../../server/server-settings/#web-server).

For information on how to set up SNC via X.509 certificate, refer to the [Knowledge Base Article: Enable Secure Network Communication (SNC) via X.509 certificate](../../../knowledge-base/enable-snc-using-pse-file/).

### Configure SNC in the SAP Source

Follow the steps below to set up an SAP connection that uses SNC:

1. Create or open an SAP source. For more information, see [Connect to SAP](../).

1. In the *General* tab, enter the system details of your SAP system.

   Tip

   Input values for the SAP connection can be found in the Properties of the SAP Logon Pad or they can be requested from the SAP Basis team.

1. In the *Authentication* tab, select the authentication method **Secure Network Communication (SNC)**.

1. Enter the SAP username and password of an SAP system or dialogue user in the fields **User** and **Password**.

1. Enter the path to the SNC library in the field **SNC library**, e.g., `C:\Program Files\SAP\FrontEnd\SecureLogin\sapcrypto.dll`.

1. Enter the SAP Partner Name configured for the SAP application server in the field **SNC Partner Name**, e.g., `p:SAPserviceERP/Alice@THEOBALD.LOCAL`.

1. In the subsection *When running extractions*, select one of the following SNC implementations:

   - [Use static SAP credentials / Windows service account](../settings/#use-static-sap-credentials-windows-service-account)
   - [Request SAP credentials from caller](../settings/#request-sap-credentials-from-caller)
   - [SSO - Log in as caller via External ID](../settings/#sso-log-in-as-caller-via-external-id)
   - [SSO - Impersonate caller via Kerberos](../settings/#sso-impersonate-caller-via-kerberos)
   - [SSO - Enroll certificate on behalf of caller](../settings/#sso-enroll-certificate-on-behalf-of-caller)

1. Click **[Test designer connection]** to validate the connection between the Xtract Universal Designer and the SAP system.

1. Click **[Test server connection]** to validate the connection between the Xtract Universal Server and the SAP system.

1. Click **[OK]** to save the SAP source.

### Download Kerberos DLLs

It is possible to use Kerberos libraries for encryption between the client and the SAP server. For more information, see [SAP Note 2115486](https://launchpad.support.sap.com/#/notes/2115486).\
Different DLLs for 32-bit (`gsskrb5.dll`) and 64-bit (`gx64krb5.dll`) platforms are provided with [SAP Note 2115486](https://launchpad.support.sap.com/#/notes/2115486).

______________________________________________________________________

#### Related Links

- [Knowledge Base Article: SSO with External ID](../../../knowledge-base/sso-with-external-id/)
- [Knowledge Base Article: SSO with Kerberos SNC](../../../knowledge-base/sso-with-kerberos-snc/)
- [Knowledge Base Article: SSO with Client Certificates](../../../knowledge-base/sso-with-client-certificates/)

This page shows how to use SSO via Logon-Tickets between Xtract Universal and SAP. Note that this connection is not encrypted.

### Prerequisites

- SSO with Logon-Tickets must be configured on the SAP system and and the application server, see [Knowledge Base Article: SSO with Logon-Ticket](../../../knowledge-base/sso-with-logon-ticket/#requirements).
- The Xtract Universal service must run under an [XU Service Account](../../server/service-account/).
- The Xtract Universal server must be set up to use the HTTPS protocol, see [Server Settings](../../server/server-settings/#web-server).

### Configure SSO with Logon-Ticket in the SAP Source

Follow the steps below to set up an SAP connection that uses SSO with Logon-Ticket:

1. Create or open an SAP source. For more information, see [Connect to SAP](../).

1. In the *General* tab, enter the system details of your SAP system.

   Tip

   Input values for the SAP connection can be found in the Properties of the SAP Logon Pad or they can be requested from the SAP Basis team.

1. In the *Authentication* tab, select the authentication method **SAP Logon Ticket**.

1. Enter the URL of an Application Server Java (AS Java) that is configured to issue logon tickets in the field **Ticket issuer Url**. For more information, see [SAP Help: Configuring the AS Java to Issue Logon Tickets](https://help.sap.com/doc/saphelp_nw75/7.5.5/EN-US/4a/412251343f2ab1e10000000a42189c/frameset.htm).

1. To open the connection in the Windows Active Directory user context of the caller, activate the option **Impersonate caller when running extractions (Kerberos SSO)**. Otherwise the connection is opened in the context of the service account under which the Xtract Universal Windows service runs.

1. Click **[Test designer connection]** to validate the connection between the Xtract Universal Designer and the SAP system.

1. Click **[Test server connection]** to validate the connection between the Xtract Universal Server and the SAP system.

1. Click **[OK]** to save the SAP source.

______________________________________________________________________

#### Related Links

- [Single Sign-On (SSO)](../#single-sign-on-sso)
- [Knowledge Base Article: SSO with Logon-Ticket](../../../knowledge-base/sso-with-logon-ticket/)

Xtract Universal is created for [distributed use](../introduction/#software-architecture). The Xtract Universal Service can be installed on a central server instance, e.g., a company-wide application server. As a result, several users can connect their Xtract Universal Designer to the server instance to create and modify extractions. The users must have access to the `C:\Program Files\XtractUniversal\config` directory on the server instance.

Tip

Use the [access restrictions](../access-restrictions/) in Xtract Universal to allow only users with administrator rights to perform fundamental changes on the central repository.

### Install the Xtract Universal Server

When executing *XtractUniversalSetup.exe*, the Xtract Universal server is installed and started as a standard Windows service. Make sure to mark the option **Server > Install Service** during the [Installation](../setup/installation/).

The Xtract Universal Service can be installed and removed using the `C:\Program Files\XtractUniversal\XtractService.exe` application. XtractService.exe is used via the Windows command line and supports the following commands:

- `/i` - install Windows service
- `/u` - uninstall Windows service

**Example:**

```console
C:\Program Files\XtractUniversal>XtractService.exe /i
C:\Program Files\XtractUniversal>XtractService.exe /u

```

Tip

There is a standalone version of the Xtract Universal Designer that can be used to connect to a central Xtract Universal server instance without any software installation, see [Installation of the Standalone Designer](../setup/installation/#installation-of-the-standalone-designer).

### Windows Service

After installation, the Windows service can be started, stopped and configured via the Windows Services administration or the Task Manager. For information on how to run the service under a dedicated Windows service account, see [Change Service Account](service-account/).

### Ports

The Xtract Universal Server runs as a Windows service with the main process being the XtractService.exe that is located in the installation directory of Xtract Universal, e.g., `C:\Program Files\XtractUniversal\XtractService.exe`. The XtractService.exe starts two listener processes that listen on the following ports by default:

| Listener Process | Default Port | Comment | | --- | --- | --- | | [*Theobald.Xu.Web.Listener.exe*](server-tasks/#theobaldxuweblistenerexe) | 8065 (HTTP) and 8165 (HTTPS) | Port can be changed. The web server accepts extraction calls via HTTP(S). | | [*Theobald.Xu.Rpc.Listener.exe*](server-tasks/#theobaldxurpclistenerexe) | 8064 | Port can be changed. The configuration server communicates with the Xtract Universal Designer via a dedicated port. |

If manual adjustments of the port number are necessary, ask your network team for the correct ports.

Note

In the case of distributed use, create corresponding [Inbound Port Rules](https://docs.microsoft.com/en-us/windows/security/threat-protection/windows-firewall/create-an-inbound-port-rule) for the protocol type **TCP** of the listener processes mentioned above.

Warning

**Changing the default ports**\
After manually adjusting the default ports, the Xtract Universal service does not start. The stored ports are already assigned and must be adapted in the following config files. Administrator access rights are required for altering the [config files](../setup/migration/#configuration-files).\
`C:\Program Files\XtractUniversal\config\server\config\general.json`\
`C:\Program Files\XtractUniversal\config\server\web\general.json`

This page contains an overview of the server settings of Xtract Universal. To open the server settings, navigate to **[Server] > [Settings]** in the main window of the Designer.

Note

The settings are stored in the following directory: `C:\Program Files\XtractUniversal\config\server` (by default).

## Configuration Server

The configuration server communicates with the Xtract Universal Designer.

### Designer Connection

#### Port

Defines the port number for communication between Server and Designer. The default is 8064. If you set a different port, add the new port number to the host name on the [logon screen](../../designer/#connect-the-designer-to-a-server) (`[host name]:[port]`).

#### Max. age of log files (days)

Defines the maximum age of the config server [log files](../../logs/#access-server-logs) in days. After this period the log files are deleted.

### Authentication Methods

Defines the authentication methods that are available when connecting a Designer to the Xtract Universal Server. For more information, see [Authentication Between Designer and Server](../../access-restrictions/#authentication-between-designer-and-server).

#### Select X.509 certificate

Select an X.509 certificate that is used for for transport encryption and authentication, when [custom users](../../access-restrictions/user-management/) connect a Designer to the Server. For more information, see [Install an X.509 certificate](../../access-restrictions/install-x.509-certificate/).

### Access Management

Defines which users and user groups have access to the Designer. For more information, see [Access Management](../../access-restrictions/).

## Web Server

The web server accepts extraction calls via HTTP(S).

### Protocol

| Protocol | Description | | --- | --- | | **HTTP - Unrestricted** | Runs extractions as an HTTP-URL. | | **HTTPS - Unrestricted** | Runs extractions as an HTTPS-URL. This enables secure data transfer via HTTPS. | | **HTTPS - Restricted to AD users with Designer read access** | This setting enables an additional access control for running an extraction. Extractions can only be executed by Windows AD users with at least a read permission (*Read*) in the *Configuration Server* tab. For more information, see [Restrict Access to Windows AD Users (Kerberos Authentication)](../../access-restrictions/restrict-server-access/#restrict-access-to-windows-ad-users-kerberos-authentication). | | **HTTPS - Restricted to custom users with Designer read access** | This setting enables an additional access control for running an extraction. Extractions can only be executed by custom users with at least a read permission (*Read*) in the *Configuration Server* tab. For more information, see [Restrict Access to Custom Users (Basic Authentication)](../../access-restrictions/restrict-server-access/#restrict-access-to-custom-users-basic-authentication). |

Note

To receive data via HTTPS, the [installation of a TLS certificate](../../access-restrictions/install-x.509-certificate/) is required on the server that runs the Xtract Universal service.

#### HTTP Port

Defines the port number, on which the Server receives HTTP requests of an extraction.

#### HTTPS Port

Defines the port number, on which the Server receives HTTPS requests of an extraction.

#### Select X.509 certificate

Select an X.509 certificate that is used for for transport encryption and authentication, when running extractions. For more information, see [Install an X.509 certificate](../../access-restrictions/install-x.509-certificate/).

### Misc

#### Keep log files (days)

Defines the maximum age of the web server [log files](../../logs/#access-extraction-logs) in days. After this period the log files are deleted.

#### Collect Usage Data

When this checkbox is active, usage data is stored in your local installation directory. If this checkbox is deactivated, no usage data is collected.

#### Upload Usage Data

When this checkbox is active, usage data is sent to Theobald Software for analysis. An internet connection is required to use this option.

#### Transfer SAP Object Names

When this checkbox is active, the names of the SAP objects that are used in extractions are included in the usage data upload.

#### Enable setup distribution for clients

Defines whether the setup of the product version that runs on the server needs to be downloaded. When an older Designer version is connected to a newer Server version, you are prompted to download and update the Designer with the product version.

### Result cache

The *Cache results* option is only available in [pull destinations](../../destinations/) (e.g., PBI, Qlik etc.).

#### Target directory

Sets the directory for the buffer files. The default directory is the result-cache folder in the installation directory, e.g., `C:\Program Files\XtractUniversal\result-cache`.

#### Max. cached runs

Defines the maximum count of results of different extractions in the buffer.

#### Max. age (minutes)

Defines the maximum age in minutes of an extraction in the buffer.

This page contains an overview of the Xtract Universal server tasks and the underlying server architecture to execute the tasks. The server performs two main tasks:

- Run extractions stored in the [Config](../../setup/migration/#configuration-files) directory.
- Make extractions stored in the [Config](../../setup/migration/#configuration-files) directory available to the Designer.

### Run Extractions on the Server

The execution of an extraction is triggered by an HTTP request. The HTTP request can be triggered from the target environment for [pull destinations](../../destinations/) or from the xu command line tool (xu.exe /xu.elf), see [Execute and Automate Extractions](../../execute-and-automate/call-via-commandline/).

Tip

The process can be traced in the [Extraction Log](../../logs/#access-extraction-logs).

1. The Theobald.xu.Web.Worker.exe checks the authentication and authorization of the request.
1. The target environment is prepared for writing the extracted data (e.g. establish database connection, create file).
1. The license is checked.
1. A connection to the SAP source system is established.
1. The data of the defined extraction type is requested.
1. Each extracted data package is written to the target environment.
1. After all packages are received, the Theobald.xu.Web.Worker.exe terminates the connection to the SAP source system and informs the target environment that the extraction is complete.

Tip

The Theobald.xu.Web.Worker.exe logs its actions in log files. The log files are located in the logs subdirectory of the program directory:`C:\Program Files\XtractUniversal\logs\servers\web\worker` (default) The logs can also be displayed in the Designer under **[Server]>[Logs (Web Worker)]**.

### Access the Settings using the Designer

The steps below show what happens when a Designer connects to the Server and changes settings.

1. The Theobald.xu.Rpc.Worker.exe checks the authentication and authorization of the request.
1. The Designer requests a certain setting, e.g., list of all extractions.
1. The Theobald.xu.Rpc.Worker.exe reads the requested settings from the [config directory](../../setup/migration/#configuration-files) and sends these settings to the Designer.
1. The user changes settings in the Designer, e.g., destination settings.
1. The Designer sends the changed settings back to the Theobald.xu.Rpc.Worker.exe. Theobald.xu.Rpc.Worker.exe saves the changed settings in the [config directory](../../setup/migration/#configuration-files).

Tip

The Theobald.xu.Rpc.Worker.exe logs its actions in log files. The log files are located in the logs subdirectory of the program directory: `C:\Program Files\XtractUniversal\logs\server\rpc\worker` (default).

### Server Architecture

The server runs as a Windows Service and the main process of the service is the XtractService.exe. The XtractService.exe starts two listener processes:

- Theobald.xu.Web.Listener.exe
- Theobald.xu.Rpc.Listener.exe

Both listener processes listen on the [Ports](../#ports) defined in the [Server Settings](../server-settings/).

Tip

The XtractService.exe logs its actions in ServiceLog.txt The log file is located in the logs subdirectory of the program directory: `C:\Program Files\XtractUniversal\logs` (default).

#### Theobald.xu.Rpc.Listener.exe

The Theobald.xu.Rpc.Listener.exe waits for new connection requests from the Designer.\
For each TCP connection the Theobald.xu.Rpc.Listener.exe starts a new instance of Theobald.xu.Rpc.Worker.exe, which processes all Designer requests coming in over the particular TCP connection, see [Access the Settings using the Designer](#access-the-settings-using-the-designer).

Tip

The Theobald.xu.Rpc.Listener.exe logs its actions in log files. The log files are located in the logs subdirectory of the program directory: `C:\Program Files\XtractUniversal\logs\server\rpc\listener` (default).

#### Theobald.xu.Web.Listener.exe

The Theobald.xu.Web.Listener.exe waits for HTTP requests.\
For each TCP connection the Theobald.xu.Web.Listener.exe starts a new instance of Theobald.xu.Web.Worker.exe, which processes all HTTP requests coming in over the particular TCP connection, see [Run Extraction on the Server](#run-extractions-on-the-server).

The following HTTP requests are possible:

- running an extraction
- emptying the result cache of an extraction
- canceling all runs of an extraction
- [Web API requests](../../../web-api/)

Tip

The Theobald.xu.Web.Listener.exe logs its actions in log files. The log files are located in the logs subdirectory of the program directory: `C:\Program Files\XtractUniversal\logs\server\web` (default).

This page shows how to run the Xtract Universal service under a dedicated Windows domain user account. After the installation, the Xtract Universal service runs under a [virtual service account](https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/manage/understand-service-accounts#virtual-accounts) by default.

The following scenarios require the service to run under a dedicated Windows domain user account:

- Enabling [Kerberos authentication](../../access-restrictions/restrict-server-access/#restrict-access-to-windows-ad-users-kerberos-authentication) for the Xtract Universal web server
- Enabling Windows authentication for an Xtract Universal destination that allows Windows credentials for log on, e.g., SQL Server destination, PostgreSQL destination.
- Enabling [SSO with Kerberos SNC](../../../knowledge-base/sso-with-kerberos-snc/)
- Enabling [SSO with SAP Logon Tickets](../../../knowledge-base/sso-with-logon-ticket/)

### Basic Settings

1. Create a Windows AD service account and assign an SPN (Service Principle Name) to the service account in the following format: `HTTP/[FQDN of XU Server]`.

   Tip

   Use the `setspn` command to check the SPNs of a user account.

1. Grant access rights to the installation folder of Xtract Universal and all sub folders to the service account as shown in the following screenshot:

1. If applicable, make sure the service account has *Read* access to the private key of the [X.509 certificate](../../access-restrictions/install-x.509-certificate/) used by Xtract Universal.

1. Let the Xtract Universal service run under the service account. Make sure to use the correct domain, e.g., *.company.local* instead of *.company.com*.

1. In the Xtract Universal Designer startup window "Connect to Xtract Universal Server", set **Authentication** to *Windows credentials* or *Custom Credentials (Kerberos authentication)*.

1. Enter the User Principal Name (UPN) of the service account in the **Target Principal** field. For more information, see [Knowledge Base Article: Target Principal Field](../../../knowledge-base/target-principal-TPN/).

### Settings for SSO with Kerberos SNC

When using [SSO with Kerberos SNC](../../../knowledge-base/sso-with-kerberos-snc/) additional steps are necessary:

1. Set *constrained delegation* for the Windows domain account under which the Xtract Universal Service runs.
1. Enter the SPN of the service account under which the SAP ABAP application server is running (SAP Service Account), e.g., `SAPServiceERP/do_not_care`.\
   For more information about the partner name notation in SAP, see the [SAP Help: Preparing the Primary Application Server Instance](https://help.sap.com/viewer/e815bb97839a4d83be6c4fca48ee5777/7.5.9/en-US/440ebb40b9920d1be10000000a114a6b.html).

______________________________________________________________________

#### Related Links

- [Microsoft Documentation: About Service Logon Accounts](https://docs.microsoft.com/en-us/windows/win32/ad/about-service-logon-accounts)
- [Microsoft Documentation: Service Principal Names](https://docs.microsoft.com/en-us/windows/win32/ad/service-principal-names)
- [Knowledge Base Article: Target Principal Field](../../../knowledge-base/target-principal-TPN/)

This section covers installation and maintenance topics for IT admins. This includes system requirements, backups and license information.

### Popular Topics

- [System Requirements](requirements/)
- [Backup & Update](update/)
- [Supported SAP Releases & Databases](requirements/#supported-sap-systems-and-releases)
- [License Installation](license/#about-the-licensing-concept-of-xtract-universal)
- [Configuration Files](migration/#configuration-files)
- [Migration to a Different Machine](migration/#migration-to-a-different-machine)

### Related Topics

### Download Xtract Universal

You can download a 30 day trial version from the [Theobald Software website](https://theobald-software.com/en/download-trial/). The trial version is only time limited and works otherwise without any restrictions.

### Evaluate Xtract Universal

You are guaranteed to get unrestricted support by the Theobald Software support team during the evaluation phase. In case of questions or doubts, feel free to contact Theobald Software at any time:

- [Support Portal](https://support.theobald-software.com)
- [Contact Forms](https://theobald-software.com/en/contact/)

### Technical Support

Theobald Software offers support in English and German.

1. Open a ticket in our [Support Portal](https://support.theobald-software.com).
1. Provide as much information as possible for the Theobald Software support team to understand and analyze the issue.
1. If there is an error message, copy and paste the error message into the ticket.
1. Copy and paste the software logs in the ticket, see [Required Support Information for Xtract Universal](https://support.theobald-software.com/helpdesk/KB/View/14457-required-support-information-for-xtract-universal).

Disclaimer

SAP versions that are no longer supported by the manufacturer are excluded from the Theobald Software support. Issues specific to custom BAPIs (Z function modules) or custom reports (Z reports) are also excluded from the scope of support.

This page shows how and where to install Xtract Universal.

### Prerequisites

Administrator permissions are required to install Xtract Universal.

### Setup

`XtractUniversalSetup.exe` is an industry standard setup. Execute the `XtractUniversalSetup.exe` file and follow the instructions of the setup program.

When starting the installation program, optional components can be selected during the setup.

| Component | Description | | --- | --- | | Main Product Files | All required files to use Xtract Universal. | | Designer | Installs the Designer application, uncheck this option if you want to use Xtract Universal without a graphical interface. | | Server | Installs the Xtract Universal Server | | Convert config files | Converts extractions, sources, destinations, etc. from previous version format to new format. Crucial when installing major releases and upgrading from e.g., version 3.x to 4.x. | | Install Service | Installs the server component as a windows service that runs under a [virtual service account](https://learn.microsoft.com/en-us/windows-server/identity/ad-ds/manage/understand-service-accounts#virtual-accounts) with the least required privileges, see [Server](../../server/). | | XtractUniversal Report Server Plugin | Plugin required by the [Power BI Report Server](../../destinations/server-report-services/) destination. | | Start Menu Shortcuts | Component that adds shortcuts to the start menu. |

Note

Make sure to mark the option “Server > Install Service” during the Installation, as installing a server without the service is only used for development purposes.

For information on how to install a license, see [Licensing](../license/#install-the-xtract-universal-license).

Warning

**Errors due to different version numbers.**\
Differences in version numbers of Designer and Server may lead to errors.\
Make sure that Designer and Server have identical version numbers.

### Installation Directory Files

The list below shows several most important files that are placed into the default directory `C:\Program Files\XtractUniversal` after installation:

| Filename | Description | | --- | --- | | ABAP directory | Directory with SAP function modules. Read the readme.txt within the directory for more information. See also [SAP Customization](../../setup-in-sap/). | | Alteryx directory | Directory with a plugin setup for the [Alteryx destination](../../destinations/alteryx/#requirements). | | logs directory | Directory with server and extraction etc. logs, see [Logs](../../logs/). | | config directory | Directory containing all SAP connections, extractions, destinations and other settings. See also [Backup and Migration](../migration/). | | powerbi directory | Directory containing files related to [Power BI Connector destination](../../destinations/Power-BI-Connector/). | | private directory | Directory containing keys for encrypted SAP passwords. This directory contains sensitive information and must be to be secured accordingly. | | result-cache directory | Directory with extraction cache files, only applicable for [Pull Destinations](../../destinations/). | | xu.exe | Command line tool used for executing extractions, see [Schedule an Extraction](../../execute-and-automate/call-via-scheduler/). | | xu-config.exe | Command line tool used for creating extractions, see [Create Extractions without the Xtract Universal Designer GUI](../../../knowledge-base/config-command-line-tool/). | | XtractCleanup.exe | Application that deletes all cached results and log files, depending on the [Web Server settings](../../server/server-settings/#web-server) of Xtract Universal. | | XtractDesigner.exe | [Xtract Universal Designer](../../designer/) application to create, test and monitor extractions. | | ConfigConverter.exe | Application that converts extractions, sources, destinations, etc. from previous version format to new format. Crucial when installing major releases and upgrading from e.g., version 3.x to 4.x. | | uninstall.exe | Tool for uninstalling and removing Xtract Universal with all its components from your machine. | | XtractUniversalSetup.exe | Setup of the currently installed version, see [Migration to a Different Machine](../migration/#migration-to-a-different-machine). | | XtractUniversal Report Server Plugin | Plugin required by the [Power BI Report Server](../../destinations/server-report-services/) destination. | | Eula_XtractUniversal.rtf | Document containing the license agreement for the use of the software Xtract Universal. | | XtractUniversalLicense.json | License file with information about the server, the component and runtime. |

Note

The Xtract Universal Server can be started as a console program for test purposes. For more details on starting Xtract Universal Server as a console program, see [Knowledge Base Article: Target Principal Field](../../../knowledge-base/target-principal-TPN/).

The installation of Xtract Universal can be initiated unattended without the GUI in a non-interactive way via the Windows Command Prompt. Execute the `XtractUniversalSetup.exe` via command line and use the switch `--unattended`.

```console
start /wait XtractUniversalSetup.exe --unattended

```

`XtractUniversalSetup.exe` is Windows applications, meaning the Windows Command Prompt does not wait until the installation is complete. To wait until the installation is complete, use the [start](https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/start) command with the `/wait` switch.

Note

All switches are case sensitive.

### Version Number

When installing Xtract Universal, the version of the product is displayed in the installation menu. To check the version of the installed product in Windows, open the Windows Settings and navigate to **Apps > Installed apps**.

### Installation of the Standalone Designer

[My Theobald Software - Portal for Customers and Partners](https://my.theobald-software.com/) offers a download option for a standalone version of the Xtract Universal Designer. The standalone Designer can be used to connect to a central Xtract Universal repository without any software installation.

#### Prerequisites

The [Xtract Universal Server](../../server/) has to be installed on a central server instance e.g., a company-wide application server. As a result, access to a common Xtract Universal repository, e.g., `C:\Program Files\XtractUniversal\config` is possible and can be used by several Xtract Universal users.

#### How to Use the Standalone-Designer

After the standalone Designer is downloaded, unpack the files from the `.zip` folder to any directory. The folder contains two executable files:

- `XtractDesigner.exe` starts the Designer
- `xu.exe` is the command line tool used for executing extractions, see [Call via Commandline](../../execute-and-automate/call-via-commandline/)

Use the `XtractDesigner.exe` file to start the Designer. Before you connect to a central Xtract Universal Server, make sure you have access rights to the server and repository, see [Access Management](../../access-restrictions/).

Note

When updating the software, you have to manually download the latest version of the standalone Designer from [My Theobald Software - Portal for Customers and Partners](https://my.theobald-software.com/) and replace the old files.

Note

The standalone Designer might be classified as 'dangerous' by antivirus software. Make sure the Designer is not blocked by antivirus software.

______________________________________________________________________

#### Related Links

- [Start the Designer](../../designer/)
- [Server Settings](../../server/server-settings/)
- [Access Management](../../access-restrictions/)

### About the Licensing Concept of Xtract Universal

Xtract Universal is licensed per Windows server. The license is bound to your company and a specific server name. If you run the Xtract Universal Designer and the Xtract Universal Server on different machines, it is only necessary to replace the license on the server.

A trial license is automatically installed with the installation of Xtract Universal.\
A regular license is provided in the [customer portal - My Theobald Software](https://my.theobald-software.com/) after purchasing the product.

The license defines the following properties:

- The name of the server that runs the Xtract Universal Server.
- The destinations to which you can extract data to.
- The number of extractions you can define.

These properties are checked when the XtractUniversal Server runs an extraction.

Note

The number of defined extractions and other license information is displayed in the status bar at the bottom of the Designer.

Recommendation

According to our experience, medium-sized businesses use less than 100 extractions.

### Install the Xtract Universal License

Place the "XtractUniversalLicense.json" file that is provided in the [Customer Portal - My Theobald Software](https://my.theobald-software.com) into the installation directory on the server, e.g., `C:\Program Files\XtractUniversal`.

Tip

To inspect your current license data, open the Xtract Universal Designer and navigate to **[Help] > [Info]** in the main menu bar. Alternatively, press **[F12]**.

### Move a License to a new Server

When moving your setup to a new server, a new license file must be issued for that server. Contact our sales team at [sales@theobald-software.com](mailto:sales@theobald-software.com) to let us know the name of the new server.

For more information on the migration process, see [Migration to a Different Machine](../migration/#migration-to-a-different-machine).

### Maintenance

Contact the [sales department](mailto:sales@theobald-software.com) for information about available maintenance options.\
In case of technical difficulties, contact our [support](https://support.theobald-software.com/helpdesk).

______________________________________________________________________

#### Related Links

- [My Theobald Software - Portal for Customers and Partners](https://my.theobald-software.com/)

This page shows how to migrate the Xtract Universal configuration from one machine to another.

### Prerequisites

- [Create a backup](../update/#create-a-backup) of the `config` folder that is located in the installation directory of the current machine, e.g., `C:\Program Files\XtractUniversal\config`.
- Make sure that the same version of Xtract Universal is installed on both machines.

Tip

To install a matching version of Xtract Universal on the new machine, use the `XtractUniversalSetup.exe` from the installation directory of the current machine, e.g., in `C:\Program Files\XtractUniversal`.

### Migration to a Different Machine

The Xtract Universal configuration can be migrated entirely (full migration) or partially.

- Conduct a **full migration** when replacing a machine completely.\
  For a full migration, restore the entire `config` and `private` folders from your backup on the new machine (full migration).
- Conduct a **partial migration** when moving parts of your setup to a different machine, e.g., if you transport defined extraction types from a test environment to a production environment.\
  For a partial migration, restore the parts of the backup you want to migrate by copying only the relevant [configuration files](#configuration-files), e.g., the `sources` or `extractions` folder, on the new machine.

Restart the [Xtract Universal Service](../../server/) if you copy the `server` folder to the new machine.

Note

When migrating to a new server, a new license file must be issued for that server. Contact our sales team at [sales@theobald-software.com](mailto:sales@theobald-software.com) for more information.

### Configuration Files

All configuration files of defined extraction types, sources and the server are stored in the sub-directory `config` of the Xtract Universal installation directory, e.g., `C:\Program Files\XtractUniversal\config`. Use the `config` directory to set up a version control system or to create manual backups.

| Filename | Description | | --- | --- | | `extractions` | Directory containing the defined extraction types. Each sub-directory contains an extraction of the same name. | | `servers` | Directory containing the [server settings](../../server/server-settings/). | | `connections` | Directory containing [SAP connections](../../sap-connection/) and [Destinations](../../destinations/). Each sub-directory contains a source or destination of the same name. | | `version.txt` | Text file containing the product version of the current configuration, see [Installation](../installation/#installation-directory-files). |

______________________________________________________________________

#### Related Links

- [Installation](../installation/)
- [Changelog](../../../changelog/)

This page contains information about system requirements and compatible SAP systems for Xtract Universal.

### Supported SAP Systems and Releases

The following SAP Systems are supported:

- All SAP ABAP based systems that provide RFC connectivity and all SAP S/4 HANA (Cloud) systems that provide OData connectivity are supported.
- SAP ABAP systems on any database are supported (including HANA). The database used by the SAP system is irrelevant, because the integration occurs at SAP application server level.
- SAP systems running on Big Endian and Little Endian hardware are supported.
- SAP industry solutions like IS-U, IS-R, etc. are supported.
- SAP releases 4.6C and newer are supported.
- All operating systems are supported.

Examples

- [SAP S/4HANA](/xtract-universal/knowledge-base/supported-sap-and-hana-versions)
- mySAP
- SAP Application Server ABAP
- Message Server
- Router
- Standalone Gateway
- SAP Business Suite 7 (CRM, SRM, SCM etc.)
- SAP Business All in One, CAR, APO, PI
- SAP BW 3.1 and later
- SAP BW/BI 7.x
- SAP ERP / ECC 5.0 / ECC 6.0 (including all EhPs)
- SAP R/3 Version 4.6C and later
- SAP BW/4HANA
- ...and more.

**Disclaimer**

While SAP versions that are no longer supported by SAP still work with Theobald Software products, they are excluded from the Theobald Software support service.

#### Not Supported SAP Systems

The following SAP Systems are not supported:

- SAP systems that don’t run on ABAP systems
- SAP systems that don't provide RFC connectivity.

Examples

- Business By Design
- Business One
- Business Objects
- Sybase
- Ariba
- Success Factors
- Concur

### HANA Database

You can use Operational Data Provisioning (ODP) to connect the SAP HANA database of an SAP ABAP source system. Communication is done via RFC. With the ODP context for SAP HANA (HANA) the following HANA View types are available for extracting:

- Analysis Views
- Calculation Views
- Associated Attribute Views

Direct access to a HANA database without an SAP ABAP source system running on the corresponding HANA database is not supported.

### Ports

The following ports between the Windows server that runs Xtract Universal and the SAP server, must be open:

| SAP NetWeaver Component | Port (NN = System number of the SAP system) | | --- | --- | | SAP Application Server | 33<NN> | | SAP Message Server | 36<NN> | | Secure Network Communication (SNC) | 48<NN> | | SAP-Router | 3299 |

For more information, see [SAP Help: TCP/IP Ports of All SAP Products](https://help.sap.com/viewer/ports).

### Installation and Configuration on SAP

| Extraction Type | SAP Release | Requirements on the SAP System | | --- | --- | --- | | [Table](/xtract-universal/documentation/table/) | Rel. > 4.6C | Installation of a custom function module [/THEO/READ_TABLE](/xtract-universal/documentation/setup-in-sap/custom-function-module-for-table-extraction/#installation-of-theoread_table) is recommended. | | [BAPI](/xtract-universal/documentation/bapi/) | Rel. > 4.6C | No requirements. Only remote-enabled functions are supported. | | [Query](/xtract-universal/documentation/query/) | Rel. > 4.6C | No requirements. | | [Report](/xtract-universal/documentation/report/) | Rel. > 4.6C | Installation of a custom function module [Z_XTRACT_IS_REMOTE_REPORT](/xtract-universal/documentation/setup-in-sap/custom-function-module-for-reports/) is required. | | [BWCube](/xtract-universal/documentation/bwcube/) | Rel. > BW 3.1 | No requirements. BEx Queries require external access ("Allow External Access to this Query" option must be active). | | [BW Hierarchy](/xtract-universal/documentation/hierarchy/) | Rel. > BW 3.1 | No requirements. | | [ODP](/xtract-universal/documentation/odp/) | SAP_BASIS >= 730, BW >= 7,3X | No requirements. | | [DeltaQ](/xtract-universal/documentation/deltaq/) | Rel. > 4.6C | Customization required, see [Customization for DeltaQ](/xtract-universal/documentation/setup-in-sap/customization-for-deltaq/). | | [OHS](/xtract-universal/documentation/ohs/) | Rel. > BW 3.5 | Customization required, see [Customization for OHS in BW](/xtract-universal/documentation/setup-in-sap/customization-for-ohs-in-bw/). | | [Table CDC](/xtract-universal/documentation/table-cdc/) | SAP ECC 5.0 or above | Installation of a custom function modules is necessary, see [Table CDC Requirements](/xtract-universal/documentation/table-cdc/#prerequisites). |

For Information about the installation of the custom function modules and the SAP customization, check the section [SAP Customization](/xtract-universal/documentation/setup-in-sap/).

### SAP Licenses

Additional SAP licenses might be required for extracting data from SAP. Contact SAP to verify these requirements.

### Operating Systems

- Windows 11
- Windows Server 2022 (until 2031-10-14)
- Windows 10 (until 2025-10-14)
- Windows Server 2022
- Windows Server 2019 (until 2029-01-09)
- Windows Server 2016 (until 2027-01-12)

### Other Applications and Frameworks

- .NET Framework 4.7.2 or higher, see [Download .NET Framework from Microsoft](https://support.microsoft.com/en-us/help/4054530/microsoft-net-framework-4-7-2-offline-installer-for-windows).

### Hardware

The following requirements apply to the hardware:

| Hardware | Minimum Requirement | | --- | --- | | Processor Cores | 2 Cores, 1 additional core is required for each additional parallel extraction | | Processor Speed | 1.4 GHz, recommended: 2.0 GHz or faster | | Main Memory | 8 GB, recommended: 12 GB for up to two parallel extractions 4 GB dedicated memory in average for each additional parallel extraction. Check the exact storage requirements of an extraction in your scenario. | | Disk Space | min. 150 MB total for Installation (when using specific destinations additional disc space may be required) | | 64-Bit Environment | 64-Bit operating systems only | | Display Resolution | 1920x1080x1.25 with 1.25 display scaling |

### Destinations

Depending on the [destination](../../destinations/), an appropriate driver or library can be required. For more information, refer to the requirements section of each destination.

Additional disk space may be required when using specific destinations (e.g. Alteryx, Power BI, Tableau, Qlik, KNIME).

This page shows how to backup and update an already installed Xtract Universal version.

### Preparations

Recommendations:

- Use a working test environment with a valid license and maintenance. The test environment is a copy of the current production environment.
- Update the software regularly, see [recommended update interval](../../../changelog/#recommended-update-interval).
- Create a [backup](#create-a-backup) of the current installation.

Before updating, check the [Changelog](../../../changelog/) for breaking changes:

- Make any necessary preparations and changes as described in the Release Notes.
- Note the current product version so that you can switch back to the same version in the event of an error.

Tip

Subscribe to our [technical newsletter](https://theobald-software.com/en/newsletter/) that informs you about new software releases.

### Create a Backup

On the machine that runs the Xtract Universal service, copy the following files and directories and store them in a secure location:

- copy the complete `config` and `private` directories into the Xtract Universal installation directory, e.g., `C:\Program Files\XtractUniversal\config`.
- optionally, copy the complete `logs` directory to create a backup of the log files.
- copy the `XtractUniversalLicense.json` file to backup your license.

Warning

**Data security.**\
The `config`, `private` and `logs` directories may contain sensitive information. Unauthorized access may lead to data breaches.\
Make sure to secure the backup location.

It is recommended to create backups at regular intervals or use a versioning tool to switch back to the previous version, if necessary.

Recommendation

A version control system helps manage the data transfer from test environment to production environment by keeping track of all modifications. Git or Azure DevOps are version control systems that can be used to ensure the following:

- The deployment of new extractions, sources and destinations can **not** cause fundamental damage to the data load of the productive landscape.
- User rights and other features allow for changes to be checked and corrected in advance.
- Quick and easy rollbacks of changes.

For more information on how to set up version control, refer to the knowledge base article [Deploy Extractions Using Git Version Control](../../../knowledge-base/deploy-extractions-using-Git-version-control/).

#### Restore a Backup

1. On the machine that runs the Xtract Universal service, remove the `config` directory from the installation directory of Xtract Universal, e.g., `C:\Program Files\XtractUniversal`.
1. Remove `private` directory from the installation directory.
1. Copy the backup of the `config` and `private` directories into the installation directory.
1. To restore the license, replace the `XtractUniversalLicense.json` file in the installation directory with the backup.
1. Restore the previous Xtract Universal version as listed in the `version.txt`within the `config` directory.

Warning

**Data loss.**\
Restoring the backup of the `config` and `private` directories replaces the existing configuration.\
Make sure to remove the existing files before restoring the backups.

### Update a Test Environment

A newer version can be installed over the old version. It is not necessary to uninstall the previous version. New versions are available in the [Theobald Software Customer Portal](https://my.theobald-software.com).

During the installation, the setup program checks whether other processes are running in the background that are related to the software being installed. If this is the case, a message with various options is displayed:

| Option | Description | | --- | --- | | **Check Again** | Close the affected programs and click **[Check Again]** to continue the installation. | | **Kill Process** | Kill the affected process and continue with the installation. | | **Ignore** | Ignore the message, but the installation may be canceled due to the locking of certain files. | | **Quit Install** | Cancel the installation and resume it at another time. |

#### Upgrade with Breaking Changes

The setup checks if an older version of Xtract Universal is already installed on the machine. If an older version is found, the setup displays a list of all (breaking) changes that were released between the old version and the current version. If there are breaking changes, make sure to follow the instructions in the release notes of the breaking changes to ensure a smooth update.

#### Upgrade Major Releases - ConfigConverter

When upgrading from one major product version to another, it can be necessary to convert components of your extractions. For this purpose, Xtract Universal provides the conversion tool *ConfigConverter*. The *ConfigConverter* ensures that all defined extraction types, source systems, server and user settings from the previous version are available in the new version.

There are two ways to use the *ConfigConverter*:

1. Execute the Xtract Universal setup.
1. During the setup, activate the option **Convert config files**. The option **Convert config files** starts the *ConfigConverter* application.
1. Confirm the conversion process in the command line pop-up window.

If the *ConfigConverter* is not executed automatically during installation, the converter can also be started manually. Run the *ConfigConverter.exe* in the Xtract Universal installation directory, e.g., `C:\Program Files\XtractUniversal`.

Note

Any issues that may occur during the conversion process are displayed in the command line window. Copy and send the command line context to the support team, if required.

#### Test the Update

Install the software update on the test environment and test the new version carefully.

1. Test all your existing extractions.
1. After successful testing, install the current version on the production environment.
1. In the case of an error, create a ticket in the [Support Portal](https://support.theobald-software.com).\
   Register if you do not have a customer account yet. Use your backup until a solution is provided.

### Update a Production Environment

After successfully testing the update on the test environment, install the update on the production environment. Make sure to create a backup beforehand and conduct all the necessary preparations and changes.

Warning

**Critical errors. Support cannot be provided.**\
The versions of the test environment and of the production environment must be identical. Different versions can cause critical errors. No support can be provided, if the versions are not identical.\
Make sure to keep the versions of the test environment and production environment identical - upgrade or downgrade, if needed.

______________________________________________________________________

#### Related Links

- [Knowledge Base: Deploy Extractions Using Git Version Control](../../../knowledge-base/deploy-extractions-using-Git-version-control/)
- [Customer Portal](https://my.theobald-software.com)
- [Changelog](../../../changelog/)
- [Support Portal](https://support.theobald-software.com)

This section covers [user rights](sap-authority-objects/) and customization topics for the SAP Basis.\
For information on [supported SAP systems](../setup/requirements/#supported-sap-systems-and-releases) and other IT topics, see [Setup](../setup/).

### Custom Function Modules

The following extraction types require the installation of a custom function module:

| Extraction Type | Custom Function Module | Installation | | --- | --- | --- | | Report | [Z_XTRACT_IS_REMOTE_REPORT](custom-function-module-for-reports/) | Required | | Table | [/THEO/READ_TABLE](custom-function-module-for-table-extraction/) | Recommended | | Table CDC | [/THEO/READ_TABLE](custom-function-module-for-table-extraction/) and [THEO_CDC](custom-function-module-for-tablecdc/) | Required |

Theobald Software distributes custom function modules as part of Xtract Universal. The custom function modules can be installed in SAP using transport requests. The transport requests are available in the installation directory of Xtract Universal, e.g., `C:\Program Files\XtractUniversal\ABAP\`.

For information on how to use transport request, see [Import an SAP Transport Request](/xtract-universal/knowledge-base/import-an-sap-transport-request).

Note

All objects that come with any of the transport requests, can be deleted by importing the `Z_THEO_READ_TABLE-deletion_request.zip` transport request.

### Customizations in SAP

The following extraction types require a customization in the SAP system:

- DeltaQ, see [Customization for DeltaQ](customization-for-deltaq/).
- OHS, see [Customization for OHS in BW](customization-for-ohs-in-bw/).

### Popular Topics

- [SAP User Rights](sap-authority-objects/)
- [Download SAP Roles](sap-authority-objects/#sap-authorization-profiles)
- [Function Module for Table Extractions](custom-function-module-for-table-extraction/)
- [Function Module for TableCDC Extractions](custom-function-module-for-tablecdc/)
- [Function Module for Report Extractions](custom-function-module-for-reports/)
- [Customization for DeltaQ](customization-for-deltaq/)
- [Customization for OHS](customization-for-ohs-in-bw/)

The extraction of reports requires the installation of a custom function module in your SAP system.\
If you cannot install the function module, turn to your SAP Basis team for help.

Note

As of version 1.2 of the custom function module `Z_XTRACT_IS_REMOTE_REPORT` access to reports must be explicitly granted, see [Authority Objects for Z_XTRACT_IS_REMOTE_REPORT](#authority-objects-for-z_xtract_is_remote_report).

### Installation of Z_XTRACT_IS_REMOTE_REPORT

Install the function module using the transport request *Z_XTRACT_IS_REMOTE_REPORT-transport.zip*.\
The transport request is located in the following installation directory: `C:\Program Files\[XtractProduct]\ABAP\Report\Z_XTRACT_IS_REMOTE_REPORT-transport.zip`.

The transport request needs to be imported into SAP by your SAP Basis team.

### Authority Objects for Z_XTRACT_IS_REMOTE_REPORT

As of Z_XTRACT_IS_REMOTE_REPORT version 1.2 access to reports must be explicitly granted.\
There are 2 ways to verify that the SAP user is allowed to extract a report:

- Use authentication groups, see [Authorizing Access to Reports via Authorization Groups](../../../knowledge-base/authorize-access-to-specific-reports/).
- Use the custom authorization object Z_TS_PROG.

______________________________________________________________________

#### Related Links

- [Knowledge Base: Import an SAP Transport Request](../../../knowledge-base/import-an-sap-transport-request/)
- [Knowledge Base: Authorizing Access to Specific Reports](../../../knowledge-base/authorize-access-to-specific-reports/)
- [SAP Help: Create Function Group](https://help.sap.com/viewer/bd833c8355f34e96a6e83096b38bf192/7.52.0/en-US/d1801ef5454211d189710000e8322d00.html)

SAP customization for the Table extraction type is optional. The installation of the custom function module `/THEO/READ_TABLE` is recommended to improve performance and to bypass restrictions of the SAP standard function module RFC_READ_TABLE.

### RFC_READ_TABLE Restrictions

Especially with older SAP releases you may encounter a few restrictions when using the SAP standard function module (RFC_READ_TABLE) for table extraction:

- The overall width of all columns to be extracted must not exceed 512 bytes.
- It is not possible to extract data from tables that contain one or more columns of the data type f (FLTP, floating point), DEC (decimal, e.g. for percentage) or x (RAW, LRAW).
- Poor extraction performance with larger tables. Can cause also duplicates.
- Depending on the SAP version there may be other restrictions.

When facing restrictions, install the Theobald Software custom function module [/THEO/READ_TABLE](./#installation-of-theoread_table) on your SAP system.

Warning

**Converting issues**\
Error while converting value '\*.0' of row 1530, column 3.\
The SAP standard module *RFC_READ_TABLE* for table extraction can only extract the ABAP data type DEC to a limited extent. This leads to the mentioned example error during extraction. Use the function module */THEO/READ_TABLE*.

### Installation of /THEO/READ_TABLE

An SAP transport request for the installation of the function module is provided in the installation directory of Xtract Universal: `C:\Program Files\XtractUniversal\ABAP\`.\
Transport requests are imported into SAP by your SAP Basis team. For more information, see [Knowledge Base: Import an SAP Transport Request](/xtract-universal/knowledge-base/import-an-sap-transport-request/).

Note

Take a look at the README.pdf in the installation directory (e.g.,`C:\Program Files\XtractUniversal\ABAP\README.pdf`) before installing any custom function modules.

It is recommended to install the latest custom function module THEO/READ_TABLE:

| Transport Request | Compatible SAP Systems | | --- | --- | | `THEO_READ_TABLE_740SP05.zip` | ABAP version 7.40 SP05 and higher | | `THEO_READ_TABLE_710.zip` | ABAP version 7.10 to 7.40 SP04 | | `THEO_READ_TABLE_640.zip` | ABAP versions from 6.40 until 7.03 | | `THEO_READ_TABLE_46C.zip` | ABAP versions from 4.6C |

When importing the transport requests on older SAP releases a syntax error may occur. Contact [Theobald Support](https://support.theobald-software.com) and send the dedicated error message text.

Warning

**Generating Short Dumps.**\
Testing the function modules on an SAP system is not possible. Function modules /THEO/READ_TABLE and Z_THEO_READ_TABLE can only be called by Theobald Software products due to the callback function of the module. Avoid calling function modules /THEO/READ_TABLE and Z_THEO_READ_TABLE directly from an SAP system.

### Supported Features

| Supported Features by THEO_READ_TABLE | \_740SP05 | \_710 | \_640 | 46C | | --- | --- | --- | --- | --- | | WHERE Clause | | | | | | HAVING Clause | | | | | | INNER JOIN | | | | | | LEFT OUTER JOIN | | | | | | Conversion exits | | | | | | Aggregate functions | | | | | | SQL expressions (subqueries) | | | | | | Background jobs | | | | |

______________________________________________________________________

#### Related Links

- [Knowledge Base: Import an SAP Transport Request](/xtract-universal/knowledge-base/import-an-sap-transport-request/)
- [Table Extraction Type](../../table/)

The Table CDC extraction type requires the installation of the custom function modules /THEO/CDC and /THEO/READ_TABLE in your SAP system. If you cannot install the function modules, turn to your SAP Basis team for help.

### Installation of THEO_CDC_ECC or THEO_CDC_S4

All transport requests for the custom function groups are provided in the installation directory of Xtract Universal, e.g., `C:\Program Files\XtractUniversal`. Make sure to install the correct transport request for your SAP system:

| SAP System | Function Group | Directory | | --- | --- | --- | | SAP ECC Systems | THEO_CDC_ECC | *C:\\Program Files\\XtractUniversal\\ABAP\\TableCDC\\THEO_CDC_ECC.zip* | | SAP S/4 Systems with SAP_BASIS Version < 7.55 | THEO_CDC_S4 | *C:\\Program Files\\XtractUniversal\\ABAP\\TableCDC\\THEO_CDC_S4.zip* | | SAP S/4 Systems with SAP_BASIS Version ≥ 7.55 | THEO_CDC_S4_755 | *C:\\Program Files\\XtractUniversal\\ABAP\\TableCDC\\THEO_CDC_S4_755.zip* |

The transport request needs to be imported into SAP by your SAP Basis team.\
The function groups /THEO/CDC_ECC and /THEO/CDC_S4 both contain the following function modules:

| Function Modules | Description | | --- | --- | | /THEO/CLEAR_LOGTAB | Clear entries of log tab up to a given sequence number | | /THEO/COUNT_LOGTAB_ENTRIES | Count log table entries | | /THEO/CREATE_LOG_TABLE | Function module for creating log tables | | /THEO/CREATE_TRIGGERS | Function module for creating DB triggers for CDC | | /THEO/DELETE_LOG_TABLE | Function module for deleting log tables | | /THEO/DELETE_TRIGGERS | Function module for deleting DB triggers for CDC | | /THEO/GET_DB | Get database system identifier | | /THEO/GET_INFO | Get package information | | /THEO/GET_TRIGGERS | Function module to retrieve triggers |

Note

Take a look at the README.pdf in the installation directory (e.g.,`C:\Program Files\XtractUniversal\ABAP\README.pdf`) before installing any custom function modules.

### Installation of /THEO/READ_TABLE

Install the custom function module /THEO/READ_TABLE, see [Function Module for Tables](../custom-function-module-for-table-extraction/#installation-of-theoread_table).

Note

Take a look at the README.pdf in the installation directory (e.g.,`C:\Program Files\XtractUniversal\ABAP\README.pdf`) before installing any custom function modules.

______________________________________________________________________

#### Related Links

- [Knowledge Base: Import an SAP Transport Request](../../../knowledge-base/import-an-sap-transport-request/)
- [Knowledge Base: Delta Mechanism of Table CDC](../../../knowledge-base/table-cdc-mechanism/)
- [Table CDC - Prerequisites](../../table-cdc/#prerequisites)

Before using the DeltaQ extraction type, an RFC destination has to be created in the SAP system. This page shows how to create and configure the RFC destination.

Follow the steps on this page in the following order:

1. [Create an RFC Destination of Type R/3](#create-an-rfc-destination-of-type-r3)
1. [Execute the Function Module RSAP_BIW_CONNECT_40](#execute-the-function-module-rsap_biw_connect_40)
1. [Delete the RFC Destination](#delete-the-rfc-destination)
1. [Create an RFC Destination of Type T](#create-an-rfc-destination-of-type-t)
1. [Execute the Function Module RSAS_RBWBCRL_STORE](#execute-the-function-module-rsas_rbwbcrl_store)
1. [Register the RFC Server](#register-the-rfc-server)
1. [qRFC Monitor (QOUT Scheduler)](#qrfc-monitor-qout-scheduler)

### Create an RFC Destination of Type R/3

1. Go to SAP transaction *SM59* to create an RFC destination of type *R/3*.
1. Enter a name in the field **RFC Destination**, e.g., *XTRACT01*. The name of the RFC Destination is needed again for later configuration.

### Execute the Function Module RSAP_BIW_CONNECT_40

Note

Executing the Function Module RSAP_BIW_CONNECT_40 can be performed on modifiable that the SAP systems.

The function module RSAP_BIW_CONNECT_40 creates a connection to a Business Information Warehouse. Go to SAP transaction *SE37* and execute the function module RSAP_BIW_CONNECT_40 with the following import parameters:

| Import Parameter | Example Value | Comment | | --- | --- | --- | | I_LANGU | EN | | | I_SLOGSYS | T90CLNT090 | Logical name of the source system. If you do not know this, look in table **T000** for the respective client (LOGSYS field). | | I_LOGSYS | XTRACT01 | | | I_STEXT | Xtract Destination | | | I_BASIC_IDOC | ZXTIDOC | Unique name of the RFC destinations. | | I_TSPREFIX | XT | Unique name of the RFC destinations. | | I_SAPRL | 700 | Automatically set by the SAP system. | | I_RESTORE | X | |

### Delete the RFC Destination

Go to SAP transaction *SM59* and delete the RFC destination of type *R/3* via **Detailed View > Menu > Delete**.

### Create an RFC Destination of Type T

1. Go to SAP transaction *SM59* to create an RFC destination of type *T=TCP/IP* that has the same name as the deleted RFC destination of type *R/3*.

1. Select the activation type **Registered Server Program**.

1. Set the following parameters:

   | Tab | Field | Example Value | Comment | | --- | --- | --- | --- | | Technical Settings | RFC Destination | XTRACT01 | | | Technical Settings | Connection Type | TCP/IP Connection | | | Technical Settings | Description 1 | Xtract Destination | | | Technical Settings | Activation Type | Registered Server Program | | | Technical Settings | Program ID | XTRACT01 | | | Technical Settings | Gateway Host | sap-erp-as05.example.com | Name or IP address of the SAP system. | | Technical Settings | Gateway service | sapgw00 | In the form sapgwnn, where nn is the SAP instance number (between *00* and *99*). | | Special Options | Serializer | Classic Serializer | Select the "Classic serializer". |

### Execute the Function Module RSAS_RBWBCRL_STORE

Go to SAP transaction *SE37* and execute the function module **RSAS_RBWBCRL_STORE** to activate the target system.

| Import Parameter | Example Value | Comment | | --- | --- | --- | | I_RBWBCRL | 700 | The current SAP system release number | | I_RLOGSYS | XTRACT01 | |

### Register the RFC Server

Note

Registering an RFC Server in SAP Releases is only necessary for SAP kernel version 720 and higher.

1. [Set the profile parameter *gw/acl_mode*](../../../knowledge-base/register-rfc-server-in-sap-releases-in-kernel-release-720-and-higher/#change-the-profile-parameter-gwacl_mode) to 0 (default is 1).
1. [Add the RFC Destination to the whitelist](../../../knowledge-base/register-rfc-server-in-sap-releases-in-kernel-release-720-and-higher/#define-a-whitelist-of-programs-at-the-sap-gateway) of programs that can register at the SAP Gateway.

For more information, refer to the [Knowledge Base Article: Register an RFC Server in SAP with Kernel Release 720 and higher](../../../knowledge-base/register-rfc-server-in-sap-releases-in-kernel-release-720-and-higher/).

### qRFC Monitor (QOUT Scheduler)

1. Go to SAP transaction *SMQS*.
1. Select the created RFC destination, e.g., *XTRACT01*.
1. Click **Register without activation** and set the parameter **Max.Verb.** to 10. Increase this value in case of parallel execution of several DeltaQ extraction types on the same RFC destination.

Note

For DeltaQ customizing errors, refer to the [DeltaQ Troubleshooting](../../../troubleshooting/#deltaq-troubleshooting).

Before using the OHS extraction type, an RFC destination has to be created in the SAP system. This page shows how to set up the RFC destination and SAP process chain.

Depending on the SAP release, OHS can be used as follows:

| SAP Release | SAP Object | Details | | --- | --- | --- | | BI < 7.0 | InfoSpoke | [InfoSpokes and Process Chains (BI < 7.0)](#infospokes-and-process-chains-bi-70) | | BI >= 7.0 | OHS-Destination | [OHS Destinations and Data Transfer Processes (BI >= 7.0)](#ohs-destinations-and-data-transfer-processes-bi-70) |

### Create an RFC destination

1. Go to SAP transaction *SM59* to create an RFC destination of type *TCP/IP* .
1. Select the activation type **Registered Server Program** .
1. Enter a name in the field **Program ID** , e.g., *XTRACT01*. The name of the Program ID is needed again for later configuration.

### InfoSpokes and Process Chains (BI < 7.0)

1. Go to SAP transaction *RSA1* to open the Administrator Workbench.
1. Navigate to **Tools > Open Hub Service > Infospoke** to create an InfoSpoke.
1. Store data source in the InfoSpoke, e.g. an ODS object or a cube.
1. Define an InfoSpoke for data extraction into an external system in the *Destination* tab.
1. Specify the RFC destination created in advance.
1. Fill in the columns to be transferred and, if necessary, a selection.
1. Save and activate the InfoSpoke.
1. Generate a process chain with transaction *RSA1* in the menu **Edit -> Process Chains**.
1. Activate **Start by API** in the variant for the process chain.
1. Insert the created InfoSpoke into the process chain.
1. Save and activate the process chain.

### OHS Destinations and Data Transfer Processes (BI >= 7.0)

1. Go to SAP transaction *RSA1* to open the Administrator Workbench.
1. Navigate to **Open Hub Destination** in the left tree and right-click on an InfoArea. Select **Create Open Hub Destination** in the context menu.
1. In the edit mode of the destination, set the *Destination Type* to **Third Party Tool** and select your RFC destination.
1. Save and activate the OHS destination.
1. Click on the newly created OHS destination in the middle tree of the InfoAreas and select **Create Data Transfer Process** to create a new data transfer process (DTP).
1. Save and activate the DTP. If necessary, change the extraction type from *Delta* to *Full* before activating) The arrangement of Destination, Transformations and DTP in the OHS tree is done afterwards.
1. Create a process chain that contains the new DTP.
1. Make sure that the planning option **Start Using Meta Chain or API** is selected in the process chain start variant.
1. Activate the process chain.

______________________________________________________________________

#### Related Links

- [SAP Note 2437637](https://launchpad.support.sap.com/#/notes/2437637)
- [SAP Note 1983356](https://launchpad.support.sap.com/#/notes/1983356)

Before using the DeltaQ extraction type, an RFC destination has to be created in the SAP system. This page shows how to create and configure the RFC destination.

Follow the steps on this page in the following order:

1. [Create an RFC Destination of Type R/3](#create-an-rfc-destination-of-type-r3)
1. [Execute the Function Module RSAP_BIW_CONNECT_40](#execute-the-function-module-rsap_biw_connect_40)
1. [Delete the RFC Destination](#delete-the-rfc-destination)
1. [Create an RFC Destination of Type T](#create-an-rfc-destination-of-type-t)
1. [Execute the Function Module RSAS_RBWBCRL_STORE](#execute-the-function-module-rsas_rbwbcrl_store)
1. [Register the RFC Server](#register-the-rfc-server)
1. [qRFC Monitor (QOUT Scheduler)](#qrfc-monitor-qout-scheduler)

### Create an RFC Destination of Type R/3

1. Go to SAP transaction *SM59* to create an RFC destination of type *R/3*.
1. Enter a name in the field **RFC Destination**, e.g., *XTRACT01*. The name of the RFC Destination is needed again for later configuration.

### Execute the Function Module RSAP_BIW_CONNECT_40

Note

Executing the Function Module RSAP_BIW_CONNECT_40 can be performed on modifiable that the SAP systems.

The function module RSAP_BIW_CONNECT_40 creates a connection to a Business Information Warehouse. Go to SAP transaction *SE37* and execute the function module RSAP_BIW_CONNECT_40 with the following import parameters:

| Import Parameter | Example Value | Comment | | --- | --- | --- | | I_LANGU | EN | | | I_SLOGSYS | T90CLNT090 | Logical name of the source system. If you do not know this, look in table **T000** for the respective client (LOGSYS field). | | I_LOGSYS | XTRACT01 | | | I_STEXT | Xtract Destination | | | I_BASIC_IDOC | ZXTIDOC | Unique name of the RFC destinations. | | I_TSPREFIX | XT | Unique name of the RFC destinations. | | I_SAPRL | 700 | Automatically set by the SAP system. | | I_RESTORE | X | |

### Delete the RFC Destination

Go to SAP transaction *SM59* and delete the RFC destination of type *R/3* via **Detailed View > Menu > Delete**.

### Create an RFC Destination of Type T

1. Go to SAP transaction *SM59* to create an RFC destination of type *T=TCP/IP* that has the same name as the deleted RFC destination of type *R/3*.

1. Select the activation type **Registered Server Program**.

1. Set the following parameters:

   | Tab | Field | Example Value | Comment | | --- | --- | --- | --- | | Technical Settings | RFC Destination | XTRACT01 | | | Technical Settings | Connection Type | TCP/IP Connection | | | Technical Settings | Description 1 | Xtract Destination | | | Technical Settings | Activation Type | Registered Server Program | | | Technical Settings | Program ID | XTRACT01 | | | Technical Settings | Gateway Host | sap-erp-as05.example.com | Name or IP address of the SAP system. | | Technical Settings | Gateway service | sapgw00 | In the form sapgwnn, where nn is the SAP instance number (between *00* and *99*). | | Special Options | Serializer | Classic Serializer | Select the "Classic serializer". |

### Execute the Function Module RSAS_RBWBCRL_STORE

Go to SAP transaction *SE37* and execute the function module **RSAS_RBWBCRL_STORE** to activate the target system.

| Import Parameter | Example Value | Comment | | --- | --- | --- | | I_RBWBCRL | 700 | The current SAP system release number | | I_RLOGSYS | XTRACT01 | |

### Register the RFC Server

Note

Registering an RFC Server in SAP Releases is only necessary for SAP kernel version 720 and higher.

1. [Set the profile parameter *gw/acl_mode*](../../../knowledge-base/register-rfc-server-in-sap-releases-in-kernel-release-720-and-higher/#change-the-profile-parameter-gwacl_mode) to 0 (default is 1).
1. [Add the RFC Destination to the whitelist](../../../knowledge-base/register-rfc-server-in-sap-releases-in-kernel-release-720-and-higher/#define-a-whitelist-of-programs-at-the-sap-gateway) of programs that can register at the SAP Gateway.

For more information, refer to the [Knowledge Base Article: Register an RFC Server in SAP with Kernel Release 720 and higher](../../../knowledge-base/register-rfc-server-in-sap-releases-in-kernel-release-720-and-higher/).

### qRFC Monitor (QOUT Scheduler)

1. Go to SAP transaction *SMQS*.
1. Select the created RFC destination, e.g., *XTRACT01*.
1. Click **Register without activation** and set the parameter **Max.Verb.** to 10. Increase this value in case of parallel execution of several DeltaQ extraction types on the same RFC destination.

Note

For DeltaQ customizing errors, refer to the [DeltaQ Troubleshooting](../../../troubleshooting/#deltaq-troubleshooting).

To use Xtract Universal you need an SAP connection user with sufficient authorization in SAP. Authorizations are assigned via authorization objects in SAP. Redirect this page to your SAP Basis administrators to get the relevant authorization objects for your SAP connection user.

The authorizations in the section [General authorization objects](#general-authorization-objects) are required to establish an SAP connection with the SAP application server. The required authorizations for each extraction type are listed in their respective section.

### SAP Authorization Profiles

Theobald Software collected and combined the necessary authorizations for all extraction types into SAP roles. You can download the SAP profiles and upload them to your SAP system:

| Extraction Type | SAP Role File | | --- | --- | | [General Authorization Objects](#general-authorization-objects) | [ZXTGENERAL.SAP](/xtract-universal/assets/files/sap_roles/ZXTGENERAL.SAP) | | [BAPI](#bapi) | [ZXTBAPI.SAP](/xtract-universal/assets/files/sap_roles/ZXTBAPI.SAP) | | [BW Cube](#bw-cube-bw-query) | [ZXTBWCUBE.SAP](/xtract-universal/assets/files/sap_roles/ZXTBWCUBE.SAP) | | [BW Hierarchy](#bw-hierarchy) | [ZXTBWHIERARCHY.SAP](/xtract-universal/assets/files/sap_roles/ZXTBWHIERARCHY.SAP) | | [ODP (Operational Data Provisioning)](#odp) | [ZXTODP.SAP](/xtract-universal/assets/files/sap_roles/ZXTODP.SAP) | | [OHS (Open Hub Services)](#ohs) | [ZXTOHS.SAP](/xtract-universal/assets/files/sap_roles/ZXTOHS.SAP) | | [Query](#query) | [ZXTQUERY.SAP](/xtract-universal/assets/files/sap_roles/ZXTQUERY.SAP) | | [Report](#report) | [ZXREPORT.SAP](/xtract-universal/assets/files/sap_roles/ZXREPORT.SAP) | | [Table](#table) | [ZXTABLE.SAP](/xtract-universal/assets/files/sap_roles/ZXTABLE.SAP) | | [Table CDC](#table-cdc) | [ZXTABLECDC.SAP](/xtract-universal/assets/files/sap_roles/ZXTABLECDC.SAP) | | [DeltaQ](#deltaq) | [ZXTDELTAQ.SAP](/xtract-universal/assets//files/sap_roles/ZXTDELTAQ.SAP), [DELTAQ_CUSTOMIZING_CHECK](/xtract-universal/assets/files/sap_roles/DELTAQ_CUSTOMIZING_CHECK.SAP) |

Note

If you still get an authorization error, ask SAP Basis to record an ST01-/ or SU53-authorization trace in SAP. This trace shows which authorizations objects are missing.

### General Authorization Objects

The following authorization objects are required to establish a connection to SAP.

Necessary SAP authorizations

```text
S_RFC           RFC_TYPE=FUGR; RFC_NAME=SYST; ACTVT=16
S_RFC           RFC_TYPE=FUGR; RFC_NAME=SRFC; ACTVT=16
S_RFC           RFC_TYPE=FUGR; RFC_NAME=RFC1; ACTVT=16
S_RFC           RFC_TYPE=FUGR; RFC_NAME=OCS_CRM; ACTVT=16

```

[Download SAP profile for general authorization](/xtract-universal/assets/files/sap_roles/ZXTGENERAL.SAP)

### BAPI

Necessary SAP authorizations

```text
S_RFC            ACTVT=16; RFC_TYPE=FUGR; RFC_NAME=DDIF_FIELDINFO_GET, SDIFRUNTIME     

```

[Download SAP profile for BAPI Extractions](/xtract-universal/assets/files/sap_roles/ZXTBAPI.SAP)

### BW Cube / BW Query

Authorizations for the underlying Queries, Cubes, InfoAreas and analysis need to be assigned via:

- `S_RS_COMP`
- `S_RS_COMP1`
- `S_RS_AUTH`

Necessary SAP authorizations

```text
S_RFC            RFC_TYPE=FUGR; RFC_NAME=RSOB; ACTVT=16
S_RFC            RFC_TYPE=FUGR; RFC_NAME=RRX1; ACTVT=16
S_TABU_NAM       ACTVT=03; TABLE=RSRREPDIR
S_TABU_NAM       ACTVT=03; TABLE=RSZGLOBV
S_TABU_NAM       ACTVT=03; TABLE=RSADMINC

```

Necessary SAP authorizations

```text
S_RFC            RFC_TYPE=FUGR;RFC_NAME=SYST;ACTVT=16;type=RF;name=RFCPING;
S_RFC            RFC_TYPE=FUGR; RFC_NAME=RSOBJS_RFC_INTERFACE; ACTVT=16; type=RF;name=RSOBJS_GET_NODES;
S_RFC            RFC_TYPE=FUGR;RFC_NAME=RSAO_CORE;ACTVT=16;type=RF;name=RSAO_BICS_SESSION_INITIALIZE
S_RFC            RFC_TYPE=FUGR;RFC_NAME=RSBOLAP_BICS_CONSUMER;ACTVT=16;type=RF;name=BICS_CONS_CREATE
S_RFC            RFC_TYPE=FUGR;RFC_NAME=RSBOLAP_BICS_PROVIDER;ACTVT=16;type=RF;name=BICS_PROV_OPEN;
S_RFC            RFC_TYPE=FUGR;RFC_NAME=RSBOLAP_BICS_PROVIDER_VAR;ACTVT=16;type=RF;name=BICS_PROV_VA
S_ADMI_FCD       S_ADMI_FCD=PADM;

```

Necessary SAP authorizations

```text
S_TABU_NAM       ACTVT=03; TABLE=DD03L

```

Alternatively, you can assign the SAP role template `S_RS_RREPU`.

[Download SAP profile for BW Cube / BW Query](/xtract-universal/assets/files/sap_roles/ZXTBWCUBE.SAP)

### BW Hierarchy

Necessary SAP authorizations

```text
S_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=RSNDI_SHIE; ACTVT=16
S_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=BAPI_IOBJ_GETDETAIL; ACTVT=16
S_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=DDIF_FIELDINFO_GET; ACTVT=16
S_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=RFC1; ACTVT=16
S_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=RFC_GET_FUNCTION_INTERFACE; ACTVT=16
S_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=RFC_READ_TABLE; ACTVT=16
S_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=RSBAPI_IOBJ; ACTVT=16 
S_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=RSNDI_SHIE; ACTVT=16
S_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=SDIFRUNTIME; ACTVT=16
S_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=SDTX; ACTVT=16
S_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=SYST; ACTVT=16
S_RS_ADMWB     RSADMWBOBJ=INFOOBJECT; ACTVT=03
S_TABU_DIS     ACTVT=03; DICBERCLS=BWC
S_TABU_DIS     ACTVT=03; DICBERCLS=BWG
S_TABU_NAM     ACTVT=03; TABLE=/BIC/*
S_TABU_NAM     ACTVT=03; TABLE=ENLFDIR
S_TABU_NAM     ACTVT=03; TABLE=RSHIEDIR

```

[Download SAP profile for BW Hierarchy](/xtract-universal/assets/files/sap_roles/ZXTBWHIERARCHY.SAP)

### ODP

For a complete and detailed list of authorization objects refer to [SAP Note 2855052](https://launchpad.support.sap.com/#/notes/2855052) - Authorizations required for ODP Data Replication API 2.0.

Necessary SAP authorizations

```text
S_TABU_NAM       ACTVT=03; TABLE=TCURX

```

[Download SAP profile for ODP](/xtract-universal/assets/files/sap_roles/ZXTODP.SAP)

### OHS

Alternatively, you can assign the SAP role template `S_BI-WHM_RFC`.

Necessary SAP authorizations

```text
S_RFC      RFC_TYPE=FUGR; RFC_NAME=RSB3RD; ACTVT=16
S_RFC      RFC_TYPE=FUGR; RFC_NAME=SDTX; ACTVT=16
S_RFC      RFC_TYPE=FUGR; RFC_NAME=BAPT; ACTVT=16
S_RFC      RFC_TYPE=FUGR; RFC_NAME=BATG; ACTVT=16
S_RFC      RFC_TYPE=FUGR; RFC_NAME=RSPC_API; ACTVT=16
S_TABU_DIS RC=0 ACTVT=03; DICBERCLS=&NC& 
S_RS_PC    RSPCCHAIN=*;RSPCAPPLNM=*; RSPCPART=DEFINITION; ACTVT=03
S_RS_PC    RSPCCHAIN=*;RSPCAPPLNM=*; RSPCPART=RUNTIME; ACTVT=16
S_CTS_ADMI CTS_ADMFCT=TABL
S_RS_DTP   RSTLDTPSRC=CUBE; RSSTDTPSRC=*; RSONDTPSRC=0D_DECU; RSTLDTPTGT=DEST; RSSTDTPTGT=*; ACTVT=16
S_BTCH_ADM BTCADMIN=Y
S_BTCH_JOB JOBGROUP=*; JOBACTION=RELE
S_RS_TR    RSTLOGOSRC=CUBE; RSSTTRSRC=*; RSOBJNMSRC=0D_DECU; RSTLOGOTGT=DEST; RSSTTRTGT=' '; RSOBJNMTG=*; ACTVT=16
S_RS_AUTH  BIAUTH=0BI_ALL
S_ADMI_FCD S_ADMI_FCD=ST0R

```

[Download SAP profile for OHS](/xtract-universal/assets/files/sap_roles/ZXTOHS.SAP)

### Query

Necessary SAP authorizations

```text
S_RFC            RFC_TYPE=FUGR; RFC_NAME=AQRC; ACTVT=16 

```

[Download SAP profile for SAP Query](/xtract-universal/assets/files/sap_roles/ZXTQUERY.SAP)

### Report

Necessary SAP authorizations

```text
S_RFC            RFC_TYPE=FUGR; RFC_NAME=ZXTRACTABAP; ACTVT=16
S_TABU_NAM       ACTVT=03; TABLE=TRDIR, TRDIRT, TSTC, VARID
S_GUI            ACTVT=61 
S_TABU_DIS       ACTVT=03; DICBERCLS=&NC& 
S_TABU_DIS       ACTVT=03; DICBERCLS=SS
S_BTCH_ADM       BTCADMIN=Y
S_BTCH_JOB       JOBGROUP=*; JOBACTION=RELE

```

Note

The necessary transport request for function group *ZXTRACTABAP* is located in the following path: `C:\Program Files\[XtractProduct]\ABAP\Report\Z_XTRACT_IS_REMOTE_REPORT-transport.zip` of the default installation.

[Download SAP profile for Report](/xtract-universal/assets/files/sap_roles/ZXREPORT.SAP)

To execute a report with Xtract Universal, the SAP connection user needs explicit authorization to execute the report. Authorization can be granted using one of the following methods:

- [Assign the authorization object Z_TS_PROG](../../../knowledge-base/create-the-custom-authority-object-z-ts-prog/)
- [Assign an authorization group](../../../knowledge-base/authorize-access-to-specific-reports/)

### Table

Necessary SAP authorizations

```text
S_RFC            ACTVT=16; RFC_TYPE=FUGR; RFC_NAME=SDTX, SDIFRUNTIME, /THEO/READ_TABLE                   
S_TABU_DIS       ACTVT=03; DICBERCLS=XXXX
S_TABU_NAM       ACTVT=03; TABLE=DD02V, DD17S, DD27S, ENLFDIR
S_DSAUTH         ACTVT=16;    

```

XXXX (stands for a placeholder) is the authorization group for the table. To determine, which authorization group belongs to which table, check the table TDDAT - Maintenance Areas for Tables. If the table is not listed, the authorization group is &NC&. For authorizing specific tables use authorization object S_TABU_NAM instead of S_TABU_DIS.

Note

The transport request for function group */THEO/READ_TABLE* and *Z_THEO_READ_TABLE* is located in the following path: `C:\Program Files\[XtractProduct]\ABAP\Table` of the default installation.

Additional options:

Necessary SAP authorizations

```text
S_BTCH_ADM       BTCADMIN=Y
S_BTCH_JOB       JOBGROUP=*; JOBACTION=RELE

```

Necessary SAP authorizations

```text
S_RFC            RFC_TYPE=FUNC; RFC_NAME=EM_GET_NUMBER_OF_ENTRIES; ACTVT=16  

```

Necessary SAP authorizations

```text
S_TABU_NAM       ACTVT=03; TABLE=TCURX

```

[Download SAP profile for Table](/xtract-universal/assets/files/sap_roles/ZXTABLE.SAP)

### Table CDC

Necessary SAP authorizations

```text
S_RFC            ACTVT=16; RFC_TYPE=FUGR, FUNC; RFC_NAME=SDTX, SDIFRUNTIME, /THEO/CDC_*, /THEO/READ_TABLE            
S_TABU_DIS       ACTVT=02, 03; DICBERCLS=*
S_TABU_CLI       CLIIDMAINT=X 
S_TABU_NAM       ACTVT=03; TABLE=DD02V, D17S, D27S, ENLFDIR
S_DEVELOP        ACTVT=02; DEVCLASS=$TMP; OBJNAME=ZTSCDC_*; OBJTYPE=*; P_GROUP=*

```

XXXX (stands for a placeholder) is the authorization group for the source table. To determine, which authorization group belongs to which table, check the table TDDAT - Maintenance Areas for Tables. If the table is not listed, the authorization group is &NC&. For authorizing specific tables use authorization object S_TABU_NAM instead of S_TABU_DIS.

Note

The transport requests for the required function groups */THEO/READ_TABLE* are located in `C:\Program Files\[XtractProduct]\ABAP\TableCDC` and `C:\Program Files\[XtractProduct]\ABAP\Table`.

[Download SAP profile for Table CDC](/xtract-universal/assets/files/sap_roles/ZXTABLECDC.SAP)

### DeltaQ

Necessary SAP authorizations

```text
S_RFC           RFC_TYPE=FUGR; RFC_NAME=SUSR; ACTVT=16  
S_RFC           RFC_TYPE=FUNC; RFC_NAME=RFC_GET_SYSTEM_INFO; ACTVT= 16 
S_ADMI_FCD      S_ADMI_FCD=NADM
S_TABU_DIS      ACTVT = 02; DICBERCLS=SA
S_TABU_DIS      ACTVT = 03; DICBERCLS=SA
S_TABU_NAM      ACTVT = 02; TABLE=EDIPOA
S_TABU_NAM      ACTVT = 03; TABLE=EDIPOA

```

Necessary SAP authorizations

```text
S_RFC            RFC_TYPE=FUGR; RFC_NAME=SDIFRUNTIME; ACTVT=16 
S_RFC            RFC_TYPE=FUGR; RFC_NAME=RSAG; ACTVT=16 
S_TABU_DIS       ACTVT=03; DICBERCLS=SS                                                
S_TABU_DIS       ACTVT=03; DICBERCLS=SC                                               
S_IDOCDEFT       EDI_TCD=WE30; ACTVT=01; EDI_CIM=*; EDI_DOC=*                            
S_IDOCDEFT       EDI_TCD=WE30; ACTVT=03; EDI_CIM=*; EDI_DOC=*   

```

Necessary SAP authorizations

```text
S_RFC           RFC_TYPE=FUGR; RFC_NAME=SDIFRUNTIME; ACTVT=16
S_TABU_DIS      ACTVT=03; DICBERCLS=SS                                 
S_TABU_DIS      ACTVT=03; DICBERCLS=SC                                               
S_IDOCDEFT      EDI_TCD=WE30; ACTVT=02; EDI_CIM=*; EDI_DOC=*

```

Necessary SAP authorizations

```text
S_RFC            RFC_TYPE=FUGR; RFC_NAME=EDI1; ACTVT=16
S_RFC            RFC_TYPE=FUGR; RFC_NAME=BATG; ACTVT=16
S_RFC            RFC_TYPE=FUGR; RFC_NAME=EDIMEXT; ACTVT=16 
S_RFC            RFC_TYPE=FUGR; RFC_NAME=ARFC; ACTVT=16 
S_RFC            RFC_TYPE=FUGR; RFC_NAME=ERFC; ACTVT=16 
S_RFC            RFC_TYPE=FUGR; RFC_NAME=EDIN; ACTVT=16 
S_RFC            RFC_TYPE=FUGR; RFC_NAME=/BIC/*; ACTVT=16 
S_RFC            RFC_TYPE=FUGR; RFC_NAME=/BI0/*; ACTVT=16
S_RFC            RFC_TYPE=FUGR; RFC_NAME=RSAK; ACTVT=16
S_RFC            RFC_TYPE=FUGR; RFC_NAME=EDIW; ACTVT=16
S_RFC            RFC_TYPE=FUGR; RFC_NAME=SDTX; ACTVT=16
S_RFC            RFC_TYPE=FUGR; RFC_NAME=EDIMEXT; ACTVT=16
S_RFC            RFC_TYPE=FUGR; RFC_NAME=SYSU; ACTVT=16
S_RFC            RFC_TYPE=FUGR; RFC_NAME=RSC1; ACTVT=16
S_RFC            RFC_TYPE=FUNC; RFC_NAME=RSAP_REMOTE_HIERARCHY_CATALOG;  ACTVT=16
S_RFC            RFC_TYPE=FUNC; RFC_NAME=RSA1_OLTPSOURCE_GET_ALL;  ACTVT=16
S_RFC            RFC_TYPE=FUNC; RFC_NAME=RSA1_OLTPSOURCE_GET_SELECTIONS;  ACTVT=16
S_RFC_ADM        ACTVT=03; RFCDEST=XTRACT*; RFCTYPE = T; ICF_VALUE=* 
B_ALE_RECV       EDIMES=RSRQST
S_IDOCMONI       ACTVT=03; EDI_DIR=*; EDI_MES=*; EDI_PRN=*; EDI_PRT=*; EDI_TCD=WE*
S_IDOCDEFT       EDI_TCD=WE30; ACTVT=02; EDI_CIM=*; EDI_DOC=*
S_IDOCDEFT       EDI_TCD=WE30; ACTVT=03; EDI_CIM=*; EDI_DOC=*
S_TABU_DIS       ACTVT=03; DICBERCLS=SS                                   
S_TABU_DIS       ACTVT=03; DICBERCLS=SC
S_TABU_DIS       ACTVT=03; DICBERCLS=&NC&  
S_BTCH_ADM       BTCADMIN=Y          
S_BTCH_JOB       JOBGROUP=*; JOBACTION=RELE
S_SPO_DEV        SPODEVICE=*
S_RO_OSOA        OLTPSOURCE=*; OSOAAPCO=*; OSOAPART=DATA; ACTVT=03;  | Only in SAP Releases  7.0 and higher

```

[Download SAP profile for DeltaQ](/xtract-universal/assets/files/sap_roles/ZXTDELTAQ.SAP)

[Download SAP profile for DeltaQ Customizing Check](/xtract-universal/assets/files/sap_roles/DELTAQ_CUSTOMIZING_CHECK.SAP)

______________________________________________________________________

#### Related Links

- [SAP Help: Role Administration](https://help.sap.com/doc/saphelp_nw74/7.4.16/en-us/52/6714a9439b11d1896f0000e8322d00/content.htm)
- [SAP ONE Support Launchpad](https://launchpad.support.sap.com/#/notes/2855052)

This page shows how to use the Table extraction type.\
The Table extraction type can be used to extract data from SAP Tables and Views.

### Supported SAP Objects

- Transparent tables
- Views
- ABAP CDS Views (without parameters)
- HANA CDS Views
- Pool tables (joining not possible)
- Cluster tables (joining not possible)

### RFC_READ_TABLE Restrictions

Especially with older SAP releases you may encounter a few restrictions when using the SAP standard function module (RFC_READ_TABLE) for table extraction:

- The overall width of all columns to be extracted must not exceed 512 bytes.
- It is not possible to extract data from tables that contain one or more columns of the data type f (FLTP, floating point), DEC (decimal, e.g. for percentage) or x (RAW, LRAW).
- Poor extraction performance with larger tables. Can cause also duplicates.
- Depending on the SAP version there may be other restrictions.

When facing restrictions, install the Theobald Software custom function module [/THEO/READ_TABLE](../setup-in-sap/custom-function-module-for-table-extraction/#installation-of-theoread_table) on your SAP system.

Warning

**Converting issues**\
Error while converting value '\*.0' of row 1530, column 3.\
The SAP standard module *RFC_READ_TABLE* for table extraction can only extract the ABAP data type DEC to a limited extent. This leads to the mentioned example error during extraction. Use the function module */THEO/READ_TABLE*.

### Prerequisites

- A connection to an SAP system is available, see [SAP Connection](../sap-connection/).
- The SAP user has sufficient user rights, see [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#table).

Warning

**Missing Authorization.**\
To use the Table extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#table) accordingly.

### Create a Table Extraction

1. In the main window of the Designer, click **[New]** to create a new extraction. The window "Create Extraction" opens.
1. Select an [SAP Connection](../sap-connection/) of type **RFC** from the drop-down menu **Source**.
1. Enter a unique name for your extraction.
1. Select the extraction type **Table**.
1. Click **[OK]**. The main window of the extraction type opens automatically.

The majority of the functions of the extraction type can be accessed in the main window.

### Look up an SAP Table

1. In the main window of the extraction type, click **[Add]** to add a table. The window "Table Lookup" opens.
1. In the field **Table Name**, enter the name of the table to extract . Use wildcards (\*) if needed.
1. Click **[]** . Search results are displayed.
1. Select a table and click **[OK]**.

All relevant metadata information of the table is retrieved from SAP. The application returns to the main window of the extraction type.

### Define the Table Extraction Type

The Table extraction type offers the following options for table extractions:

1. Select the columns you want to extract. By default all columns are selected. Deselect the columns you do not want to extract.
1. Optional: Define a [WHERE clause](where-clause/) or a [HAVING clause](having-clause/) to filter table records. By default all data is extracted.
1. Optional: Join two or more tables and extract the result of the join. For more information, see [Table Joins](table-join/).
1. Click **[Load live preview]** to display a live preview of the first 100 records.
1. Check the [Extraction Settings](settings/) and the [General Settings](general-settings/) before running the extraction.
1. Click **[OK]** to save the extraction type.

You can now run the extraction, see [Execute and Automate Extractions](../execute-and-automate/).

______________________________________________________________________

#### Related Links

- [Knowledge Base Article: Delta Table Extraction](../../knowledge-base/delta-table-extraction/)
- [Knowledge Base Article: Read data from Cluster Fields in Tables PCL1 and PCL2 (Payroll)](../../knowledge-base/read-data-from-cluster-fields-in-the-tables-pcl1-and-pcl2-payroll/)

Runtime parameters are are placeholders for values that are passed at runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom). They can be created in context of using the [WHERE Clause](../where-clause/).

## Create Runtime Parameters

There are two types of runtime parameters:

- [Scalar parameters](#scalar-parameters) represent a single value.
- [List parameters](#list-parameters) represent multiple values.

### Scalar Parameters

Follow the steps below to create a scalar runtime parameter:

1. Open the *Edit Runtime Parameters* tab.

1. Click **[Add Scalar]** to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.

   | Type | Description | | --- | --- | | *Text* | Can be used for any type of SAP selection field. | | *Number* | Can be used for numeric SAP selection fields. | | *Flag* | Can only be used for SAP selection fields THAT require an ‘X’ (true) or a blank ‘‘ (false) as input value. |

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

### List Parameters

Follow the steps below to create a list runtime parameter:

1. In the main window of the component click **Edit runtime parameters**. The window “Edit Runtime Parameters” opens.

1. Click **[Add List]** to define list parameters that contain multiple values separated by commas e.g., 1,10 or “1”, “10”. The placeholders need to be populated with actual values at runtime.

   Tip

   Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.

1. Click **[OK]** to confirm.

The runtime parameters are now available in the extraction type.

## Use Runtime Parameters in the WHERE Clause Editor

1. Navigate to the WHERE clause tab in the main window of the extraction and open the WHERE clause editor.
1. Add a new WHERE clause criteria that uses the **[Default with Parameter]** template.
1. Click the *Parameter* component. A drop-down list that displays all available parameters opens. Select a parameter from the list.
1. To test the WHERE clause, click **[Load live preview]**. Provide parameter values when prompted.

## Use Runtime Parameters in the WHERE Clause Text Mode

Add an @ symbol before a value to mark it as a runtime parameter, e.g., enter `@myParameter` instead of a value. Example: `T001W~WERKS BETWEEN @PlantLow AND @PlantHigh`.

Pass values during runtime, see [Extraction Parameters - Custom](../../parameters/extraction-parameters/#custom).

This page contains an overview of the settings in the window "General Settings".\
To open the general settings, click **General Settings** in the main window of the extraction type.

### Misc. Tab

The *Misc.* tab covers cache settings, column encryption and keywords of an extraction type.

#### Cache results

The *Cache results* option is only available in [pull destinations](../../destinations/), e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times. To decrease the SAP server load, you can select the **Cache results** option, this way the pull destination pulls the data from cache and not from the SAP.

Cache results increases the performance and limits the impact on the SAP system. If this behavior is undesirable (for example, because the data must be always 100% up-to-date), the cache option must be explicitly disabled.

#### Keywords

One or more keywords (tags) can be assigned to an extraction. Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the "Search Extractions" window. To open the "Search Extractions" window, click **[ Search]** in the main window of the Designer.

Tip

To add keywords to multiple extractions at once, select the extractions in the main window of the Designer.\
Right-click + **Add/Remove keywords** opens the window "Add/Remove Keywords To/From Multiple Extractions".

### Primary Key Tab

Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.

The depicted example shows the SAP object *MAKT* with its primary key inherited from SAP in the general settings of the Designer. In this example the primary key consists of *MANDT*, *MATNR* and *SPRAS*. The primary key is also taken over in the destination.

Note

A defined primary key field in a table is a prerequisite for merging data.

#### Generate Surrogate Key Column

If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs. The surrogate keys are hash values of type signed 8 byte integer, e.g., `#-3008591679982390000`. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.

### Security Tab

Restrict user access to the extraction. For more information, see [Restrict Designer Access](../../access-restrictions/restrict-designer-access/).

### Columns Order Tab

The "Columns Order" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns. Index 0 defines the first column in the result set, index 1 the second columns, etc.

| Option | Description | | --- | --- | | **Enabled** | When this option is active, the defined column order is applied when running the extraction. | | **Swap** | Swaps the index of 2 columns. All other columns keep their indexes. | | **Insert** | Inserts the selected column into the selected index. All other indexes are recalculated. | | **Reset default** | Restores the original column order. |

The HAVING clause can be used to filter groups of rows, see [SAP ABAP Documentation: SELECT-HAVING](https://help.sap.com/doc/abapdocu_750_index_htm/7.50/en-US/abaphaving_clause.htm).

### Create a HAVING Clause

The following example shows how many materials are assigned to a material type (MTART). After applying the HAVING Clause, the preview shows only the material types with more than 10 materials assigned.

1. Select an aggregate function for a field you want to use in the HAVING-clause.

   Note

   Aggregate functions are only supported for numeric field types, which is why the field BRGEW (Gross Weight) is used in the example.

1. Navigate to tab *HAVING Clause* .

1. Enter a HAVING Clause using the [syntax](#having-clause-syntax) in accordance to your SAP Release, e.g., `COUNT(BRGEW) > 10`.

1. Click **[Load live preview]** to display the results in the *Preview* section.

Note

When fields with the same name exist in different tables, the field names must be formatted as [Table]~[Column], e.g., MAKT~MATNR. This can be the case with [table joins](../table-join/).

### HAVING Clause Syntax

With regard to syntax and formulas, the same rules apply as for the [WHERE Clause](../where-clause/#where-clause-syntax). Depending on your SAP release the syntax may varry, see [SAP Help - ABAP SQL - SQL Expressions sql_exp](https://help.sap.com/doc/abapdocu_latest_index_htm/latest/en-US/abenabap_sql_strictmode_754.htm) .

This section contains an overview and description of the *Tables and Fields* tab in the main window of the Table extraction type.

### Tables

The subsection *Tables* lists all SAP Tables and Views that were added to the extraction type.

- Click **[Add]** to add an SAP Table or View to the extraction type.
- Click **[Remove]** to remove an SAP Table or View from the extraction type.

Tip

Select an SAP Table to define the corresponding settings, e.g, output columns, WHERE clause, etc. You can switch between the SAP Tables.

### Fields

The subsection *Fields* displays all columns in the selected SAP Table or View.

The Table extraction type imports and highlights the dedicated indexes of an SAP Table, such as primary key and/or sorting options.

Note

Use the indicated fields for filtering increases performance when applying WHERE-clause.

| Columns | Description | | --- | --- | | / | Defines whether or not a table column is added to the output of the extraction type. By default, all table columns are extracted. | | **Name** | Name of a column in the SAP Table. The column name can be filtered. | | **Description** | Description of the column. The column description can be filtered. | | **Aggregate Function** | Applies [aggregate functions](#aggregate-functions) to numeric fields. | | **Conv.** | Activates SAP [conversion routines](#conversion-routines). |

#### Aggregate Functions

Aggregate functions are only available for numeric field data types, e.g., INT, FLOAT, DECIMAL. The following aggregation functions are available:

- *None*: No aggregation
- *MEAN*: Average
- *COUNT*: Number
- *MAX*: Maximum
- *MIN*: Minimum
- *SUM*: Total

#### Conversion Routines

Conversion routines are stored in the Data Dictionary that is used for the respective fields. Activating the conversion routines in SAP leads to significant performance decrease. After the conversion, the value is displayed as it appears in the transaction *SE16N* in the SAP GUI.

Examples:

- Language keys, e.g., `D` in the database becomes `DE` after conversion
- Project numbers, e.g., `T000012738GT` becomes `T/12738/GT` after conversion.

| State | Description | Safety | | --- | --- | --- | | | no conversion routines selected | - | | | conversion routines enabled requires the function module Z_XTRACT_IS_TABLE_COMPRESSION | no data type safety | | | conversion routines enabled requires the function module [/THEO/READ_TABLE](../../setup-in-sap/custom-function-module-for-table-extraction/) | assured data type safety |

### Preview

The subsection *Preview* displays a real-time preview of the first 100 fields of the SAP table when clicking the button **[Load live preview]**.

Note

The use non-indexed fields in the WHERE-clause can lead to timeouts during the preview of large tables.

### Buttons

**[Load live preview]**\
Allows a real-time preview of the extraction data without executing the extraction.\
You can also preview the data with aggregation functions.

**[Count rows]**\
Returns the number of rows/data records of an extraction, considering the WHERE and HAVING clauses stored.

**[Refresh metadata]**\
A new lookup is performed on the selected table(s). Existing mappings and field selections are retained, which is not the case when the table is added again.\
It can be necessary to refresh the metadata when a table has been adjusted in SAP, when another source system is assigned to the extraction type, or when the source system was updated. In such cases, data inconsistencies can occur that are resolved by this function.

This page contains an overview of the extraction settings in the Table extraction type.\
The extraction settings are located in the tab *Extraction Settings*.

Warning

**Could not load list of available function modules because permission for table ENLFDIR is missing**\
This warning appears if a technical SAP user does not have authorization rights to access the SAP table *ENLFDIR*. Confirm the warning as the user can **still** adjust the extraction settings.

### Extraction Settings

#### Package Size

The extracted data is split into packages of the defined size. The default value is 50000 lines.\
A package size between 20000 and 50000 is advisable for large amounts of data. 0 means no packaging. Not using packaging can lead to an RFC timeout for large data extractions.

Warning

**RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table**\
To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.

#### Row Limit

Specifies the maximum number of extracted records. 0 extracts all data. You can use this option to perform tests with a small amount of data by entering a row limit of e.g., 1000.

### Function Module

Specifies the name of the function module used for data extraction. This field is filled automatically depending on what function modules are installed on your SAP system. Custom function modules are supported.

The following function modules can be used to extract tables:

- RFC_READ_TABLE (TAB512)
- /BODS/RFC_READ_TABLE (TAB2048)
- /SAPDS/RFC_READ_TABLE (TAB2048)
- /BODS/RFC_READ_TABLE2
- /SAPDS/RFC_READ_TABLE2
- Z_THEO_READ_TABLE
- [/THEO/READ_TABLE](/xtract-universal/documentation/setup-in-sap/custom-function-module-for-table-extraction) (recommended)

Warning

**Duplicates in the target environment.**\
The SAP standard modules for table extraction do not have pointers for table fields. In larger tables this may cause low performance and duplicates in the target environment. Use the function module [THEO/READ_TABLE](../../setup-in-sap/custom-function-module-for-table-extraction/#installation-of-theoread_table) from Theobald Software to ensure smooth extractions.

Note the necessary [SAP Authority Objects](../../setup-in-sap/sap-authority-objects/#table):

```text
S_TABU_NAM ACTVT=03; TABLE=ENLFDIR

```

### Extract Data in Background Job

If *Background job timeout (seconds)* checkbox is activated, the table extraction is executed as a background job in SAP. This setting is optional and is supported in combination with function module THEO/READ_TABLE or Z_THEO_READ_TABLE version 2.0. Activate the setting *Background job timeout (seconds)* for long-running extractions with a large amounts of data that may run into a timeout error (“Time limit exceeded”), when using the foreground mode.

Tip

The extraction jobs can be found in the SAP JobLog (**SM37**) under the JobName *theo_read_table*.

Warning

**Shared Memory ran out of memory!**\
If this error message pops up when running an extraction in the background, adjust the size of the Shared Memory. SAP recommends a Shared Memory size of 800MB~1.5GB for a production/test system or 2GB~4GB for S/4 systems, see [SAP Support: How to solve SYSTEM_NO_SHM_MEMORY runtime error](https://ga.support.sap.com/dtp/viewer/#/tree/1080/actions/12107).

### Advanced Settings

#### Background Job Timeout (seconds)

Sets a timeout period for extractions that run in background mode.\
The default value is 180 seconds. The maximum timeout value is 3600 seconds.

This option can be used if the data transfer to a destination takes a lot of time, e.g., when bulk-inserts are deactivated for database destinations.

Note

The background job timeout setting only takes effect if the extractions run in background mode using [/THEO/READ_TABLE](../../setup-in-sap/custom-function-module-for-table-extraction/).

#### Adjust Currency Decimals

The default number of decimal places for a currency in the SAP database is 2 decimals. Currencies that do not have decimals are also stored in this format, e.g. JPY, VND, KRW, etc.

Example:

| Currency | Actual Amount | Amount stored in SAP database | | --- | --- | --- | | JPY | 100 | 1.00 | | KRW | 10000 | 100.00 |

When extracting currencies with no decimals, the amount stored in SAP is returned e.g., 100 JPY are extracted as 1.00. To correct the decimal placement of the extracted data, activate **Adjust Currency Decimals**. If **Adjust Currency Decimals** is active, currencies without decimals are multiplied by a factor that balances out the decimals.

**Adjust Currency Decimals** also requires the extraction of the corresponding CURRENCY field that can be used as a reference for the multiplication factor. Use the **[Load live preview]** function to find the correct currency field/s.

- If the currency field is part of the table, add it to the output.
- If the currency field is in another table, join the tables.
- If the reference is not part of a table, **Adjust Currency Decimals** cannot be used.

Note

The multiplication factor used in *Adjust Currency Decimals* is determined by the SAP currency table TCURX. To access the table, the following SAP Authority objects must be set in SAP: *S_TABU_NAM ACTVT=03; TABLE=TCURX*.

The *Join* functionality allows joining two or more tables and extract the result of the join. To perform the extraction the corresponding SQL command is generated dynamically and the join is executed on the SAP server.

Possible scenarios can be joining tables for header and item data or tables for master data and texts.

### Supported Join Types

The following join types are supported:

- Inner Join
- Left Outer Join, also referred to as *Left Join*.

For more information on join types see [SAP Help: Inner Join and Outer Join](https://help.sap.com/doc/saphelp_nwpi71/7.1/en-US/cf/21ec77446011d189700000e8322d00/content.htm?no_cache=true).

Note

Joining of cluster or pool tables is not supported. Cluster or pool tables need to be extracted individually and joined in the destination.

### Prerequisites

To use table join, the function module [/THEO/READ_TABLE](../../setup-in-sap/custom-function-module-for-table-extraction/#installation-of-theoread_table) needs to be available in SAP.

### Join Tables

The following example shows how to join the tables KNA1 and KNVV.

1. In the tab *Tables and Fields*, click **[Add]** to add two tables (e.g., KNA1 and KNVV).

1. Select both tables on the left and check the fields you want to extract .

1. Optional: Switch to the *WHERE clause* tab and specify a [WHERE clause](../where-clause/).

1. Switch to the *Joins* tab . A Join condition with default values is automatically available. The join condition is based on the foreign key relationship of the joined tables.

1. Click **[]** to edit the join condition. The window "Join" opens.

1. Select a table column in the *Left Table* field and in the *Right Table* field to map the table contents. In the depicted example a left outer join on tables KNA1 (left table) and KNVV (right table) on the field KUNNR is performed. It is possible to add multiple join conditions.

   - Click **[Add]** to extend the join condition to more fields.
   - Click **[]** to delete existing joins.

   Tip

   Different tables can have identical field / column names. Defining a join condition based on the identical field names not always delivers the expected result, e.g., VBAK~VBELN \<> LIPS~VBELN. Make sure the fields you use in a join condition contain the same content/data.

1. Click **[OK]** to save the join.

You can join additional tables by adding tables more in the tab *Tables and Fields*.

Warning

**RFC_ERROR_SYSTEM_FAILURE - Illegal access to the right table of a LEFT OUTER JOIN.**\
Using a WHERE clause on the right table of a LEFT OUTER JOIN is only possible as of SAP Release 7.40, SP05.

#### Auto Mapping

The **[Auto-map]** button deletes existing join conditions and performs a new field mapping based on the foreign key relationship of the joined tables.

Recommendation

To avoid poor extraction performance, do not join more than five tables.

A WHERE clause can be used to filter table records, see [SAP ABAP Documentation: SELECT-WHERE](https://help.sap.com/doc/abapdocu_750_index_htm/7.50/en-us/abapwhere.htm). Enter WHERE clauses manually in *Text mode* or use the [WHERE Clause Editor](#where-clause-editor) in *Editor Mode*.

## Create a WHERE Clause

1. Open a Table extraction type.
1. Navigate to the tab *WHERE Clause*.
1. Enter a WHERE clause [manually](#where-clause-text-mode) or use the [WHERE Clause Editor](#where-clause-editor).
1. Click **[Load live preview]** to display the results in the *Preview* section.

## WHERE Clause Text Mode

The WHERE clause text mode allows you to directly enter a WHERE clauses. The text mode of the WHERE clause supports script expressions.

Warning

**Extraction fails due to incorrect syntax.**\
The extractions fail, if incorrect syntax is used in the WHERE clause.\
Make sure to use correct SAP OpenSQL syntax. Several important syntax rules are listed in this help section.

Tip

To check the syntax of the WHERE clause, click **[Load live preview]**. This way there is no need to run an extraction to see, if the syntax is correct.

### WHERE Clause Syntax

The WHERE Clause syntax generally uses the following structure:

```bash
[Table]~[Column][Space][Operator][Space][Filter-Value]

```

**Example:**

```text
KNA1~LAND1 = 'US'

```

The following rules apply to filter values:

| Rule | Correct | Wrong | | --- | --- | --- | | Enter a space before and after the equal sign | *YEAR = '1999'* | *YEAR= '1999 '*, *YEAR ='1999'* or *YEAR='1999'* | | Set floating point numbers in single quotation mark | *KMENG > '10.3'* | *KMENG > 10.3* | | Values must use the internal SAP representation: Date: YYYYMMDD Year Period: YYYYPPP Numbers with leading zeroes, e.g., customer numbers | 19990101 1999001 0000001000 | 01.01.1999 001.1999 1000 |

The following operations are supported in the WHERE clause:

| Operator | Description | | --- | --- | | =, EQ | True if the content of operand1 is equal to the content of operand2. | | \<>, NE | True if the content of operand1 is not equal to the content of operand2. | | \<, LT | True if the content of operand1 is less than the content of operand2. | | >, GT | True if the content of operand1 is greater than the content of operand2. | | \<=, LE | True if the content of operand1 is less than or equal to the content of operand2. | | >=, GE | True if the content of operand1 is greater than or equal to the content of operand2. | | (NOT) LIKE | True if the value of operand1 matches (does not match) the pattern in operand2. | | (NOT) IN | True if the content of operand1 is (not) part of the content of operand2. Operand2 must be of type LIST or SQL. |

For more details on the OpenSQL syntax, see [SAP Help: Select WHERE](https://help.sap.com/doc/abapdocu_752_index_htm/7.52/en-US/abapwhere.htm?file=abapwhere.htm)

Tip

To increase extracting performance, make sure to place the indexed fields as the first selection filter operation in the WHERE clause.

Note

When fields with the same name exist in different tables, the field names must be formatted as [table name]~[field name], e.g., MARC~MATNR. This can be the case when extracting multiple tables.

### Script Expressions

The **[Text Mode]** of the WHERE clause supports script expressions. Script expressions are usually used to determine a dynamic date based on the current date. When using script expressions in a WHERE Clause, the value must be entered in single quotation marks.

**Syntax:**

```text
[Table]~[Column][Space][Operator][Space]'#[Script-Expression]#'

```

`BKPF~BUDAT >= '#{DateTime.Now.AddYears(-5).ToString("yyyyMMdd")}#'`

**Examples:**

| Input | Description | | --- | --- | | `#{ DateTime.Now.ToString("yyyyMMdd") }#` | Current date in SAP format (yyyyMMdd) | | `#{ String.Concat(DateTime.Now.Year.ToString(), "0101") }#` | Current year concatenated with "0101" (yyyy0101) | | `#{ String.Concat(DateTime.Now.ToString("yyyy"), "0101") }#` | Current year concatenated with "0101" (yyyy0101) | | `#{ String.Concat(DateTime.Now.ToString("yyyyMMdd").Substring(0,4), "0101") }#` | Current year concatenated with "0101" (yyyy0101) |

For more information on script expression, see [Script Expressions](../../parameters/script-expressions/).

### Subqueries

Note

The usage of subqueries is only possible as of SAP Release 7.40, SP05.

A subquery is an SQL query nested inside a larger query. Subqueries are nested queries that provide data to the enclosing query. Subqueries need be enclosed with parenthesis and can return individual values or a list of records. Get more details about subqueries on the [SAP Help: Conditions](https://help.sap.com/doc/abapdocu_752_index_htm/7.52/en-US/abenwhere_logexp_in_subquery.htm).

**Example:**

In the following example a subquery is used with the *IN* operator. The following statement returns all the *active* customers (rows in the table KNA1) that have i.e. a sales document in the table VBAK for sales document header data.

```text
KUNNR IN ( SELECT KUNNR FROM VBAK )

```

## WHERE Clause Editor

The WHERE clause editor offers a toolkit for those who are not familiar with the syntax of the WHERE clause. Click **[Editor mode]** to open the editor.

There are 2 options for adding criteria to the WHERE clause:

- **[Add Criteria]** adds single criteria.
  - The default structure for a single criteria with static values is `[Table~Column][Operator][Filer-Value]` e.g., *MARC~WERKS = 1000*.
  - The default structure for a single criteria with parameters is `[Column][Operator][Parameter]` e.g., *MARC~WERKS = [p_WERKS]*.
- **[Add Criteria Group]** adds a group of criteria.
  - The default structure for a criteria group is `([Table~Column1][Operator1][Filter-Value1][Boolean][Table~Column2][Operator2][Filter-Value2])` e.g., *(MARC~PSTAT = 'L' OR MARC~PSTAT = 'LB')*.

Tip

Combine multiple criteria and criteria groups to create complex filters e.g., *MARC~WERKS = 1000 AND (MARC~PSTAT = 'L' OR MARC~PSTAT = 'LB')* extracts only data where the column WERKS equals 1000 and the column PSTAT equals either 'L' or 'LB'.

### Components of the WHERE Clause Editor

The following buttons and options are available in the WHERE Clause Editor:

| Icon | Component | Function | | --- | --- | --- | | | Delete row | deletes a criteria. | | | Move row up | changes the sequence of the criteria. The selected criteria moves up. The sequence of criteria can also be changed with Drag and drop. | | | Move row down | changes the sequence of the criteria. The selected criteria moves down. The sequence of criteria can also be changed with Drag and drop. | | | Column | adds a column. Click on the component to select a column from the available tables. | | | SQL | adds an Open SQL statement, see [SAP Help: Open SQL](https://help.sap.com/doc/abapdocu_750_index_htm/7.50/en-us/abenopensql.htm). | | | Operator | adds an operator e.g., =, \<, >, etc. | | | Value | adds a static value of type *String*, *Number*, *Flag* or *List*. *List* offers a separate editor to create lists of type *String*, *Number* or *Select*. *Select* enables usage of SELECT statements. For more information, see [Working with Lists in the WHERE-Clause Editor](../../../knowledge-base/where-clause-editor-lists/). | | | Criteria | adds a new criteria after the selected criteria. | | | Group | adds a new group of criteria the selected criteria. | | | Parameter | adds a previously defined runtime parameter, see [Runtime Parameters](../edit-runtime-parameters/). |

Note

When adding or editing a criteria only the relevant components are displayed e.g., **Add Operator** is only available if there is a column or SQL statement to use an operator on.

#### Edit and Delete Components

- Click on a component to edit it. All areas that are marked green can be edited.
- To delete a component, click the (x) icon above the component.

### SAP System Fields

You can use [SAP system fields for date and time](https://help.sap.com/doc/abapdocu_751_index_htm/7.51/en-US/abentime_system_fields.htm) in a WHERE clause. The usage of SAP system fields requires SAP NW 7.4 SP5 or higher and the custom function module [/THEO/READ_TABLE](../../setup-in-sap/custom-function-module-for-table-extraction/#installation-of-theoread_table).

Example:

1. Navigate to [WHERE Clause Editor](#where-clause-editor) and select a column of the type Date *here: BUDAT* .
1. Delete the criterion "Value" and use the criterion "SQL" .
1. Within the "SQL" criterion, use the supported system fields for date and time with a preceding "@" character, *here: @sy-datum* .
1. Click **[Load live preview]** to check the result.

______________________________________________________________________

#### Related Links

- [Knowledge Base Article: Delta Table Extraction](../../../knowledge-base/delta-table-extraction/)
- [Knowledge Base Article: Working with Lists in the WHERE-Clause Editor](../../../knowledge-base/where-clause-editor-lists/)
- [Knowledge Base Article: LIKE Operand in WHERE Clauses](../../../knowledge-base/like-operand-where-clause/)

This page shows how to use the Table CDC extraction type.\
The Table CDC extraction type can be used to map data from SAP Tables to various target environments. Unlike a full data load, where all records in a table are transferred from the source to the target environment, Table CDC captures only the data changes since the last load.

Note

Table CDC is an Add-On that requires a separate license. For more information, contact Theobald Software's sales team at [sales@theobald-software.com](mailto:sales@theobald-software.com).

### About Table CDC

Change Data Capture (CDC) is a method to keep track of data changes such as insert, update and delete in SAP tables. The Table CDC component creates a log table in SAP that records any changes made to a selected table. The content of the log table is cleared after every successful run of the extraction. Data that was not extracted is not cleared from the log table.

For a detailed overview of this process, refer to the Knowledge Base article [Delta Mechanism of TableCDC](../../knowledge-base/table-cdc-mechanism/).

Note

Clusters, pool tables and views are not supported by the Table CDC extraction type.

### System Requirements

- The Table CDC extraction type is compatible with SAP ECC 5.0 and higher
- Supported databases:
  - HANA
  - SQL Server
  - Oracle
  - IBM Db2
  - IBM for i (Db4)
  - IBM for z/os (Db6)
  - MaxDB

### Prerequisites

- SAP custom function module [/THEO/CDC_ECC or /THEO/CDC_S4](../setup-in-sap/custom-function-module-for-tablecdc/) is installed in SAP.
- SAP custom function module [/THEO/READ_TABLE](../setup-in-sap/custom-function-module-for-table-extraction/) is installed in SAP.
- A connection to an SAP system is available, see [SAP Connection](../sap-connection/).
- The SAP user has sufficient user rights, see [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#table-cdc).

The SAP transport requests for the function modules are provided in the installation directory: `C:\Program Files\XtractUniversal\ABAP\`, see [Custom function module for TableCDC](../setup-in-sap/custom-function-module-for-tablecdc/).

Warning

**Missing Authorization.**\
To use the Table CDC extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust [SAP Authority Objects](../setup-in-sap/sap-authority-objects/#table) accordingly.

### Create a Table CDC Extraction

1. In the main window of the Designer, click **[New]** to create a new extraction. The window "Create Extraction" opens.
1. Select an [SAP Connection](../sap-connection/) of type **RFC** from the drop-down menu **Source**.
1. Enter a unique name for your extraction.
1. Select the extraction type **Table CDC**.
1. Click **[OK]**. The main window of the extraction type opens automatically.

The majority of the functions of the extraction type can be accessed in the main window.

### Look Up a Table

1. In the main window of the extraction type, click **[]**. The window “Table Lookup” opens.
1. In the field **Table Name**, enter the name of the table to track and extract . Use wildcards (\*) if needed.
1. Click **[]** . Search results are displayed.
1. Select a table and click **[OK]**.

All relevant metadata information of the table is retrieved from SAP. The application returns to the main window of the extraction type.

### Define the Table CDC Extraction Type

The Table CDC extraction type offers the following options for tracking SAP tables:

1. Select the table columns you want to track and extract. By default all columns are selected. Deselect the columns you do not want to extract.

1. If you want to extract the table when first running the extraction, activate **[Extract table on first run]**.

1. Optional: You can set a maximum number of rows that the log table can contain. The default is disabled (Value 0). If the row limit is reached, the extraction fails with an exception. The maximum row limit of a log table is 500.000.

   Note

   Once a log table is initialized, you cannot change the size limit anymore.

1. Optional: Define a [WHERE Clause](where-clause/) to filter table records. By default all data is extracted.

1. Click **[Load live preview]** to display a live preview of the first 1000 records.

   - The column *TS_TIMESTAMP* contains a timestamp of when the data was last changed.
   - The column *TS_OPERATION* indicates if a row was inserted (I), updated (U) or deleted (D).

1. Check the [Extraction Settings](settings/) and the [General Settings](general-settings/) before running the extraction.

1. Click **[OK]** to confirm your settings.

To initialize the tracking of the selected SAP table, run the extraction once.

### Run the Extraction for the First Time

Run the extraction for the first time to create a log table in SAP that records any changes made to a selected table or view.

1. Select the extraction in the main window of the Designer.
1. Click **[ Destination]** to assign the destination where you want to write data to .
1. Click **[Run]** . The window "Run Extraction" opens.
1. Click **[Run]** to run the extraction.
1. If the extraction is successful, the status in the *General Info* section of the window changes to "finished successfully". If an error occurred, you can find information on the error in the *Log* section of the window.

The log table in SAP is now available for the Table CDC component. The extracted SAP table is now available in your destination.

Note

When running the extraction regularly the content of the log table in SAP is extracted and written to the destination. The content of the log table in SAP is cleared after every successful run of the extraction. Data that was not extracted is not cleared.

### Delete Log Table and Triggers

When a Table CDC extraction is no longer in use or if you need to change the structure of the source table, simply deleting the extraction is not enough. To delete the log table and all associated triggers from your SAP system, open the Table CDC extraction and click **[Delete CDC resources]**.

To delete the SAP resources of multiple extractions or extractions that are already deleted, see [Active CDC Watches](active-cdc-watches/).

Warning

**Table change not possible**\
The source table cannot be changed, if any CDC-related resources connected to the source table in SAP exist.\
Clear the CDC-related resources connected to the source table in SAP, see [SAP Note 2284776](https://launchpad.support.sap.com/#/notes/2284776).

This page shows how to keep track of Table CDC log tables and triggers in your SAP system.\
The *Active CDC Watches* menu lists all active log tables and their DB triggers in the SAP source system. All listed Table CDC resources can be deleted from the SAP source system.

To open the *Active CDC Watches* menu, click **Show Active CDC Watches** in the main window of the extraction type.

### Active CDC Watches

#### Source Table

Name of the source table that is tracked by the Table CDC component.

#### Log Table

Name of the log table that tracks changes in the source table.

#### Created on

Timestamp when the log table was created.

#### Created by

SAP username that was used to create the initial log table.

#### Rows

Number of rows in the log table.

#### Show Details

Click **[]** to display more information about the DB triggers in the "CDC watch details" window.

#### Delete CDC resources

Click **[]** to delete the log table and all associated triggers from your SAP system.\
It is not possible to delete the log table and the triggers of the current extraction, see [Delete Log Table and DB Triggers](../#delete-log-table-and-triggers).

This page contains an overview of the settings in the window "General Settings".\
To open the general settings, click **General Settings** in the main window of the extraction type.

### Misc. Tab

The *Misc.* tab covers cache settings, column encryption and keywords of an extraction type.

#### Cache results

The *Cache results* option is only available in [pull destinations](../../destinations/), e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times. To decrease the SAP server load, you can select the **Cache results** option, this way the pull destination pulls the data from cache and not from the SAP.

Cache results increases the performance and limits the impact on the SAP system. If this behavior is undesirable (for example, because the data must be always 100% up-to-date), the cache option must be explicitly disabled.

#### Keywords

One or more keywords (tags) can be assigned to an extraction. Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the "Search Extractions" window. To open the "Search Extractions" window, click **[ Search]** in the main window of the Designer.

Tip

To add keywords to multiple extractions at once, select the extractions in the main window of the Designer.\
Right-click + **Add/Remove keywords** opens the window "Add/Remove Keywords To/From Multiple Extractions".

### Primary Key Tab

Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.

The depicted example shows the SAP object *MAKT* with its primary key inherited from SAP in the general settings of the Designer. In this example the primary key consists of *MANDT*, *MATNR* and *SPRAS*. The primary key is also taken over in the destination.

Note

A defined primary key field in a table is a prerequisite for merging data.

#### Generate Surrogate Key Column

If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs. The surrogate keys are hash values of type signed 8 byte integer, e.g., `#-3008591679982390000`. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.

### Security Tab

Restrict user access to the extraction. For more information, see [Restrict Designer Access](../../access-restrictions/restrict-designer-access/).

### Columns Order Tab

The "Columns Order" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns. Index 0 defines the first column in the result set, index 1 the second columns, etc.

| Option | Description | | --- | --- | | **Enabled** | When this option is active, the defined column order is applied when running the extraction. | | **Swap** | Swaps the index of 2 columns. All other columns keep their indexes. | | **Insert** | Inserts the selected column into the selected index. All other indexes are recalculated. | | **Reset default** | Restores the original column order. |

This page contains an overview of the extraction settings in the Table CDC extraction type.\
To open the extraction settings, click ****Extraction Settings**** in the main window of the extraction type.

### Initial Load

#### Package size

The extracted data is split into packages of the defined size. The default value is 50000 lines.\
A package size between 20000 and 50000 is advisable for large amounts of data. 0 means no packaging. Not using packaging can lead to an RFC timeout for large data extractions.

Warning

**RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table**\
To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.

#### Extract data in background job

If *Background job timeout (seconds)* checkbox is activated, the table extraction is executed as a background job in SAP. This setting is optional and is supported in combination with function module THEO/READ_TABLE or Z_THEO_READ_TABLE version 2.0. Activate the setting *Background job timeout (seconds)* for long-running extractions with a large amounts of data that may run into a timeout error (“Time limit exceeded”), when using the foreground mode.

Tip

The extraction jobs can be found in the SAP JobLog (**SM37**) under the JobName *theo_read_table*.

Warning

**Shared Memory ran out of memory!**\
If this error message pops up when running an extraction in the background, adjust the size of the Shared Memory. SAP recommends a Shared Memory size of 800MB~1.5GB for a production/test system or 2GB~4GB for S/4 systems, see [SAP Support: How to solve SYSTEM_NO_SHM_MEMORY runtime error](https://ga.support.sap.com/dtp/viewer/#/tree/1080/actions/12107).

#### Background job timeout (seconds)

Sets a timeout period for extractions that run in background mode.\
The default value is 180 seconds. The maximum timeout value is 3600 seconds.

This option can be used if the data transfer to a destination takes a lot of time, e.g., when bulk-inserts are deactivated for database destinations.

Note

The background job timeout setting only takes effect if the extractions run in background mode using [/THEO/READ_TABLE](../../setup-in-sap/custom-function-module-for-table-extraction/).

A WHERE clause can be used to filter table records, see [SAP ABAP Documentation: SELECT-WHERE](https://help.sap.com/doc/abapdocu_750_index_htm/7.50/en-us/abapwhere.htm). Enter WHERE clauses manually in *Text mode* or use the [WHERE Clause Editor](#where-clause-editor) in *Editor Mode*.

## Create a WHERE Clause

1. Open a Table CDC extraction type.
1. Navigate to the tab *WHERE Clause*.
1. Enter a WHERE clause using the [WHERE Clause Editor](#where-clause-editor).
1. Click **[Load live preview]** to display the results in the *Preview* section.

## WHERE Clause Syntax

The WHERE Clause syntax generally uses the following structure:

```bash
[Table~Column][Operator][Filter-Value]

```

Filter values in the WHERE clause must use the internal SAP representation:

| Examples | Correct | Wrong | | --- | --- | --- | | Date: YYYYMMDD | 19990101 | 01.01.1999 | | Year Period: YYYYPPP | 1999001 | 001.1999 | | Numbers with leading zeroes, e.g., customer numbers | 0000001000 | 1000 |

The following operations are supported in the WHERE clause:

| Operator | Description | | --- | --- | | =, EQ | True if the content of operand1 is equal to the content of operand2. | | \<>, NE | True if the content of operand1 is not equal to the content of operand2. | | \<, LT | True if the content of operand1 is less than the content of operand2. | | >, GT | True if the content of operand1 is greater than the content of operand2. | | \<=, LE | True if the content of operand1 is less than or equal to the content of operand2. | | >=, GE | True if the content of operand1 is greater than or equal to the content of operand2. | | (NOT) LIKE | True if the value of operand1 matches (does not match) the pattern in operand2. | | (NOT) IN | True if the content of operand1 is (not) part of the content of operand2. Operand2 must be of type LIST or SQL. |

For more details on the OpenSQL syntax, see [SAP Help: Select WHERE](https://help.sap.com/doc/abapdocu_752_index_htm/7.52/en-US/abapwhere.htm?file=abapwhere.htm)

Tip

To increase extracting performance, make sure to place the indexed fields as the first selection filter operation in the WHERE clause.

## WHERE Clause Editor

There are 2 options for adding criteria to the WHERE clause:

- **[Add Criteria]** adds single criteria.

  - The default structure for a single criteria with static values is `[Table~Column][Operator][Filer-Value]` e.g., *MARC~WERKS = 1000*.

- **[Add Criteria Group]** adds a group of criteria.

  - The default structure for a criteria group is `([Table~Column1][Operator1][Filter-Value1][Boolean][Table~Column2][Operator2][Filter-Value2])` e.g., *(MARC~PSTAT = 'L' OR MARC~PSTAT = 'LB')*.

Tip

Combine multiple criteria and criteria groups to create complex filters e.g., *MARC~WERKS = 1000 AND (MARC~PSTAT = 'L' OR MARC~PSTAT = 'LB')* extracts only data where the column WERKS equals 1000 and the column PSTAT equals either 'L' or 'LB'.

### Components of the WHERE Clause Editor

The following buttons and options are available in the WHERE Clause Editor:

| Icon | Component | Function | | --- | --- | --- | | | Delete row | deletes a criteria. | | | Move row up | changes the sequence of the criteria. The selected criteria moves up. The sequence of criteria can also be changed with Drag and drop. | | | Move row down | changes the sequence of the criteria. The selected criteria moves down. The sequence of criteria can also be changed with Drag and drop. | | | Column | adds a column. Click on the component to select a column from the available tables. | | | SQL | adds an Open SQL statement, see [SAP Help: Open SQL](https://help.sap.com/doc/abapdocu_750_index_htm/7.50/en-us/abenopensql.htm). | | | Operator | adds an operator e.g., =, \<, >, etc. | | | Value | adds a static value of type *String*, *Number*, *Flag* or *List*. *List* offers a separate editor to create lists of type *String*, *Number* or *Select*. *Select* enables usage of SELECT statements. For more information, see [Working with Lists in the WHERE-Clause Editor](../../../knowledge-base/where-clause-editor-lists/). | | | Criteria | adds a new criteria after the selected criteria. | | | Group | adds a new group of criteria the selected criteria. |

Note

When adding or editing a criteria only the relevant components are displayed e.g., **Add Operator** is only available if there is a column or SQL statement to use an operator on.

#### Edit and Delete Components

- Click on a component to edit it. All areas that are marked green can be edited.
- To delete a component, click the (x) icon above the component.

### SAP System Fields

You can use [SAP system fields for date and time](https://help.sap.com/doc/abapdocu_751_index_htm/7.51/en-US/abentime_system_fields.htm) in a WHERE clause. The usage of SAP system fields requires SAP NW 7.4 SP5 or higher and the custom function module [/THEO/READ_TABLE](../../setup-in-sap/custom-function-module-for-table-extraction/#installation-of-theoread_table).

Example:

1. Navigate to [WHERE Clause Editor](#where-clause-editor) and select a column of the type Date *here: BUDAT* .
1. Delete the criterion "Value" and use the criterion "SQL" .
1. Within the "SQL" criterion, use the supported system fields for date and time with a preceding "@" character, *here: @sy-datum* .
1. Click **[Load live preview]** to check the result.

______________________________________________________________________

#### Related Links

- [Knowledge Base Article: Working with Lists in the WHERE-Clause Editor](../../../knowledge-base/where-clause-editor-lists/)
- [Knowledge Base Article: LIKE Operand in WHERE Clauses](../../../knowledge-base/like-operand-where-clause/)
# Knowledge Base Articles

This section contains in-depth articles and sample use cases for Xtract Universal.

### Xtract Universal

- [Call Extractions via Script](call-extraction-via-script/)
- [Certificate Renewal for TLS](certificate-renewal/)
- [Create Extractions via Commandline](config-command-line-tool/)
- [Deactivate Usage Data Collection](deactivate-usage-analytics/)
- [Deploy Extractions Using Git Version Control](deploy-extractions-using-Git-version-control/)
- [Execute & Schedule all Extractions using an SSIS Package](execute_all_defined_xu_extractions/)
- [Insert Extraction Events into Windows Logs](insert-extraction-events-into-the-windows-logs/)
- [Load Balancing](load-balancer/)
- [Proxy Server Settings](proxy-server-settings/)
- [SAP Access with Xtract Universal and Powershell](sap-access-with-xtract-universal-and-powershell/)
- [Run Xtract Universal in a VM on AWS EC2](run-xu-in-aws/)
- [Target Principal Field (TPN)](target-principal-TPN/)
- [SharePoint Lists Notification using Intelligent Merge Procedure](sharepoint-lists-notification-using-intelligent-merge-procedure/)

______________________________________________________________________

### SAP

- [Check the Accessibility to an SAP System](check-the-accessibility-to-an-sap-system/)
- [Access Data in the SAP Public Cloud using RFC BAPI](access-data-in-the-sap-public-cloud/)
- [Authorize Access to Reports via Authorization Groups](authorize-access-to-specific-reports/)
- [Create / Configure the Custom Authorization Object Z_TS_PROG](create-the-custom-authority-object-z-ts-prog/)
- [Create a Client PSE to connect to SAP Cloud Systems](create-personal-security-environment/)
- [Enable Secure Network Communication (SNC) via X.509 certificate](enable-snc-using-pse-file/)
- [Import an SAP Transport Request](import-an-sap-transport-request/)
- [Register an RFC Server in SAP with Kernel Release 720 and higher](register-rfc-server-in-sap-releases-in-kernel-release-720-and-higher/)
- [Supported SAP S/4HANA Versions](supported-sap-and-hana-versions/)

______________________________________________________________________

### SSO Scenarios

- [SSO with Client Certificates](sso-with-client-certificates/)
- [SSO with External ID](sso-with-external-id/)
- [SSO with Kerberos SNC](sso-with-kerberos-snc/)
- [SSO with Logon Ticket](sso-with-logon-ticket/)

______________________________________________________________________

## Destinations

The following articles refer to specific [destinations](../documentation/destinations/) defined in Xtract Unviersal.

### Microsoft Azure

- [Authentication via Microsoft Entra ID for Azure Storage](authentication-via-entra-id-with-azure-storage/)
- [Integration in Azure Data Factory using Commandline](adf-integration-using-command-line/)
- [Integration in Azure Data Factory using Webservices](adf-integration-using-webservices/)
- [Call Dynamic Extractions with Variables in ADF](call-dynamic-extractions-with-variables-in-adf/)
- [Run an ADF Pipeline when an SAP Extraction File is uploaded to Azure Storage](run-an-ADF-pipeline-when-sap-extraction-file-is-successfully-uploaded-to-Azure-storage/)

### Microsoft Fabric (OneLake)

- [Integration in Microsoft Fabric using Webservices](fabric-integration-using-webservices/)

### Microsoft SQL Server

- [Collation Settings for MSSQL Server Destination](collation-sql-server/)
- [Post-Processing Column Name Style](adjust-column-name-style/)

### Power BI Report Server

- [Connect Xtract Universal to Power BI Service](connect-to-power-bi-service/)
- [Use Computed Query Parameters for SSRS](xu-ssrs-parameterizing-in-vs/)

### Google Cloud

- [Set Up OAuth 2.0 for the Google Cloud Storage Destination](google-cloud-storage-oauth/)

### KNIME

- [Dynamic Runtime Parameter in KNIME Workflows](dynamic-runtime-paramater-within-KNIME-workflow/)

### Snowflake

- [Integrate SAP Data into a Snowflake Data Warehouse](integrate_sap_data_into_a_Snowflake_data_warehouse/)
- [SAP Integration with Matillion Data Loader](create-a-custom-cennector-in-matillion-data-loader/)

### Tableau

- [Link a BEx query with a Hierarchy in Tableau](link-bex-query-with-hierarchy/)

### Amazon Redshift

- [Configure AnySQL Maestro to Manage Amazon Redshift](configuring-anysql-maestro-to-manage-amazon-redshift/)

______________________________________________________________________

## Extraction Types

The following articles refer to specific [extraction types](../documentation/introduction/#extraction-types) defined in Xtract Unviersal.

### BAPI

- [Access Data in the SAP Public Cloud using RFC BAPI](access-data-in-the-sap-public-cloud/)
- [Read Data from Cluster Fields in Tables PCL1 and PCL2 (Payroll)](read-data-from-cluster-fields-in-the-tables-pcl1-and-pcl2-payroll/)

### DeltaQ

- [Create Generic DataSources](create-generic-datasource-using-function-module-and-timestamps/)
- [Register an RFC Server in SAP with Kernel Release 720 and higher](register-rfc-server-in-sap-releases-in-kernel-release-720-and-higher/)

### OData

- [Create OData Services using the SAP Gateway Builder](create-odata-services-using-the-sap-gateway-builder/)

### ODP

- [Alternatives for the ODP Extraction Type](alternatives-for-odp/)

### Report

- [Authorize Access to Reports via Authorization Groups](authorize-access-to-specific-reports/)
- [Create / Configure the Custom Authorization Object Z_TS_PROG](create-the-custom-authority-object-z-ts-prog/)
- [Extract Reports with ALV Layouts](extract-report-layouts/)
- [Parse Reports in Xtract Universal](parse-reports/)

### Table

- [Extraction Mechanism of Table](table-extraction-mechanism/)
- [Delta Table Extraction](delta-table-extraction/)
- [Read data from Cluster Fields in Tables PCL1 and PCL2 (Payroll)](read-data-from-cluster-fields-in-the-tables-pcl1-and-pcl2-payroll/)
- [Working with Lists in the WHERE-Clause Editor](where-clause-editor-lists/)
- [Working with LIKE operand in WHERE-Clauses](like-operand-where-clause/)

### Table CDC

- [Delta Mechanism of Table CDC](table-cdc-mechanism/)
- [Initial Table Load in SAP Versions < 7.10](table-cdc-initial-table-load/)

The following article shows how to access data from the SAP S/4HANA Public Cloud. Xtract Universal can access BAPIs / Function Modules that are available via SAP *Communication Scenarios*.

### About Communication Scenarios

*Communication Scenarios* are used to exchange data between the SAP Cloud and external systems.\
You can use the **Display Communication Scenarios** app in the SAP Public Cloud to look up available *Communication Scenarios* that provide access to BAPIs / Function Modules, e.g., the *Communication Scenario* SAP_COM_0109 includes BAPIS that are used for Sales Order Integration.

In *Communication Scenarios* BAPIs / Function Modules are listed as Service Type *RFC*:

### Prerequisites

- The Administrator role in the SAP Public Cloud is required to set up communication on the SAP side.
- The *Communication Scenario* SAP_COM_0636 (Remote Function Call - RFC Metadata Integration) must be accessible in the SAP Public Cloud.
- [Download the SAP Cryptographic Library](https://help.sap.com/doc/saphelp_em900/9.0/en-US/48/a324e7ccfc062de10000000a42189d/frameset.htm) (sapcrypto.dll) from the SAP Service Marketplace.
- [Create a Client PSE to connect Xtract Universal to SAP Cloud Systems](../create-personal-security-environment/).
- Make sure to use Xtract Universal version 6.4.2.0 (2024-04-19) or higher.

### Enable Communication in the SAP Public Cloud

Use the **Communication Management** apps in the SAP Public Cloud to make BAPIs / Function Modules accessible for external systems:

1. Create a *Communication User* in the **Maintain Communication Users** app.
1. Create a *Communication System* that handles inbound service calls in the **Communication Systems** app.
1. Assign a *Communication User* to the *Communication System* in the subsection *Users for Inbound Communication*.
1. Create *Communication Arrangements* to define, which *Communication Scenarios* can be accessed by the *Communication System* and the *Communication User*. The *Communication Scenario* SAP_COM_0636 (Remote Function Call - RFC Metadata Integration) is mandatory for using the BAPI extraction type in Xtract Universal.

For more information, see [SAP Help: Setting up Communication Management](https://learning.sap.com/learning-journeys/implement-sap-s-4hana-cloud-public-edition-for-sourcing-and-procurement/setting-up-communication-management_a913171c-c96d-47a9-81ec-dc9ee8754320).

### Enable Communication in Xtract Universal

Follow the steps below to create an Xtract Universal source that connects to an SAP Public Cloud:

1. In the main window of the Designer, navigate to the menu bar and select **Server > Manage Sources**. The window "Manage Sources" opens.
1. Click **[Add]** to add a new SAP connection or click **[]** to edit an existing connection. The window "Change Source" opens.
1. Enter a name for the SAP connection in the field **Name**, e.g., *SAPCloud*.
1. In the *General* tab, select the connection method **WebSocket**.
1. Fill out the connection details, see [Connection Settings](../../documentation/sap-connection/settings/#general).
   - In the field **Library**, enter the path to the SAP Cryptographic Library (sapcrypto.dll).
   - In the field **Client PSE**, enter the path to the client .pse file.
1. In the *Authentication* tab, enter the username and password of the SAP *Communication User*.
1. Activate the checkbox **User name is alias**.
1. Click **[Test designer connection]** to validate the connection between the Xtract Universal Designer and the SAP system.
1. Click **[Test server connection]** to validate the connection between the Xtract Universal Server and the SAP system.
1. Click **[OK]** to save the SAP source.

### Look Up and Execute BAPIs

Follow the regular workflow to look up and execute BAPIS:

1. [Create a new BAPI / Function Module extraction](../../documentation/bapi/#create-a-bapi-extraction).

1. [Look up a BAPI / Function Module](../../documentation/bapi/#look-up-a-function-module-bapi) included in the *Communication Scenarios* that your *Communication User* has access to.

   Note

   The lookup returns a list of all BAPIS in the SAP Public Cloud with no consideration for *Communication Scenarios*. Make sure to only select BAPIS that are available via *Communication Scenarios*, see [About Communication Scenarios](#about-communication-scenarios).

1. [Define all mandatory parameters of the BAPI](../../documentation/bapi/#define-the-bapi-extraction-type).

1. [Run the extraction](../../documentation/execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

______________________________________________________________________

#### Related Links

- [SAP Help: Setting up Communication Management](https://learning.sap.com/learning-journeys/implement-sap-s-4hana-cloud-public-edition-for-sourcing-and-procurement/setting-up-communication-management_a913171c-c96d-47a9-81ec-dc9ee8754320)
- [SAP Community: Calling BAPIs in the SAP S/4HANA Public Cloud](https://community.sap.com/t5/enterprise-resource-planning-blogs-by-sap/calling-bapis-in-the-sap-s-4hana-public-cloud/ba-p/13411902)
- [Knowledge Base Article: Create a Client PSE to connect to SAP Cloud Systems](../create-personal-security-environment/)

The following article describes a scenario that uses Azure Data Factory (ADF) to trigger and automate SAP data movements using Xtract Universal's [command line tool](../../documentation/execute-and-automate/call-via-commandline/). This article targets customers that utilize ADF as a platform for orchestrating data movement and transformation.

Note

The depicted scenario is no best practice or recommendation. The following is a suggestion of how an orchestration of Xtract Universal extractions from ADF can look like, see also [Integration in Azure Data Factory using Webservices](../adf-integration-using-webservices/).

### Prerequisites

- Xtract Universal is installed on a cloud VM and is accessible remotely over http(s).
- The extraction uses a [push-destination](../../documentation/destinations/), e.g., Azure Blob Storage or Azure SQL Server.
- The extraction runs successfully when called from a remote machine via commandline, see [Execute and Automate - Call via Commandline](../../documentation/execute-and-automate/call-via-commandline/). This ensures that the XU server is reachable.
- Access to the Azure portal and Azure Data Factory.
- Knowledge on how to build ADF pipelines.

### Configure an Azure Batch Account

When creating a batch account in the Azure portal, make sure to consider the following:

- A **Storage account** needs to be associated with your Batch account. This can a be new storage account dedicated to batch processing, or an existing storage account. Microsoft recommends a general-purpose v2 storage account in the same region as your Batch account (for better performance).
- The **Pool allocation mode** under **Advanced** can be the default **Batch service** (no need to select **User subscription**).

For information on how to configure a batch account in the Azure portal, see [Microsoft Documentation: Create a Batch account in the Azure portal](https://docs.microsoft.com/en-us/azure/batch/batch-account-create-portal).

### Add a Pool to the Azure Batch Account

The pool provides the computing resources (VM) to execute a task, in this case running the commandline tool xu.exe. When creating a pool from a managed image in the Azure portal, make sure to consider the following:

- The commandline tool xu.exe is not a very resource-intensive application, but if Azure Batch is used for other processing, choose an appropriately sized resource for your needs. Note that there is an Azure cost associated with the selected Pool. The depicted example uses a Window Server 2019 Datacenter with small disk configuration.
- When creating the pool, set the **Scale** property **Target dedicated nodes** to at least 1.

For information on how to create a pool, see [Microsoft Documentation: Use a managed image to create a custom image pool](https://docs.microsoft.com/en-us/azure/batch/batch-custom-images).

### Upload xu.exe to Storage Account

Follow the steps below to make the command line tool xu.exe available in Azure:

1. Create a container for the Xtract Universal commandline tool in the Azure storage account associated with the Azure Batch account. In the depicted example, the container is named ‘xuexe’.
1. Upload the files xu.exe and xu.exe.config from the Xtract Universal server installation to the Azure storage account. The files are located in `C:\Program Files\XtractUniversal`.

Note

Do not confuse the xu.exe.config file with the xu.config file.

### Create a Linked Service to Azure Batch in ADF

Follow the steps below to create a *Batch Linked Service* and a *Storage Linked Service* in Azure Data Factory:

1. In ADF, navigate to **Manage > Connections > Linked Services** and click **[New]** . The menu "New linked service" opens.
1. In the tab *Compute* category, select **Azure Batch** and click **[Continue]** .
1. Specify the **Batch Account**, **Access Key**, **Batch URL** and **Pool name** of the batch account. The data is available in the key settings of the batch account.
1. In **Storage linked service name**, select *New* to create a new linked service that references the storage account that contains the xu.exe file in the linked service.

### Create an ADF Pipeline with Custom Activity

Follow the steps below to create a pipeline that runs extractions:

1. Create a new **Pipeline** in ADF.
1. Drag the **Custom Activity** under *Batch Service* into your pipeline.
1. In the *General* tab, provide a name for the activity, e.g., ‘KNA1’ .
1. In the *Azure Batch* tab, reference the *Batch Linked Service* from [Create a Linked Service to Azure Batch in ADF](#create-a-linked-service-to-azure-batch-in-adf).
1. In the *Settings* tab, specify the xu.exe command that you want to execute , e.g., `xu.exe [protocol]://[host or IP address]:[port]/?name=[name of the extraction]` to run an extraction.
1. Reference the *Storage Linked Service* from [Create a Linked Service to Azure Batch in ADF](#create-a-linked-service-to-azure-batch-in-adf) in the **Advanced Settings** .
1. Specify the container / folder path where the xu.exe file is located in the Azure storage account .
1. Click **[Debug]** to testrun the SAP data extraction.

When the activity is finished, review the output of the activity in the *Output* tab. If the exitcode from xu.exe is 0, the data extraction was successful and the following folders / files are available in the Azure storage account:

- the storage account contains a folder *adfjobs*.
- for every pipeline execution, there is a subfolder with log information.
- the files *stderr.txt* and *stdout.txt* contain the output from xu.exe.

The pipeline can be run in debug mode or can be triggered via a scheduler.

______________________________________________________________________

#### Related Links

- [Call Dynamic Extractions with Variables in ADF](../call-dynamic-extractions-with-variables-in-adf/).
- [Integration in Azure Data Factory using Webservices](../adf-integration-using-webservices/)

The following article describes a scenario that uses Azure Data Factory (ADF) to trigger and automate SAP data movements using Xtract Universal's [webservices](../../web-api/).\
This article targets customers that utilize ADF as a platform for orchestrating data movement and transformation.

Note

The depicted scenario is no best practice or recommendation. The following is a suggestion of how an orchestration of Xtract Universal extractions from ADF can look like, see also [Integration in Azure Data Factory using Commandline](../adf-integration-using-command-line/).

### Prerequisites

- A [self-hosted Integration runtime](https://docs.microsoft.com/EN-US/azure/data-factory/create-self-hosted-integration-runtime#create-a-self-hosted-ir-via-azure-data-factory-ui) is set up on the server Xtract Universal runs on. This ensures that Xtract Universal's web server is accessible from ADF over http(s).
- The extraction uses a [push-destination](../../documentation/destinations/), e.g., Azure Blob Storage or Azure SQL Server.
- The extraction runs successfully when called from a web browser, see [Web-API](../../web-api/#run-extractions).
- Access to Azure Data Factory.
- Knowledge on how to build ADF pipelines.

### Basic Principles

The depicted scenario builds upon the following basic principles:

- Xtract Universal offers a [Web-API](../../web-api/#run-extractions) through which various actions can be performed via http(s) calls.
- Microsoft's self-hosted Integration runtime enables access to on-prem resources, such as Xtract Universal, from ADF.
- Microsoft's ADF offers a *Web Activity* that allows calling resources via http(s) and a self-hosted Integration runtime.

The depicted scenario uses two ADF pipelines to run extractions from ADF:

- a [Child Pipeline](#child-pipeline) that extracts data from SAP.
- a [Master pipeline](#master-pipeline) that executes the child pipeline for different extractions.

### Child Pipeline

Follow the steps below to create a child pipeline that extracts data from SAP:

1. Run an extraction using a web activity , see [Web-API - Run Extractions](../../web-api/#run-extractions).
1. Query the extraction status in regular intervals using a web activity , see [Web-API - Get Status of an Extraction](../../web-api/#get-status-of-an-extraction).
1. Add a condition that checks the extraction status and executes follow up activities in case the extractions fails.\
   Example: When the extraction fails, use a web activity to query the extraction log, see [Web-API - Get Extraction Logs](../../web-api/#get-extraction-logs), and write the logs to an Azure Blob Storage account. A follow up event can then be triggered by the *Storage event*, e.g., sending a notification email.

The pipeline functions as a standalone solution. It can be run in debug mode or can be triggered via a scheduler.

### Master Pipeline

Follow the steps below to create a master pipeline that executes the child pipeline multiple times, each time for a different extraction. This allows automatic iteration through all extractions defined in Xtract Universal.

1. Query a list of extractions using a web activity, see [Web-API - Get Extraction Details](../../web-api/#get-extraction-details).
1. Loop over the list of extractions .
1. In the loop, pass the name of the current extraction to the *Child pipeline* and execute the *Child pipeline* for that extraction.

### Variables and Parameters

Parameters and variables are used in both pipelines:

- Parameters provide constant values that are used in multiple activities.
- Variables provide dynamic values at runtime and are used to pass on data between different activities or pipelines.

The following parameters and variables are used in the depicted scenario:

| Parameter / Variable | Name | Data Type | Defined in | Description | | --- | --- | --- | --- | --- | | Parameter | p_global_XU_HOST | String | global | This parameter contains the base URL of the Xtract Universal webserver, here: `https://MyOnPremXuServer.theobald.local:8165`. The parameter is used in every Web Activity. | | Variable | v_XU_extractions_array | Array | Master pipeline | This variable stores the list of XU extractions returned by *Web* activity *Get_List_of_XU_extractions*. The variable's value is set in the *Set variable* activity *Set variable_extraction array*. | | Parameter | p_extractionName_from_Master | String | Child pipeline | This parameter takes on the value (extraction name) of the current iteration *For Each* activity \*ForEach extraction in v_extraction array. As a default name, you assign a name of an extraction. This allows running the Child pipeline w/o being triggered from the Master pipeline. | | Variable | v_TIMESTAMP | String | Child pipeline | This variable stores the extraction's timestamp returned by *Web* activity *XU_START_JOB*. The variable’s value is set in the *Set variable* activity *TIMESTAMP*. The variable is later used in *Web* activities *CHECK_XU_JOB_STATUS* and *XU_Get_Extraction_Log*. | | Variable | v_JOB_STATUS | String | Child pipeline | This variable stores the extraction's run status returned by *Web* activity*CHECK_XU_JOB_STATUS*. The variable’s value is set in the *Set variable* activity *JOB_STATUS*. As long as the variable has the status "Running", the *Until* activity *IS_JOB_RUNNING* is executed. Other values this variable can can have are "FinishedNoErrors" and "FinishedErrors". | | Variable | v_Log | String | Child pipeline | This variable stores the extraction's log returned by *Web* activity*XU_Get_Extraction_Log*. The variable’s value is set in the *Set variable* activity *Set_variable_XU_Log*. The value of this variable is appended to the log file in the *Copy data* activity *Copy Extraction Log to Blob*. |

For more information on variables in ADF, see [Call Dynamic Extractions with Variables in ADF](../call-dynamic-extractions-with-variables-in-adf/).

### Download JSON Templates

Downloads for the child and master pipeline are provided below:

[Download CHILD pipeline as json](../../assets/files/xu/CHILD_pipeline_Execute_single_XU_extraction.json) [Download MASTER pipeline as json](../../assets/files/xu/MASTER_pipeline_Loop_over_XU_extractions.json)

______________________________________________________________________

#### Related Links

- [Call Dynamic Extractions with Variables in ADF](../call-dynamic-extractions-with-variables-in-adf/).
- [Integration in Azure Data Factory using Commandline](../adf-integration-using-command-line/)

The following section describes a common business scenario to rename column name styles within the Microsoft SQL-Server environment. The given example shows how to use Custom SQL in the *Finalization* step of the database transaction within the Xtract Universal destination settings.

### About Column Name Styles

Xtract Universal offers 4 different [Column Name Styles](../../documentation/destinations/microsoft-sql-server/#column-name-style) for naming the SAP table columns in databases:

- Code - `[FieldName]`
- Prefixed Code - `[TabName]~[FieldName]`
- CodeAndText - `[FieldName]_[FieldDescription]`
- TextAndCode - `[FieldDescription]_[FieldName]`

The depicted example uses the Column Name Style 'Prefixed Code', which connects each table field in the format *[TabName][ColumnName]* with the SAP standard separator '~'. This naming of table columns is mainly used for table joins, because identical column identifiers exist in the different tables. A typical example is the table join of 'EKKO' (Purchasing Document Header) and 'EKPO' (Purchasing Document Item). Both tables have the following identical column descriptions:

- 'MANDT'
- 'EBELN'

When selecting the standard Column Name Style 'Code' in the [destination settings](../../documentation/destinations/microsoft-sql-server/#destination-settings), the following error occurs on the SQL side when selecting these fields:

```text
> System.Data.SqlClient.SqlException (0x80131904): Column names in each table must be unique. Column name 'MANDT' in table 'EKKO_JOIN' is specified more than once.

```

### Adjust Standard Separator using Custom SQL

Follow the steps below to adjust SAP standard separator from '~' to '\_':

1. Adjust the Column Name Style e.g. 'PrefixedCode' .

1. Insert the generic SQL Code below into the *Finalization* step using **[Edit SQL]**.

   ```sql
   declare @table_name nvarchar(128) = '#{ Extraction.TableName }#'
   declare @old_name nvarchar(128)
   declare @new_name nvarchar(128)

   declare cur CURSOR LOCAL for
       select COLUMN_NAME
       from INFORMATION_SCHEMA.COLUMNS
       where TABLE_NAME = @table_name

   open cur

   while (1 = 1)
   begin
       fetch next from cur into @old_name
       IF @@FETCH_STATUS != 0 BREAK

       SET @new_name = REPLACE(@old_name, '~', '_')
       SET @old_name = '[' + @table_name + '].[' + @old_name + ']'
       EXEC sp_rename @old_name, @new_name, 'COLUMN'
   end

   close cur
   deallocate cur

   ```

1. Click **[OK]** to confirm your input .

1. Run the extraction.

1. Check the Column Name Style changes and results in SQL Server Management Studio (SSMS).

### Create Stored Procedure (sp) using SSMS

Create a stored procedure that contains above mentioned T-SQL code for renaming column name styles and call this stored procedure in the *Finalization* step of the destination settings. This approach allows to easily change the renaming logic within the DB or SQL server instance. You only have to adapt the stored procedure instead of each *Finalization* step.

Follow the steps below to adjust the SAP standard separator from '~' to '\_':

1. Create T-SQL Stored Procedure using SQL Server Management Studio. For more information, see [Microsoft Documentation: Create a stored procedure](https://docs.microsoft.com/en-us/sql/relational-databases/stored-procedures/create-a-stored-procedure?view=sql-server-ver15).

1. Assign a name for the Stored Procedure e.g., ColumnNameStyle.

1. Insert the SQL-Code below and **[Execute]** the statement to save the process.

   ```sql
   CREATE PROCEDURE ColumnNameStyle 
       @table_name nvarchar(128)
   AS 

   BEGIN

   declare @old_name nvarchar(128)
   declare @new_name nvarchar(128)
   declare cur CURSOR LOCAL for

       select COLUMN_NAME
           from INFORMATION_SCHEMA.COLUMNS
           where TABLE_NAME = @table_name

   open cur
   while (1 = 1)
   begin

       fetch next from cur into @old_name
       IF @@FETCH_STATUS != 0 BREAK
       SET @new_name = REPLACE(@old_name, '~', '_')
       SET @old_name = '[' + @table_name + '].[' + @old_name + ']'
       EXEC sp_rename @old_name, @new_name, 'COLUMN'

   end
   close cur
   deallocate cur

   END

   ```

1. Open the destination settings in Xtract Universal and select a Column Name Style, e.g., 'PrefixedCode' .

1. Insert following SQL Code into the *Finalization* step using **[Edit SQL]**.

   ```text
   EXEC ColumnNameStyle '#{ Extraction.TableName }#'

   ```

1. Click **[OK]** to confirm your input.

1. Execute the selected extraction and check the Column Name Style changes and results in SSMS.

The following article shows how to replace most ODP [provider contexts](../../documentation/odp/provider-context/) with other extraction types.

Aside from the extraction types presented in this article, Xtract Universal offers ODP data extraction via OData protocol. While [OData](../../documentation/odata/) covers all provider contexts, the extraction time for large data sets is longer compared to components that use the standard RFC protocol.

| Provider Context | Alternative | | --- | --- | | ABAP Core Data Services \[[ABAP_CDS](../../documentation/odp/provider-context/#abap-cds-views)\] | [Table](../../documentation/table/) | | HANA Information Views \[[HANA](../../documentation/odp/provider-context/#hana-views)\] | [Table](../../documentation/table/) [BWCube](../../documentation/bwcube/) | | SAP NetWeaver Business Warehouse or BW4/HANA \[[BW](../../documentation/odp/provider-context/#bw-infoproviders)\] | [BWCube](../../documentation/bwcube/) [Table](../../documentation/table/) [Table CDC](../../documentation/table-cdc/) [DeltaQ](../../documentation/deltaq/) [OHS](../../documentation/ohs/) | | Datasources/Extractors \[[SAPI](../../documentation/odp/provider-context/#extractors)\] | [DeltaQ](../../documentation/deltaq/) | | SAP LT Queue Alias \[[SLT~your_queue_alias](../../documentation/odp/provider-context/#slt-server)\] | [Table](../../documentation/table/) [Table CDC](../../documentation/table-cdc/) |

### Extract Data from CDS Views

Alternative for the *ABAP Core Data Services [ABAP_CDS]* provider context:\
To extract data from CDS Views, use the [Table](../../documentation/table/) extraction type.

Note

When looking up CDS Views in Xtract Universal, make sure to look up the name of the DDL SQL View (CDS Database View) instead of the CDS View name.

### Extract Data from HANA Views

Alternative for the *HANA Information Views [HANA]* provider context:\
To extract data from HANA Views, use the [BWCube](../../documentation/bwcube/) extraction type or the [Table](../../documentation/table/) extraction type.

- **Extraction via Table**

  ______________________________________________________________________

  - In SAP, expose the HANA View to ABAP CDS Views, see [SAP Community: How to consume HANA Calculation views in S/4HANA CDS views](https://community.sap.com/t5/technology-blogs-by-members/how-to-consume-hana-calculation-views-in-s-4hana-cds-views/ba-p/13476798).
  - Extract the CDS View using the [Table](../../documentation/table/) extraction type.

- **Extraction via BWCube**

  ______________________________________________________________________

  - In SAP, expose the HANA View to CompositeProviders, see [SAP Learning: Consuming a Calculation View with SAP BW/4HANA](https://learning.sap.com/learning-journeys/upgrading-your-sap-bw-skills-to-sap-bw-4hana/consuming-a-calculation-view-with-sap-bw-4hana_ac069075-173b-41fb-bb35-b950b213d407)
  - Extract the CompositeProvider using the [BWCube](../../documentation/table/) extraction type.

### Extract Data from BW Systems (CompositeProviders, Cubes, InfoObjects, etc.)

Alternative for the *SAP NetWeaver Business Warehouse or BW4/HANA [BW]* provider context:

- To extract data from CompositeProviders, InfoCubes, MultiProviders, aDSO/DSOs etc., use the [BWCube](../../documentation/bwcube/) extraction type.
- To extract data from InfoObjects (Master data, Text data or Hierarchies), look up the respective tables (P, T, H) in the [Table](../../documentation/table/) extraction type.
- To extract data from Hierarchy InfoObjects, use the [BW Hierarchy](../../documentation/hierarchy/) extraction type.
- To extraxt data from Infocubes and DSOs, convert the Infocubes and DSOs to DataSources and extract the DataSources using the [DeltaQ](../../documentation/table/) extraction type.
- If you use Open Hub Service (OHS) destinations that can receive data from Cubes, DSOs, aDSOs etc., the data can be extracted using the [OHS](../../documentation/ohs/) extraction type. You can also run the process chain that writes data to OHS within SAP BW.
  - To extract the data from OHS tables, use the [Table](../../documentation/table/) extraction type.
  - To extract delta changes (Inserts, Updates, Deletes) from an OHS table, use the [Table CDC](../../documentation/table-cdc/) extraction type.

### Extract Data from Extractors

Alternative for the *Datasources/Extractors [SAPI]* provider context:\
To extract data from Extractors, use the [DeltaQ](../../documentation/deltaq/) extraction type.

### Extract Data from Tables

Alternative for the *SAP LT Queue Alias [SLT~your_queue_alias]* provider context:

- To extract data from regular (application) tables, cluster or pooled tables, use the [Table](../../documentation/table/) extraction type.
- To extract delta changes (Inserts, Updates, Deletes), use the [Table CDC](../../documentation/table-cdc/) extraction type.

______________________________________________________________________

#### Related Links

- [Guidance on SAP Note 3255746 for Theobald Software Xtract Products](https://theobald-software.com/en/products-technology-en/guidance-on-sap-note-3255746-for-theobald-software-xtract-products/).
- [Create (ODP) OData Services using the SAP Gateway Builder](../create-odata-services-using-the-sap-gateway-builder/)

The following article shows how to connect to the Azure Storage destination using Authentication via Microsoft Entra ID (formerly Azure Active Directory). The article leads you through the following process:

1. Register a new app with your Entra ID tenant.
1. Assign access rights for the new app in Azure Storage using the Storage Blob Data Contributor role.
1. In Xtract Universal, connect to Azure Storage using the Microsoft Entra ID method.

### App Registration

Follow the steps below to register a new app with your Entra ID tenant:

1. Open the Azure portal and navigate to **App Registrations**.
1. Click **[New registration]** to register a new app with your Entra ID tenant.
1. Enter the name of the application.
1. In the Redirect UI section, select *Public Client /native (mobile and desktop)* and assign `https://login.microsoftonline.com/common/oauth2/nativeclient` as the redirect URI.
1. Click **Register**.
1. Open the new application and navigate to **API Permissions > Add a permission > Azure Storage**.
1. Click **Grant admin consent**.

The app is now registered.

### Access Rights in Azure Storage

Follow the steps below to assign access rights for the [new Azure app](#app-registration) in Azure Storage using the Storage Blob Data Contributor role:

1. Open the Azure portal and navigate to **Access Control (IAM)**.
1. Click **[Add role assignment]**.
1. Select the **Storage Blob Data Contributor** role and click **[Next]**.
1. Click **+ Select members** and add the new Azure app created in [App Registration](#app-registration) to the members.
1. Click **[Next]** to continue, then click **[Review + assign]** to assign the access rights.

Access rights are now assigned.

### Connect to Azure Storage

Follow the steps below to connect Xtract Universal to the Azure Storage destination using Authentication via Microsoft Entra ID:

1. Open Xtract Universal and create a new Azure Storage destination or edit an existing destination.
1. Select the connection type **Azure active directory**.
1. Enter the name of your storage account.
1. Copy and paste the Application (client) ID and the Directory (tenant) ID from the Azure app created in [App Registration](#app-registration).
1. Click **[Connect]**. The window "Azure OAuth 2.0" opens.
1. When prompted, provide the credentials of a Microsoft service user for the OAuth connection. Make sure that the user meets the following requirements:
   - The user has the 'Storage Blob Data Contributor' or 'Owner' role in Azure Storage.
   - The user does not use Multifactor Authentication (MFA) as extractions fail when the MFA of the user expires.
1. If the connection is successful, a "Connection successful" message is displayed in a pop-up window.

The destination is now ready to use.

The following article shows how to set up access to reports by assigning authorization groups to reports.\
Access is then granted through the S_PROGRAM authorization object, see [SAP Note 338177](https://launchpad.support.sap.com/#/notes/338177).

### Set Up Access to Specific Reports

1. Log into SAP and use transaction code SE38 to open the ABAP Editor.
1. Enter the name of the report you want to access and select **Attributes** as the *Subobjects*.
1. Click **[Change]**. A window that contains the program attributes opens.
1. Assign an authorization group.
1. Edit or create a user role you want to grant access to (transaction code PFCG).
1. Manually assign the authorization object S_PROGRAM to the user role.
1. Select the actions **SUBMIT** and **BTCSUBMIT** in the S_PROGRAM object field *P_ACTION*.
1. Assign the same authorization group that is assigned to the report to the S_PROGRAM object field *P_GROUP*.
1. Save and generate the authorization.
1. Assign the user role to users.

______________________________________________________________________

#### Related Links

- [Create the Custom Authorization Object Z_TS_PROG](../create-the-custom-authority-object-z-ts-prog/)
- [SAP Authorization Objects for Reports](../../documentation/setup-in-sap/sap-authority-objects/#report)
- [Documentation: Report](../../documentation/report/)

The following article shows how to call Xtract Universal extractions dynamically from Azure Data Factory (ADF) using user-defined variables. The depicted example runs extractions daily to write data added or updated on the day before to the destination.

### Call Dynamic Extractions with Variables

The depicted example calls an extraction with a date parameter in ADF. The date parameter is set dynamically using a user-defined variable.

1. Create an extraction in Xtract Universal that uses runtime parameters.\
   The depicted example uses an extraction called *0COSTCENTER_0101_HIER* with a date parameter called *myDate*.
1. Create a pipeline in ADF that stores yesterday's date in a variable .
1. Format the date to the internal SAP date format (YYYYMMDD).\
   The type and format of the input variable must match the type and format of the actual parameter in Xtract Universal.
1. Add a web activity that calls extractions . The URL used to call static extractions has the following format:\
   `[Protocol]://[HOST or IP address]:[Port]/?name=[Name of the Extraction]`
1. To set runtime parameters of an extraction, add the corresponding variables to the extraction URL using the *@concat* command. The concatenated string has the following format:\
   `@concat('[Protocol]://[HOST or IP address]:[Port]/?name=[Name of the Extraction]&[Name of the Parameter in XU]=',variables('[Name of the Variable in ADF'))`
1. Run the pipeline and check the result.

Tip

You can copy the URL of an extraction from the Run window in Xtract Universal, see [Documentation: Run an Extraction](../../documentation/execute-and-automate/run-an-extraction/#run-extraction-window).

______________________________________________________________________

#### Related Links

- [Run an ADF pipeline when an SAP extraction file is successfully uploaded to Azure storage](../run-an-ADF-pipeline-when-sap-extraction-file-is-successfully-uploaded-to-Azure-storage/)
- [Integration in Azure Data Factory using Webservices](../adf-integration-using-webservices/)
- [Integration in Azure Data Factory using Command Line](../adf-integration-using-command-line/)
- [Documentation: Web API](../../web-api/)

This section shows how to call an extraction from a [Windows script (.bat)](#call-via-windows-script-bat) or [PowerShell script](#call-via-powershell-script) using the command line tool xu.exe.

### Call via Windows script (.bat)

Follow the steps below to run an extraction using a Windows script that calls the command line tool *xu.exe*.

1. Create a new batch file.

1. Define the following variables:

   - Standard output (*XUOutputfile*)
   - Standard error output (*XULogfile*)
   - Path to the command line tool (*XUCmd*)
   - XU server name (*XUServer*)
   - XU server port (*XUPort*)
   - Name of the extraction (*XUExtraction*)

   ```bat
   :: Execute an Xtract Universal extraction using the command tool xu.exe
   :: clear screen  
   cls
   :: Turns off the command echoing feature
   @echo off
   :: write the output to a file
   set XUOutputfile="C:\Data\xubatch\output.txt"
   :: write the log to a file
   set XULogfile="C:\Data\xubatch\log.txt"
   :: set the path to the installation folder
   set XUCmd="C:\Program Files\XtractUniversal\xu.exe"
   :: default is also localhost, so you skip it or change it  
   set XUServer=localhost
   :: default port is also 8065, so you skip it or change it  
   set XUPort=8065
   set XUExtraction=customers 

   ```

1. If the extraction requires input parameters, set dynamic parameters, e.g., *v_country* for the language key.

   ```bat
   :: the extraction has a variable Country that needs a country code of length 2, e.g. US
   :: Skip this block if you don't use variable  
   set v_country=US
   :: Turns on the command echoing feature
   @echo on

   ```

1. Run the extraction by calling the command line tool with the corresponding parameters.

   ```bat
   :: run the command tool with the right parameters
   %XUCmd% -s %XUServer% -p %XUPort% -n %XUExtraction% -o Country=%v_country% 1>%XUOutputfile% 2>%XULogfile%

   ```

   ```bat
   @echo off 
   :: create an array with extraction names separated by empty space 
   :: in this example there are two extractions named *customers* and *materials*.
   set extraction_list=customers materials 
   :: alternative 
   :: set extraction_list[0] = customers 
   :: set extraction_list[1] = materials 
   @echo on

   for %%e in (%extraction_list%) do ( 
       %XUCmd% -s %XUServer% -p %XUPort% -n %%e 1>>%XUOutputfile% 2>>%XULogfile%
   )

   :: The output in this example is added to the existing file with >>.

   ```

1. Check the return code and write a corresponding message. The return code *0* indicates a successful execution. Other [Return Codes](../../documentation/execute-and-automate/call-via-commandline/#return-codes) indicate errors during execution.

   ```bat
   :: check the last exit code
   :: 0: successful
   :: else unsuccessful
   @echo off 
   IF %ERRORLEVEL% EQU 0 ( 
    echo extraction %XUExtraction% is successful 
   ) ELSE (
    echo extraction %XUExtraction% is not successful. Error Code %ERRORLEVEL%. See log for details.
   )
   @echo on

   ```

1. Optional: extractions can be added to the Windows logs. They can be displayed in the Event Viewer.

### Call via PowerShell Script

Follow the steps below to run an extraction using a PowerShell script that calls the command line tool *xu.exe*.

1. Define the following variables:

   - Standard output (*XUOutputfile*)
   - Standard error output (*XULogfile*)
   - Path to the command line tool (*XUCmd*)
   - XU server name (*XUServer*)
   - XU server port (*XUPort*)
   - Name of the extraction (*XUExtraction*)

   ```shell
   # Execute an Xtract Universal extraction using the command tool xu.exe 
   # clear screen  
   clear
   # write the output to a file
   $XUOutputfile = "C:\Data\powershell\output.txt"
   # write the log to a file
   $XULogfile = "C:\Data\powershell\log.txt"
   # set the path to the installation folder
   $XUCmd = 'C:\Program Files\XtractUniversal\xu.exe'
   $XUServer = "localhost"
   $XUPort = "8065"
   $XUExtraction = "SAPSalesCube" 

   ```

1. If the extraction requires input parameters, set dynamic parameters, e.g., *myCalendarMonth* for the current month in the format "yyyyMM".

   ```shell
   # the extraction has a variable CalendarMonth that needs a value in the format "yyyyMM", e.g. 201712
   # Skip this block if you don't use variable
   # generate the calender month from the current date to be used as a variable
   # e.g. Tuesday, December 19, 2017 10:40:32 AM

   $myyear = (Get-Date -format "yyyy")
   $mymonth = (Get-Date -format "MM")

   # 201712
   $myCalendarMonth = "$myyear$mymonth"
   # another option Get-Date -format "yyyyMM"
   # just if you use variables
   # the extraction has a variable CalendarMonth, its value has the format YYYYMM
   # set the variable for calendar month e.g. 201712

   ```

1. Run the extraction by calling the command line tool with the corresponding parameters.

   ```shell
   :: run the command tool with the right parameters
   %XUCmd% -s %XUServer% -p %XUPort% -n %XUExtraction% -o Country=%v_country% 1>%XUOutputfile% 2>%XULogfile%

   ```

1. Check the return code and write a corresponding message. The return code *0* indicates a successful execution. Other [Return Codes](../../documentation/execute-and-automate/call-via-commandline/#return-codes) indicate errors during execution.

   ```shell
   # check the last exit code
   # 0: successful
   # else unsuccessful
   if($LASTEXITCODE -eq 0) {           
   write-host -f Green "The last command executed successfully"          
   } else {           
   write-host -f Red "The last execution failed with error code $LASTEXITCODE!"
   write-host $errorMessage
   }

   ```

For more examples on how to use PowerShell scripts with Xtract Universal, see [SAP Access with Xtract Universal and Powershell](../sap-access-with-xtract-universal-and-powershell/)

______________________________________________________________________

#### Related Links:

- [Create Extractions via Commandline](../config-command-line-tool/)
- [Insert Extraction Events into Windows Logs](../insert-extraction-events-into-the-windows-logs/)
- [SAP Access with Xtract Universal and Powershell](../sap-access-with-xtract-universal-and-powershell/)

The following article shows how to manually and automatically renew a X.509 certificate used for TLS in Xtract Universal.

Warning

**Expired Certificate.**\
The Cryptographic key pair associated with the certificate is no longer valid and this may cause security risks. Always use a valid certificate. To access the Designer after a certificate has expired, delete the `tls.json` file in the Xtract Universal installation directory (`C:\Program Files\XtractUniversal\config\servers\`) and restart the Xtract Universal service. This resets all TLS settings in Xtract Universal, including the certificate selection.

### Renew a Certificate Manually

1. Before the old certificate expires, install a new certificate on the server machine.

1. Open the Xtract Universal Designer and reference the new certificate, see [Install an X.509 Certificate](../../documentation/access-restrictions/install-x.509-certificate/#integrate-the-x509-certificate).

1. Delete the old certificate from the Microsoft Certificate Store.

1. Block external access to the Xtract Universal server using the firewall.

1. Open the Xtract Universal Designer and navigate to **Settings > Server**.

1. In the *Web Server* tab, select the protocol **HTTP - Unrestricted** to disable TLS.

1. Click **[OK]** to save the settings. When prompted to restart the service, click **[OK]** again.

1. Renew the certificate with the same key using Windows AD Certificate Services.

1. Open the Xtract Universal Designer and enable TLS with the new certificate, see [Activate TLS Encryption](../../documentation/access-restrictions/restrict-server-access/#activate-tls-encryption).

1. Click **[OK]** to save the settings. When prompted to restart the service, click **[OK]** again.

1. Allow external access to the Xtract Universal server using the firewall.

Note

If you use TLS encryption for the communication with the Xtract Universal Designer, make sure to also reference the new certificate in the [*Configuration Server*](../../documentation/server/server-settings/#configuration-server) tab of the server settings.

### Renew a Certificate Automatically

If you're using [win-acme](https://www.win-acme.com/reference/plugins/installation/script) for the renewal of Letsencrypt certificates, run the following PowerShell script with the same client that runs win-acme.

[Download PowerShell Script for Letsencrypt Certificate Renewal](../../assets/files/xu/xu-le.ps1)

#### About win-acme

win-acme creates a scheduled task for the renewal process. When this process is triggered, it issues a new certificate and stores it in the Windows Certificate Store. The old certificate is deleted.

#### About the PowerShell Script

The `xu-le.ps1` script replaces the old certificate in the Xtract Universal settings with the new certificate. No manual changes in Xtract Universal are required.

The `xu-le.ps1` script requires 2 input parameters:

- the thumbprint of the old certificate
- the thumbprint of the new certificate

______________________________________________________________________

#### Related Links

- [Xtract Universal Documentation: Install an X.509 Certificate](../../documentation/access-restrictions/install-x.509-certificate/)
- [Xtract Universal Documentation: Server Settings](../../documentation/server/server-settings/)
- [Enable Secure Network Communication (SNC) via X.509 certificate](../enable-snc-using-pse-file/)

The following article shows how to load data incrementally (daily) from an SAP Table with no delta pointers / date fields. The depicted example scenario uses two tables:

- MAKT (Material Descriptions), which has no date fields.
- CDHDR (Change Documents Header), which holds the header information of the changed records. CDHDR is used to determine the delta information of MAKT and other tables.

### Prerequisites

- Prepare a record for the delta extraction:\
  Change the description of a material in MAKT, e.g., change the description “ABC” of material 2593 to "Test_delta”.
- Check if CDHDR registered the change:\
  Filter the field UPDATE for today's date. The change made in MAKT should be listed.

### Daily Data Extraction

The following steps describe how to only extract the data from MAKT that has been changed on today’s date.

1. Create a new Table extraction.

1. Look up the tables MAKT and CDHDR.

1. Select the fields OBJECTID and UDATE from CDHDR for the output.

   - OBJECTID contains information about the Key on which the changes are made. This field is used for joining the tables and to get the delta data from MAKT.
   - UDATE contains the date on which updates occurred. This field is used to filter the data for specific dates.

1. Select the fields you want to extract from MAKT for the output (MATNR is mandatory).

1. Open the tab *Joins* and click **[Add]**. The window "Join" opens.

1. Select the join type *INNER_JOIN* to combine the tables CDHDR and MAKT. The OBJECTID from CDHDR and MATNR from MAKT have same entries and thus form an inner join condition.

1. Click **[Add]** and confirm your selection with **[OK]**.

1. Open the tab *WHERE Clause* and enter the following filter criteria:

   ```text
   CDHDR~UDATE = '#{ DateTime.Now.ToString("yyyyMMdd") }#'

   ```

   This criteria uses script expressions to get the current date in the SAP format ("yyyyMMdd").

1. Click **[Load live review]** to check the results. Only the data in MAKT that has been changed on today's date is extracted.

1. Schedule the extraction daily.

Tip

To extract all changes of the day before, change the WHERE clause to `CDHDR~UDATE >= '#{ DateTime.Now.AddDays(-1).ToString("yyyyMMdd") }#'` and schedule the extraction every night at 1p.m. or later.

______________________________________________________________________

#### Related Links

- [Delta Table Extraction](../delta-table-extraction/)
- [Table: WHERE Clause](../../documentation/table/where-clause/)

The following article shows how to check the accessibility of an SAP system using the [paping.exe](https://code.google.com/archive/p/paping) tool. Alternatively, you can also use the Microsoft [telnet](https://learn.microsoft.com/de-de/windows-server/administration/windows-commands/telnet) tool.

Sometimes the firewall is blocking the traffic to the SAP System. This could be a local firewall, but also a firewall in the destination network. You can use the paping.exetool to ping the port and to check if the firewall is open.

### Ping the SAP System

Use the following syntax with paping.exe:

```console
paping.exe SAPServer -p port -c 3*

```

```console
paping.exe 192.168.0.9 -p 3305 -c 3

```

### Port Numbers

If an SAP-Router is used, the ports are 3299 and 3399.\
If not, the ports are 32XX and 33XX. XX is usually the system number, e.g., 00, or 05.

Some important port numbers:

```console
sapdp00 3200/tcp # SAP Dispatcher. 3200 + Instance-Number
sapgw00 3300/tcp # SAP Gateway. 3300 + Instance-Number
sapsp00 3400/tcp # 3400 + Instance-Number
sapms00 3500/tcp # 3500 + Instance-Number
sapmsSID 3600/tcp # SAP Message Server. 3600 + Instance-Number
sapgw00s 4800/tcp # SAP Secure Gateway 4800 + Instance-Number

```

The following article describes a common problem that occurs when pushing SAP data into an SQL server database when collation is not set to case-sensitive. The depicted example shows how to customize the *Drop & Create* SQL server statement within the Xtract Universal destination settings to accommodate these issues.

### About Collation SQL Server

Collations in SQL Server provide sorting rules, case, and accent sensitivity properties for your data. Collations that are used with character data types, such as *char* and *varchar*, difine the code page and corresponding characters that can be represented for the corresponding data type.

Collation can be set up on three different levels:

- [Server collation](https://docs.microsoft.com/en-us/sql/relational-databases/collations/set-or-change-the-server-collation?view=sql-server-ver15)
- [Database collation](https://docs.microsoft.com/en-us/sql/relational-databases/collations/set-or-change-the-database-collation?view=sql-server-ver15)
- [Column collation](https://docs.microsoft.com/en-us/sql/relational-databases/collations/set-or-change-the-column-collation?view=sql-server-ver15)

MSSQL server offers different collation statements. The following excerpts provide the necessary adaptions for the given example:

| **Option** | **Description** | | --- | --- | | Case-sensitive (\_CS) | Distinguishes between uppercase and lowercase letters. If this option is selected, lowercase letters sort ahead of their uppercase versions. If this option isn't selected, the collation is case-insensitive. Which means, SQL Server considers the uppercase and lowercase versions of letters to be identical for sorting purposes. You can explicitly select case insensitivity by specifying \_CI. | | Accent-sensitive (\_AS) | Distinguishes between accented and unaccented characters. For example, "a" is not equal to "ấ". If this option isn't selected, the collation is accent-insensitive. Which means, SQL Server considers the accented and unaccented versions of letters to be identical for sorting purposes. You can explicitly select accent insensitivity by specifying \_AI. |

For information on usable collations, see [Microsoft Documentation: Collation](https://docs.microsoft.com/en-us/sql/relational-databases/collations/collation-and-unicode-support?redirectedfrom=MSDN&view=sql-server-ver15#Collation_Defn) and [Microsoft Documentation: Collation and Unicode support](https://docs.microsoft.com/en-us/sql/relational-databases/collations/collation-and-unicode-support?redirectedfrom=MSDN&view=sql-server-ver15). The depicted example shows the column collation within Xtract Universal with a [Custom SQL](../../documentation/destinations/microsoft-sql-server/#custom-sql-statements) statement.

### SQL Server Management Studio (SSMS)

Check the database settings. The following collation statement is displayed: `Latin1_General_100_CI_AI`.

| **Option** | **Description** | | --- | --- | | \_CI | case-insensitive | | \_AI | accent-insensitive |

### Setup in Xtract Universal

Follow the steps below to extract the SAP table *MAKT* from SAP:

1. Create a table extraction, see [Documentation: Table](../../documentation/table/). The look-up process loads the corresponding metadata from our SAP object *MAKT*:
   1. The composite primary key consists of the table fields *MANDT*, *MATNR*, *SPRAS*, each with a unique constraint.
   1. The SAP field *SPRAS* is of data type *LANG* with a length *1*.
1. Create a simple WHERE clause, e.g., `MATNR = '000000000000000038' AND ( SPRAS = 'd' OR SPRAS = 'D' )`.
1. Click **[Load live preview]**.\
   The results show that the SAP database interprets the data records with upper-case 'D' and lower-case 'd' in the field *SPRAS* as different data records.
1. Assign an MSSQL server destination to the extraction and click **[Run]**.

The MSSQL server returns the following error:

```text
> System.Data.SqlClient.SqlException (0x80131904): Violation of PRIMARY KEY constraint 'PK__makt__3483F06C110B42CD'. 
> Cannot insert  duplicate key in object 'dbo.makt'.The duplicate key value is (800, 000000000000000038, d)

```

### Workaround

As shown in [Setup in Xtract Universal](#setup-in-xtract-universal) the data of MAKT cannot be pushed into the MSSQL server destination due to the collation statement of the database. In this case, the user has to customize the SQL statement *Preparation* of the MSSQL destination settings.

1. Change the default value *Drop & Create* to *Custom SQL*.

1. Click **[Edit SQL]** to enter an SQl statement.

1. Select the *Drop & Create* entry from the drop-down menu and click on **[Generate Statement]** for table *MAKT*.

1. Customize the column collation for field *SPRAS* using the following code:

   ```sql
   IF (object_id('MAKT') IS NOT NULL)
   BEGIN
      DROP TABLE [MAKT];
   END;

   CREATE TABLE [MAKT]  
   (
      [MANDT] NATIONAL CHARACTER VARYING(3) NOT NULL,
      [MATNR] NATIONAL CHARACTER VARYING(18) NOT NULL,
      [SPRAS] NATIONAL CHARACTER VARYING(1) COLLATE Latin1_General_100_CS_AS NOT NULL,
      [MAKTX] NATIONAL CHARACTER VARYING(40),
      [MAKTG] NATIONAL CHARACTER VARYING(40),
      PRIMARY KEY
      (
         [MANDT], 
         [MATNR], 
         [SPRAS]
      )

   );

   ```

1. Click **[OK]** to confirm your input.

1. Run the extraction again.

The MSSQL server now returns the following message:

```text
Extraction finished successfully

```

The command line tool xu-config.exe tool creates extractions, sources and destinations outside of the Xtract Universal Designer. The tool is available in the installation directory of Xtract Universal, e.g. `C:\Program Files\XtractUniversal\xu-config.exe`.

The xu-config.exe tool supports the creation of the following extraction types and destinations:

| Extraction Types | Destinations | | --- | --- | | [Table](../../documentation/table/) | [Azure Storage](../../documentation/destinations/azure-storage/#destination-details) | | [Table CDC](../../documentation/table-cdc/) | [Amazon AWS S3](../../documentation/destinations/amazon-aws-s3/#destination-details) | | [ODP](../../documentation/odp/) | | | [DeltaQ](../../documentation/deltaq/) | |

### Prerequisites

The execution of PowerShell scripts must be authorized on your system, see [Microsoft Documentation: Managing the execution policy with PowerShell](https://docs.microsoft.com/en-us/powershell/module/microsoft.powershell.core/about/about_execution_policies?view=powershell-7.2#managing-the-execution-policy-with-powershell).

Note

From Xtract Universal version 5.0.0 (2021-10-18) up to version 2024.8.6.35, the xu-config.exe tool must be run by the same Windows AD account that runs the [Xtract Universal Service](../../documentation/server/service-account/). This means, you either run the Windows command prompt as the respective user or you use the `runas` command in the command prompt. This is necessary, because passwords are encrypted for the user account that runs the xu-config.exe tool and can only be decrypted by the same account.

### Create an SAP Source using Windows Command Prompt

Note

the xu-config.exe tool only supports SAP connections with plain authentication.

1. Start the Windows command prompt application with administrator rights .

1. Navigate to the installation directory of Xtract Universal .

1. Run the following shell command to create an encrypted password for your SAP source :

   ```text
   powershell ./protect-password.ps1

   ```

1. Use the following command to select the `xu-config.exe` command line tool from the Xtract Universal installation directory and to create a new SAP source:

   ```text
   xu-config.exe --source <name> <host> <instance-number> <client> <language> <user> <protected-password>

   ```

1. Replace the parameters in `< >` with actual values . The parameters are not case sensitive.

1. Press **[Enter]** to run the command.

1. Check the generated source in the Xtract Universal Designer or in the directory `C:\Program Files\XtractUniversal\config\sources`.

### Create a Destination using Windows Command Prompt

1. Start the Windows command prompt application with administrator rights .

1. Navigate to the installation directory of Xtract Universal .

1. Run the following shell command to create encrypted passwords or keys necessary for the destination :

   ```text
   powershell ./protect-password.ps1

   ```

1. Use one the following commands to select the *xu-config.exe* command line tool from the Xtract Universal installation directory and to create a new destination:

   ```text
   xu-config.exe --azure <account> <accesskey> <container> <folder(opt)>

   ```

   ```text
   xu-config.exe --s3 --auth user <key> <secretkey> <bucket> <region> <folder(opt)>

   ```

   ```text
   xu-config.exe --s3 --auth iam <bucket> <region> <folder(opt)>

   ```

1. Replace the parameters in `< >` with actual values . The names of the parameters are not case sensitive.

1. Press **[Enter]** to run the command.

1. Check the generated destination in the Xtract Universal Designer or in the directory `C:\Program Files\XtractUniversal\config\destinations`.

### Create a Table Extraction using Windows Command Prompt

1. Start the Windows command prompt application with administrator rights .

1. Navigate to the installation directory of Xtract Universal.

1. Use the following command to select the *xu-config.exe* command line tool from the Xtract Universal installation directory and to create a new Table extraction:

   ```text
   xu-config.exe --extraction <source> <destination> --table <table>

   ```

1. Replace the parameters in `< >`with actual values .

1. Enter a [defined SAP Connection, Destination](../../documentation/setup/migration/#configuration-files) and an SAP Table object for the parameters \<source>, \<destination> and \<table>. The names of the parameters are not case sensitive.

1. Press **[Enter]** to run the command.

1. Check the generated table extraction in the Xtract Universal Designer or in the directory `C:\Program Files\XtractUniversal\config\extractions`.

Note

The following table settings are set by default after creation: **Package Size** (50000), **Extract data in background job** (enabled), all columns are selected for output.

Tip

Use the command `xu-config.exe -h` to look up the syntax for Table, Table CDC, ODP and DeltaQ extractions.

### Examples for all Extraction Types

| Extraction Type | Command | | --- | --- | | Table | `xu-config.exe --extraction ec5 sql-server --table TCURR` | | DeltaQ | `xu-config.exe --extraction ec5 sql-server --table TCURR` | | ODP (ABAP Core Data Services) | `xu-config.exe --extraction bw2 sql-server --odp ABAP_CDS UCONRFC_ATTR$F` | | ODP (SAP NetWeaver Business Warehouse) | `xu-config.exe --extraction bw2 sql-server --odp BW 0ADDR_SHORT$T` | | ODP (SAP HANA Information Views) | `xu-config.exe --extraction S4H sql-server --odp HANA HCCT232H1KHY32F7UL59IH224$F` | | ODP (DataSources/Extractors) | `xu-config.exe --extraction ec5 sql-server --odp SAPI 2LIS_11_VAITM` | | Table CDC (extract table on first run) | `xu-config.exe --extraction ec5 csv --tablecdc KNA1 true 5000` | | Table CDC (do not extract table on first run) | `xu-config.exe --extraction ec5 csv --tablecdc KNA1 false 10000` |

### Create Multiple Extractions using PowerShell

Multiple extractions can be generated semi-automatically using suitable scripts. The scripts for creating extractions can be used to contribute to the generation of an SAP data warehouse.

PowerShell Script to Create Multiple Tbale Extractions

```shell
# read table list
$tableList = "KNA1","LFA1","MARA","CSKT","SKA1"
# set the path to the installation folder
$XUConfig = 'C:\Program Files\XtractUniversal\xu-config.exe'
# source sytem
$source = "ec5"
# destination
$destination = "sqlserver2019"

# loop the tables
foreach ($tableName in $tableList) {
    # create the extraction e.g.
    # xu-config.exe --extraction ec5 sqlserver2019 --table KNA1 
    Try {                   
        write-host -f Green "$tableName : Creation of Extraction is starting "  (Get-Date)                      
        &$XUConfig --extraction $source $destination --table $tableName    

        # check the last exit code
        # 0: successful
        # else unsuccessful
        if($LASTEXITCODE -eq 0) {                           
            write-host -f Green "$tableName : Creation of Extraction  is successful"  (Get-Date)            
        } else {           
            write-host -f Red "$tableName : Creation of Extraction failed with error code $LASTEXITCODE!"  (Get-Date)
            #Write-Host $errorMessage
        }                
    }
    Catch {
        write-host -f Red "$tableName : Creation of Extraction failed with Exception ! " + (Get-Date)  $_.Exception.Message
    }         
}

```

______________________________________________________________________

#### Related Links

- [Documentation: SAP Connection](../../documentation/sap-connection/)
- [Documentation: Define a Table Extraction](../../documentation/table/#define-the-table-extraction-type)
- [Documentation: Run an Extraction](../../documentation/execute-and-automate/run-an-extraction/)
- [Documentation: WHERE Clause](../../documentation/table/where-clause/)
- [Documentation: Schedule Extractions](../../documentation/execute-and-automate/call-via-scheduler/)

The following article shows an example of how to configure and use AnySQL Maestro to manage your Amazon Redshift database. The example works for any other [Database](https://docs.aws.amazon.com/redshift/latest/mgmt/connecting-using-workbench.html).

### Create Database Profiles

1. Download and install [AnySQL Maestro](http://www.sqlmaestro.com/de/products/anysql/maestro/download/).
1. Download and install [ODBC driver for PostgreSQL](https://ftp.postgresql.org/pub/odbc/versions.old/msi/psqlodbc_08_04_0200.zip).
1. Launch AnySQL Maestro.
1. Click "Create Database Profiles".
1. In the Create Database Profiles Wizard, click the button next to the connection string field.
1. Navigate to *Connection* and enable *Use connection string* then click **[Build...]**.
1. Navigate to *Machine Data Source* and Click **[New...]**.
1. Click **[Next]** and choose *PostgreSQL Unicode*, then click **[Next]** and **[Finish]**.

Note

A warning might pop up. Close the warning and click **[OK]**.

### Test the result

1. Enter your credentials, click "Test" to check if they are correct and click "Save".
1. Choose your connection and click **[OK]**.
1. Enter the database name and your credentials and select SSL-Mode *allow*, then click **[OK]**.
1. Enter your credentials and select the initial catalog, then click **[OK]**.
1. Click **[Next]**, then **[Ready]**. Now the database is ready.

______________________________________________________________________

#### Related Links

- [Amazon AWS: Getting Started - Backup & Restore with AWS](https://aws.amazon.com/backup-restore/getting-started/?nc1=h_ls)
- [Amazon Redshift: Getting Started with Amazon Redshift](hhttps://docs.aws.amazon.com/redshift/latest/gsg/getting-started.html)

The following article shows how to connect Xtract Universal to Power BI Service via an on-premises data gateway.

### Prerequisites

To connect Power BI Service with Xtract Universal, the following components are required:

- [Power BI Account](https://powerbi.microsoft.com/en-us/landing/signin/)
- [On-premises Data Gateway](https://powerbi.microsoft.com/en-us/gateway/)

### Setup On-Premises Data Gateway

To set up the on-premises Data Gateway using the Power BI Custom Connector, follow the steps below.

Note

The connection to Xtract Universal can be created using [Power Query M-script](../../documentation/destinations/Power-BI-Connector/#power-query-m-script) or [Power BI Custom Connector](../../documentation/destinations/Power-BI-Connector/#power-bi-custom-connector). Using Power Query M-script does not require further configuration.

1. Install the on-premises Data Gateway on the Xtract Universal application server.
1. Configure the Data Gateway, see [Use the on-premises data gateway app](https://docs.microsoft.com/en-us/data-integration/gateway/service-gateway-app).
1. Switch to the *Connectors* tab and define the path to the Power BI Custom Connector e.g., in `[Documents]\Power BI Desktop\Custom Connectors`.\
   The *XtractUniversalExtension* will then be displayed as a Custom Data Connector.

### Add Xtract Universal as a Data Source

Note

Make sure that [enabling and usage of custom connectors in Power BI](https://learn.microsoft.com/en-us/power-bi/connect-data/service-gateway-custom-connectors#enable-and-use-custom-connectors) is activated. If the Xtract Universal entry is not available in the drop-down menu, check the configuration in the [Setup On-Premises Data Gateway](#setup-on-premises-data-gateway) section.

The configured on-premises data gateway is integrated into the Power BI service environment. A DataSource to the Xtract Universal Server must then be set up:

1. In the Power BI service, navigate to **Settings > Manage connections and gateways** and click **[New]**. The "New Connection" window opens.
1. Select the connection type *On-premises* (default setting). Fill in the necessary fields:
1. Select the previously created "Data Gateway" from the **Gateway cluster name** drop-down menu.
1. Assign a name to the connection under **Connection name**.
1. Select *Xtract Universal Extraction* under **Connection type**. If the Xtract Universal entry is not available in the drop-down menu, check the configuration in [Setup On-Premisess Data Gateway](#setup-on-premises-data-gateway).
1. Specify the Xract Universal Server URL under **Server**, see [Connect to an Xtract Universal Server](../../documentation/designer/#connect-the-designer-to-a-server).
1. Decide on the appropriate authentication method under **Authentication Method**, see [Single Sign On and SAP Authentication](../../documentation/destinations/Power-BI-Connector/#single-sign-on-and-sap-authentication).
1. Click **[Create]**. A connection is created and an automatic connection test is performed.

### Data Source Status

1. Check the Data Source Status and other settings, e.g., **Schedules Refresh**.
1. Under **[Workspace settings]** navigate to the settings of the dataset **[...]**.
1. Expand the entry **Gateway and cloud connections**.
1. Configure the connection of the uploaded Power BI dataset.
1. Select the defined gateway and select the name of the connection (here XtractUniversal) from the drop-down menu under the option **Extention**.\
   The status of the connection is checked and reported back in the **Status** field.

______________________________________________________________________

#### Related Links

- [Documentation: Power BI Connector](../../documentation/destinations/Power-BI-Connector/)

The following article shows how to create a custom connector in Matillion Data Loader that loads SAP data via Xtract Universal into Snowflake. Matillion Data Loader is a cloud based data loading platform that extracts data from popular sources and loads it into cloud destinations, see [Official Website: Matillion Data Loader](https://www.matillion.com/products/data-loader/).

### Prerequisites

- Matillion Hub Account, see [Official Website](https://hub.matillion.com/).
- Snowflake Destination for the Matillion Data Loader pipeline, see [Matillion Documentation: Destinations - Set up Snowflake](https://docs.matillion.com/data-productivity-cloud/batch/docs/set-up-snowflake/).
- Xtract Universal must be accessible via the internet, e.g., by hosting Xtract Universal on a webserver with a static IP address or via third party tools like [ngrock](https://ngrok.com/).

### Setup in Xtract Universal

1. Create an extraction in Xtract Universal, see [Getting Started: Create an Extraction](../../getting-started/#create-an-extraction).\
   The depicted example scenario extracts the SAP table KNA1 (General Data in Customer Master).
1. Assign the `http-json` destination to the extraction, see [Documentation: Assign Destinations](../../documentation/destinations/json-via-http/).

### Create a Custom Connector in Matillion

To extract SAP data via Xtract Universal you must define a custom connector that contains the connection details of Xtract Universal, see [Matillion Documentation: Matillion Custom Connector Overview](https://docs.matillion.com/data-productivity-cloud/custom-connector/docs/custom-connector-overview/).

1. Open the website <https://create-connector.matillion.com/> and log in to create the custom connector.
1. Click **[Add Connector]** to create a new custom connector.
1. Click to change the name of the connector .
1. Copy the URL the extraction into the designated input field and select `GET` as the http method . The URL has the following format:\
   `<Protocol>://<HOST or IP address>:<Port>/?name=<Name of the Extraction>{&<parameter_i>=<value_i>}`\
   Example: the URL `https://6606-185-114-89-133.eu.ngrok.io/?name=kna1` calls the extraction "kna1" via ngrock. For more information about calling extractions via web services, see [Web API](../../web-api/#run-extractions).
1. To test the connection, enter your authentication details and click **[Send]** . If the connection is successful, the http response contains the SAP customer data extracted by Xtract Universal .
1. Click to edit the structure (names and data types) of the http response .\
   The structure is used when loading data into your destination. This example scenario only extracts the KNA1 columns *City_ORT01*, *Name 1_NAME1*, *Country Key_LAND1* and *Customer Number_KUNNR*.
1. Optional: If your extraction uses parameters, open the *Parameters* tab and define the parameters.
1. Click **[Save]** to save the connector.

The custom connector can now be used in a Matillion Data Loader pipeline.

Note

The Matillion Custom Connector must be set to the same region as Matillion Data Loader, e.g., US (N. Virginia).

### Create a Pipeline in Matillion Data Loader

Create a pipeline that triggers the extraction and writes the data to a destination, see [Matillion Documentation: Create a pipeline with custom connectors](https://docs.matillion.com/data-productivity-cloud/custom-connector/docs/custom-connector-batch-pipeline/).

1. Open the [Matillion Data Loader dashboard](https://dataloader.matillion.com/dashboard).
1. Click **[Add Pipeline]** to create a new pipeline .
1. Open the *Custom Connectors* tap to select the custom connector , that contains the connection settings for Xtract Universal.
1. Select the endpoint that calls the Xtract Universal extraction and use the arrow buttons to add the endpoint to the list **Endpoints to extract and load**. Note that a custom connector can have multiple endpoints.
1. Click **[Continue with x endpoint]** .
1. In the *General* tab enter a name for the target table under **Data warehouse table name**.
1. Open the *Authentication* tab and enter the authentication details for the Xtract Universal webservice.
1. Open the *Behaviour* tab and select the elements you want to include as columns in the target table. By default, all elements are selected.
1. Optional: If your endpoint uses parameters, open the *Parameters* tab to define the parameters.
1. Open the *Keys* tab and select a key column that is used to match existing data and prevent duplicates, e.g., *Customer Number_KUNNR* .
1. Click **[Continue]** .
1. Select the destination to which the data is written to, e.g., Snowflake . For more information on how to connect to Snowflake, see [Matillion Documentation: Connect to Snowflake](https://docs.matillion.com/data-productivity-cloud/batch/docs/connect-to-snowflake/).
1. Configure the destination, see [Matillion Documentation: Configure Snowflake](https://docs.matillion.com/data-productivity-cloud/batch/docs/connect-to-snowflake/#configure-snowflake).
1. Click **[Continue]**.
1. Enter a name for the pipeline .
1. Select at which interval pipeline is to be executed . The pipeline first runs after it is created and then continues with the specified frequency.
1. Click **[Create pipeline]** to create and run the pipeline . The pipeline is now listed in your dashboard.
1. Check if the data was successfully uploaded to the destination.

The pipeline now runs automatically at the specified frequency.

______________________________________________________________________

#### Related Links

- [Matillion Documentation: Snowflake Destination](https://docs.matillion.com/data-productivity-cloud/batch/docs/set-up-snowflake/)
- [Matillion Documentation: Matillion Custom Connector Overview](https://docs.matillion.com/data-productivity-cloud/custom-connector/docs/custom-connector-overview/)
- [Matillion Documentation: Create a pipeline with custom connectors](https://docs.matillion.com/data-productivity-cloud/custom-connector/docs/custom-connector-batch-pipeline/).

This article shows how to create generic DataSources for delta extractions in SAP using a function module and timestamps.

### About Delta Functionality with Delta fields

To use the delta functionality, a delta field is required. Certain tables like VBAK (Sales Document: Header Data) do not have a timestamp field for creation/change that can be uses as a unique delta field, but they have separate fields for creation date (**ERDAT**), creation time (**ERZET**) and change date (**AEDAT**). To get the delta data of the VBAK table, we create a generic DataSource using a custom function module that implements the necessary logic.

This article leads you through all necessary steps to create an extraction structure that has a timestamp field that can be used to implement the delta functionality.

There are two template function modules that can be copied and used:

- RSAX_BIW_GET_DATA_SIMPLE: A function module with simple interface for *Full Load* with no support of delta loads.
- RSAX_BIW_GET_DATA: : A function module with complete interface that supports *Delta Load*.

### Step 1: Create an Extract Structure

Follow the steps below to create the extract structure for the DataSource:

1. Use SAP transaction SE11 to create a new structure ZZVBAK.
1. Insert the table VBAK as an include into the structure.
1. Add a field ZTMSTMP(Data element: TZNTSTMPS, it is of datatype DEC with Length 15). This field holds the timestamp and allow us to use the extraction for delta purposes.
1. Save and activate the structure.

### Step 2: Create the Function Module

1. Use SAP transaction SE80 to copy the function group RSAX to the new function group Z_RSAX and to copy the function module RSAX_BIW_GET_DATA to Z_RSAX_BIW_GET_DATA_VBAK.

1. Be sure to copy and activate all the related objects (interfaces, datatypes etc.).

1. Use SAP transaction SE37 to open and edit the function module Z_RSAX_BIW_GET_DATA_VBAK. In the tab *Tables*, set the parameter E_T_DATA to associated type ZZVBAK.

1. Navigate to the tab *source code* and paste the following ABAP Code.

   ABAP Code for the Custom Function Module

   ```ABAP
   FUNCTION Z_RSAX_BIW_GET_DATA_VBAK.
   *"----------------------------------------------------------------------
   *"*"Local Interface:
   *"  IMPORTING
   *"     VALUE(I_REQUNR) TYPE  SBIWA_S_INTERFACE-REQUNR
   *"     VALUE(I_ISOURCE) TYPE  SBIWA_S_INTERFACE-ISOURCE OPTIONAL
   *"     VALUE(I_MAXSIZE) TYPE  SBIWA_S_INTERFACE-MAXSIZE OPTIONAL
   *"     VALUE(I_INITFLAG) TYPE  SBIWA_S_INTERFACE-INITFLAG OPTIONAL
   *"     VALUE(I_UPDMODE) TYPE  SBIWA_S_INTERFACE-UPDMODE OPTIONAL
   *"     VALUE(I_DATAPAKID) TYPE  SBIWA_S_INTERFACE-DATAPAKID OPTIONAL
   *"     VALUE(I_PRIVATE_MODE) OPTIONAL
   *"     VALUE(I_CALLMODE) LIKE  ROARCHD200-CALLMODE OPTIONAL
   *"     VALUE(I_REMOTE_CALL) TYPE  SBIWA_FLAG DEFAULT SBIWA_C_FLAG_OFF
   *"  TABLES
   *"      I_T_SELECT TYPE  SBIWA_T_SELECT OPTIONAL
   *"      I_T_FIELDS TYPE  SBIWA_T_FIELDS OPTIONAL
   *"      E_T_ZZVBAK STRUCTURE  ZZVBAK OPTIONAL
   *"  EXCEPTIONS
   *"      NO_MORE_DATA
   *"      ERROR_PASSED_TO_MESS_HANDLER
   *"----------------------------------------------------------------------


   * THE INPUT PARAMETER I_DATAPAKID IS NOT SUPPORTED YET !


   * EXAMPLE: INFOSOURCE CONTAINING TADIR OBJECTS
   *  TABLES: TADIR.


   * AUXILIARY SELECTION CRITERIA STRUCTURE
     DATA: L_S_SELECT TYPE SBIWA_S_SELECT,
           STARTDATE LIKE SY-DATUM,
           STARTTIME LIKE SY-UZEIT,
           ENDDATE LIKE SY-DATUM,
           ENDTIME LIKE SY-UZEIT,
           TSTAMP LIKE TZONREF-TSTAMPS.


   * MAXIMUM NUMBER OF LINES FOR DB TABLE
     STATICS: L_MAXSIZE TYPE SBIWA_S_INTERFACE-MAXSIZE.


   * SELECT RANGES
     RANGES:  L_R_TMPSTMP FOR ZZVBAK-ZTMSTMP.


   * PARAMETER I_PRIVATE_MODE:
   * SOME APPLICATIONS MIGHT WANT TO USE THIS FUNCTION MODULE FOR OTHER
   * PURPOSES AS WELL (E.G. DATA SUPPLY FOR OLTP REPORTING TOOLS). IF THE
   * PROCESSING LOGIC HAS TO BE DIFFERENT IN THIS CASE, USE THE OPTIONAL
   * PARAMETER I_PRIVATE_MODE (NOT SUPPLIED BY BIW !) TO DISTINGUISH
   * BETWEEN BIW CALLS (I_PRIVATE_MODE = SPACE) AND OTHER CALLS
   * (I_PRIVATE_MODE = X).
   * IF THE MESSAGE HANDLING HAS TO BE DIFFERENT AS WELL, DEFINE YOUR OWN
   * MESSAGING MACRO WHICH INTERPRETS PARAMETER I_PRIVATE_MODE. WHEN
   * CALLED BY BIW, IT SHOULD USE THE LOG_WRITE MACRO, OTHERWISE DO WHAT
   * YOU WANT.


   * INITIALIZATION MODE (FIRST CALL BY SAPI) OR DATA TRANSFER MODE
   * (FOLLOWING CALLS) ?
     IF I_INITFLAG = SBIWA_C_FLAG_ON.

   ************************************************************************

   * INITIALIZATION: CHECK INPUT PARAMETERS
   *                 BUFFER INPUT PARAMETERS
   *                 PREPARE DATA SELECTION
   ************************************************************************


   * THE INPUT PARAMETER I_DATAPAKID IS NOT SUPPORTED YET !


   * INVALID SECOND INITIALIZATION CALL -> ERROR EXIT
       IF NOT G_FLAG_INTERFACE_INITIALIZED IS INITIAL.

         IF 1 = 2. MESSAGE E008(R3). ENDIF.
         LOG_WRITE 'E'                    "MESSAGE TYPE
                   'R3'                   "MESSAGE CLASS
                   '008'                  "MESSAGE NUMBER
                   ' '                    "MESSAGE VARIABLE 1
                   ' '.                   "MESSAGE VARIABLE 2
         RAISE ERROR_PASSED_TO_MESS_HANDLER.
       ENDIF.


   * CHECK INFOSOURCE VALIDITY
       CASE I_ISOURCE.
         WHEN 'ZDSVBAK' OR ''.
         WHEN OTHERS.
           IF 1 = 2. MESSAGE E009(R3). ENDIF.
           LOG_WRITE 'E'                  "MESSAGE TYPE
                     'R3'                 "MESSAGE CLASS
                     '009'                "MESSAGE NUMBER
                     I_ISOURCE            "MESSAGE VARIABLE 1
                     ' '.                 "MESSAGE VARIABLE 2
           RAISE ERROR_PASSED_TO_MESS_HANDLER.
       ENDCASE.


   * CHECK FOR SUPPORTED UPDATE MODE
       CASE I_UPDMODE.
         WHEN 'F' OR ''.
         WHEN 'C'.  "
         WHEN 'R'.  "
         WHEN 'S'.  " DELTA INITIALIZATION
         WHEN 'I'.    " DELTA INIT FOR NON CUMMULATIVE
         WHEN 'D'.   " DELTA UPDATE
         WHEN OTHERS.
           IF 1 = 2. MESSAGE E011(R3). ENDIF.
           LOG_WRITE 'E'                  "MESSAGE TYPE
                     'R3'                 "MESSAGE CLASS
                     '011'                "MESSAGE NUMBER
                     I_UPDMODE            "MESSAGE VARIABLE 1
                     ' '.                 "MESSAGE VARIABLE 2
           RAISE ERROR_PASSED_TO_MESS_HANDLER.
       ENDCASE.


   * CHECK FOR OBLIGATORY SELECTION CRITERIA
   *    READ TABLE I_T_SELECT INTO L_S_SELECT WITH KEY FIELDNM = 'ZTMSTMP'.
   *    IF SY-SUBRC <> 0.
   *      IF 1 = 2. MESSAGE E010(R3). ENDIF.
   *      LOG_WRITE 'E'                    "MESSAGE TYPE
   *                'R3'                   "MESSAGE CLASS
   *                '010'                  "MESSAGE NUMBER
   *                'PGMID'                "MESSAGE VARIABLE 1
   *                ' '.                   "MESSAGE VARIABLE 2
   *      RAISE ERROR_PASSED_TO_MESS_HANDLER.
   *    ENDIF.

       APPEND LINES OF I_T_SELECT TO G_T_SELECT.


   * FILL PARAMETER BUFFER FOR DATA EXTRACTION CALLS
       G_S_INTERFACE-REQUNR    = I_REQUNR.
       G_S_INTERFACE-ISOURCE   = I_ISOURCE.
       G_S_INTERFACE-MAXSIZE   = I_MAXSIZE.
       G_S_INTERFACE-INITFLAG  = I_INITFLAG.
       G_S_INTERFACE-UPDMODE   = I_UPDMODE.
       G_S_INTERFACE-DATAPAKID = I_DATAPAKID.
       G_FLAG_INTERFACE_INITIALIZED = SBIWA_C_FLAG_ON.


   * FILL FIELD LIST TABLE FOR AN OPTIMIZED SELECT STATEMENT
   * (IN CASE THAT THERE IS NO 1:1 RELATION BETWEEN INFOSOURCE FIELDS
   * AND DATABASE TABLE FIELDS THIS MAY BE FAR FROM BEEING TRIVIAL)
       APPEND LINES OF I_T_FIELDS TO G_T_SEGFIELDS.

     ELSE.                 "INITIALIZATION MODE OR DATA EXTRACTION ?

   ************************************************************************

   * DATA TRANSFER: FIRST CALL      OPEN CURSOR + FETCH
   *                FOLLOWING CALLS FETCH ONLY
   ************************************************************************


   * FIRST DATA PACKAGE -> OPEN CURSOR
       IF G_COUNTER_DATAPAKID = 0.
         "LOOP AT I_T_SELECT INTO L_S_SELECT WHERE FIELDNM = 'ZTMSTMP'.
          LOOP AT G_T_SELECT INTO L_S_SELECT WHERE FIELDNM = 'ZTMSTMP'.

           TSTAMP = L_S_SELECT-LOW.
           CONVERT TIME STAMP TSTAMP TIME ZONE SY-ZONLO INTO DATE STARTDATE TIME STARTTIME.
           TSTAMP = L_S_SELECT-HIGH.
           CONVERT TIME STAMP TSTAMP TIME ZONE SY-ZONLO INTO DATE ENDDATE TIME ENDTIME.
         ENDLOOP.

   * FILL RANGE TABLES FOR FIXED INFOSOURCES. IN THE CASE OF GENERATED
   * INFOSOURCES, THE USAGE OF A DYNAMICAL SELECT STATEMENT MIGHT BE
   * MORE REASONABLE. BIW WILL ONLY PASS DOWN SIMPLE SELECTION CRITERIA
   * OF THE TYPE SIGN = 'I' AND OPTION = 'EQ' OR OPTION = 'BT'.
   *      LOOP AT G_T_SELECT INTO L_S_SELECT WHERE FIELDNM = 'PGMID'.
   *        MOVE-CORRESPONDING L_S_SELECT TO L_R_PGMID.
   *        APPEND L_R_PGMID.
   *      ENDLOOP.


   *      LOOP AT G_T_SELECT INTO L_S_SELECT WHERE FIELDNM = 'OBJECT'.
   *        MOVE-CORRESPONDING L_S_SELECT TO L_R_OBJECT.
   *        APPEND L_R_OBJECT.
   *      ENDLOOP.


   * DETERMINE NUMBER OF DATABASE RECORDS TO BE READ PER FETCH STATEMENT
   * FROM INPUT PARAMETER I_MAXSIZE. IF THERE IS A ONE TO ONE RELATION
   * BETWEEN INFOSOURCE TABLE LINES AND DATABASE ENTRIES, THIS IS TRIVIAL.
   * IN OTHER CASES, IT MAY BE IMPOSSIBLE AND SOME ESTIMATED VALUE HAS TO
   * BE DETERMINED.
         L_MAXSIZE = G_S_INTERFACE-MAXSIZE.
         IF ENDDATE <> '00000000' AND ENDTIME <> '000000'.
           OPEN CURSOR WITH HOLD G_CURSOR FOR
       SELECT * FROM VBAK
         WHERE
         (
           ( ERDAT >= STARTDATE AND ERZET >= STARTTIME AND ERDAT <= ENDDATE AND ERZET <= ENDTIME )
           OR ( AEDAT >= STARTDATE AND  AEDAT <= ENDDATE )
         ).
         ELSE.
           OPEN CURSOR WITH HOLD G_CURSOR FOR
            SELECT * FROM VBAK.
         ENDIF.
       ENDIF.                             "FIRST DATA PACKAGE ?


   * FETCH RECORDS INTO INTERFACE TABLE. THERE ARE TWO DIFFERENT OPTIONS:
   * - FIXED INTERFACE TABLE STRUCTURE FOR FIXED INFOSOURCES HAVE TO BE
   *   NAMED E_T_'NAME OF ASSIGNED SOURCE STRUCTURE IN TABLE ROIS'.
   * - FOR GENERATING APPLICATIONS LIKE LIS AND CO-PA, THE GENERIC TABLE
   *   E_T_DATA HAS TO BE USED.
   * ONLY ONE OF THESE INTERFACE TYPES SHOULD BE IMPLEMENTED IN ONE API !
       FETCH NEXT CURSOR G_CURSOR
                  APPENDING CORRESPONDING FIELDS
                  OF TABLE E_T_ZZVBAK
                  PACKAGE SIZE 1000.
   * PACKAGE SIZE L_MAXSIZE.

       IF SY-SUBRC <> 0.
         CLOSE CURSOR G_CURSOR.
         RAISE NO_MORE_DATA.
       ENDIF.

       G_COUNTER_DATAPAKID = G_COUNTER_DATAPAKID + 1.

     ENDIF.              "INITIALIZATION MODE OR DATA EXTRACTION ?

   ENDFUNCTION.

   ```

1. Save and activate the function module.

### Step 3: Create the DataSource

1. Use SAP transaction RSO2 to create a new DataSource for transaction data and name it to ZDSVBAK.

1. Set the Application component and the description texts.

1. Click **[Extraction by FM]**. Enter the name of the function module Z_RSAX_BIW_GET_DATA_VBAK and the extract structure ZZVBAK.

1. Click **[Generic Delta]**. Select the timestamp field ZTMSTMP and activate the option **[Time stamp]**.

1. Optional: set the *Safety Interval Lower Limit*.

1. Click **[Save]** twice. In the following screen you can set the selection fields. The timestamp field is disabled, because it is automatically populated as part of the delta process.

1. Use SAP transaction RSA2 to see the details of our DataSource ZDSVBAK. The extraction method is set to *F2 (Simple Interface)*. Change it to *F1 (Complete Interface)* by executing the following ABAP code.

   Tip

   Use SAP transaction SE38 to create a new report with this ABAP code. Unfortunately this is not possible in the GUI.

   ```ABAP
   REPORT ZABAPDEMO
   UPDATE roosource
   SET delta = 'E'
   exmethod = 'F1'
   genflag = 'X'
   WHERE oltpsource = 'ZDSVBAK'

   ```

1. Use SAP transaction RSA2 to display the status of the DataSource ZDSVBAK. Confirm that the **Extraction Mode** is set to *F1*.

1. Check for errors.

1. Optional: use SAP transaction SE37 and call it twice to test the function module. The first call is for the initialization and the second call reads the data. Make sure that the table E_T_DATA contains the data.

### Step 4: Test the DataSource

Use SAP transaction RSA3 to test the datasource.

Now the DataSource is created and you can use the DeltaQ extraction type to read it. Be sure to activate the DataSource using the **[Activate]** button in the main window of the extraction type.

### Result

The DataSource supports the *Full* and *Update Delta* mode. To use the*Update Delta* mode, the first call must have the update type *C (Delta Initialization)*. All following calls must have the update type *D (Delta Update)*.

The delta process of the DataSource can be monitored and maintained in SAP transaction RSA7 (Delta Queue).

This article shows how to use the SAP Gateway Builder to create OData services that provide ODP based data for the [OData extraction type](../../documentation/odata/) of Xtract Universal.

Note

This article focuses on SAP on-premise systems.\
For information on how to create OData services in the SAP cloud, refer to [SAP Help: Setting up Communication Management](https://learning.sap.com/learning-journeys/implement-sap-s-4hana-cloud-public-edition-for-sourcing-and-procurement/setting-up-communication-management_a913171c-c96d-47a9-81ec-dc9ee8754320).

### Prerequisites

- SAP NetWeaver Gateway Release 2.0 Support Package 4 or higher. In SAP S/4HANA, the Gateway is embedded, but needs to be activated.
- SAP user with access to the SAP Gateway Service Builder.
- To extract data using the BW InfoProvider (ODP context BW), an open hub license is required.
- Optional: to transport services from one SAP system to another, e.g., from a test environment to a production environment, authorization for CTS (Change and Transport System) is required.

Tip

To test services directly in SAP, activate the SAP Gateway Client via transaction /IWFND/GW_CLIENT.

### Workflow

The creation of OData services can be divided into the following phases:

1. **Service Definition Phase:**\
   Create a project to bundle all artifacts that are needed to develop a service.
1. **Data Model Definition Phase:**\
   Define the model that the service is based on, e.g., entity types, associations, etc.).
1. **Service Implementation Phase:**\
   Implement the operations that are supported by the service.
1. **Service Maintenance Phase:**\
   Register and activate the service in an SAP Gateway system.

### Service Definition

Follow the steps below to create a project that bundles all artifacts necessary to develop a service:

1. Log in to the SAP GUI.
1. Go to transaction SEGW. The SAP Gateway Builder opens.
1. Click **Create Project** ().
1. Enter a name and description for the OData service.
1. In the field **Project Type**, select *Service with SAP Annotations*.
1. If you want to transport the service to another SAP system, define a transport package.\
   Otherwise, enter *$TMP* (temporary objects).
1. Click **Continue** () to create the project.

The new project is listed in the main window of the SAP Gateway Builder.

### Data Model Definition

Follow the steps below to define the model the service is based on:

1. In the main window of the SAP Gateway Builder, select your project and navigate to the *DataModel* directory of the project.

1. Right-click the *DataModel* directory and select **Redefine > ODP Extraction** from the context menu.

   The window "Wizard step 1: OData access for Operational Data Provisioning" opens.

1. In the field **ODP Context**, select a provider context that can access the SAP data source you want to extract. For more information, see [Documentation: Provider Context](../../documentation/odp/provider-context/). The depicted example uses the *DataSources/Extractors* context.

1. In the field **ODP Name**, select an existing ODP object. The depicted example uses the *0CUSTOMER_ATTR* extractor.

1. Click **[Add ODP]** to add the ODP object to the data model.

   Tip

   An OData service can access multiple ODP objects.

1. Click **Next** to continue on to step 2 of the Wizard.

1. Fill out any missing information and click **Next**. The window "Create Object Directory Entry" opens.

1. If you want to transport the service to another SAP system, define a transport package. If not, click **Save** () to continue on to step 3 of the Wizard.

1. Use the file tree menu to select the entities and artifacts you want to add to the service.

   Note

   - *EntityOf* defines the structure of your data. Select the fields you want to add to the service.
   - *DeltaLinksOfEntity* contains delta links that are required to retrieve only changes (new, updated, or deleted records) to the dataset instead of fetching the entire dataset. Select all items to activate delta updates.
   - *SubscribedToEntity* checks whether a client is subscribed to tracking delta changes. Select this artifact to activate delta updates.
   - *TerminateDeltasForEntity* allows the client to stop tracking delta changes. Select this artifact to clean up unnecessary delta subscriptions.
   - *entityOf* defines the relationship between entities in the OData services, e.g., it links the main entity (*EntityOf*) with its delta-tracking entity (*DeltaLinksOfEntity*).

1. Click **Finish** ().

The subdirectories of the *DataModel* directory now contain the selected entities and artifacts.

### Service Implementation

When using the Wizard during the [Data Model Definition](#data-model-definition), the service implementation is auto-generated based on the underlying ODP framework.

The following use cases require manual adjustments:

- Modify how data is retrieved, e.g., to filter or transform the data before returning it to the client.
- Add custom business logic, e.g., to trigger additional actions when data is created/updated.
- Expose custom function imports, e.g., to execute a custom SAP function.

### Generate the Service

Before the service can be activated, follow the steps below to generate the runtime objects of the service:

1. In the main window of the SAP Gateway Builder, select your project.
1. Click **Generate Runtime Objects** (). The window "Model and Service Definition" opens.
1. Click **Continue** (). The window "Create Object Directory Entry" opens.
1. If you want to transport the service to another SAP system, define a transport package. If not, click **Save** ().

The runtime objects for the service are now generated.

### Service Maintenance

Follow the steps below to register and activate the service:

1. Go to transaction /n/IWFND/MAINT_SERVICE. The "Activate and Maintain Services" menu opens.

1. Click **Add Service**. The menu "Add Selected Services" opens.

1. In the field **System Alias**, select *LOCAL*.

1. Click **Get Services** to display a list of all services on your SAP system.

1. Select the service you want to register.

   Tip

   Use **Find** () to look up the service.

1. Click **Add Selected Services**. The window "Add Service" opens.

1. Check the service information and click **Continue** (). The window "Add Service" closes.

1. Exit the "Add Selected Services" menu and check if the service is now listed in the service catalog.

The service can now be consumed by Xtract Universal's OData extraction type.

______________________________________________________________________

#### Related Links

- [SAP Help: SAP Gateway Service Builder](https://help.sap.com/docs/SAP_NETWEAVER_AS_ABAP_751_IP/68bf513362174d54b58cddec28794093/cddd22512c312314e10000000a44176d.html)
- [SAP Learning: Building OData Services with SAP Gateway](https://learning.sap.com/learning-journeys/building-odata-services-with-sap-gateway)
- [SAP Help: ODP-Based Data Extraction via OData](https://help.sap.com/doc/saphelp_nw75/7.5.5/en-US/11/853413cf124dde91925284133c007d/frameset.htm)

The following article shows how to create a client [PSE (Personal Security Environment)](https://help.sap.com/saphelp_nw73/helpdata/en/4c/61a6c6364012f3e10000000a15822b/frameset.htm) that can be used to connect to SAP cloud systems via WebSocket RFC.

### Prerequisites

- SAP Cloud API URL, e.g., `https://my123456-api.s4hana.ondemand.com`.
- [Download the SAP Cryptographic Library](https://help.sap.com/doc/saphelp_em900/9.0/en-US/48/a324e7ccfc062de10000000a42189d/frameset.htm) (sapgenpse.exe and sapcrypto.dll) from the SAP Service Marketplace.

### Creating a Client PSE

Follow the steps below to create a client PSE file that trusts the server certificate of the SAP cloud system.

1. Enter the SAP Cloud API URL in a browser of your choice.

1. View the certificate in the browser.

   Navigate to **View site information > Connection is secure > Certificate is valid**.

   Click the pad lock icon left of the URL, navigate to **Connection secure > More information**, then click **[View Certificate]**.

1. Download the certificate chain from the browser. The certificate chain contains all certificates that are signed by the server certificate.

   Open the *Details* tab and click **[Export...]**.\
   Make sure to save the file in the format *Base64-encoded ASCII, certificate chain (\*.pem;\*.crt)*.

   Scroll to the *Miscellaneous* section of the certificate and in the download row, click *PEM (chain)*.

1. Use the sapgenpse tool to create a client PSE file:

   ```text
   sapgenpse.exe gen_pse -p client.pse -v [Distinguished name]

   ```

   - Replace `[Distinguished name]` with the distinguished name of the server that runs the Xtract product, e.g., `"CN=COMPUTER.theobald.local, C=DE, S=BW, O=TS, OU=DEV"`.
   - Optionally, replace `client.pse` with a custom file name for the .pse file.
   - The tool creates its own repository in a standard path, unless the path is changed by the environment variable SECUDIR or by specifying an absolute path.

   Warning

   **Restricted Access**\
   The PSE must be created without a password/pin, otherwise reading is not possible. Make sure not to secure the PSE.

1. Use the following command to add the certificate chain from step 3 to the client PSE:

   ```text
   sapgenpse.exe maintain_pk -a <[chain.pem]> -p <client.pse>

   ```

   Replace `[chain.pem]` with the name of the downloaded .pem file, e.g., `s4hana-cloud-sap-chain.pem`. For more information on how to use the sapgenpse.exe, run the command `sapgenpse -h`.

The .pse file can now be used to connect Xtract Universal to the SAP cloud.

______________________________________________________________________

#### Related Links

- [Knowledge Base: Access Data in the SAP Public Cloud](../access-data-in-the-sap-public-cloud/)

The following article shows how to create the Z_TS_PROG authorization object for the custom function module The Theobald Software custom function module [Z_XTRACT_IS_REMOTE_REPORT](../../documentation/setup-in-sap/custom-function-module-for-reports/) enables the extractions of reports from SAP systems.

The Theobald Software custom function module Z_XTRACT_IS_REMOTE_REPORT enables the extractions of reports from SAP systems. If no authorization group is assigned to a report, Z_XTRACT_IS_REMOTE_REPORT uses a custom authorization object Z_TS_PROG to verify whether the SAP user is allowed to extract a report. The access to reports is granted based on the name of the report.

### Create the Custom Authorization Object Z_TS_PROG

1. Use transaction SU21 to create a new authorization object.
1. Expand the *Create* menu and click **[Authorization Object]**. The window "Create Authorization Object" opens.
1. Enter the following values:\
   **Object:** Z_TS_PROG\
   **Text:** Theobald Software Report Authorization
1. Click **[Continue]** to enable editing of the section *Authorization fields*.
1. Manually enter S_NAME as the first entry in *Authorization fields*.
1. Click **[Save]** to save the authorization object.

### Configure the Custom Authorization Object Z_TS_PROG

1. Either download the provided [SAP role](../../documentation/setup-in-sap/sap-authority-objects/#sap-authorization-profiles) or manually [create the Custom Authorization Object Z_TS_PROG](#create-the-custom-authorization-object-z_ts_prog).
1. Enter the change mode within the "Role Maintenance" window to configure the object.
1. Navigate to *Authorizations* and click *Change Authorization Data* (). The window "Change Role: Authorizations" opens.
1. Expand the tree view **Basis: Administration > Theobald Software authorization for reports**.
1. Click **[]**. The window "Field values" opens.
1. Enter the names of relevant ABAP reports and transfer the corresponding values.

______________________________________________________________________

#### Related Links

- [Authorize Access to Reports via Authorization Groups](../authorize-access-to-specific-reports/)
- [SAP Authorization Objects for Reports](../../documentation/setup-in-sap/sap-authority-objects/#report)

This article shows how to opt-out of the usage data collection by Xtract Universal.

### About Usage Analytics

Usage data is collected to improve Xtract Universal in a way that benefits our customers, see [Benefits of the Usage Analytics Feature](https://theobald-software.com/en/xu-beyond/). Typical usage data include the source type and extraction types that are in use, referenced SAP objects, statistics about axtraction runs, etc. For more information, refer to our [Privacy Policy](https://theobald-software.com/en/privacy-policy/#productanalytics).

### Opt-Out

1. Open the **Server Settings**.
1. Open the *Web Server* tab.
1. Deactivate the following checkboxes:
   - [Collect Usage Data](../../documentation/server/server-settings/#collect-usage-data)
   - [Upload Usage Data](../../documentation/server/server-settings/#upload-usage-data)
   - [Transfer SAP Object Names](../../documentation/server/server-settings/#transfer-sap-object-names)

You can opt-in again by reactivating the checkboxes.

The following article shows how to load data incrementally (daily) from an SAP Table based on date fields. The depicted example scenarios use the table VBAK (SAP Sales Document Header), which has two date fields:

- ERDAT for creation date
- AEDAT for update date

### Extract Data using a Date Parameter

The depicted example extracts data that was created or changed after a specific date. The date is provided as a parameter at runtime.

1. Create a new Table extraction.

1. Look up a table you want to extract data from, e.g., VBAK.

1. Open the WHERE clause tab of theTable extraction type and enter the following criterion:

   ```text
   ( VBAK~ERDAT GE @LastDate AND VBAK~AEDAT EQ '00000000' ) OR VBAK~AEDAT GE @LastDate

   ```

   This criterion extracts data if one of the following conditions is true:

   - The data was created (ERDAT) after the date provided by the parameter `@LastDate` and it has not been changed (AEDAT).
   - The data has changed (ARDAT) after the date provided by the parameter `@LastDate`.

1. Click **[OK]** to confirm your input.

1. Open the *Run Extraction* menu and navigate to the *Custom* tab for runtime parameters.

1. Enter a value for the runtime parameter `@LastDate` in the format `YYYYmmDD`.

1. Click **[Run]** and check the results.

### Daily Data Extraction

The depicted example extracts data that was created or changed the day before. The depicted example uses script expressions to query the current date.

1. Create a new Table extraction.

1. Look up a table you want to extract data from, e.g., VBAK.

1. Open the WHERE clause tab of the Table component and enter the following criterion:

   ```text
   (ERDAT >= '#{ DateTime.Now.AddDays(-1).ToString("yyyyMMdd") }#' AND AEDAT = '00000000') OR AEDAT >= '#{ DateTime.Now.AddDays(-1).ToString("yyyyMMdd") }#'` <br>

   ```

   This criterion extracts data if one of the following conditions is true:

   - The data was created (ERDAT) the day before the current date and it has not been changed (AEDAT).
   - The data has changed (ARDAT) the day before the current date.

1. Click **[OK]** to confirm your input.

1. Run the extraction.

The extraction can be scheduled every night at 1p.m. or later to extract all changes of the day before. Providing extraction dates is not necessary.

______________________________________________________________________

#### Related Links

- [Table: WHERE Clause](../../documentation/table/where-clause/)

The following article describes versioning of newly developed extractions using [Git](https://gitforwindows.org/).

Versioning ensures a seamless synchronization of new developments on a test environment with the productive system. The implementation of various [Git](https://gitforwindows.org/) security techniques ensures an error-free transfer without compromising the production system.

Note

Target audience: Customers who use a production and a development environment for SAP data replications.

### Prerequisites

- Technically separate development and production environment.
- Developers have [read-only access](../../documentation/access-restrictions/) to Xtract Universal on the production system.
- Git client installed locally on the development environment and the production environment, e.g., [Fork](https://fork.dev).
- The Xtract Universal installation must have the identical release version on the servers.

### Set Up a new Environment

1. Create a new Git repository. How to do this depends on the technology you use, e.g.:
   - [GitHub](https://docs.github.com/en/get-started/quickstart/create-a-repo)
   - [Azure DevOps](https://docs.microsoft.com/en-us/azure/devops/repos/git/create-new-repo?view=azure-devops)
   - [Git-scm](https://git-scm.com/book/en/v2/Git-on-the-Server-Setting-Up-the-Server)
1. Clone the repository into the installation directory of Xtract Universal, e.g., `C:\Program Files\XtractUniversal`.\
   The repository directory **must** be named `config`!
1. Use dedicated branches for test and development environments and the main branch for the production system.

Tip

The initial initialization of the Git version control requires an empty `config` folder. For this reason, the existing `config` folder must first be renamed and then filled with the configuration files.

### Set Up an existing Environment

1. Create a new Git Repository. How to do this depends on the technology you use, e.g.:
   - [GitHub](https://docs.github.com/en/get-started/quickstart/create-a-repo)
   - [Azure DevOps](https://docs.microsoft.com/en-us/azure/devops/repos/git/create-new-repo?view=azure-devops)
   - [Git-scm](https://git-scm.com/book/en/v2/Git-on-the-Server-Setting-Up-the-Server)
1. Initialize a local repository in the installation directory of Xtract Universal, e.g., `C:\Program Files\XtractUniversal\config` using the command `git init`.
1. Attach the remote repository to your local one using the command `git remote add origin [ssh/https]://git-server-address/path/to/repo.git`.
1. Commit the current `config` folder.

### Advantage & Usage of Versioning

- By using the main branch for the production environment and development branches for the development or test environments, the former is separated from the latter.
- Applying appropriate and established techniques such as [pull requests](https://www.git-scm.com/docs/git-request-pull) and Git user rights allows the changes to be checked and corrected in advance.
- New developments can **not** cause fundamental damage to the data load of the productive landscape with this approach.
- Versioning enables quick and easy rollbacks of changes by reverting them in Git and is agnostic towards the use of containers (such as Docker).

Note

This scenario can also be implemented with Azure DevOps and Git. For more information, see [Microsoft Documentation](https://docs.microsoft.com/en-us/azure/devops/repos/?view=azure-devops).

______________________________________________________________________

#### Releated Links

- [GitForWindows Download](https://gitforwindows.org/)
- [Manual Migration to a Different Machine](../../documentation/setup/migration/)
- [Getting Started - About Version Control](https://git-scm.com/book/en/v2/Getting-Started-About-Version-Control)
- [Git Basics - Getting a Git Repository](https://git-scm.com/book/en/v2/Git-Basics-Getting-a-Git-Repository)
- [Git Branching - Branches in a Nutshell](https://git-scm.com/book/en/v2/Git-Branching-Branches-in-a-Nutshell)
- [Distributed Git - Distributed Workflows](https://git-scm.com/book/en/v2/Distributed-Git-Distributed-Workflows)
- [Git Basics - Undoing Things](https://git-scm.com/book/en/v2/Git-Basics-Undoing-Things)

The following article describes how user-defined runtime parameters in Xtract Universal can be transferred to the SAP Reader (Theobald Software) of a KNIME workflow. This reduces transaction costs and improves the use of a delta-mechanism on standard SAP tables.

The depicted example uses the field *AEDAT* (Changed On) of the standard table *EKKO* (Purchasing Document Header). A runtime parameter in KNIME is used to extract and process only the entries from the SAP table EKKO that changed since the last run.

### Requirements

Basic knowledge of T-SQL, KNIME Analytics Platform and the creation of table extractions in Xtract Universal is required.\
The following BI architecture must be available and configured:

- Latest version of the [KNIME Analytics Platform](https://www.knime.com/downloads).
- Installed KNIME extension [SAP Reader (Theobald Software)](https://hub.knime.com/knime/extensions/org.knime.features.sap.theobald/latest/org.knime.sap.theobald.node.v2.SAPTheobaldReader2NodeFactory).
- Existing table object in a Microsoft database (SQL-Server).
- Latest version of Xtract Universal, obtained from the [Theobald Software website](https://theobald-software.com/en/download-trial).
- Existing table extraction of table *EKKO - Purchasing Document Header* in Xtract Universal, see [Define a Table Extraction](../../documentation/table/#define-the-table-extraction-type).
- Use of a WHERE condition with [runtime parameters](../../documentation/table/edit-runtime-parameters/) of the table extraction in Xtract Universal e.g.: `EKKO~AEDAT > @maxAEDAT`.

### KNIME Analytics Platform Workflow

1. Configuration of the KNIME node **Microsoft SQL Server Connector**:

1. Configuration of the KNIME node **DB Table Selector**:\
   Use the following SQL statement for the table object *EKKO* in the **Custom Query** .

   ```sql
   SELECT MAX(REPLACE(AEDAT, '-', '')) AS maxAEDAT FROM #table#

   ```

1. Use the KNIME node **DB Reader** to read the result value of the Custom Query `maxAEDAT` and propagate the data to all connected nodes.

1. Configuration of the KNIME node **Table Row to Varaiable** :

1. Right-click on the node **SAP Reader (Theobald Software)** to display the flow variable ports.

1. Configuration of the KNIME node **SAP Reader (Theobald Software)**, see [KNIME Integration via SAP Reader (Theobald Software)](../../documentation/destinations/knime/#knime-integration-via-sap-reader):

1. In the **Parameters** tab, use **Fetch Parameters** to add the user-defined variable to the WHERE condition and enter a default value .

1. In the **Flow Variables** tab, select the variable `maxAEDAT` in the **Custom Parameters** section using the drop-down menu .

1. Pass the results of the extraction in the workflow to the KNIME node **DB Writer** and execute the workflow.

Tip

Check the correct execution of the extraction with user-defined runtime parameters in the [Extraction Log](../../documentation/logs/#read-extraction-logs) of Xtract Universal.

______________________________________________________________________

#### Releated Links

- [Documentation: KNIME Destination](../../documentation/destinations/knime/)
- [Read or download documentation for KNIME Software](https://docs.knime.com/)
- [KNIME Flow Control Guide](https://docs.knime.com/2021-06/analytics_platform_flow_control_guide/index.html#introduction)

The following article describes how to establish an SNC connection to an SAP source system.

The depicted approach uses an X.509 certificate that provides the logon data of the Windows AD user. The correctness of this X.509 certificate is ensured via the company's internal certification authority (ca).

### Workflow

1. Upon connection start, the Secure Login Client retrieves the SNC name from the SAP NetWeaver AS ABAP.
1. The Secure Login Client uses the authentication profile for this SNC name.
1. The user unlocks the security token, for example, by entering the PIN or password.
1. The Secure Login Client receives the X.509 certificate from the user security token.
1. The Secure Login Client provides the X.509 certificate for single sign-on and secure communication between SAP GUI or Web GUI and the AS ABAP.
1. The user is authenticated and the communication is secured.

Tip

The configuration of the X.509 certificate should be implemented by the network & SAP Basis team and requires basic knowledge in this area.

### Requirements

The following system settings are a prerequisite for using this SNC solution:

- Install the [Secure Login Client](https://help.sap.com/viewer/df185fd53bb645b1bd99284ee4e4a750/3.0/en-US/da610fd072e4409baa8b6a96973b5c67.html).
- The SAP application server is configured and activated for Secure Network Communication (SNC).
- The SNC standard library *sapcryptolib* is used as the SNC solution.
- The following SNC parameters are configured:

| SNC parameter | Value | Example | | --- | --- | --- | | snc/gssapi_lib | Path and file name where the SAP Cryptographic Library is located. | *$(DIR_EXECUTABLE)\\sapcrypto.dll* | | snc/identity/as | Application server's SNC name Syntax: p:\<Distinguished_Name> The Distinguished Name part must match the Distinguished Name that you specify when creating the SNC PSE. | *p:CN=saperp.theobald.local* |

### Step-by-Step Guide

Follow the steps below to set up the SNC connectivity ins SAP:

1. Generate the certificate for the application server and AD-user context from common Certificate Authority (ca).

   Note

   The X.509 certificate is available when placed in folder **Certmgr > Personal > Certificates** within Windows certificate store (user).

1. Convert the .pfx file to SAP PSE format e.g., `sapgenpse.exe import_p12 -p cert.pse cert.pfx`.

1. Import the created PSE file via **TA STRUST > Edit mode > PSE Import > PSE Save as SNC Libcrypto**.

1. Edit the SNC configuration of the corresponding SAP user via transaction SU01 > **SNC** > **SNC Name** = p:\<Full Distinguished_Name>, e.g., `p:EMAIL="RandomUser@domain",CN="Random User",OU="Users",OU="TheobaldSoftware",DC="theobald",DC="local"`.

1. Set up SNC authentication in the Xtract Universal SAP connection settings, see [Configure SNC in the SAP Source](../../documentation/sap-connection/snc-authentication/#configure-snc-in-the-sap-source).

______________________________________________________________________

#### Releated Links

- [Certificate Renewal](../certificate-renewal/)
- [SAP Help: Workflow with X.509 Certificate without Secure Login Server](https://help.sap.com/viewer/df185fd53bb645b1bd99284ee4e4a750/3.0/en-US/06d9e59a0fd44aa4aa082ffad7d618e3.html)
- [SAP Help: Secure Network Communications (SNC)](https://help.sap.com/doc/saphelp_nw70/7.0.31/en-us/e6/56f466e99a11d1a5b00000e835363f/content.htm?no_cache=true)
- [SAP Help: Configuring SNC: External Programs AS ABAP Using RFC](https://help.sap.com/doc/saphelp_nwpi71/7.1/en-US/d9/e8a740bbaa4d8f8bee6f7b173bd99f/content.htm?loaded_from_frameset=true)
- [SAP Help: Setting the SNC Profile Parameters](https://help.sap.com/doc/saphelp_nw73ehp1/7.31.19/en-US/19/164442c1a1c353e10000000a1550b0/content.htm?no_cache=true)
- [SAP Help: Configuring SAP GUI and SAP Logon for Single Sign-On](https://help.sap.com/doc/saphelp_nw73ehp1/7.31.19/en-US/44/0ea40dc6970d1ce10000000a114a6b/content.htm?no_cache=true)
- [SAP Help: Secure Login Client](https://help.sap.com/viewer/df185fd53bb645b1bd99284ee4e4a750/3.0/en-US/ba21970855064e54a9246b6c6de67fb2.html)
- [SAP Additional Content: List of SNC Error Codes](https://wiki.scn.sap.com/wiki/display/Security/List+of+SNC+Error+Codes)

The following article shows how to execute and schedule all defined extractions in the Xtract Universal Designer using an SSIS-package.

The implementation of this scenario is realized via the integration of an SSIS-package. The package uses the onboard SSIS tasks to sequentially call all existing extractions in Xtract Universal. The execution is implemented via the SQL Server Agent as part of the SSIS-DB.

### Requirements

The following programs and tools are required for using the SSIS-package:

- Install the [SQL Server](https://www.microsoft.com/en-us/sql-server/sql-server-downloads)
- Install the [SQL Server Management Studio (SSMS)](https://docs.microsoft.com/en-us/sql/ssms/download-sql-server-management-studio-ssms?view=sql-server-ver15)
- Configure the [SQL Server Agent](https://docs.microsoft.com/en-us/sql/ssms/agent/configure-sql-server-agent?view=sql-server-ver15)
- Install the [Xtract Universal Server](../../documentation/setup/installation/)
- Download the following SSIS-package:

[Download XtractUniversalScheduler.dtsx](../../assets/files/xu/XtractUniversalScheduler.dtsx)

The Xtract Universal Server must be installed on the same environment as the SQL-Server / SSIS-DB using the default directory `C:\Program Files\XtractUniversal`.

### Step-by-Step Guide

1. Import the SSIS-package *XtractUniversalScheduler.dtsx* into the SSIS-DB using the Microsoft SQL Server Management Studio.

1. Adjust the SSIS parameters to the correct values of the customer installation.

   | SSIS Parameter | Data type | Sensitive | Required | Example Value | Info | | --- | --- | --- | --- | --- | --- | | XtractUniversalServer | String | False | True | *[ServerName.theobald.local]* | [Xtract Universal Server](../../documentation/designer/) | | XtractUniversalServerPort | String | False | True | *8065* | [XU-Server Ports](../../documentation/server/#ports) |

1. Schedule the deployed SSIS package using the integrated SQL Server Agent execution jobs. For more information, see [Microsoft Documentation: Create a SQL-Server Agent Job](https://docs.microsoft.com/en-us/sql/ssms/agent/create-a-job?view=sql-server-ver15).

1. Execute the created step of the SQL Server Agent.

1. Check the correct execution of all defined extractions within the Xtract Universal Designer window.

1. Check the results in the extraction destination(s).

### About the SSIS Package

The SSIS package contains the following SSIS variables:

| SSIS Variables | Data type | Example Value | Expression | | --- | --- | --- | --- | | extraction_arguments | String | *-s todd.theobald.local -p 8065 -n ExtractionName* | `"-s " + @[$Package::XtractUniversalServer] + " -p " + @[$Package::XtractUniversalServerPort] + " -n " + REPLACE ( SUBSTRING( @[User::extraction_folder_name], 53, LEN(@[User::extraction_folder_name]) - 52 ) , "\\general.json", "")` | | extraction_folder_name | String | *C:\\Program Files\\XtractUniversal\\config\\ extractions\\ExtractionName\\general.json* | - |

______________________________________________________________________

#### Releated Links

- [Call via Commandline](../../documentation/execute-and-automate/call-via-commandline/)
- [Run Integration Services (SSIS) Packages](https://docs.microsoft.com/en-us/sql/integration-services/packages/run-integration-services-ssis-packages?view=sql-server-ver15)
- [Create an SQL-Server Agent Job](https://docs.microsoft.com/en-us/sql/ssms/agent/create-a-job?view=sql-server-ver15)

The following article shows how to extract ABAP reports that use an SAP ABAP List Viewer (ALV) layout.

### About Layouts

Layouts in ABAP reports define and control the visual presentation of report data. They determine how data is displayed to the user, including column visibility, column order, column width, aggregation (totals, subtotals), sorting, filtering and other formatting options. Layouts do not affect the actual data.

### Option 1 - Use Layouts in Variants

Some ALV-based reports have a **Layout** field in the selection screen . This field can be used to select a layout before running the report. If the selection screen includes a **Layout** field, that field's value can be saved as part of a [Report Variant](../../documentation/report/variants-and-selections/) . This way, every time the report is executed using that variant, it will automatically use this layout.

Make sure to select the variant in your Xtract Universal [Report extraction](../../documentation/report/variants-and-selections/#choose-a-variant).

### Option 2 - User-Specific Default Layouts

A user-specific default layout is a layout that is saved together with your SAP user ID. This way, SAP automatically applies the layout each time your SAP user runs the report, if no layout is explicitly chosen.

To use a user-specific default layout in Xtract Universal, create the layout for the SAP user that is used in your [SAP Connection](../../documentation/sap-connection/).

For more information on how to create a user-specific layout in ALV, see [SAP Help: Working with the SAP List Viewer (ALV) - Saving Layouts](https://help.sap.com/docs/ABAP_PLATFORM_NEW/b1c834a22d05483b8a75710743b5ff26/4d620265d79751b0e10000000a42189c.html?utm_source=chatgpt.com).

______________________________________________________________________

#### Related Links

- [Report Extraction Type](../../documentation/report/)
- [SAP Help: Working with the SAP List Viewer (ALV)](https://help.sap.com/docs/ABAP_PLATFORM_NEW/b1c834a22d05483b8a75710743b5ff26/4d5edc88767161bee10000000a42189b.html?utm_source=chatgpt.com)

The following article describes a scenario that uses Microsoft Fabric Data Pipelines to trigger and automate SAP data movements using Xtract Universal's [web services](../../web-api/). This article targets customers that utilize Fabric Data Pipelines as a platform for orchestrating data movement and transformation.

Note

The depicted scenario is not a best practice or recommendation.\
The following is a suggestion of how an orchestration of Xtract Universal extractions from Microsoft Fabric Data Pipelines can be implemented.

### Prerequisites

- A [Microsoft On-premises Data Gateway](https://www.microsoft.com/en-us/download/details.aspx?id=53127) is set up on the server where Xtract Universal runs.\
  This ensures that Xtract Universal's web server is accessible from Fabric over HTTP(S).
- The extraction uses a [push-destination](../../documentation/destinations/), such as Microsoft Fabric OneLake or Azure Blob Storage.
- The extraction runs successfully when called from a web browser. See [Web-API](../../web-api/#run-extractions).
- Access to Microsoft Fabric OneLake.
- A Microsoft Fabric-enabled Workspace.
- Knowledge of how to build Data Pipelines.

### Basic Principles

The depicted scenario builds upon the following basic principles:

- Xtract Universal offers a [Web-API](../../web-api/#run-extractions) through which various actions can be performed via HTTP(S) calls.
- Microsoft's On-premises Data Gateway enables access to on-premises resources, such as Xtract Universal, from Fabric Data Pipelines.
- Microsoft's Fabric Data Pipelines provide a *Web Activity* that allows calling resources via HTTP(S) through a Data Gateway.

The scenario utilizes two data pipelines to run extractions from Microsoft's Fabric:

- Pipeline 1 ([Trigger an Extraction](#trigger-an-extraction)) extracts data from SAP.
- Pipeline 2 ([Retrieve Extraction Logs](#retrieve-extraction-logs)) retrieves the extraction logs of the executed Xtract Universal extraction.

### Trigger an Extraction

Follow the steps below to create **Pipeline 1**, which extracts data from SAP:

1. Create a Data Pipeline under your workspace in Microsoft Fabric.
1. Start with a blank Canvas Pipeline and select *Web Activity*.
1. Set up the connection using the Data Gateway and use the URL that runs the Xtract Universal extraction.
1. Save and run the Data Pipeline.
1. Verify in the Xtract Universal server if the extraction has been triggered.

The pipeline works as a standalone solution. It can be run in debug mode or triggered via a scheduler.

### Retrieve Extraction Logs

Follow the steps below to create **Pipeline 2**, which retrieves the extraction logs:

1. Create a Data Pipeline under your workspace in Microsoft Fabric.
1. Start with a blank Canvas Pipeline and select *Web Activity*.
1. Set up the connection using the Data Gateway and use the URL that retrieves the extraction logs of the executed Xtract Universal extraction.
1. Save and run the extraction pipeline to see the output, which contains the retrieved extraction logs.

______________________________________________________________________

#### Related Links

- [Microsoft Fabric - End-to-End Pipeline Tutorial](https://learn.microsoft.com/en-us/fabric/data-factory/tutorial-end-to-end-pipeline)
- [Microsoft Fabric - Data Factory Documentation](https://learn.microsoft.com/en-us/fabric/data-factory/)

The following article shows how to enable OAuth 2.0 for the google cloud storage destination.

Xtract Universal supports OAuth 2.0 for authentication with the Google servers. To enable the OAuth 2.0 protocol, configure an OAuth flow with the required access permissions to Xtract Universal.

Note

Google initially classifies third-party applications generally as unsafe and issues a warning. The verification process is optional. Official app verification involves ongoing charges.

Note

As of version 5.11.16 Xtract Universal also supports authentication via service account credentials, see [Documentation: Google Cloud Storage - Destination Details](../../documentation/destinations/google-cloud-storage/#destination-details).

### GCP console

The GCP console allows configuring of all resources and services. To get to the overview dashboard, navigate to the [Google Cloud Storage](https://cloud.google.com/storage) page and click **[Console]** or **[Go to console]**.

To access all settings and services use the navigation menu on the upper left side.

### Set Up OAuth 2.0

1. Open the GCP console. In the navigation menu, select **APIs & Services > Credentials**.
1. In the "Credentials" section select **Create Credentials > OAuth client ID**.
1. Click **[Configure consent screen]**. The "Configure consent screen" is processed with the OAuth flow that is started when a connection is established in the Xtract Universal Designer.
1. 1. If your account belongs to an organization, you can restrict the usage of Xtract Universal in combination with GCS to your organization. To do so, select **internal** in the *User Type* option . The restriction option is only available, if you are a [GSuite](https://gsuite.google.com/) user.
   1. Alternatively you can allow any user with access to the OAuth credentials to grant Xtract Universal the permission to write data to your GCS buckets. To do so, select **external** in the *User Type* option.
1. Click **[Create]** to continue .
1. In the "App information" section enter an app name of your choice.\
   **Support email** and **Developer contact information** are also mandatory fields. Click **[Save and continue]** to get to the next section.
1. In the following section click **[Add or remove scopes]**.\
   Xtract Universal needs read and write permissions for its operations, which are configured in the "Scopes" section.
1. Enter `https://www.googleapis.com/auth/devstorage.read_write` under **Manually add scopes** and click **[Add to table]**.
1. The newly added scope is the first entry in the table. Click **[Update]** to create the entry.
1. Click **[Continue]** when the "Verfification required" window is prompted.
1. Click **[Save and continue]** twice.
1. Click **[Back to dashboard]** to return to the dashboard.
1. Return to the "Credentials" menu, click **[Create credentials]** and select *OAuth client ID*.\
   Select *Desktop app* as application type , enter a name for the app .
1. Click **[Create]** .

Your OAuth client is now created.\
The **Client ID** and the **Client secret** are needed for the destination configuration in Xtract Universal, see [Documentation: Google Cloud Connection Settings](../../documentation/destinations/google-cloud-storage/#destination-details).

______________________________________________________________________

#### Related Links

- [Documentation: Google Cloud Storage Destination](../../documentation/destinations/google-cloud-storage/)
- [Google Cloud Storage Documentation: Cloud Storage-Authentifizierung](https://cloud.google.com/storage/docs/authentication)
- [Google Cloud Storage Documentation](https://cloud.google.com/storage/docs#docs)

The following article shows how to import transport requests for custom functions modules that are included in the installation directory of your product, e.g., `C:\Program Files\XtractUniversal\ABAP`.

### Upload SAP Transport Requests to SAP

If you have access to the file system of SAP, you can copy and paste the files of your transport request directly into the `data` and `cofiles` folders of your SAP system. If you don't have access to the file system, follow the steps below to upload the files of your transport request using the SAP function module ARCHIVFILE_CLIENT_TO_SERVER:

1. Unzip the transport request provided in the installation directory of your product, e.g., `C:\Program Files\XtractUniversal\ABAP`.
1. Open SAP and go to transaction AL11.
1. Find the entry DIR_TRANS in the column *Name of Directory Parameter*. Note or copy the path shown in the column *Directory*.
1. Go to SAP transaction SE37.
1. Enter name of function module ARCHIVFILE_CLIENT_TO_SERVER and click **[Test/Execute]**.
1. In the field **PATH** you select your request file from from step 1. The name of the file starts with an "R", e.g., R900472.
1. In the field **TARGET PATH** you construct your target path using the following pattern:\
   `{copied path from step 2}\data\{request file name}`.
1. Enable case-sensitivity and click **[Execute]**. When prompted, confirm the upload.
1. In the field **PATH** you select your cofile from from step 1. The name of the file starts with a "K", e.g., K900472.
1. In the field **TARGET PATH** you construct your target path using the following pattern:\
   `{copied path from step 2}\cofiles\{cofile name}`.
1. Enable case-sensitivity and click **[Execute]**. When prompted, confirm the upload.

The files are now available in SAP.

Note

Another method for uploading files to SAP is the SAP transaction CG3Z. This transaction is only available on ERP systems.

### Import SAP Transport Requests

Follow the steps below to add the transport requests to the import queue and import them:

1. Go to SAP transaction STMS to open the transport management system.
1. Click **[Import Overview]** ( icon).
1. Double click on the import queue in which you want to load the transport request into.
1. Open the transport request selection dialog via **More > Extras > Other Requests > Add**.
1. Select the transport request and confirm. If prompted, confirm the import.
1. Select your transport request from the list and click **[Import Request]** ( icon). The window "Import Transport Request" opens.
1. Enter the target client. If the version of the SAP system where the transport request was created differs from your SAP system version, select the option **Ignore Invalid Component Version**.
1. Confirm your settings.

The transport request is imported.

### Check the Status of Transport Requests

The import overview of the transport management system (transaction STMS) lists all transport requests.\
The status of the transport requests is displayed in the column "RC".

A green bar indicates that the import was successful. In case of warnings or errors, double click on the icon to view the error messages.

The following article shows how to create a batch file that writes extraction logs into the windows log. The logs can be viewed in the Windows Event Viewer.

### Create a Batch File that Writes Events into the Windows Log

1. Create a new batch file, e.g., *xubatch.bat*.

1. Add the following code to the batch file:

   Insert Extraction Events into the Windows Log

   ```bat
   echo off
   "C:\Program Files\XtractUniversal\xu.exe" %1
   IF %ERRORLEVEL% == 0 eventcreate /ID 1 /L APPLICATION /T INFORMATION /SO "XtractUniversal" /D "Extraction successfully executed"
   IF %ERRORLEVEL% == 1001 eventcreate /ID 101 /L APPLICATION /T ERROR /SO "XtractUniversal" /D "An undefined error occured"
   IF %ERRORLEVEL% == 1002 eventcreate /ID 102 /L APPLICATION /T ERROR /SO "XtractUniversal" /D "Could not find the specified file"
   IF %ERRORLEVEL% == 1013 eventcreate /ID 113 /L APPLICATION /T ERROR /SO "XtractUniversal" /D "Invalid input data"
   IF %ERRORLEVEL% == 1014 eventcreate /ID 114 /L APPLICATION /T ERROR /SO "XtractUniversal" /D "The number of arguments is invalid"
   IF %ERRORLEVEL% == 1015 eventcreate /ID 115 /L APPLICATION /T ERROR /SO "XtractUniversal" /D "The parameter name is unknown"
   IF %ERRORLEVEL% == 1016 eventcreate /ID 116 /L APPLICATION /T ERROR /SO "XtractUniversal" /D "The argument is not valid"
   IF %ERRORLEVEL% == 1053 eventcreate /ID 153 /L APPLICATION /T ERROR /SO "XtractUniversal" /D "Something is wrong with your URL"
   IF %ERRORLEVEL% == 1087 eventcreate /ID 187 /L APPLICATION /T ERROR /SO "XtractUniversal" /D "The parameter is invalid"
   IF %ERRORLEVEL% == 404 eventcreate /ID 404 /L APPLICATION /T ERROR /SO "XtractUniversal" /D "Extraction does not exist"

   ```

1. If you do not use the default folder for your Xtract Universal installation, change the following line to your installation path:

   ```bat
   "C:\Program Files\XtractUniversal\xu.exe" %1

   ```

1. Execute the following line with the name of the extraction as an argument in the windows command line:

   ```text
   xubatch.bat -n MAKT

   ```

1. You can see the following event in the Event Viewer:

______________________________________________________________________

#### Related Links

- [Schedule Extractions using xu.exe command line](../../documentation/execute-and-automate/call-via-scheduler/)

The following article describes a customer use case, where Theobald Software has partnered with Snowflake to build a showcase for integrating SAP ERP data into a Snowflake cloud data warehouse.

### Customer Need

The customer was looking to replace an existing on-premises data warehouse and analytics toolset with primarily cloud-based services. Critical business data would have to be sourced from SAP ECC (on-premise), in addition to data stored in various databases or flat files.

The customer was also looking for a fully automated process to bring data from different sources together in a Snowflake cloud data warehouse. The data load process had to be fast and support incremental data loads, to ultimately enable near real-time insights for the business users.

### SAP Data Extraction

The customer used our [Xtract Universal](https://theobald-software.com/en/xtract-universal/) product to extract data from their SAP ECC system. Incremental data feeds are enabled with the built-in Table and DeltaQ components in Xtract Universal. The Table component is easy to configure and delivers performant data extraction from even the largest SAP tables. The DeltaQ component provides reliable delta feeds, based on SAP’s native DataSource extractors.

### Cloud Storage

Once the data is extracted from SAP, it can be directly stored in one of currently 20+ supported target environments. It’s a direct pass-through of the data from SAP into the target. In the process, SAP data types are mapped to the data types of the target environment.\
Xtract Universal can write SAP data directly into a database, cloud storage or data warehouse. In this case, the customer wants to write the data from Xtract Universal to Azure blob storage first and then to Snowflake DB. With this approach an additional staging datalake can be maintained in Azure. The DDLs to create the proper tables in Snowflake can be auto-generated in Xtract Universal. An initial, full data load from the Blob container can be done in Snowflake with the COPY command. Subsequent, incremental data loads can be done with a MERGE statement.

### Automation

The SAP data extractions and the load process in Snowflake can be fully automated, using utilities like the Windows Task Scheduler or Cron, or your scheduling tool of choice. This can also work in conjunction with an existing ETL tool for centralized monitoring and management of all data movement processes.

______________________________________________________________________

#### Related Links

- [Xtract Universal Product Information](https://theobald-software.com/en/xtract-universal/)
- [Documentation: Snowflake Destination](../../documentation/destinations/snowflake/)

The following article shows how to use the LIKE operand in WHERE-Clauses of the Table extraction type.\
The LIKE operand represents a pattern using the following wildcard characters:

- "%" is any character string (including an empty string)
- "\_" is any character

The pattern is case-sensitive. Trailing blanks in operands are ignored. This also applies in particular to operands of the type string with trailing blanks that are otherwise respected in ABAP.

### Examples

| WHERE-Condition | Description | | --- | --- | | `MSEG~MJAHR LIKE '20__'` | Filter back all fiscal years of the table column MSEG~MJAHR that start with *20*. | | `MSEG~MBLNR LIKE '0049%'` | Filters all records of the table column MSEG~MBLNR that return the Number of Material Document starting with the value *0049*. | | `BKPF~BUKRS LIKE '__1_'` | Filters all records that have a *1* in the third digit of the value for the company code (BKPF~BUKRS). |

______________________________________________________________________

#### Related Links

- [SAP Help: Open SQL - Operands and Expressions - LIKE](https://help.sap.com/doc/abapdocu_752_index_htm/7.52/en-US/abenwhere_logexp_like.htm)

This articles shows how to link BEx Query and BW Hierarchy extractions in the Tableau destination. By linking the extractions, you can blend data from both data sources into a single sheet.

### Setup in Xtract Universal

1. Create a BEx Query extraction, see [Define a BW Cube Extraction](../../documentation/bwcube/#define-the-bwcube-extraction-type).

1. Open the extraction settings of the BEx Query and activate *Formatted Values*.

1. Open the Destination Settings of the extraction and set *Text* as the **Column Name Style** for Tableau. Note that using *Text* as the column name style can result in column names that are not unique.

1. Create a BW Hierarchy extraction, see [Define a Hierarchy Extraction](../../documentation/hierarchy/#define-the-bw-hierarchy-extraction-type).

1. Open the Extraction Settings of the Hierarchy and set **Representation** to *Natural*:

### Create a Relationship in Tableau

1. Load both extractions into Tableau .
1. Create a relationship between the data sources by dragging the sheets into the canvas . The window "Edit Relationships" opens.
1. Select one pair of fields that is to be matched. Add multiple field pairs to create a compound relationship. Matched pairs must have the same data type.
1. Close "Edit Relationships" and switch to the Worksheet view .

The data sources are now linked and data from both data sources can be blended in a single sheet.

### "Not Assigned" Nodes

In BW Hierarchies, the values that are not assigned to a hierarchy node are gathered under the "Not Assigned" node.

In Tableau the "Not Assigend" node is labeled "Null" and is only displayed if the BEx Query extraction acts as the primary data source. If the BW Hierarchy extraction is acts as primary data source, the "Null" node is not displayed.

______________________________________________________________________

#### Related Links

- [How to Create Relationships in Tableau](https://help.tableau.com/current/pro/desktop/en-us/relate_tables.htm#create-a-relationship)
- [Tutorial: Relationships in Tableau](https://www.tableau.com/learn/tutorials/on-demand/relationships)

The following article shows how to operate Xtract Universal with load balancing.\
In this context, load balancing means to distribute the network traffic across multiple Windows servers to avoid server overloads.

### About Load Balancing

In today's highly interconnected world, a load balancer is a crucial tool for managing traffic effectively. When distributing network traffic caused by Xtract Universal, it is necessary to use a load balancer between different Windows servers that run the Xtract Universal.

Typical use cases for load balancing include:

- Improved Performance: By distributing network traffic across multiple Xtract Universal servers, a load balancer can significantly improve the performance of your network. This can be especially important for high-traffic applications that require fast and reliable access.
- Increased Reliability: Load balancing can help increase the reliability of your network by ensuring that no one Xtract Universal server becomes overloaded with traffic. This helps to prevent downtime and ensures that your network is always available.
- Scalability: A load balancer can also help improve the scalability of your network by allowing you to easily add or remove Xtract Universal servers as needed. This can be important for businesses that experience sudden spikes in traffic.
- Centralized Management: A load balancer allows you to manage multiple servers from a single location. This makes it easier to monitor your network, troubleshoot issues, and make necessary adjustments.
- Parallel Processing: A load balancer allows running multiple extractions at the same time on different servers. This increases the amount of extractions that can be run in parallel.

### Prerequisites

- Every Xtract Universal servers needs a server license, see [Documentation: Licensing](../../documentation/setup/license/).
- All Xtract Universal servers must share the same configuration folder, e.g., by mapping the configuration folder to an external shared storage. The configuration folder contains the settings for the destinations, extractions, sources, server and users. The folder is located in the installation directory of Xtract Universal, e.g., `C:\Program Files\XtractUniversal\config`. For versioning of the configuration folder you can use [git version control](../deploy-extractions-using-Git-version-control/).
- All Xtract Universal installations must use the same software version to avoid any version compatibility issues.

### The Process

A load balancer setup with two Xtract Universal servers uses the following process when processing requests:

1. A client sends a web request to the load balancer. The load balancer is the entry point for all incoming web requests, so it's the first component to receive the request.
1. The load balancer uses a predefined algorithm (e.g., round-robin, least connections, or IP hash) to select the Xtract Universal server to handle the incoming web request.
1. The load balancer forwards the request to the selected Xtract Universal server.
1. The selected Xtract Universal server processes the incoming request and sends the response back to the load balancer.
1. The load balancer receives the response from the selected server and forwards the response to the client.
1. For subsequent web requests, the load balancer repeats this process.

This setting can also be configured to act as Active / Standby servers.\
This means you have multiple servers with identical configurations and applications where only one server is active, while the others remain passive or on standby until a failover event occurs.

The load balancer forwards incoming requests to the active server. If the active server fails, the load balancer will automatically switch to one of the passive servers. The goal is to provide redundancy and ensure high availability and reliability of critical applications and services.

______________________________________________________________________

#### Related Links:

- [Deploying Extractions Using Git Version Control](../deploy-extractions-using-Git-version-control/)
- [Documentation: Installation and Update - Program Directory Files](../../documentation/setup/installation/#installation-directory-files)

This articles shows how to use Xtract Universal to parse ABAP reports that contain groups of data sets and merge them into a single data set. The depicted example uses the report *FBL3N* that contains multiple groups where each group has its own header and calculation rows.

The article leads you through the following steps to merge all groups into a single data set:

- [Remove rows](#remove-rows) that calculate the content of the rows before.
- [Select a header](#select-a-header) for the report and remove duplicates.
- [Parse rows that contain additional headers as columns.](#parse-rows-with-additional-headers-as-columns)

### Remove Rows

Follow the steps below to remove rows that calculate the content of the rows before. Calculation rows are marked with an * symbol.

1. [Look up](../../documentation/report/#look-up-a-report-or-transaction) a report that contains groups of data. The depicted example uses report FBL3N (RFITEMGL).
1. If the report has variants, [select a variant](../../documentation/report/variants-and-selections/#choose-a-variant).
1. Click **[Load Preview]** to load a preview of the first 100 rows.
1. Click **[Automatically detect columns]** to define the columns of the report.
1. Open the tab *Skip and Parse Rows*.
1. Click **[Add rows to skip]**.
1. To remove calculation rows, enter the keyword \*.\
   All rows that contain the keyword are displayed in a gray font in the preview section.
1. To remove additional rows that separate calculation rows and headers, enter the keyword ---.
1. Click **[OK]** to save the changes.

The calculation rows are skipped during the report extraction.\
For more information on how to manipulate rows in report extractions, see [Documentation: Define Rows](../../documentation/report/report-rows-define/).

### Select a Header

Follow the steps below to select one of multiple available headers as the report header:

1. Open the report extraction from [Remove Rows](#remove-rows) or any other report that contains multiple headers.

1. Click **[Load Preview]** to load a preview of the first 100 rows.

1. Open the tab *Skip and Parse Rows*.

1. In the preview section, right-click the header row and select **Select as header** from the context menu. All rows that contain the header are displayed in a blue font in the preview section.

   Tip

   If the header is not part of the preview (first 100 rows of the report), enter a pattern in the field [**Header pattern**](../../documentation/report/report-rows-define/#header-pattern) to scan all report rows for the pattern.

1. Click **[OK]** to save the changes.

The report parses the selected header as the report header during report extraction.\
Any duplicates of the header are removed from the result set.

### Parse Rows with Additional Headers as Columns

Follow the steps below to parse header rows that contain information about a data set as columns. The depicted example parses the header rows containing the *G/L Account* number and the *Company Code* of a data set as columns.

1. Open the report extraction from [Select a Header](#select-a-header) or any other report that contains multiple headers.

1. Click **[Load Preview]** to load a preview of the first 100 rows.

1. Open the tab *Skip and Parse Rows*.

1. Click **[Add row to parse as column]** to add a new column.

1. Click **[]**. The window "Parse Helper" opens and a new preview of the report data is fetched. This can take a while.

1. Under **Search keyword** enter a unique keyword from the row you want to parse as a column. Matching rows are displayed in the preview section of the window.

1. Make sure only header rows are displayed in the preview section. If the preview includes regular rows, edit the keyword until only header rows remain.

1. Under **New Column Name** enter a name for the new column.

1. Mark the content of the column in the preview section by pressing and dragging the mouse pointer over the length of the content.

1. Click **[OK]** to save the column properties. All rows that will be parsed as a column are displayed in a blue font in the preview section.

1. Repeat steps 4 to 9 for the *Company Code*.

   Tip

   If the *Company Code* is not relevant for your report analysis, remove the corresponding header row from the result set, see [Remove Rows](#remove-rows).

1. Click **[OK]** to save the changes.

1. Run the extraction and check if the rows are parsed correctly.

The extracted report returns a single data set that includes the content of all original data set groups.

______________________________________________________________________

#### Related Links

- [Report](../../documentation/report/)
- [Skip Rows by Keyword](../../documentation/report/report-rows-define/#skip-rows-by-keyword)

The following article shows how to configure a proxy server in Xtract Universal.

Due to corporate network regulations it can be the required that all web traffic needs to go through a web proxy. This means that Xtract Universal must also connect to a destination via a proxy server.

### Proxy Server Settings in Xtract Universal

By default, Xtract Universal uses the proxy configuration defined by the Windows environment variables. For information about the necessary environment variable and how to configure them, refer to the [Microsoft Documentation: Configure a proxy using environment variables](https://learn.microsoft.com/en-us/dotnet/azure/sdk/configure-proxy?tabs=cmd#configure-using-environment-variables).

The following article shows how to extract data from the SAP HCM tables PCL1 and PCL2.\
The data can only be extracted using a custom function module and the BAPI extraction type. Data extraction via Table extraction type is not supported.

### Custom Function Module Z_HR_CLUSTER_READ

Create the following custom function module in SAP:

1. Use SAP transaction SE37 to create a remote enabled custom function module Z_HR_CLUSTER_READ.

1. Create the following parameters:

   ```text
   PERNR            TYPE=PC2B0-PERNR;
   ACTIONID         TYPE=CHAR2; DEFAULT = 'P1'
   STARTDATE        TYPE=DATS;
   ENDDATE          TYPE=DATS;

   ```

   ```text
   ERT        LIKE=PC2B8
   ST         LIKE=PC2B5
   CRT        LIKE=PC208

   ```

1. Copy and paste the following ABAP source code into the source code area of the function module.

   ```abap
        FUNCTION Z_HR_CLUSTER_READ.

        *"----------------------------------------------------------------------

        *"*"Local Interface:
        *"  IMPORTING
        "     VALUE(PERNR) TYPE  PC2B0-PERNR OPTIONAL
        *"     VALUE(ACTIONID) TYPE  CHAR2 DEFAULT 'P1'
        *"     VALUE(STARTDATE) TYPE  DATS OPTIONAL
        *"     VALUE(ENDDATE) TYPE  DATS OPTIONAL
        *"  TABLES
        *"      ERT STRUCTURE  PC2B8 OPTIONAL
        *"      ST STRUCTURE  PC2B5 OPTIONAL
        *"      CRT STRUCTURE  PC208 OPTIONAL
        *"----------------------------------------------------------------------
        DATA : BEGIN OF it_pcl1 OCCURS 0,
        srtfd TYPE pcl1-srtfd,
        END OF it_pcl1.

        DATA BEGIN OF b1_key.
             INCLUDE STRUCTURE pdc10.
        DATA END OF b1_key.

        IF actionid = 'P1'.

        SELECT srtfd
        FROM pcl1
        INTO TABLE it_pcl1
        WHERE relid EQ 'B1'
        AND srtfd EQ pernr
        AND srtf2 EQ 0.

       LOOP AT it_pcl1.
         MOVE it_pcl1-srtfd TO b1_key.
         IMPORT st ert FROM DATABASE pcl1(b1) ID b1_key.
         IF sy-subrc EQ 0.
         ENDIF.
       ENDLOOP.

        ENDIF.

        IF actionid = 'P2'.
        DATA : it_rgdir TYPE TABLE OF pc261 INITIAL SIZE 0,
              wa_rgdir LIKE LINE OF it_rgdir,
              it_crt TYPE pay99_result-inter-crt,
              wa_crt LIKE LINE OF it_crt,
              wa_payrollresult TYPE pay99_result,
              v_molga TYPE molga.

       DATA : BEGIN OF wa_out,
               pernr TYPE pernr-pernr,
               gross TYPE pc207-betrg, "Amount
               net TYPE pc207-betrg,
              END OF wa_out,
              it_outtab LIKE TABLE OF wa_out.

        wa_out-pernr = PERNR.
        CALL FUNCTION 'CU_READ_RGDIR'
        EXPORTING
        persnr          = PERNR
        IMPORTING
        molga           = v_molga
        TABLES
        in_rgdir        = it_rgdir
        EXCEPTIONS
        no_record_found = 1
        OTHERS          = 2.
        IF sy-subrc = 0.



       LOOP AT it_rgdir INTO wa_rgdir
                          WHERE fpbeg GE startdate AND
                                fpend LE enddate AND
                                srtza EQ 'A'.  "Current result
           CALL FUNCTION 'PYXX_READ_PAYROLL_RESULT'
             EXPORTING
               clusterid                    = 'RD'
               employeenumber               = PERNR
               sequencenumber               = wa_rgdir-seqnr
               READ_ONLY_INTERNATIONAL      = 'X'
             CHANGING

               payroll_result               = wa_payrollresult
             EXCEPTIONS
               illegal_isocode_or_clusterid = 1
               error_generating_import      = 2
               import_mismatch_error        = 3
               subpool_dir_full             = 4
               no_read_authority            = 5
               no_record_found              = 6
               versions_do_not_match        = 7
               error_reading_archive        = 8
               error_reading_relid          = 9
               OTHERS                       = 10.

           IF sy-subrc = 0.
             LOOP AT wa_payrollresult-inter-crt INTO wa_crt.
               CASE wa_crt-lgart.
                 WHEN '/101'.  " Gross
                   APPEND wa_crt TO crt.
               ENDCASE.
               CLEAR wa_out.
             ENDLOOP.
           ENDIF.
         ENDLOOP.
       ENDIF.
     ENDIF.
   ENDFUNCTION.

   ```

1. Save the function module.

### How to use the Custom Function Module

Depending on the table you want to access, define the following parameters:

1. Look up the function module using the BAPI extraction type.

1. Populate the import parameter ACTIONID with value *P1*.

1. Populate the field PERNR with a value for Personnel Number. The personnel number has to be entered with leading zeroes.

1. Run the extraction. The data is available in the table parameter ST.

1. Look up the function module using the BAPI extraction type.

1. Populate the import parameter ACTIONID with value *P2*.

1. Enter a start date and end date using the parameters STARTDATE and ENDDATE. The date fields have the format YYYYmmDD.

1. Run the extraction. The data is available in the table parameter CRT.

The following article shows how to register an RFC server in SAP releases with Kernel release 720 and higher.

Warning

**RFC server is not working, please check gateway info.**\
As of SAP Kernel Release 720, you can use the parameter *gw/acl_mode* to set an initial security environment to start and register external programs, e.g., RFC Server required for DeltaQ processing / customizing check. If this value is set to *1*, the DeltaQ extraction type cannot register the RFC Server and the Customizing Check returns the an error.

There are two options to avoid this error:

- Set the Profile Parameter *gw/acl_mode* to 0.
- Define a whitelist of programs that can register at the SAP Gateway.

### Change the Profile Parameter *gw/acl_mode*

When setting the profile parameter *gw/acl_mode* to 0 (default is 1), all RFC destinations / RFC servers with different Program IDs can register.

1. Use SAP transaction RZ10 to open the "Edit Profile" menu.
1. Select the profile name *Default* and *Extended Maintenance*.
1. Click **[Change]** and set the profile parameter *gw/acl_mode* value to *0*

The Customizing Check now executes without error messages.

### Define a Whitelist of Programs at the SAP Gateway

To define a whitelist of programs that can register at the SAP Gateway, create two files named *secinfo* and *reginfo*. Both files don't exist per default, see the example below.

1. Create the files *secinfo* and *reginfo*. The files must have the following content:

   ```abap
   #VERSION=2
   P TP=* HOST=internal,local CANCEL=internal,local ACCESS=internal,local
   # the following line should be the LAST line in the secinfo file
   P TP=XTRACT01 USER=* USER-HOST=* HOST=* 

   ```

   ```abap
   #VERSION=2
   # the following line should be the LAST line in the reginfo file
   P TP=XTRACT01

   ```

1. Copy both files to the following directory (data path): `/usr/sap/<SID>/<INSTANCE>/data/`

1. Extend the following two parameters to the Profile Parameter in the SAP transaction RZ10:

   - gw/reg_info = $(DIR_DATA)/reginfo
   - gw/sec_info = $(DIR_DATA)/secinfo

1. Restart the gateway or re-read the security parameters using SAP transaction SMGW (navigate to **Menu -> Goto -> Expert Functions -> External Security -> Reread**).

The Customizing Check now executes without error messages.

Warning

**Registration of the RFC-server fails.**\
The content of both files *secinfo* and *reginfo* override the parameter *gw/acl_mode*.\
Make sure that both files *secinfo* and *reginfo* allow the registration of the RFC-server.

______________________________________________________________________

#### Related Links

- [SAP Help: Gateway Security Files secinfo and reginfo](https://help.sap.com/doc/saphelp_crm700_ehp01/7.0.1.15/en-US/48/b2096b7895307be10000000a42189b/frameset.htm)
- [SAP Note 1850230](https://launchpad.support.sap.com/#/notes/0001850230)
- [SAP Community blog](http://scn.sap.com/docs/DOC-42463)

The following article shows how to run an event-driven pipeline in Azure Data Factory to process SAP data extracted with [Xtract Universal](https://theobald-software.com/en/xtract-universal/) into an Azure Storage.

### About

The depicted example extracts and uploads SAP customer master data to Azure Storage.\
An event then triggers an ADF pipeline to process the SAP parquet file, e.g. with [Databricks](https://learn.microsoft.com/en-us/azure/databricks/). Xtract Universal supports different file formats for Azure storage, the depicted example uses [Apache Parquet](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/read-parquet), which is a column file format that provides optimizations to speed up queries and is more efficient than CSV or JSON.

Target audience: Customers, who utilize Azure Data Factory (ADF) as a platform for orchestrating data movement and transformation.

Note

The following sections describe the basic principles for triggering an ADF pipeline. Keep in mind, this is not a best practice document or a recommendation.

#### Azure Storage

Xtract Universal extracts SAP data and loads it into an Azure Storage as a parquet file. An Azure Storage event trigger is used to run an ADF pipeline for further processing of the SAP file.

#### ADF Pipelines and Storage Event Triggers

The *Master pipeline* is triggered by an Azure Storage event and calls a child pipeline for further processing. The *Master pipeline* has an event trigger based on Azure storage.

The Master pipeline has 2 activities:

- write a log to an Azure SQL database (optional)
- call a *Child pipeline* to process the parquet file with Databricks

This article focuses on the *Master pipeline*. The *Child pipeline* processes the parquet file e.g., with Databricks. The *Child pipeline* in this example is a placeholder.

#### Use Azure SQL for logging (optional)

In the scenario depicted, the ADF pipeline executes a stored procedure to log various details of the pipeline run into an Azure SQL table.

### Prerequisites

- Basic knowledge of Xtract Universal, see [Getting Started](../../getting-started/).
- Basic knowledge of [Azure Storage](https://docs.microsoft.com/en-us/azure/storage/common/storage-introduction).
- You can successfully run extractions from a web browser, see [Web API: Run Extractions](../../web-api/#run-extractions).
- [Azure Storage Destination](../../documentation/destinations/azure-storage/) is set up and running.
- Access to [Azure Data Factory](https://docs.microsoft.com/en-us/azure/data-factory/); basic knowledge of building ADF pipelines.
- Basic knowledge of ADF pipeline triggers, especially [triggering a pipeline in response to a storage event](https://docs.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger?tabs=data-factory).

### Procedure

1. Define an SAP extraction and set the destination to Azure Storage.\
   The depicted example uses a storage account *xtractstorage* and a container called *ke-container*:

1. Define two pipelines in ADF:

   - The master pipeline *ProcessBlogStorageFile* contains 2 activities.\
     The first activity *sp_pipelinelog* executes an SQL stored procedure to write a log entry to an Azure SQL table. The second activity runs a dummy subpipeline . As both activities are out of the scope of this article, there are no further details.
   - The child pipeline *ProcessWithDataBricks* processes the parquet file e.g., with Databricks.

1. Define the following parameters:

   - `fileName`: contains the file Name in the Azure Storage.
   - `folderPath`: contains the file path in the Azure Storage.

1. Click **[New/Edit]** to add a new Storage Event Trigger in the ADF Pipeline.

1. Adjust the details and use the Storage account name and Container name defined in the Xtract Universal Azure Storage destination:

1. Adjust the event trigger parameters that are used as input parameters for the *Master Pipeline*:

   - `@triggerBody().fileName`
   - `@triggerBody().folderPath`

1. Publish the pipeline.

1. Run the extraction in Xtract Universal.

1. When the extraction finishes successfully, check the Azure Storage.

1. Check the log table in Azure SQL. The log table contains an entry, each for the master and child pipeline.

1. Check the trigger and pipeline runs in ADF.

### Download JSON Templates

Downloads for the trigger and the master pipeline are provided below:

[Download Trigger as json](../../assets/files/xu/BlobEventsTrigger01.json) [Download MASTER pipeline as json](../../assets/files/xu/ProcessBlobStorageFile.json)

The following article shows how to run Xtract Universal in a virtual machine on an AWS EC2 instance.

AWS enables running virtual servers (instances) in the cloud, see [AWS Documentation: EC2](https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/concepts.html). Theobald Software offers Xtract Universal as an [Amazon Machine Image (AMI)](https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/AMIs.html) for the following customer purposes:

- Xtract Universal evaluation
- Hosting of Xtract Universal in the cloud

The Xtract Universal AMI can be selected when launching an instance in AWS.

## Pre-Configured Settings

When starting an Xtract Universal instance, the following settings are pre-configured:

| | Configuration | Resources | | --- | --- | --- | | License | Xtract Universal is already installed and running with a 30-days trial license. You can replace the trial license with your regular license. | [Documentation: Licensing](../../documentation/setup/license/) | | Software Updates | The pre-installed version of Xtract Universal is displayed in the AWS marketplace. Make sure to keep Xtract Universal up-to-date with the latest software releases, see [Xtract Universal Changelog](../../changelog/). | [Documentation: Installation and Update](../../documentation/setup/update/) | | Server Settings | The webserver is pre-configured with a self-signed TLS certificate and can be accessed in a browser via `https://xtractuniversal:8165/` from within the rdp session. | [Documentation: Server Settings](../../documentation/server/server-settings/) | | Data Extractions | There are no default SAP data extractions. To get started with Xtract Universal, create an SAP connection and start creating extractions. | [Getting Started on AWS](#getting-started-on-aws) |

## Prerequisites

- Access to an AWS account
- Access to the [EC2](https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/concepts.html) console

## Set Up Xtract Universal in AWS

There are multiple ways to set up Xtract Universal in EC2:

1. Log in to AWS marketplace and open the [Xtract Universal product page in AWS](https://aws.amazon.com/marketplace/pp/prodview-anarfo2osmhl4?sr=0-1&ref_=beagle&applicationId=AWSMPContessa#pdp-reviews).

1. Click **[View Purchase Options]** and [subscribe](https://docs.aws.amazon.com/marketplace/latest/buyerguide/buyer-ami-contracts.html) to Xtract Universal.

1. Click **[Continue to Configuration]** to select a software version and a region for hosting Xtract Universal.

1. Click **[Continue to Launch]**.

1. In **Choose Action** select *Launch through EC2* to access all options for configuring a virtual machine in EC2.

1. Click **[Launch]**. The EC2 console opens.

1. Set up your virtual machine, see [AWS Documentation: Launch an Instance](https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/EC2_GetStarted.html#ec2-launch-instance). The Xtract Universal AMI is already selected.

1. Start the EC2 instance and connect to it, see [AWS Documentation: Connect to an Instance](https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/EC2_GetStarted.html#ec2-connect-to-instance-windows).

1. When connected, the Xtract Universal Designer is located on the Desktop. Start the Xtract Universal Designer.

1. Set up an SAP connection and extractions, see [Documentation: Getting Started](../../getting-started/).

1. Open the [Amazon EC2 console](https://console.aws.amazon.com/ec2/).

1. In the EC2 console dashboard, click **[Launch instance]**.

1. In the *Quick Start* tab of **Application and OS Images (Amazon Machine Image)** click **[Browse more AMIs]**.

1. Enter "Xtract Universal" in the search bar. Xtract Universal is listed under *AWS Marketplace AMIs*.

1. Click **[Select]**. The application returns to the EC2 console.

1. Set up your virtual machine, see [AWS Documentation: Launch an Instance](https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/EC2_GetStarted.html#ec2-launch-instance). The Xtract Universal AMI is already selected.

1. Start the EC2 instance and connect to it, see [AWS Documentation: Connect to an Instance](https://docs.aws.amazon.com/AWSEC2/latest/WindowsGuide/EC2_GetStarted.html#ec2-connect-to-instance-windows).

1. When connected, the Xtract Universal Designer is located on the Desktop.

1. Start the Xtract Universal Designer and click **[ Connect]** to [connect to the Xtract Universal Server](../../documentation/designer/#connect-the-designer-to-a-server).

Note

If you are not already [subscribed](https://docs.aws.amazon.com/marketplace/latest/buyerguide/buyer-ami-contracts.html) to Xtract Universal, launching the EC2 instance automatically adds the subscription.

## Getting Started on AWS

After Xtract Universal is set up on AWS, the following steps are required to start data extracting data from SAP:

- [Connect Xtract Universal to an SAP system](../../getting-started/#connect-to-sap).
- [Create an extraction](../../getting-started/#create-an-extraction) that defines which SAP data to extract.
- [Testrun the extraction](../../getting-started/#run-an-extraction) in the Xtract Universal Designer to preview the extracted data.
- [Add a destination](../../getting-started/#write-data-to-a-target-environment) to connect Xtract Universal to load the data into a target environment.
- Optional: [Automate your extractions](../../documentation/execute-and-automate/) using a scheduler, script or ETL tool.

This article contains examples of PowerShell scripts that query information from Xtract Universal and run extractions.\
The following sample scripts are available:

- [Run an Extraction](#run-an-extraction)
- [Run an Extraction with a Parameter](#run-an-extraction-with-a-parameter)
- [Run an Extraction with a Parameter using PowerShell Variables](#run-an-extraction-with-a-parameter-using-powershell-variables)
- [Run an Extraction with Multiple Parameters](#run-an-extraction-with-multiple-parameters)
- [Create a Function to Run an Extraction](#create-a-function-to-run-an-extraction)
- [Loop an Array with Different Parameter Values](#loop-an-array-with-different-parameter-values)
- [Run multiple Extractions in Sequence](#run-multiple-extractions-in-sequence)
- [Run multiple Extractions in Parallel](#run-multiple-extractions-in-parallel)
- [Get a List of Defined Extractions](#get-a-list-of-defined-extractions)
- [Get the Latest Log of Extractions](#get-the-latest-log-of-extractions)
- [Get the Metadata of Extractions](#get-the-metadata-of-extractions)

### Run an Extraction

Execute an Xtract Universal extraction using the command tool xu.exe in a PowerShell script as shown below:

Run an Extraction

```shell
# execute an Xtract Universal extraction using the command tool xu.exe in a PowerShell script
# 2>&1 redirects standard error (the 2) to the same place as standard output (the 1)
&'C:\Program Files\XtractUniversal\xu.exe' -s "localhost" -p "8065" -n "SAPSalesCube" 1>$null 2>1

```

### Run an Extraction with a Parameter

Here is an example of an extraction with a variable *CalendarMonth* that needs a value in the format YYYYMM, e.g. 201712:

Run an Extraction with a Parameter

```shell
# the extraction has a variable CalendarMonth that needs a value in the format YYYYMM, e.g. 201712
&'C:\Program Files\XtractUniversal\xu.exe' -s "localhost" -p "8065" -n "SAPSalesCube" -o CalendarMonth='200401' 1>$null 2>&1

```

### Run an Extraction with a Parameter using PowerShell Variables

Here is an example of an extraction with a parameter using a PowerShell variable:

Run an Extraction with a Parameter using PowerShell Variables

```shell
# set the path to the installation folder
$XUCmd = 'C:\Program Files\XtractUniversal\xu.exe'
# XU server &amp; port
$XUServer = "localhost"
$XUPort = "8065"
# extraction name
$XUExtraction = "SAPSalesCube"

# Setting Calendar month variable
# the extraction has a variable CalendarMonth that needs a value in the format YYYYMM, e.g. 201712
$myCalendarMonth = (Get-Date -format "yyyyMM")

# run an extraction with one parameter
&$XUCmd -s $XUServer -p $XUPort -n $XUExtraction -o CalendarMonth=$myCalendarMonth 1>$null 2>&1

```

### Run an Extraction with Multiple Parameters

Here is an example of an extraction with multiple parameters:

Run an Extraction with Multiple Parameters

```shell
# run an extraction with multiple parameters
&$XUCmd -s $XUServer -p $XUPort -n $XUExtraction -o CalendarMonth=$myCalendarMonth -o clearBuffer=true 1>$null 2>&1

```

### Create a Function to Run an Extraction

Here is an example of a function that runs an extraction, checks the exit code and writes an output:

Create a Function to Run an Extraction

```shell
# Function to run an XU extraction
Function XURun($XUCmd, $XUServer, $XUPort, $XUExtraction, $XUParameters)
{
Try {
$parameters = $XUCmd + " " + $XUServer + " " + $XUPort + " " + $XUExtraction + " " + $XUParameters

if([string]::IsNullOrEmpty($XUParameters)){
&$XUCmd -s $XUServer -p $XUPort -n $XUExtraction 1>$null 2>&1
} else {
&$XUCmd -s $XUServer -p $XUPort -n $XUExtraction -o $XUParameters 1>$null 2>&1
}

# check the last exit code
# 0: successful
# else unsuccessful
if($LASTEXITCODE -eq 0) {

write-host -f Green "The last command $parameters has been executed successfully" (Get-Date)

} else {

write-host -f Red "The last execution for $parameters failed with error code $LASTEXITCODE!" (Get-Date)
Write-Host $errorMessage
}
}
Catch {

write-host -f Red "Error running XU extraction!" + (Get-Date) $_.Exception.Message
}
}

# define error message
$errorMessage = @'
If the command completes an operation successfully, it returns an exit code of zero (0).
In case of an error, it will return one of the following (http status) codes:
HTTP Statuscodes (e.g. 404 when the extraction does not exist)
1001 An undefined error occured
1002 Could not find the specified file
1013 Invalid input data
1014 The number of arguments is invalid
1015 The parameter name is unknown
1016 The argument is not valid
1053 Something is wrong with your URL
1087 The parameter is invalid
'@

# run an extraction with multiple parameters
$XUParameters = "clearBuffer=True -o CalendarMonth=$myCalendarMonth"
$XUResult = XURun -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtraction $XUExtraction -XUParameters $XUParameters

```

### Loop an Array with Different Parameter Values

The depicted example uses a loop to run an extraction with different parameter values. The parameters values are defined in an array.

Loop an Array with Different Parameter Values

```shell
$Months = @("200401","200402","200403")
foreach($Month in $Months){
XURun -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtraction $XUExtraction -XUParameters CalendarMonth=$Month
}

```

### Run multiple Extractions in Sequence

The depicted example uses a loop tu run multiple extractions in sequence. The extraction names are defined in an array.

Run multiple Extractions in Sequence

```shell
Function XURun-Multi ($XUCmd, $XUServer, $XUPort, $XUExtractions,$XUParameters){

foreach($XUExtraction in $XUExtractions){

XURun -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtraction $XUExtraction -XUParameters $XUParameters
}
}

#$XUExtractions = "SAPCustomers", "SAPPlants";,"PSSAPCustomers", "PSSAPPlants";
$XUResult = XURUN-Multi -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtractions $XUExtractions

```

### Run multiple Extractions in Parallel

There are multiple ways to run parallel commands in PowerShell. One of them is using [PowerShell Workflow](https://docs.microsoft.com/en-us/system-center/sma/overview-powershell-workflows). Here is an example of using a ThrottleLimit to limit the number of parallel running extractions.

Run multiple Extractions in Parallel

```shell
# Define Workflow 1
# Run multiple Extractions in parallell using powershell workflow
Workflow XURun-Parallel { param ($XUCmd, $XUServer, $XUPort, $XUExtractions, $XUParameters, $ThrottleLimit)

foreach -parallel -throttlelimit $ThrottleLimit ($XUExtraction in $XUExtractions){

InlineScript{
if([string]::IsNullOrEmpty($XUParameters)){
&$Using:XUCmd -s $Using:XUServer -p $Using:XUPort -n $Using:XUExtraction 1>$null 2>&1
} else {
&$Using:XUCmd -s $Using:XUServer -p $Using:XUPort -n $Using:XUExtraction -o $Using:XUParameters 1>$null 2>&1
}
}

}
}

# 4 parallel extractions
$ThrottleLimit = 4
XURun-Parallel -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtractions $XUExtractions -XUParameters $XUParamters -ThrottleLimit $ThrottleLimit

# Define Workflow 2

# Run multiple Extractions using PowerShell workflow

Workflow XURun-Parallel2{ param ($XUCmd, $XUServer, $XUPort, $XUExtractions, $XUParameters, $ThrottleLimit)

foreach -parallel -throttlelimit $ThrottleLimit ($XUExtraction in $XUExtractions){

InlineScript{

Try {
$parameters = $Using:XUCmd + " " + $Using:XUServer + " " + $Using:XUPort + " " + $Using:XUExtraction + " " + $Using:XUParameters

if([string]::IsNullOrEmpty($Using:XUParameters)){
&$Using:XUCmd -s $Using:XUServer -p $Using:XUPort -n $Using:XUExtraction 1>$null 2>&1
} else {
&$Using:XUCmd -s $Using:XUServer -p $Using:XUPort -n $Using:XUExtraction -o $Using:XUParameters 1>$null 2>&1
}

# check the last exit code
# 0: successful
# else unsuccessful
if($LASTEXITCODE -eq 0) {

write-host -f Green "The last command $Using:parameters has been executed successfully" (Get-Date)

} else {

write-host -f Red "The last execution for $Using:parameters failed with error code $LASTEXITCODE!" (Get-Date)
Write-Host $errorMessage
}
}
Catch {

write-host -f Red "Error running XU extraction!" + (Get-Date) $_.Exception.Message
}
}

}
}

# 4 parallel extractions
$ThrottleLimit = 4
XURun-Parallel2 -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtractions $XUExtractions -XUParameters $XUParamters -ThrottleLimit $ThrottleLimit

```

### Get a List of Defined Extractions

Xtract Universal offers an HTTP API to access the defined extractions, their metadata and log, the server log and further information. The following function queries the list of extractions from the repository. The output will have the following format for each extraction.

*Name : BWCubeFIGL*\
*Type : BWCube*\
*Source : sapbw*\
*Destination : tableau*\
*LastRun : 2018-04-25_12:44:02.422*\
*RowCount : 2733787*\
*LastChange : 2018-02-16_12:18:29.475*\
*Created : 2018-02-14_11:25:47.718*

Get a List of Defined Extractions

```shell
<pre>Function XUGet-Extractions($XUServer, $XUPort){
$XUExtractions= (Invoke-WebRequest "http://$XUServer`:$XUPort").Content | ConvertFrom-CSV
return $XUExtractions
}
$XUExtractions = XUGet-Extractions $XUServer $XUPort
[code lang='powershell']
The following functions gets the list of extraction names from repository. This list can be then used e.g. to run the extraction or to check their logs.
[code lang='powershell']
Function XUGet-ExtractionNames($XUServer, $XUPort){
$XUExtractions = XUGet-Extractions $XUServer $XUPort
$XUExtractionNames = $XUExtractions | foreach { $_.Name } #| where{$_ -like "*PSSAP*"}
return $XUExtractionNames
}
$XUExtractionNames = XUGet-ExtractionNames $XUServer $XUPort

# run all the extractions in the list
XURun-Parallel2 -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtractions $XUExtractionNames

```

### Get the Latest Log of Extractions

The following script gets the latest log of the extractions and writes a colorful output depending on the log status.

Get the Latest Log of Extractions

```shell
Function XUGet-Log($XUServer, $XUPort){
$XUExtractionNames = XUGet-ExtractionNames $XUServer $XUPort
$XULog = @{}
foreach ($XUExtractName in $XUExtractionNames) {
# concatenate URL
$XUURL = "http://$XUServer`:$XUPort/log/?req_type=extraction&amp;name=$XUExtractName"
# get log, convert it to csv, sort by timestamp and select the newest log
$newestLog = (Invoke-WebRequest $XUURL).Content | ConvertFrom-CSV | Sort-Object Timestamp | Select-Object -Last 1
# chech log status
Switch ($newestLog.StateDescr) {
"FinishedNoErrors"{ write-host -f Green $XUExtractName $newestLog}
"FinishedErrors" {write-host -f Red $XUExtractName $newestLog}
"Running" {write-host -f Yellow $XUExtractName $newestLog}
"NotAvailable"{write-host -f Blue $XUExtractName $newestLog}
}
$XULog.Add($XUExtractName, $newestLog)
}
return $XULog
}
$XULog = XUGet-Log $XUServer $XUPort

```

### Get the Metadata of Extractions

This function gets the metadata of the extractions, including field names, data types etc. The output has the following format for each extraction:

*POSITION,NAME,DESC,TYPE,LENGTH,DECIMALS*\
*0,WERKS,Plant,C,4,0*\
*1,NAME1,Name,C,30,0*\
*2,KUNNR,Customer number of plant,C,10,0*\
*3,NAME2,Name 2,C,30,0*

Get the Metadata of Extractions

```shell
# Get Metadata
# http://[host]:[port]/metadata/?name=[extractionName]
Function XUGet-Metadata($XUServer, $XUPort){
$XUExtractionNames = XUGet-ExtractionNames $XUServer $XUPort
$XUMetadata = @{}
foreach ($XUExtractName in $XUExtractionNames) {
# concatenate URL
$XUURL = "http://$XUServer`:$XUPort/metadata/?name=$XUExtractName"
# get log, convert it to csv, sort by timestamp and select the newest log
$tmpmeta = (Invoke-WebRequest $XUURL).Content | ConvertFrom-CSV
$XUMetadata.Add($XUExtractName, $tmpmeta)
}
return $XUMetadata
}
$XUMetadata = XUGet-Metadata $XUServer $XUPort

```

______________________________________________________________________

#### Related Links:

- [Get all scripts from GitHub](https://github.com/KhoderElzein/theobaldsoftware/blob/master/XURunScript.ps1)
- [Documentation: Call extractions from command line](../../documentation/execute-and-automate/call-via-commandline/)
- [Microsoft PowerShell Documentation](https://docs.microsoft.com/en-us/powershell/)

The following article shows how to create SharePoint lists notification using intelligent merge procedure of Xtract Universal.

### Procedure

As the first step, customer master data fron table KNA1 is be loaded into SharePoint.

Afterwards, the alert functionality for the SharePoint list is set up.

Note

The alert setup functionality is available in SharePoint 2013 as well as SharePoint Online by default.

As the following step, the customer master data is changed in SAP:

Next, the complete table KNA1 is extracted again.

Xtract Universal's intelligent merge procedure identifies and sends only those data records to SharePoint that have either been changed or added since the last upload.

### Results

The SharePoint list now contains the updated data set.

### Email Notification

The embedded user in the SharePoint alert is informed by email about the changed records.

______________________________________________________________________

#### Related Links

- [Documentation: SharePoint](../../documentation/destinations/sharepoint/)

The following article describes the process of setting up Single-Sign-On (SSO) via Secure Network Communication (SNC) with SAP client certificates. For more information on using SSO with Xtract Universal (XU), see [Documentation: SAP Single-Sign-On](../../documentation/sap-connection/#single-sign-on-sso).

### Requirements

The usage of *SSO Certificate* requires the correct characteristics of the architecture:

- Implement SAP SSO with X.509 certificates without Secure Login Server, see [SAP-Documentation: Authentication Methods without Secure Login Server](https://help.sap.com/viewer/df185fd53bb645b1bd99284ee4e4a750/LATEST/en-US/7c45fe620ab9469083f7ab50a9008c37.html).
- Implement Microsoft Certificate Store and Active Directory Certificate Templates for SAPGUI/RFC, see [Microsoft TechNet: Certificate Template](https://social.technet.microsoft.com/wiki/contents/articles/53249.active-directory-certificate-services-enterprise-ca-architecture.aspx#Certificate_Template).
- Set up an enrollment agent for Xtract Universal in AD, see [Microsoft TechNet: Establish Restricted Enrollment Agents](https://social.technet.microsoft.com/wiki/contents/articles/10942.ad-cs-security-guidance.aspx#Establish_Restricted_Enrollment_Agents).
- Install the SAP Secure Login Client on the server that runs Xtract Universal, see [SAP-Documentation: Secure Login Client](https://help.sap.com/viewer/8ac26ac20064447ba9e65b18e1bb747e/Cloud/en-US/b304e57f6393461dafd7affc2760b05b.html).\
  The Secure Login Client ensures that the correct SNC library is available for *SSO Certificate*. This library is used to open the SAP connection.
- The Xtract Universal service must run under a Windows AD Service account, see [Run the Xtract Universal Service under a Windows Service Account](../../documentation/server/service-account/).
- Set up access restrictions for the Xtract Universal Designer and the XU server, see [Restrict Access to Windows AD Users (Kerberos Authentication)](../../documentation/access-restrictions/restrict-server-access/#restrict-access-to-windows-ad-users-kerberos-authentication).

### Process

The following graphic illustrates the process of authentication via *SSO Certificate*:

1. The user of the BI tool (caller) triggers an extraction by calling the XU webservice of your Xtract product. The caller uses their Active Directory identity to authenticate against the XU webservice via HTTPS and SPNEGO.
1. The XU server checks if a certificate for the caller is available in the Windows Certificate Store. If no certificate is available for the caller, a new certificate is issued by the Windows enrollment agent.
   1. The XU server requests the Client certificate from the Windows Certificate Store via the Windows API. If a certificate is available, the process continues with step 3. If no certificate is available steps 2b) to 2e) are executed.
   1. The XU server requests an enrollment agent certificate from the Windows Certificate Store via the Windows API. The enrollment agent certificate can be used to issue client certificates.
   1. The XU server receives the enrollment agent certificate from the Windows Certificate Store.
   1. If the requested certificate from 2a) is not found in the Windows Certificate Store, the XU server enrolls a new client certificate for the caller using the enrollment agent certificate.
   1. The Windows Certificate Store receives the new client certificate from the Active Directory Services via MSRPC.
1. The XU server receives the client certificate of the caller from the Windows Certificate Store.
1. The XU server configures the SAP Secure Login Client via the Windows Registry.
1. The Secure Login Client receives the caller's client certificate as specified by the XU server in step 4 from the Windows Certificate Store.
1. The Secure Login Client uses the client certificate of the caller to authenticate the caller's identity via SNC against SAP.
1. The XU server extracts data with the identity and privileges of the caller.
1. The XU server loads the extracted data from 7 to the tool that triggered the extraction.

### Setting up SSO and SNC with Client Certificates

Create a new SAP source system in Xtract Universal to set up SSO with client certificates:

1. Navigate to **[Server > Manage Sources]** in the main menu of the Designer. The window "Manage Sources" opens.
1. Click **[Add]** to create a new SAP source.
1. Open the tab *General* and enter the connection details of your SAP system.
1. Open the tab *Authentication* and activate the option **SNC**.
1. Enter the path to the 64bit version of the SAP Crypto Library in the field *SNC library*, e.g., `C:\Program Files\SAP\FrontEnd\SecureLogin\lib\sapcrypto.dll`. The SAP Crypto Library is installed as part of the SAP Secure Login Client.
1. Enter the SNC partner name of the SAP system in the field **SNC partner name**. This is the same partner name as the SNC name used to set up the SAP GUI.
1. Activate the option **Enroll certificate on behalf of caller (Certificate SSO)**.
1. Enter the technical name of the Active Directory Certificate Template used to authenticate SAP users.
1. Enter the thumbprint of the certificate of the enrollment agent. If you don't know the name or thumbprint, consult the IT department that manages the Active Directory Certificate Services.
1. Click **[Test Designer Connection]** to test your connection settings.
1. Click **[OK]** to confirm your input.

Tip

Create new extractions in the test environment with an SAP connection that uses **Plain Authentication**. Change the SAP source when moving the extraction to the productive environment.

______________________________________________________________________

#### Related Links

- [SAP Help: Secure Network Communications](https://help.sap.com/doc/saphelp_nw73ehp1/7.31.19/en-US/e6/56f466e99a11d1a5b00000e835363f/content.htm?no_cache=true)
- [SAP Help: Secure Login Client](https://help.sap.com/viewer/8ac26ac20064447ba9e65b18e1bb747e/Cloud/en-US/b304e57f6393461dafd7affc2760b05b.html)
- [SAP Help: Logging on with Secure Login Client Using SNC](https://help.sap.com/viewer/df185fd53bb645b1bd99284ee4e4a750/3.0/en-US/68a6caca798e4adbba5608fb69ea6398.html)

The following article shows how to set up Single Sign-On (SSO) with Secure Network Communication (SNC) and External ID.\
*SSO with External ID* uses a Personal Security Environment (PSE) to create a trust relationship between the SAP application server and the service account that runs Xtract Universal. This allows Xtract Universal to impersonate any SAP user.

### Requirements

The usage of *SSO with External ID* requires:

- The Xtract Universal service must run under a Windows AD Service account, see [Run the Xtract Universal Service under a Windows Service Account](../../documentation/server/service-account/).
- Set up access restrictions for the Xtract Universal Designer and the XU server, see [Restrict Access to Windows AD Users (Kerberos Authentication)](../../documentation/access-restrictions/restrict-server-access/#restrict-access-to-windows-ad-users-kerberos-authentication).
- Windows AD users must be mapped to SAP users in the SAP table USRACL, see [SAP Help: User Authentication and Single Sign-On](https://help.sap.com/docs/SAP_NETWEAVER_750/e815bb97839a4d83be6c4fca48ee5777/e54344b6d24a05408ca4faa94554e851.html?locale=en-US).
- The SAP CommonCryptoLib must be installed on the machine that runs Xtract Universal, see [SAP Note 1848999](https://launchpad.support.sap.com/#/notes/1848999).\
  Copy the library (sapcrypto.dll) and the command line tool (sapgenpse.exe) to a local directory, e.g. `C:\PSE\`. For more information, see [SAP Help: Downloading and Installing the SAP Cryptographic Library](https://help.sap.com/docs/SAP_IDENTITY_MANAGEMENT/4773a9ae1296411a9d5c24873a8d418c/3d4ece540ae64e30997498025e37f686.html?locale=en-US).
- The environment variables SECUDIR and SNC_LIB must be set to the PSE directory that contains the SAP CommonCryptoLib.

For more information on PSE, see [SAP Help: Creating PSEs and Maintaining the PSE Infrastructure](https://help.sap.com/doc/saphelp_nw73ehp1/7.31.19/en-us/59/6b653a0c52425fe10000000a114084/frameset.htm).

### The Process

SSO with External ID uses an X.509 certificate & PSE to create a trust relationship between the SAP application server and the service account that runs Xtract Universal. This allows Xtract Universal to impersonate any SAP user.

1. Users authenticate themselves against Xtract Universal via Active Directory (Kerberos) and request data from SAP.
1. Xtract Universal opens an RFC connection via SNC and uses PSE & External ID for authentication.
1. Xtract Universal reads the SAP table USRACL to determine the SAP user that is mapped to the Active Directory user from step 1.
1. Xtract Universal then impersonates the mapped SAP user to request the SAP data via SNC.
1. Xtract Universal retrieves the requested SAP data with the privileges of the caller.
1. Xtract Universal loads the extracted SAP data to the tool that triggered the extraction.

### Setup in SAP

1. Use the SAPGENPSE command line tool to generate an X.509 certificate for the Windows service account that runs Xtract Universal.\
   Use the following command to create the certificate:

   ```console
   sapgenpse gen_pse -p theo-xu.pse

   ```

   The distinguished name of the PSE owner can be the fully qualified hostname of the Xtract Universal server, e.g., `CN=xuserver.example.com`.

1. Use the the following command to export the certificate:

   ```console
   sapgenpse export_own_cert -v -p theo-xu.pse -o theo-xu.crt

   ```

1. Use SAP transaction STRUST to add the certificate to the list of trusted PSE certificates, see [SAP Help: Adding Certificates to PSE Certificate Lists](https://help.sap.com/docs/SAP_NETWEAVER_750/280f016edb8049e998237fcbd80558e7/798e9421e00b4dc1ade3d4199ac60837-35.html?locale=en-US).

1. Use SAP transaction SNC0 to create an access control list item that allows RFC and external IDs for the Common Name (CN) of the certificate created in step 1.

1. Use SAP transaction STRUST to export the server certificate of the SAP server, see [SAP Help: Exporting the AS ABAP's Public-Key Certificate](https://help.sap.com/saphelp_SNC700_ehp01/helpdata/en/47/d84e3c719d1742e10000000a11405a/frameset.htm).

1. Copy the exported server certificate to the PSE directory of the machine that runs Xtract Universal.

1. Use the SAPGENPSE command line tool to import the server certificate to the client PSE. Example:

   ```console
   sapgenpse maintain_pk -v -a server.crt -p theo-xu.pse

   ```

1. Use the following command to create a credentials file (cred_v2), see [SAP Help: Creating the Server's Credentials Using SAPGENPSE](https://help.sap.com/saphelp_snc70/helpdata/en/32/ce2e3ad962a51ae10000000a11402f/frameset.htm).

   ```console
   sapgenpse seclogin -p theo-xu.pse –O SAPServiceUser

   ```

   The credentials file gives Xtract Universal access to the PSE without providing the password for the PSE.

The PSE directory should now contain the following files:

- the client PSE `theo-xu.pse`
- the client certificate `theo-xu.crt`
- the server certificate that was exported from your SAP system `[server].crt`
- the credentials file `cred_v2`

### Setup in Xtract Universal

Create a new SAP source system in your Xtract Universal to set up SSO with External ID:

1. Navigate to **[Server > Manage Sources]** in the main menu of the Designer. The window "Manage Sources" opens.
1. Click **[Add]** to create a new SAP source.
1. Open the tab *General* and enter the connection details of your SAP system.
1. Open the tab *Authentication* and activate the option **Secure Network Communications (SNC)**.
1. Enter the name of an SAP user in the field **User** for the initial login with Xtract Universal.\
   This user must be a technical user (SAP user with security policy set to *Technical User*) and must have privileges to read the SAP table USRACL via the function module RFC_READ_TABLE.
1. Enter the complete path to the SAP cryptographic library in the field **SNC Library**, e.g. `C:\PSE\sapcrypto.dll`.
1. Enter the SPN of the SAP service account in the field **SNC partner name**. Use the following notation: `p:[SPN]@[Domain-FQDN-Uppercase]`.
1. Enable the option **SSO - Log in as caller via External ID**.
1. Click **[Test Connection]** to verify your connection settings.
1. Click **[OK]** to save your changes.

______________________________________________________________________

#### Related Links

- [Documentation: SAP Single-Sign-On](../../documentation/sap-connection/#single-sign-on-sso)
- [Documentation: Run the Xtract Universal Service under a Windows Service Account](../../documentation/server/service-account/).
- [SAP Help: Creating PSEs and Maintaining the PSE Infrastructure](https://help.sap.com/doc/saphelp_nw73ehp1/7.31.19/en-us/59/6b653a0c52425fe10000000a114084/frameset.htm)

The following article describes the required steps for setting up Single Sign-On (SSO) with Secure Network Communication (SNC) and Kerberos encryption.

Note

SAP officially does not support the Kerberos Wrapper Library (gx64krb5.dll) anymore.

Warning

**Single Sign-On availability.**\
ABAP application server has to run on a Windows OS and SNC with Kerberos encryption setup on SAP.

### Requirements

- The SAP ABAP application server runs under a Windows operating system.
- The BI client (which calls the extraction) runs under Windows.
- The SAP Kerberos Wrapper Library (gsskrb5) is used as the SNC solution.

### "Double Hop" Issue

SNC solution must support the Windows credentials being passed on by Xtract Universal. Since Active Directory is based on Kerberos, a "Double Hop" issue may occur. Here is a possible solution to the "Double Hop" issue:

For security reasons, Kerberos does not allow credentials to be passed on. But there are Kerberos extensions from Microsoft (S4U extensions) that allow credentials passing on. These extensions are also known as "constrained delegation". Because the Kerberos Wrapper Library uses the Microsoft extensions for Kerberos to work around the "Double Hop" issue, it is only available for Windows. It therefore only works with SAP application servers under Windows and clients under Windows.

Unlike the Kerberos Wrapper Library (gsskrb5) from SAP (according to SAP), SAP's Common Crypto Library does not explicitly support credentials to be passed on. The Kerberos Wrapper Library (gsskrb5) used by multiple customers of Theobald Software.

When using an SNC solution from a third-party vendor, use either the Kerberos Wrapper Library or a corresponding solution of the third-party vendor.

### Activation of HTTPS

1. Enable access control protocol HTTPS within the tab *Web Server* settings.
1. Reference an existing [X.509 certificate](../../documentation/access-restrictions/install-x.509-certificate/) .
1. Click **[OK]** to confirm .

Make sure to check the default ports, e.g., *8165* in Xtract Universal.

### Configuration of Windows AD Service Account

Using SSO with Kerberos SNC, the Xtract Universal service must run under a dedicated service account.

Note

As of Xtract Universal version 5.0 SAP passwords are encrypted with a key that is derived from the Windows account that runs the XU service. The passwords can only be accessed from the same service account, when restoring a backup or moving the files to a different machine. If the service account changes, passwords need to be re-entered manually.

### Server Settings

Warning

**Incompatible library.**\
Xtract Universal runs on 64bit OS only. Kerberos Wrapper Library gx64krb5.dll(64-Bit version) is required.\
Download `gx64krb5.dll` from [SAP Note 2115486](https://launchpad.support.sap.com/#/notes/2115486).

1. Copy the Kerberos Wrapper Library to the file system, e.g., to `C:\SNC\gx64krb5.dll` of the machine, where the service is running.

1. Place the downloaded .dll file on each machine, where the Designer is running.

1. Open "Computer Management" by entering the following command: `compmgmt.msc`.

1. Under **Local Users and Groups** select **Groups > Administrators**.

1. Click **[Add]** to add the service account to the local admin group .

1. Open "Local Security policy" by entering the following command: `secpol.msc`.

1. Select **[Local Policies > User Rights Assignment]**:

   - Act as part of the operating system
   - Impersonate a client after authentication

1. Change the registry settings of the server machine:

   | **Field** | **Registry Entry** | | --- | --- | | SubKey | HKEY_LOCAL_MACHINE\\SOFTWARE\\SAP\\gsskrb5 | | Entry | ForceIniCredOK | | Type | REG_DWORD | | Value | 1 |

### SAP Source Settings

Note

An existing [SAP connection](../../documentation/sap-connection/) to a Single Application Server or Message Server is the prerequisite for using SSO with SNC.

1. In the main menu of the Xtract Universal Designer, navigate to **[Server > Manage Sources]**. The window "Source Details" opens.
1. Select an existing SAP source and click **[Edit]** (pencil symbol).
1. Enable the **SNC** option in the subsection *Authentication*.
1. Enable the checkbox *Impersonate authenticated caller (SSO)* .
1. Enter the complete path of the Kerberos library in the field *SNC library* e.g., `C:\SNC\gx64krb5.dll` .
1. Enter the SPN of the SAP service account in the field *Partner name* . Use the following notation: `p:[SPN]@[Domain-FQDN-Uppercase]`.
1. Click **[Test Connection]** to verify your connection settings.
1. Click **[OK]** to confirm.

Note

The SAP Logon Pad SNC settings for partner name differ from the ones used in Xtract Universal. SAP Logon Pad uses the UPN of the SAP service accounts and Xtract Universal uses the Service Principal Name (SPN). Use the following notation: *p:[SAP Service Account]@[domain]*. SPN's are case sensitive in the SNC partner name.

### SNC Activation in SAP

In SAP, apply the Kerberos SNC settings as described in the [SAP Help: Single Sign-On with Microsoft Kerberos SSP](https://help.sap.com/viewer/e815bb97839a4d83be6c4fca48ee5777/7.5.9/EN-US/440ebf6c9b2b0d1ae10000000a114a6b.html).

______________________________________________________________________

#### Related Links

- [Documentation: X.509 certificate](../../documentation/access-restrictions/install-x.509-certificate/)
- [Documentation: Run the Xtract Universal Service under a Windows Service Account](../../documentation/server/service-account/).

The following article describes the process of running extractions for [pull destinations](../../documentation/destinations/) using Single-Sign-On (SSO) with SAP Logon-Ticket. For more information on using SSO with Xtract Universal (XU), see [Documentation: SAP Single-Sign-On](../../documentation/sap-connection/#single-sign-on-sso).

### Requirements

- The Xtract Universal server must be set up to use HTTPS, see [SSO with Kerberos SNC](../sso-with-kerberos-snc/#activation-of-https).
- The Xtract Universal service must run under an [XU Service Account](../../documentation/server/service-account/).
- The XU service account must be configured for *Constrained Delegation* to the SPN of the AS Java in AD.
- An Application Server Java (AS Java) must be set up as a Ticket Issuing System, see [SAP Help: Configuring the AS Java to Issue Logon Tickets](https://help.sap.com/doc/saphelp_nw75/7.5.5/EN-US/4a/412251343f2ab1e10000000a42189c/frameset.htm).
- The AS Java must be configured for SPNEGO/Kerberos.
- A mapping between Windows AD users and SAP identities must be configured in the AS Java. The AS Java must be configured to generate SAP Logon Tickets. Consult with your SAP Basis team for more information.
- The SAP AS ABAP must be configured to trust SAP Logon Tickets from the AS Java, see [SAP Help: Using Logon Tickets with AS ABAP](https://help.sap.com/doc/saphelp_nw75/7.5.5/en-US/9d/472b83bbed4915b84b30e539c625ae/frameset.htm).

### Process of Authentication

The following graphic illustrates the process of calling an extraction with SSO via Ticket Issuer:

1. The BI tool user starts an extraction by calling the XU web service. They authenticate against the XU web service with their Active Directory identity, using HTTPS and SPNEGO.
1. The XU server contacts the Active Directory Domain Controller via Kerberos and tries to impersonate the web service caller (BI tool user) against the SAP AS Java (ticket issuer).
1. The XU server receives a Kerberos ticket from the DC that allows it to impersonate the caller against the AS Java.
1. The XU server uses the Kerberos ticket from 3. to authenticate against the AS Java as the caller via HTTPS and SPNEGO.\
   Prerequisite: The AS Java has been configured for SPNEGO/Kerberos.
1. The AS Java maps the caller's AD identity to an SAP user and generates an SAP Logon Ticket for this SAP user. The AS Java sends the SAP Logon Ticket to the XU server via HTTPS as the value of the MYSAPSSO2 cookie.
1. The XU server takes the SAP Logon Ticket that it has received from the AS Java and uses it to authenticate against the AS ABAP as the caller via RFC.
1. The XU server extracts data from the AS ABAP using the identity and privileges of the caller (BI tool user) via RFC.
1. The XU server sends the extracted data from 7. to the caller.

______________________________________________________________________

#### Related Links

- [Set Up an XU Service Account](../../documentation/server/service-account/)
- [SAP Help: Kerberos and SAP NetWeaver AS for Java](https://help.sap.com/doc/saphelp_nw75/7.5.5/EN-US/4c/8a4d292e2849a8b7cbd229be5c94a5/frameset.htm)
- [SAP Help: Using Logon Tickets with AS ABAP](https://help.sap.com/doc/saphelp_nw75/7.5.5/EN-US/d0/dc33c460a243929b7ec120f55af101/frameset.htm)
- [Youtube-Tutorial: Kerberos-Based Single Sign-On to Application Server Java Unlisted](https://www.youtube.com/watch?v=GRIkarGsU5U)

The following article gives an overview of all SAP S/4HANA systems supported by Xtract Universal.

### Supported SAP S/4HANA Cloud Systems

The following table lists all available SAP S/4HANA cloud systems and their compatibility with Xtract Universal.\
SAP S/4HANA Cloud Edition (Cloud ERP):

| | S/4HANA Public Cloud | S/4HANA Private Cloud | | --- | --- | --- | | **Compatible with Xtract Universal:** | (with limitations) | | | **Limitations:** | Only [OData services](../../documentation/sap-connection/settings/#source-type-odata) and Remote Function Modules (BAPIs) released via Communication Scenarios are supported. Function Modules are accessed using the [RFC over Websocket](../../documentation/sap-connection/settings/#general) protocol. | - |

### SAP S/4HANA On Premise Systems

The following table lists all available SAP S/4HANA on premise systems and their compatibility with Xtract Universal.\
SAP S/4HANA AnyPremise (ERP in the Cloud):

| | S/4HANA Private Cloud Managed by SAP (HEC) | SAP HEC Customer Edition | S/4HANA AnyPremise | S/4HANA AnyPremise | | --- | --- | --- | --- | --- | | Additional Information: | On-Premises Edition | Runs on Customer Data Center | Public Cloud (AZURE/AWS/GCP) | called ERP in DC (Customer Data Center) | | Compatible with Xtract Universal: | | | | |

The following article shows how to handle the initial table load for delta extractions using the Table CDC extraction type. The article applies in the following situation:

- The Table CDC extraction type is run on SAP releases < 7.10
- The option **Extract table on first run** (Delta initialization) is activated.
- The delta initialization takes longer than the maximum processing time specified in the SAP profile parameter **rdisp/max_wprun_time**.
- The extraction aborts with an error message, e.g., `ERPConnect.ABAPRuntimeException: RfcInvoke failed(RFC_ABAP_RUNTIME_FAILURE): TIME_OUT - Time limit exceeded`.

Note

The custom function module /THEO/READ_TABLE used by the Table CDC component to extract the table does not support background mode on SAP releases < 7.10. The background mode avoids the timeout mentioned above.

### Recommended Workflow

1. Create a Table CDC extraction. Make sure the option **Extract table on first run** is deactivated.
1. Run the Table CDC extraction to initialize the delta extractions. This ensures that no data is missed between table extraction and delta initialization.
1. Create a regular Table extraction using the Table extraction type. Make sure to select an SAP standard function module, e.g., RFC_READ_TABLE.
1. Run the Table extraction.

______________________________________________________________________

#### Related Links

- [Documentation: Define the TableCDC Extraction Type](../../documentation/table-cdc/#define-the-table-cdc-extraction-type).
- [Documentation: Define the Table Extraction Type](../../documentation/table/#define-the-table-extraction-type).
- [Documentation: Table Extraction Settings - Function modules and Background Jobs](../../documentation/table/settings/#extract-data-in-background-job)

The following article illustrates the process of delta table extractions using the Table CDC extraction type.

### Table CDC Process

The delta mechanism of Table CDC includes the following processes:

- Lookup process to read SAP metadata for the definition of the Table CDC extraction.
- CDC watch process to create a database trigger on the source table and to create the corresponding log table in SAP.
- Synchronize data process to run Table CDC extractions regularly.

The depicted graphic illustrates the processes in both Xtract Universal and in SAP. Click the graphic to zoom in.

Tip

Use the SAP transaction DB02 to view all triggers of Table CDC log tables in SAP.

The following article illustrates the extraction mechanism of the Table extraction type.

### Table Extraction Process

The extraction mechanism of Table includes the following processes:

- Lookup process to read SAP metadata for the definition of the Table extraction.
- Configuration of the Table extraction type definition.
- Execution of the asynchron data loading process.

The depicted graphic illustrates the mechanism in both Xtract Universal and in SAP. Click the graphic to zoom in.

The following article describes how to use the **Target Principal** field when connecting the Xtract Universal Designer to an Xtract Universal Server.

The use of a Target Principal Name (TPN) is required to use Kerberos transport encryption or to authenticate Active Directory users. The Target Principal Name (TPN) can be either a User Principal Name (UPN) or a Service Principal Name (SPN).

Note

The Target Principal Name only needs to be changed in the Xtract Universal Designer login screen if the service account of the Xtract Universal Windows service is changed.

### About Target Principal Name (TPN)

By default, the Xtract Universal Service is executed under the Local System Account.

In the Active Directory (AD), this user acts as a computer account. When dialing into a remote server where the service is not used in the local environment, both an UPN and an SPN can be used in the following form:

| Field | Syntax | Example | | --- | --- | --- | | XU Server | `[host].[domain]:[port]` | theosoftw2012r2.theobald.local:8064 | | Target Principal as **UPN** | `[AD-user]@[domain]` | svc_xusrv@theobald.local | | Target Principal as **SPN** | `[service class]/[host]@[domain]` | HTTP/theosoftw2012r2.theobald.local@THEOBALD.LOCAL |

The Target Principal Name must correspond either to the UPN of the user under which the Xtract Universal Windows service is running, or to an SPN assigned to this user. The UPN or SPN of the Xtract Universal Windows service executes the write processes for the target environments in this context.\
Accordingly, this user must have the necessary write permissions for the database.

Note

Xtract Universal can be used as a distributed application on a central application instance in the company network using appropriate UPNs or SPNs.\
All users connect to this remote server in the company network using the locally installed Xtract Universal Designer.

#### Example

### User Principal Name (UPN)

A User Principal Name identifies users in a domain. For more information, see [Microsoft Documentation: User Principal Name](https://docs.microsoft.com/en-us/windows/win32/secauthn/user-name-formats?redirectedfrom=MSDN#user-principal-name). A UPN is assigned in the following form:

| Field | Syntax | Example | | --- | --- | --- | | XU Server | `[host].[domain]:[port]` | TODD.theobald.local:8064 (or localhost:8064) | | Target Principal | `[AD-user]@[domain]` | steffan@theobald.local |

Note

After changing the user context of the Windows service, the UPN or SPN for logging in to the Xtract Universal Server must also be adjusted.

Follow the steps below to configure the service to use with UPN:

1. Open *Windows Services (Local)*.
1. Right-click the Xtract Universal service to open the service **Properties**.
1. Open the *Log-on* tab and switch to **This Account**.
1. Click **[Browse]** to look up Windows AD users.
1. Click **[Locations]** and select *Entire Directory*.
1. Select an existing UPN or SPN and confirm with **[OK]**.
1. Apply the changes by restarting the Xtract Universal service.
1. Adjust the UPN in the Target Principal field when logging on to the Xtract Universal Designer.

### Service Principal Name (SPN)

A Service Principal Name is an identifier for services within an authentication domain. For more information, see [Microsoft Documentation: Service Principal Names](<https://msdn.microsoft.com/en-us/library/ms677949(VS.85).aspx>). An SPN is assigned in the following form:

| Field | Syntax | Example | | --- | --- | --- | | XU Server | `[host].[domain]:[port]` | TODD.theobald.local:8064 (or localhost:8064) | | Target Principal | `HOST/[hostname]@[domain]` | HOST/TODD.theobald.local@THEOBALD.LOCAL |

The service class and host name are required for authenticating a service instance to a logon account. Domain Admin rights are required for processing *Manage Service Accounts* in Active Directory Users and Computers.

### Windows Service does not Start

When a service does not start, configure the service to use a user account with the following rights:

- **Local Security Policy > Local Policies > User Right Management**: *Log on as a service*
- Permissions for the installation folder and subfolders: *Modify*
- HTTP URL Access Control List e.g., `urlacl url=https://+:80/MyUri user=DOMAIN\user`

______________________________________________________________________

#### Related Links

- [Microsoft Documentation: User Principal Name](https://docs.microsoft.com/en-us/windows/win32/secauthn/user-name-formats?redirectedfrom=MSDN#user-principal-name)
- [Microsoft Documentation: Service Principal Names](<https://msdn.microsoft.com/en-us/library/ms677949(VS.85).aspx>)
- [Enable Secure Network Communcation (SNC)via X.509 Certificate](../enable-snc-using-pse-file/)

The following article shows how to use lists and SELECT statements in the [WHERE Clause Editor](../../documentation/table/where-clause/#where-clause-editor) of the Table extraction type.\
Lists can contain multiple values separated by commas e.g., `1,10` or `“1”, “10”`.

### List Parameters

1. Create a Table extraction.
1. Look up the table KNA1, see [Documentation: Define the Table Extraction Type](../../documentation/table/#define-the-table-extraction-type).
1. Click **Edit runtime parameters**. The window "Edit runtime parameters" opens.
1. Click **[Add List]** to define a list parameter, e.g., *Parameter0*.
1. Click **[OK]** to save the parameter. The window "Edit runtime parameters" closes.
1. Open the *WHERE Clause* tab and click **[Editor Mode]** to open the WHERE clause editor.
1. Click **[Add criteria]**, then **[Default with parameter]** to create an empty template in the WHERE clause editor.
1. Select the column *ORT01* from KNA1 as the data you want to filter.
1. Select *IN* as the operator. *IN* is the only operator that can be used for lists.
1. Click **[Select parameter]** in the static parameter component of the WHERE clause. A drop down list opens.
1. Select an existing list parameter from the drop down list, e.g., *Parameter0*.
1. Click **[OK]** to confirm your input.
1. Click **[Load live preview]** or run the extraction to check the output.\
   When providing values for the list parameter, use multiple values separated by commas e.g., `1,10` or `“1”, “10”`.

### Static Lists

The depicted example statement returns all active customers (rows in the table KNA1) that have an address in one of the following cities: Berlin, Stuttgart, Paris, Seattle, Hong Kong or Dongguan.

1. Create a Table extraction.
1. Look up the table KNA1, see [Documentation: Define the Table Extraction Type](../../documentation/table/#define-the-table-extraction-type).
1. Open the *WHERE Clause* tab and click **[Editor Mode]** to open the WHERE clause editor.
1. Click **[Add criteria]**, then **[Default with literal]** to create an empty template in the WHERE clause editor.
1. Select the column *ORT01* from KNA1 as the data you want to filter.
1. Select *IN* as the operator. *IN* is the only operator that can be used for lists.
1. Select *List* as the type of the static filter value.
1. Click **[Press to Edit]** in the static value component of the WHERE clause. The window "Edit List" opens.
1. Select *String* as the **Type** of the list. When working with numbers, select *Number*.
1. Click **[Add]** to add items to the list. You can edit items via double-click.
1. Click **[OK]** to confirm your input.
1. Click **[Load live Preview]** or run the extraction to check the output.\
   When providing values for the list parameter, use multiple values separated by commas e.g., `1,10` or `“1”, “10”`.

### SELECT Statement

SELECT statements can be used to select data from SAP tables, see [ABAP Documentation: Open SQL SELECT](https://help.sap.com/doc/abapdocu_750_index_htm/7.50/en-us/abapselect.htm).\
The depicted example statement returns all active customers (rows in the table KNA1) that have a sales document in the table VBAK for sales document header data.

Note

The usage of SELECT statements is only possible as of SAP Release 7.40, SP05.

1. Create a new Table extraction.

1. Look up the table KNA1, see [Documentation: Define the Table Extraction Type](../../documentation/table/#define-the-table-extraction-type).

1. Open the *WHERE Clause* tab and click **[Editor Mode]** to open the WHERE clause editor.

1. Click **[Add criteria]**, then **[Default with literal]** to create an empty template in the WHERE clause editor.

1. Select the column *KUNNR* from KNA1 as the data you want to filter.

1. Select *IN* as the operator. *IN* is the only operator that can be used for lists.

1. Select *List* as the type of the static filter value.

1. Click **[Press to Edit]** in the static value component of the WHERE clause. The window "Edit List" opens.

1. Select *SELECT* as the **Type** of the list.

1. Enter the following SELECT statement to create a list that contains all items of the column KUNNR from the SAP table VBAK:

   ```text
   SELECT KUNNR FROM VBAK

   ```

1. Click **[OK]** to confirm your input.

1. Click **[Load live Preview]** or run the extraction to check the output.

______________________________________________________________________

#### Related Links

- [Documentation: WHERE Clause Editor](../../documentation/table/where-clause/#where-clause-editor)

This article shows how to parameterize extractions in Visual Studio as part of Xtract Universal's [Power BI Report Server destination](../../documentation/destinations/server-report-services/). The depicted example demonstrates how to pass multiple parameters using a single computed query parameter.

### About Parameters

When creating reports with Visual Studio you can incorporate dynamic and optional parameters to let users filter data without having to change the report itself. To do this, the parameters are passed as runtime parameters to Xtract Universal. In Visual Studio's Report Server Projects, the runtime parameters of Xtract Universal allow only one parameter input for each runtime parameter. To pass multiple parameters, a computed query parameter can be used.

The depicted example uses the [WHERE clause](../../documentation/table/where-clause/) of a Table extraction to demonstrate how to pass multiple parameters using a single computed query parameter.

### Prerequisites

- Create a Table extraction with an SSRS destination in Xtract Universal. The depicted example uses the SAP standard table *MAKT*.
- Add the extraction as a data source in Visual Studio and create a report as described in [Power BI Report Server in Visual Studio](../../documentation/destinations/server-report-services/#add-an-extraction-as-a-data-source-in-visual-studio).

### Computed Query Parameters

The depicted example uses data from the columns SPRAS and MATNR to filter the SAP standard table *MAKT*. Input parameters for both columns are combined in a single computed parameter that is passed to the WHERE clause.

Follow the steps below to create a parameter that encapsulates 2 other parameters:

1. Create 3 query parameters:

   1. Right-click the data set in the *Report Data* section and select **Dataset Properties**. The window "Dataset Properties" opens.
   1. Switch to the tab *Parameters* and click **[Add]**.

1. Name two parameters after the columns you want to filter, e.g., "spras" and "matnr". Name the other parameter "where". This will be the computed query parameter.

1. Click the **[f(x)]** button next to the "where" parameter. The window "Expressions" opens.

1. Enter the name of the two columns and the value of their respective query parameter. Link them together using " and ":

   ```text
   = "spras " & Parameters!spras.Value & " and " & "matnr " & Parameters!matnr.Value

   ```

1. Confirm your input with **[OK]**.

1. Switch to the Query tab and click **[Query Designer…]**. The window “Query Designer” opens.

1. Select *Parameterized* as the **Behaviour** for Xtract Universal's "where" parameter.

1. Enter the name of the new query parameter under **Value**.

1. Confirm your input with **[OK]**.

Use the **Preview** mode of Visual Studio to test the inputs.

### Optional Computed Query Parameters

To use parameters as optional input, the expression for the "where" parameter needs to be edited to accommodate optional parameters. Follow the steps below to make input parameters optional:

1. Right-click the input fields of the 2 parameters you want to be optional and select *Parameter Properties*. The window "Report Parameter Properties" opens.
1. In the *General* tab, activate the checkbox **Allow null value**.
1. Confirm your input with **[OK]**. A checkbox appears next to the input field.
1. If the checkbox is activated, the parameter will send *Nothing* and thus be ignored at runtime.

### Optional Parameters in Expressions

In this example we add an optional WHERE clause to the query. The goal is to filter the SPRAS and MATNR columns, with both filters being optional.\
This results in 4 possible behaviors for the WHERE clause:

- no filtering, the runtime parameter "where" is ignored
- filter by "spras" only, e.g., `spras > 5`
- filter by "spras" and "matnr", e.g., `spras > 5 and matnr = TG0012`
- filter by "matnr" only, e.g., `matnr = TG0012`

Follow the steps below to edit the "where" expression to accommodate optional input:

1. Navigate to the query parameters:

   1. Right-click the data set in the *Report Data* section and select **Dataset Properties**. The window "Dataset Properties" opens.
   1. Switch to the tab *Parameters*.

1. Click the **[f(x)]** button next to the "where" parameter. The window "Expressions" opens.

1. Edit the expression to include the following conditions:

   Optional Parameters

   ```c++
   = IIf (IsNothing(Parameters!spras.Value) And IsNothing(Parameters!matnr.Value),Nothing,
   IIf(IsNothing(Parameters!spras.Value),"","spras " & Parameters!spras.Value) &
   IIf(Not(IsNothing(Parameters!spras.Value)) And Not(IsNothing(Parameters!matnr.Value))," and ","") &
   IIf(IsNothing(Parameters!matnr.Value),"","matnr " & Parameters!matnr.Value))

   ```

   1. The first line checks if both parameters are *Nothing*. In this case the expression returns *Nothing* and the evaluation is complete. Else either one or both parameters are not set to *Nothing*.
   1. The second line checks if "spras" is not *Nothing* and adds the column name and value to the expression if that is the case. Else the expression is left unchanged ("").
   1. The third line checks if both "spras" and "matnr" are not *Nothing* and adds an " and " to the where clause. Else the expression is left unchanged ("").
   1. The last line checks if the "matnr" is not *Nothing* and adds the column name and value to the expression if that is the case. Else the expression is left unchanged ("").

1. Confirm your input with **[OK]**.

Use the **Preview** mode of Visual Studio to test the inputs.
# API Reference

This section contains a list of all API endpoints that are available for Xtract Universal.

Xtract Universal offers a web API that allows running extractions and querying meta information and extraction logs from Xtract Universal through web calls. The web API returns the result as an http-json stream.

### Base URL

The basic URL for web calls uses the following format: `[protocol]://[host or IP address]:[port]/`.

#### Examples

| Protocol | Syntax | Example | | --- | --- | --- | | HTTP | `http://[host].[domain]:[port]` | `http://sherri.theobald.local:8065` | | HTTP | `http://[host]:[port]` | `http://localhost:8065` | | HTTPS | `https://[host].[domain]:[port]` | `https://sherri.theobald.local:8165` Requires a dedicated host name and X.509 certificate, see [web server settings](../documentation/server/server-settings/#web-server). |

Note

Make sure to use the correct ports, see [Server Ports](../documentation/server/#ports).

### Run Extractions

```text
[protocol]://[host]:[port]/run/[extraction_name]

```

Runs the specified extraction.

```text
[protocol]://[host]:[port]/start/[extraction_name]

```

Runs the specified extraction asynchronously and returns the run status immediately.

Note

If the extraction is located inside an [extraction group](../documentation/organize-extractions/), the name of the extraction group is part of the extraction name. Example: The extraction "KNA1" in the extraction group "S4HANA" becomes `S4HANA,KNA1`.

Warning

**Deprecated Endpoints:**

- `[protocol]://[host]:[port]/?name=[extraction_name]` becomes `[protocol]://[host]:[port]/run/[extraction_name]`
- `[protocol]://[host]:[port]/?name=[extraction_name]&wait=false` becomes `[protocol]://[host]:[port]/start/[extraction_name]`

#### Response

The response of the web service call contains the following information:

| | Response | Description | | --- | --- | --- | | | HTTP status code | The HTTP status code *200* indicates a successful extraction call. It does not indicate a successful execution of the extraction. The HTTP status code *404* indicates that the called extraction does not exist. Detailed information can be found in the log of the web service. | | | HTTP header | Shows the timestamp of the extraction in the HTTP header e.g., X-XU-Timestamp: *2021-04-09_19:03:09.971* | | | HTTP response body | The Response in the HTTP body depends on the destination type of the extraction. Depending on the destination type, the extracted data is returned in either CSV or JSON format. |

#### Parameters for `/start/[extraction_name]`

| Parameter | Description | | --- | --- | | `&[parameter1_name]=[value]` | Runs the specified extraction and passes values to the specified [extraction parameters](../documentation/parameters/extraction-parameters/). | | `&quiet-push=true` | Runs the specified extraction and suppresses the output of extraction logs for push destinations. This parameter has no effect on pull destinations and asynchronous extractions. |

Tip

You can use the UI in the "Run Extraction" menu to generate a URL for extraction runs, see [Run Extractions](../documentation/execute-and-automate/run-an-extraction/#run-extractions-in-the-designer).

#### Examples

http://sherri.theobald.local:8065/start/KNA1

```text
MANDT,KUNNR,LAND1,NAME1,ORT01
800,0000000313,DE,zdemo customer zr,
800,0000001824,IN,,
800,0000001832,IN,cus 3,
800,0000001834,IN,52 CUSTOMER,
800,0000001837,IN,emax,
800,0000005002,JP,One time customer,
800,0000010009,,Einmalkunde,
800,0000011113,DE,Myers Corp.,
800,0000011114,DE,Watson & Watson,
800,0000100016,DE,Einmalkunde,
800,0000100026,DE,Reference Customer for Internet,
800,0000100236,US,,
800,0000100291,IN,RELIANCE FRESH DOMESTIC CUSTOMERS,
800,0000200000,IN,MA Reddy Customer,
800,0000200001,IN,SD Customer,
800,0000200002,IN,SMR Customer,
800,0000300050,,,
800,0000300065,GB,ComputerWorld,
800,0000300320,GB,Customer 1,
800,0000300430,DE,testcompany crm,
800,0000300735,US,OTHER MASS,
800,0000300736,US,OTHER GROCRY,
800,0000300737,US,OTHER CONVENIENCE,
800,0000300738,US,OTHER MILITARY,
800,0000300739,US,OTHER FOOD,
...

```

http://sherri.theobald.local:8065/start/KNA1/?cityParam=Stuttgart&companyParam=Theobald+Software

```text
MANDT,KUNNR,LAND1,NAME1,ORT01
800,0000000779,DE,Theobald Software,Stuttgart"

```

### Abort Extractions

```text
[protocol]://[host]:[port]/stop/[extraction_name]

```

Aborts all running extractions that use the specified extraction name. If the abortion is successful, a confirmation message is returned in the HTTP body.

Note

If the extraction is located inside an [extraction group](../documentation/organize-extractions/), the name of the extraction group is part of the extraction name.

Example: The extraction "KNA1" in the extraction group "S4HANA" becomes `S4HANA,KNA1`.

#### Parameters and Options for `/stop/[extraction_name]`

| Parameter | Description | | --- | --- | | `/[yyyy-MM-dd_HH:mm:ss.SSS]` | Only aborts the extraction with the specified extraction name and the specified timestamp. |

Note

The following endpoint is still supported, but can be replaced by `/stop/[extraction_name]/[yyyy-MM-dd_HH:mm:ss.SSS]`:

```text
[protocol]://[host]:[port]/abort?name=[extraction_name]

```

Aborts the latest run of the specified extraction. If the abortion is successful, a confirmation message is returned in the HTTP body.

#### Example

http://sherri.theobald.local:8065/stop/KNA1

```text
Extraction cancellation succeeded.

```

### Get Status of an Extraction

```text
[protocol]://[host]:[port]/status/?name=[extraction_name]&timestamp=[yyyy-MM-dd_HH:mm:ss.SSS]

```

Returns the status of a (running) extraction at the specified timestamp.

Note

If the extraction is located inside an [extraction group](../documentation/organize-extractions/), the name of the extraction group is part of the extraction name.

Example: The extraction "KNA1" in the extraction group "S4HANA" becomes `S4HANA,KNA1`.

Tip

The timestamp corresponds to the *startedAt* element returned by [`[protocol]://[host]:[port]/config/extractions/`](#get-extraction-details) or [`[protocol]://[host]:[port]/logs/extractions/[extraction-name]`](#get-extraction-logs). If an extraction is triggered by an [http request](#run-extractions), the extraction's timestamp is returned in the field `X-XU-Timestamp` of the http request's response header.

#### Response

| State | Description | | --- | --- | | Running | The extraction is running. | | FinishedNoErrors | The extraction is finished without errors. | | FinishedErrors | The extraction is finished, but with at least one error. | | Cancelled | The extraction is finished and was cancelled by the user. |

#### Example

http://sherri.theobald.local:8065/status/?name=KNA1&timestamp=2024-02-05_10:23:08.025

```text
FinishedNoErrors

```

### Get Extraction Logs

```text
[protocol]://[host]:[port]/logs/extractions

```

Returns a list of extraction runs.

#### Response

The result contains the following elements:

| Item | Description | | --- | --- | | extractionName | name of the extraction (if the extraction is located inside an [extraction group](../documentation/organize-extractions/), the name of the extraction group is part of the extraction name, e.g., the extraction "KNA1" in the extraction group "S4HANA" becomes `S4HANA,KNA1`) | | runs | contains *rowCount*, *duration*, *state*, *webServerLog* and *startedAt* of extraction runs | | row count | number of extracted data records | | duration | duration of the execution | | state | status of the extraction (*Running*, *FinishedNoErrors*, *FinishedErrors*, *Cancelled*) | | webServerLog | timestamp of the corresponding [server log](../documentation/logs/#log-levels) | | startedAt | timestamp of the execution |

#### Parameters & Options for `/logs/extractions`

| Parameter | Description | | --- | --- | | `?min=[yyyy-MM-dd_HH:mm:ss.SSS]` | Returns the extraction runs after the specified date and time. | | `?max=[yyyy-MM-dd_HH:mm:ss.SSS]` | Returns the extraction runs before the specified date and time. | | `/[extraction-name]` | Returns all extraction runs of the specified extraction. | | `/[extraction-name]/[yyyy-MM-dd_HH:mm:ss.SSS]` | Returns the extraction run of the specified extraction with the specified timestamp. | | `/[extraction-name]/[yyyy-MM-dd_HH:mm:ss.SSS]/log` | Returns the extraction log of the specified extraction with the specified timestamp. |

Note

For information on how to interpret logs, see [Logs](../documentation/logs/).

#### Examples

http://sherri.theobald.local:8065/logs/extractions?min=2023-08-17_11:20:44.029

```json
{
"extractions": 
[
    {
        "extractionName": [
            "MEBEST"
        ],
        "runs": [
            {
                "rowCount": 53,
                "duration": "PT00H00M00.541S",
                "state": "FinishedNoErrors",
                "webServerLog": "2024-05-31_07:19:36.156",
                "startedAt": "2024-05-31_07:19:37.006"
            }
        ]
    },
    {
        "extractionName": [
            "S4HANA",
            "BSEG"
        ],
        "runs": [
            {
                "rowCount": 12036,
                "duration": "PT00H00M00.584S",
                "state": "FinishedNoErrors",
                "startedAt": "2024-06-12_08:56:28.066"
            }
        ]
    },
    {
        "extractionName": [
            "S4HANA",
            "KNA1"
        ],
        "runs": [
            {
                "rowCount": 10071,
                "duration": "PT00H00M03.753S",
                "state": "FinishedNoErrors",
                "webServerLog": "2024-04-12_11:10:59.505",
                "startedAt": "2024-04-12_11:11:57.977"
            },
            {
                "rowCount": 0,
                "duration": "PT00H00M03.068S",
                "state": "FinishedErrors",
                "webServerLog": "2024-05-31_07:20:06.041",
                "startedAt": "2024-05-31_07:20:06.840"
            }
        ]
    }
]

```

http://sherri.theobald.local:8065/logs/extractions/KNA1

```json
{
    "runs": [
        {
            "rowCount": 10071,
            "duration": "PT00H00M03.753S",
            "state": "FinishedNoErrors",
            "webServerLog": "2024-04-12_11:10:59.505",
            "startedAt": "2024-04-12_11:11:57.977"
        },
        {
            "rowCount": 0,
            "duration": "PT00H00M03.068S",
            "state": "FinishedErrors",
            "webServerLog": "2024-05-31_07:20:06.041",
            "startedAt": "2024-05-31_07:20:06.840"
        }
        ]
    }

```

http://sherri.theobald.local:8065/logs/extractions/KNA1/2024-04-12_11:11:57.977

```json
{
    "rowCount": 10071,
    "duration": "PT00H00M03.753S",
    "state": "FinishedNoErrors",
    "webServerLog": "2024-04-12_11:10:59.505",
    "startedAt": "2024-04-12_11:11:57.977"
}

```

http://sherri.theobald.local:8065/logs/extractions/KNA1/2024-04-12_11:11:57.977/log

```json
{
    "rowCount": 10071,
    "duration": "PT00H00M03.753S",
    "state": "FinishedNoErrors",
    "webServerLog": "2024-04-12_11:10:59.505",
    "startedAt": "2024-04-12_11:11:57.977",
    "logEntries": [
        {
            "timestamp": "2024-04-12_11:11:58.080",
            "logLevel": "Info",
            "source": "Table",
            "message": "Xtract Universal server version: 6.4.1.0"
        },
        {
            "timestamp": "2024-04-12_11:11:58.157",
            "logLevel": "Debug",
            "source": "Table",
            "message": "Attempting to load Theobald.Extractors.Table.TableExtractionDefinition information for extraction KNA1"
        },

        ...
        {
            "timestamp": "2024-04-12_11:12:01.728",
            "logLevel": "Debug",
            "source": "Table",
            "message": "Wrapping up extractor."
        },
        {
            "timestamp": "2024-04-12_11:12:01.730",
            "logLevel": "Info",
            "source": "Table",
            "message": "Extraction finished with status FinishedNoErrors."
        },
        {
            "timestamp": "2024-04-12_11:12:01.730",
            "logLevel": "Debug",
            "source": "Table",
            "message": "Writing run information."
        },
        {
            "timestamp": "2024-04-12_11:12:01.736",
            "logLevel": "Info",
            "source": "Table",
            "message": "Extraction run information was updated."
        }
    ]
}

```

### Get Server Logs

```text
[protocol]://[host]:[port]/logs/web

```

Returns a list of timestamps that correspond to server logs.

#### Parameters & Options for `/logs/web`

| Parameter | Description | | --- | --- | | `?min=[yyyy-MM-dd_HH:mm:ss.SSS]` | Returns the timestamps of server logs after the specified date and time. | | `?max=[yyyy-MM-dd_HH:mm:ss.SSS]` | Returns the timestamps of server logs before the specified date and time. | | `/[yyyy-MM-dd_HH:mm:ss.SSS]` | Returns the server log entries with the specified timestamp. |

Note

For information on how to interpret logs, see [Logs](../documentation/logs/).

#### Examples

http://sherri.theobald.local:8065/logs/web?min=2024-02-05_12:39:29.022

```json
{
    "logs": 
    [
        "2024-06-18_10:19:05.500",
        "2024-06-20_12:33:11.346",
        "2024-06-20_12:59:16.603",
        "2024-06-20_13:00:47.833",
        "2024-06-20_13:01:50.974",
        "2024-06-20_13:02:10.364",
        "2024-06-20_13:02:17.521",
        "2024-06-20_13:03:08.071",
        "2024-06-20_13:03:08.819",
        "2024-06-20_13:03:59.725",
        "2024-06-20_13:15:45.630",
        "2024-06-20_13:21:43.620"
    ]
}

```

http://sherri.theobald.local:8065/logs/web/2024-06-20_13:21:43.620

```json
{
    "logEntries": 
    [
        {
            "timestamp": "2024-06-20_13:21:43.707",
            "logLevel": "Info",
            "source": "WebServerHandler",
            "message": "Client [fe80::d3ac:77ba:ce0f:83b1%8]:56863"
        },
        {
            "timestamp": "2024-06-20_13:21:43.817",
            "logLevel": "Debug",
            "source": "HttpServer",
            "message": "Reading..."
        },
        {
            "timestamp": "2024-06-20_13:21:43.852",
            "logLevel": "Info",
            "source": "HttpServer",
            "message": "Processing /logs/extractions/S4HANA,KNA1."
        },

        ...

    ]
}

```

### Get Software Version

```text
[protocol]://[host]:[port]/version

```

Returns the software version of the Xtract Universal server installation in JSON format.

#### Example

http://sherri.theobald.local:8065/version

```json
{
  "version": "6.8.1.2"
}

```

### Get Destination Details

```text
[protocol]://[host]:[port]/destinations

```

Returns a list of all defined destinations. For a list of extractions with a specific destination, see [Get Extraction Details](#get-extraction-details).

The result contains the following elements:

| Item | Description | | --- | --- | | name | name of the target connection | | type | connection type | | host | host name, if applicable | | port | port name, if applicable | | database | database name, if applicable | | user | user name in the connection, if applicable | | schema | schema name, if applicable | | directory | directory name, if applicable |

#### Example

http://sherri.theobald.local:8065/destinations

```text
Name,Type,Host,Port,Database,User,Schema,Directory
csv,FileCSV,,,,,,"C:\Users\alice\Documents\csv\"
http-csv,CSV,,,,,,
http-json,HTTPJSON,,,,,,
json,FileJSON,,,,,,"C:\Users\alice\Documents\json"
sql-server,SQLServer,dbtest-ws2019.theobald.local,1433,alice,THEOBALD\alice,,
tableau2022,Tableau,,,,,,"C:\Users\alice\Documents\csv"

```

### Get Extraction Details

```text
[protocol]://[host]:[port]/config/extractions

```

Returns a list of all defined extractions in JSON format.

#### Response

The result contains the following elements:

| Item | Description | | --- | --- | | name | name of the extraction (if the extraction is located inside an [extraction group](../documentation/organize-extractions/), the name of the extraction group is part of the extraction name, e.g., the extraction "KNA1" in the extraction group "S4HANA" becomes `S4HANA,KNA1`) | | type | extraction type | | sapObject | name of the extracted SAP object | | source | name of the source connection | | destination | name of the target environment | | latestRun | contains *rowCount*, *duration*, *state* and *startedAt* of the latest extraction run | | rowCount | number of the last extracted data records | | duration | duration of the last execution | | state | status of the extraction (*Running*, *FinishedNoErrors*, *FinishedErrors*, *Cancelled*) | | startedAt | timestamp of the last execution | | created | contains *machine*, *timestamp* and *user* of when the extraction was created | | machine | machine on which the extraction was created | | timestamp | timestamp of the creation | | user | user that created the extraction | | lastChange | contains *machine*, *timestamp* and *user* of when the extraction was last changed | | machine | machine on which the extraction was last changed | | timestamp | timestamp of the last change | | user | user that last changed the extraction |

#### Parameters & Options for `/config/extractions`

| Parameter | Description | | --- | --- | | `/[extraction_name]/parameters` | Returns a list of runtime parameters used in the specified extraction. Every extraction has a set of *Extraction*, *Source* and *Custom* [extraction parameters](../documentation/parameters/extraction-parameters/). The parameters are available in the "Run Extraction" window. | | `/[extraction_name]/result-columns` | Returns the result columns of an extraction. | | `/[extraction_name]/result-name` | Returns the name that is used by the extracted SAP data in the target environment. Destinations without output objects return the the HTTP status code 500. | | `?destinationType=[destination]` | Returns a list of extractions that write into a specific destination. |

Note

Data fields that contain dates have the data type *ConvertedDate* if the option *Date Conversion* in the *Destination Settings* is active. If inactive, the data type is *StringLengthMax* with a length of 8 (*Date*).

#### Examples

http://sherri.theobald.local:8065/config/extractions/

```json
{
    "extractions": 
    [
        {
            "name": [
                "0MAT"
            ],
            "type": "DeltaQ",
            "technicalName": "0MATERIAL_ATTR",
            "source": "saperp",
            "destination": "GoogleCloudStorage",
            "latestRun": {
                "rowCount": 20275,
                "duration": "PT00H00M13.383S",
                "state": "FinishedNoErrors",
                "startedAt": "2023-08-17_11:24:07.770"
            },
            "created": {
                "machine": "TODD",
                "timestamp": "20221005T080618.544Z",
                "user": "THEOBALD\\steffan"
            },
            "lastChange": {
                "machine": "SHERRI",
                "timestamp": "20240129T131530.701Z",
                "user": "THEOBALD\\alice"
            }
        },
        {
            "name": [
                "0MATERIAL"
            ],
            "type": "DeltaQ",
            "technicalName": "0COSTCENTER_0101_HIER",
            "source": "saperp",
            "destination": "csv",
            "latestRun": {
                "rowCount": 200,
                "duration": "PT00H00M00.114S",
                "state": "FinishedNoErrors",
                "startedAt": "2023-08-17_11:31:44.029"
            },
            "created": {
                "machine": "SHERRI",
                "timestamp": "20230815T114651.045Z",
                "user": "THEOBALD\\alice"
            },
            "lastChange": {
                "machine": "SHERRI",
                "timestamp": "20230817T113328.786Z",
                "user": "THEOBALD\\alice"
            }
        }
    ]
}

```

http://sherri.theobald.local:8065/config/extractions/?destinationType=sqlserver

```json
{
    "extractions": 
    [
        {
            "name": "BW,2LIS",
            "type": "ODP",
            "sapObject": "2LIS_11_VAITM",
            "source": "bw2",
            "destination": "sqlserver",
            "latestRun": {
                "rowCount": 59058,
                "duration": "PT00H00M30.593S",
                "state": "FinishedNoErrors",
                "webServerLog": "2024-05-22_06:44:30.412",
                "startedAt": "2024-05-22_06:44:30.855"
            },
            "created": {
                "machine": "SHERRI",
                "timestamp": "20240522T095026.285Z",
                "user": "THEOBALD\\schipka"
            },
            "lastChange": {
                "machine": "SHERRI",
                "timestamp": "20240529T102008.256Z",
                "user": "THEOBALD\\schipka"
            }
        }
    ]
}

```

http://sherri.theobald.local:8065/config/extractions/KNA1/parameters

```json
{
    "extraction": 
    [
        {
            "name": "ignoreCache",
            "description": "Ignore the result cache",
            "type": "Flag",
            "default": "False"
        },
        {
            "name": "preview",
            "description": "Enable/disable preview mode",
            "type": "Flag",
            "default": "False"
        },
        {
            "name": "source",
            "description": "Sets the name of the source",
            "type": "Text",
            "default": "ec5"
        },
        {
            "name": "destination",
            "description": "Sets the name of the destination",
            "type": "Text",
            "default": "csv"
        },
        {
            "name": "rows",
            "description": "Maximum number of rows",
            "type": "Number",
            "default": "0"
        },
        {
            "name": "whereClause",
            "description": "Where Clause",
            "type": "Text",
            "default": null
        },
        {
            "name": "packageSize",
            "description": "Package Size",
            "type": "Number",
            "default": "50000"
        }
    ],
    "source": 
    [
        {
            "name": "lang",
            "description": "Logon Language",
            "type": "Text",
            "default": "EN"
        }
    ]
}

```

http://sherri.theobald.local:8065/config/extractions/KNA1/result-columns

```json
{
    "columns": 
    [
        {
            "name": "KUNNR",
            "description": "Customer Number",
            "type": "StringLengthMax",
            "length": 10,
            "isPrimaryKey": true,
            "isEncrypted": false,
            "referenceField": "",
            "referenceTable": ""
        },
        {
            "name": "LAND1",
            "description": "Country Key",
            "type": "StringLengthMax",
            "length": 3,
            "isPrimaryKey": false,
            "isEncrypted": false,
            "referenceField": "",
            "referenceTable": ""
        },
        {
            "name": "NAME1",
            "description": "Name 1",
            "type": "StringLengthMax",
            "length": 35,
            "isPrimaryKey": false,
            "isEncrypted": false,
            "referenceField": "",
            "referenceTable": ""
        },
        {
            "name": "ORT01",
            "description": "City",
            "type": "StringLengthMax",
            "length": 35,
            "isPrimaryKey": false,
            "isEncrypted": false,
            "referenceField": "",
            "referenceTable": ""
        },
        {
            "name": "Mean_UMSAT",
            "description": "Annual sales",
            "type": "Double",
            "isPrimaryKey": false,
            "isEncrypted": false,
            "referenceField": "UWAER",
            "referenceTable": "KNA1"
        }
    ]
}

```
