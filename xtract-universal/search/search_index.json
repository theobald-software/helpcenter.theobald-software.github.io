{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Xtract Universal HelpCenter","text":"&lt; Welcome to Xtract Universal! &gt;  <p>Xtract Universal is a flexible SAP interface for databases, analytics, BI or cloud solutions. Learn how to use Xtract Universal by browsing our user documentation, knowledge base and more.</p>"},{"location":"#latest-news","title":"Latest News","text":"<ul> <li> <p>SAP Note 3255746: Unpermitted Usage of ODP</p> <p>We are aware of the issue surrounding the SAP Note 3255746. For information on how to handle this matter, refer to our Blog post from July 2024.</p> <p> 2024-09-01</p> </li> <li> <p> Webinar: The TS Partner Ecosystem</p> <p>Join our webinar on 08/08/2024 - 11:00AM PDT to learn why and how Theobald Software works with technology partners to deliver better decision making and drive value to SAP customers. </p> <p> 2024-08-08</p> </li> <li> <p>New Version Numbers</p> <p>The version numbers in the changelog now incorporate the release date of the software, e.g., 2024.7.23.1 instead of 1.21.2.34.</p> <p> 2024-08-01</p> </li> </ul> <p> Show more</p> <p></p>"},{"location":"#additional-resources","title":"Additional Resources","text":"<p>Product Information</p><p>Visit our official Xtract Universal website to take a look at the advantages Xtract Universal offers, a list of FAQs and more.</p> <p>Success Stories</p><p>Get to know real customer scenarios with Xtract Universal by reading one of our success stories.</p> <p>Blog</p><p>Discover exciting articles and helpful tips about SAP data and cloud integration, SAP process automation and more.</p> <p>Events &amp; Webinars</p><p>Find out about upcoming events and Webinars. Join our live webinars and experience interactive communication or visit our SAP experts live at an event.</p> <p>Technical Newsletter</p><p>Our technical newsletter informs you once a month about new features, releases, breaking changes and articles.</p> <p>Feedback Form</p><p>We are constantly striving to improve our products and would love to get your feedback. Our feedback form is very short, just write down what you want us to know .</p>"},{"location":"changelog/","title":"ChangeLog","text":"<p> This section contains a version history for Xtract Universal.</p>"},{"location":"changelog/#recommended-update-interval","title":"Recommended Update Interval","text":"<p>We recommend checking for updates at least every three months. </p> <p> Download the latest version</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p> This section shows how to install and set up Xtract Universal for the first time. </p>","tags":["quickstart","quick start"]},{"location":"getting-started/#installation","title":"Installation","text":"<ol> <li>Download a 30 day trial version of Xtract Universal.</li> <li>Run the Xtract Universal executable (XtractUniversalSetup.exe) to install the Xtract Universal Designer and the Xtract Universal Server. For information on system requirements, see Requirements.</li> <li>Make sure that the Xtract Universal Service is running on your windows system and that the default port 8064 is not blocked by your firewall.</li> <li>Open the Xtract Universal Designer application and click [ plug2 icon plug2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com).  Connect] to connect to the Xtract Universal Server using the default settings.</li> </ol> <p>For more information, see Setup.</p>","tags":["quickstart","quick start"]},{"location":"getting-started/#connect-to-sap","title":"Connect to SAP","text":"<p>Before connecting to SAP for the first time, set up an SAP dialog user with the necessary SAP user rights.</p> <ol> <li>In the main window of the Designer, click [New]. The window \"Change Source\" opens. </li> <li>Enter a name for the SAP connection in the field Name, e.g., s4hana, bw, etc.</li> <li>In the General tab, enter the system details of your SAP system. Input values for the SAP connection can be found in the Properties of the SAP Logon Pad or they can be requested from the SAP Basis team. </li> <li>In the Authentication tab, enter the SAP credentials of the SAP dialog user.</li> <li>Click [Test designer connection] to validate the connection between the Xtract Universal Designer and the SAP system.</li> <li>Click [Test server connection] to validate the connection between the Xtract Universal Server and the SAP system.</li> <li>Click [OK] to save the SAP source. </li> </ol> <p>For more information, see SAP Connection.</p> <p>Tip</p> <p>To edit a source or to create new sources, navigate to Server &gt; Manage Sources in the menu bar.</p>","tags":["quickstart","quick start"]},{"location":"getting-started/#create-an-extraction","title":"Create an Extraction","text":"<p>Extractions are the main entities of Xtract Universal.  An extraction defines what data to extract from SAP and where to write the data.</p> <ol> <li>In the main window of the Designer, click [New]. The window \"Create Extraction\" opens.</li> <li>Select an SAP Connection from the drop-down menu in Source . </li> <li>Enter a name for the extraction  .</li> <li> <p>Select one of the following extraction types  :</p> Extraction Type Description  BAPI Execute BAPIs and Function Modules.  BWCube Extract data from SAP BW InfoCubes and BEx Queries.  BW Hierarchy Extract Hierarchies from an SAP BW / BI system.  DeltaQ Extract data from DataSources (OLTP) and extractors from ERP and ECC systems.  ODP Extract data via the SAP Operational Data Provisioning (ODP) framework.  OHS Extract data from InfoSpokes and OHS destinations.  Query Extract data from ERP queries. Note: BEx queries are covered by BWCube.  Report Extract data from SAP ABAP reports.  Table Extract data from SAP tables and views.  Table CDC Extract delta data from SAP tables and views. </li> <li> <p>Click [OK]. The main window of the extraction type opens automatically. Follow the instructions in the documentation of the selected extraction type to set up the extraction.</p> </li> </ol>","tags":["quickstart","quick start"]},{"location":"getting-started/#how-to-create-a-simple-extraction-for-beginners","title":"How to Create a Simple Extraction for Beginners","text":"<p>Follow the steps below to create a simple extraction that extracts customer master data from the SAP table KNA1:</p> <ol> <li>Create an extraction that uses the Table extraction type.</li> <li>In the main window of the extraction type, click [Add] to look up an SAP table. The window \"Table Lookup\" opens. </li> <li>In the field Table Name, enter the name of the table to extract (KNA1)  . Use wildcards (*) if needed. </li> <li>Click [ ] . Search results are displayed.</li> <li>Select the table KNA1   and click [OK]. The application returns to the main window of the extraction type.</li> <li>Optional: Select the table columns you want to extract. By default all columns are extracted.  For more information on filter options and advanced settings, see, Define the Table Extraction Type</li> <li>Click [Load live preview] to display a live preview of the first 100 records.</li> <li>Click [OK] to save the extraction type.</li> </ol> <p>The extraction is now listed in the main window of the Designer. To edit an extraction, double-click the extraction.</p>","tags":["quickstart","quick start"]},{"location":"getting-started/#run-an-extraction","title":"Run an Extraction","text":"<p>Extractions can be run directly in the Xtract Universal Designer or via web service and command line.</p> <ol> <li>In the main window of the Designer, select an extraction   and click [Run] . The window \"Run Extraction\" opens. </li> <li>Click [Run]  to execute the extraction. The status in the subsection General Info indicates if the extraction finished successfully.</li> <li>Open the Output tab to view the extracted data . </li> </ol> <p>For more information, see Execute and Automate.</p>","tags":["quickstart","quick start"]},{"location":"getting-started/#write-data-to-a-target-environment","title":"Write Data to a Target Environment","text":"<p>Xtract Universal allows you to load data to a wide range of target environments, including databases, cloud storages, BI tools, etc. By default, extractions use the http-csv destination as a target environment.</p> <p>Follow the steps below to add a new destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select a destination type from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination. Destination details vary depending on the destination type. For more information about destination details, select your destination:  Select a destination Alteryx Amazon S3 Amazon Redshift Azure Storage Azure Synapse Analytics EXASolution Flat File CSV Flat File JSON Flat File Parquet Google Cloud Storage HTTP CSV HTTP JSON Huawei Cloud OBS IBM Db2 KNIME Microsoft SQL Server MySQL Oracle PostgreSQL Power BI Connector QlikSense and QlikView Salesforce SAP HANA SharePoint Snowflake Power BI Report Server (SQL Server Reporting Services) Tableau </li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination is now available and can be assigned to extractions.</p>","tags":["quickstart","quick start"]},{"location":"getting-started/#assign-a-destination-to-an-extracion","title":"Assign a Destination to an Extracion","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: change the destination settings. Destination settings are specific to the selected extraction and vary depending on the destination type. For more information about destination settings, select your destination:  Select a destination Alteryx Amazon S3 Amazon Redshift Azure Storage Azure Synapse Analytics EXASolution Flat File CSV Flat File JSON Flat File Parquet Google Cloud Storage HTTP CSV HTTP JSON Huawei Cloud OBS IBM Db2 KNIME Microsoft SQL Server MySQL Oracle PostgreSQL Power BI Connector QlikSense and QlikView Salesforce SAP HANA SharePoint Snowflake Power BI Report Server (SQL Server Reporting Services) Tableau </li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination. For more information on available destinations, see Destinations.</p>","tags":["quickstart","quick start"]},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p> This section contains a list of troubleshooting articles that show you how to solve common errors.</p>"},{"location":"troubleshooting/#general","title":"General","text":"<ul> <li>DeltaQ Troubleshooting Guide</li> <li>Proxy server settings in Xtract Universal</li> <li>How to activate tracing in Xtract Universal</li> <li>Required Support Information for Xtract Universal</li> </ul>"},{"location":"troubleshooting/#sap-troubleshooting","title":"SAP - Troubleshooting","text":"<ul> <li>Allow external access to BW Queries</li> <li>Authority Check Trace</li> <li>DeltaQ Customizing - PORT_CREATION_ERROR</li> <li>ERROR service \u201a?\u2018 unknown or service sapms unknown</li> <li>Error: Theobald.Extractors.Table.TheoReadTableException: The parser produced the error</li> <li>Error while retrieving metadata for BAPI</li> <li>Error with Table extractions: Theobald.Extractors.Table.TheoReadTableException: Length of the field \"ABCD\" not valid. Given length is 00000x . Length in SAP is 00000y</li> <li>LIST_FROM_MEMORY_NOT_FOUND</li> <li>MDX parser does not start</li> <li>Missing SAP authority rights for data replication.</li> <li>Non-Unicode destinations are not supported by SAP</li> <li>No more storage space available for extending an internal table</li> <li>OPTION_NOT_VALID running table extractions error after an SAP Update</li> <li>(RFC_ABAP_RUNTIME_FAILURE): ASSIGN_BASE_WRONG_ALIGNMENT - Error in ASSIGN assignment in program SAPLSDTX</li> <li>RFC_ERROR_SYSTEM_FAILURE - A table or alias name in the FROM clause is not unique</li> <li>RFC_ERROR_SYSTEM_FAILURE - Illegal access to the right table of a LEFT OUTER JOIN.</li> <li>SAP Query Metadata/result mismatch</li> <li>Search for new Tables and/or Table extractions are empty</li> <li>SM59 RFC - Connection Test fails</li> <li>Theobald.Extractors.Table.TheoReadTableException: Waiting for package to be written to shared memory has timed out.</li> <li>When running a report a CNTL_ERROR exception is raised</li> <li>Wrong program type for SUBMIT</li> <li>Xtract Report - Screen output without connection to user</li> <li>Xtract SAP Query - DATA_TO_MEMORY_NOT_POSSIBLE</li> <li>Xtract Table - RFC_ERROR_SYSTEM_FAILURE - An error has occurred while parsing a dynamic entry</li> <li>Xtract Table - Time Limit Exceeded</li> </ul>"},{"location":"troubleshooting/#xtract-universal-troubleshooting","title":"Xtract Universal - Troubleshooting","text":"<ul> <li>Copy Command for Snowflake Destination failing / TheoReadTableException: An error occured in PROCESS_PACKAGE callback</li> <li>Insert Statement failed for empty values using Snowflake Destination</li> <li>Key not valid for use in specified state</li> <li>No more storage space available for extending an internal table</li> <li>Table Export Parameter missing for BAPI Extraction Type</li> <li>The certificate for the remote host could not be validated</li> <li>The remote host's name did not match the name on the provided certificate</li> </ul>"},{"location":"web-api/","title":"Web API","text":"<p> This section contains a list of all API endpoints that are available for Xtract Universal.</p> <p>Xtract Universal offers a web API that allows running extractions and querying meta information and extraction logs from Xtract Universal through web calls. The web API returns the result as an http-json stream.</p>"},{"location":"web-api/#base-url","title":"Base URL","text":"<p>The basic URL for web calls uses the following format: <code>[protocol]://[host or IP address]:[port]/</code>.</p>"},{"location":"web-api/#examples","title":"Examples","text":"Protocol Syntax Example HTTP <code>http://[host].[domain]:[port]</code> <code>http://sherri.theobald.local:8065</code> HTTP <code>http://[host]:[port]</code> <code>http://localhost:8065</code> HTTPS <code>https://[host].[domain]:[port]</code> <code>https://sherri.theobald.local:8165</code> Requires a dedicated host name and X.509 certificate, see web server settings. <p>Note</p> <p>Make sure to use the correct ports, see Server Ports.</p>"},{"location":"web-api/#run-extractions","title":"Run Extractions","text":"Run Extractions SynchronousRun Extractions Asynchronous <p><pre><code>[protocol]://[host]:[port]/run/[extraction_name]\n</code></pre> <p>Runs the specified extraction.</p> <p><pre><code>[protocol]://[host]:[port]/start/[extraction_name]\n</code></pre> <p>Runs the specified extraction asynchronously and returns the run status immediately.</p> <p>If the extraction is located inside an extraction group, the name of the extraction group is part of the extraction name. Example: The extraction \"KNA1\" in the extraction group \"S4HANA\" becomes <code>S4HANA,KNA1</code>.</p> <p>Warning</p> <p>Deprecated Endpoints:</p> <ul> <li><code>[protocol]://[host]:[port]/?name=[extraction_name]</code> becomes <code>[protocol]://[host]:[port]/run/[extraction_name]</code></li> <li><code>[protocol]://[host]:[port]/?name=[extraction_name]&amp;wait=false</code> becomes <code>[protocol]://[host]:[port]/start/[extraction_name]</code></li> </ul> <p>Tip</p> <p>You can use the UI in the \u201cRun Extraction\u201d menu to generate an URL for extraction runs, see Run Extractions.</p>"},{"location":"web-api/#response","title":"Response","text":"<p>The response of the web service call contains the following information:</p> Response Description HTTP status code The HTTP status code 200 indicates a successful extraction call. It does not indicate a successful execution of the extraction.  The HTTP status code 404 indicates that the called extraction does not exist. Detailed information can be found in the log of the web service. HTTP header Shows the timestamp of the extraction in the HTTP header e.g., X-XU-Timestamp: 2021-04-09_19:03:09.971 HTTP response body The Response in the HTTP body depends on the destination type of the extraction. Depending on the destination type, the extracted data is returned in either CSV or JSON format. <p></p>"},{"location":"web-api/#parameters-for-protocolhostportstartextraction_name","title":"Parameters for <code>[protocol]://[host]:[port]/start/[extraction_name]</code>","text":"Parameter Description <code>&amp;[parameter1_name]=[value]</code> Runs the specified extraction and passes values to the specified extraction parameters. <code>&amp;quiet-push=true</code> Runs the specified extraction and suppresses the output of extraction logs for push destinations. This parameter has no effect on pull destinations and asynchronous extractions. <p>Tip</p> <p>You can use the UI in the \"Run Extraction\" menu to generate a URL for extraction runs, see Run Extractions.</p>"},{"location":"web-api/#examples_1","title":"Examples","text":"http://sherri.theobald.local:8064/start/KNA1 <pre><code>MANDT,KUNNR,LAND1,NAME1,ORT01\n800,0000000313,DE,zdemo customer zr,\n800,0000001824,IN,,\n800,0000001832,IN,cus 3,\n800,0000001834,IN,52 CUSTOMER,\n800,0000001837,IN,emax,\n800,0000005002,JP,One time customer,\n800,0000010009,,Einmalkunde,\n800,0000011113,DE,Myers Corp.,\n800,0000011114,DE,Watson &amp; Watson,\n800,0000100016,DE,Einmalkunde,\n800,0000100026,DE,Reference Customer for Internet,\n800,0000100236,US,,\n800,0000100291,IN,RELIANCE FRESH DOMESTIC CUSTOMERS,\n800,0000200000,IN,MA Reddy Customer,\n800,0000200001,IN,SD Customer,\n800,0000200002,IN,SMR Customer,\n800,0000300050,,,\n800,0000300065,GB,ComputerWorld,\n800,0000300320,GB,Customer 1,\n800,0000300430,DE,testcompany crm,\n800,0000300735,US,OTHER MASS,\n800,0000300736,US,OTHER GROCRY,\n800,0000300737,US,OTHER CONVENIENCE,\n800,0000300738,US,OTHER MILITARY,\n800,0000300739,US,OTHER FOOD,\n...\n</code></pre> http://sherri.theobald.local:8064/start/KNA1/?cityParam=Stuttgart&amp;companyParam=Theobald+Software <pre><code>MANDT,KUNNR,LAND1,NAME1,ORT01\n800,0000000779,DE,Theobald Software,Stuttgart\"\n</code></pre>"},{"location":"web-api/#abort-extraction","title":"Abort Extraction","text":"<pre><code>[protocol]://[host]:[port]/abort?name=[extraction_name]\n</code></pre> <p>Aborts the specified extraction. If the abortion is successful, a confirmation message is returned in the HTTP body. </p> <p>Note</p> <p>If the extraction is located inside an extraction group, the name of the extraction group is part of the extraction name. Example: The extraction \"KNA1\" in the extraction group \"S4HANA\" becomes <code>S4HANA,KNA1</code>.</p>"},{"location":"web-api/#example","title":"Example","text":"http://sherri.theobald.local:8064/abort?name=KNA1 <pre><code>All runs of extraction 'KNA1' aborted.\n</code></pre>"},{"location":"web-api/#get-status-of-an-extraction","title":"Get Status of an Extraction","text":"<pre><code>[protocol]://[host]:[port]/status/?name=[extraction_name]&amp;timestamp=[yyyy-MM-dd_HH:mm:ss.SSS]\n</code></pre> <p>Returns the status of a (running) extraction at the specified timestamp. </p> <p>Note</p> <p>If the extraction is located inside an extraction group, the name of the extraction group is part of the extraction name. Example: The extraction \"KNA1\" in the extraction group \"S4HANA\" becomes <code>S4HANA,KNA1</code>.</p> <p>Tip</p> <p>The timestamp corresponds to the startedAt element returned by <code>[protocol]://[host]:[port]/config/extractions/</code> or <code>[protocol]://[host]:[port]/logs/extractions/[extraction-name]</code>.</p>"},{"location":"web-api/#response_1","title":"Response","text":"State Description Running The extraction is running. FinishedNoErrors The extraction is finished without errors. FinishedErrors The extraction is finished, but with at least one error."},{"location":"web-api/#example_1","title":"Example","text":"http://sherri.theobald.local:8064/status/?name=KNA1&amp;timestamp=2024-02-05_10:23:08.025 <pre><code>FinishedNoErrors\n</code></pre>"},{"location":"web-api/#get-extraction-logs","title":"Get Extraction Logs","text":"<pre><code>[protocol]://[host]:[port]/logs/extractions\n</code></pre> <p>Returns a list of extraction runs. </p>"},{"location":"web-api/#response_2","title":"Response","text":"<p>The result contains the following elements:</p> Item Description extractionName name of the extraction (if the extraction is located inside an extraction group, the name of the extraction group is part of the extraction name, e.g., the extraction \"KNA1\" in the extraction group \"S4HANA\" becomes <code>S4HANA,KNA1</code>) runs contains rowCount, duration, state, webServerLog and startedAt of extraction runs row count number of extracted data records duration duration of the execution state status of the extraction (Running, FinishedNoErrors, FinishedErrors) webServerLog timestamp of the corresponding server log startedAt timestamp of the execution"},{"location":"web-api/#parameters-options-for-protocolhostportlogsextractions","title":"Parameters &amp; Options for <code>[protocol]://[host]:[port]/logs/extractions</code>","text":"Parameter Description <code>?min=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns the extraction runs after the specified date and time. <code>?max=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns the extraction runs before the specified date and time. <code>/[extraction-name]</code> Returns all extraction runs of the specified extraction. <code>/[extraction-name]/[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns the extraction run of the specified extraction with the specified timestamp. <code>/[extraction-name]/[yyyy-MM-dd_HH:mm:ss.SSS]/log</code> Returns the extraction log of the specified extraction with the specified timestamp. <p>Note</p> <p>For information on how to interpret logs, see Logs.</p>"},{"location":"web-api/#examples_2","title":"Examples","text":"http://sherri.theobald.local:8064/logs/extractions?min=2023-08-17_11:20:44.029 <pre><code>{\n\"extractions\": \n[\n    {\n        \"extractionName\": [\n            \"MEBEST\"\n        ],\n        \"runs\": [\n            {\n                \"rowCount\": 53,\n                \"duration\": \"PT00H00M00.541S\",\n                \"state\": \"FinishedNoErrors\",\n                \"webServerLog\": \"2024-05-31_07:19:36.156\",\n                \"startedAt\": \"2024-05-31_07:19:37.006\"\n            }\n        ]\n    },\n    {\n        \"extractionName\": [\n            \"S4HANA\",\n            \"BSEG\"\n        ],\n        \"runs\": [\n            {\n                \"rowCount\": 12036,\n                \"duration\": \"PT00H00M00.584S\",\n                \"state\": \"FinishedNoErrors\",\n                \"startedAt\": \"2024-06-12_08:56:28.066\"\n            }\n        ]\n    },\n    {\n        \"extractionName\": [\n            \"S4HANA\",\n            \"KNA1\"\n        ],\n        \"runs\": [\n            {\n                \"rowCount\": 10071,\n                \"duration\": \"PT00H00M03.753S\",\n                \"state\": \"FinishedNoErrors\",\n                \"webServerLog\": \"2024-04-12_11:10:59.505\",\n                \"startedAt\": \"2024-04-12_11:11:57.977\"\n            },\n            {\n                \"rowCount\": 0,\n                \"duration\": \"PT00H00M03.068S\",\n                \"state\": \"FinishedErrors\",\n                \"webServerLog\": \"2024-05-31_07:20:06.041\",\n                \"startedAt\": \"2024-05-31_07:20:06.840\"\n            }\n        ]\n    }\n]\n</code></pre> http://sherri.theobald.local:8064/logs/extractions/KNA1 <pre><code>{\n    \"runs\": [\n        {\n            \"rowCount\": 10071,\n            \"duration\": \"PT00H00M03.753S\",\n            \"state\": \"FinishedNoErrors\",\n            \"webServerLog\": \"2024-04-12_11:10:59.505\",\n            \"startedAt\": \"2024-04-12_11:11:57.977\"\n        },\n        {\n            \"rowCount\": 0,\n            \"duration\": \"PT00H00M03.068S\",\n            \"state\": \"FinishedErrors\",\n            \"webServerLog\": \"2024-05-31_07:20:06.041\",\n            \"startedAt\": \"2024-05-31_07:20:06.840\"\n        }\n        ]\n    }\n</code></pre> http://sherri.theobald.local:8064/logs/extractions/KNA1/2024-04-12_11:11:57.977 <pre><code>{\n    \"rowCount\": 10071,\n    \"duration\": \"PT00H00M03.753S\",\n    \"state\": \"FinishedNoErrors\",\n    \"webServerLog\": \"2024-04-12_11:10:59.505\",\n    \"startedAt\": \"2024-04-12_11:11:57.977\"\n}\n</code></pre> http://sherri.theobald.local:8064/logs/extractions/KNA1/2024-04-12_11:11:57.977/log <pre><code>{\n    \"rowCount\": 10071,\n    \"duration\": \"PT00H00M03.753S\",\n    \"state\": \"FinishedNoErrors\",\n    \"webServerLog\": \"2024-04-12_11:10:59.505\",\n    \"startedAt\": \"2024-04-12_11:11:57.977\",\n    \"logEntries\": [\n        {\n            \"timestamp\": \"2024-04-12_11:11:58.080\",\n            \"logLevel\": \"Info\",\n            \"source\": \"Table\",\n            \"message\": \"Xtract Universal server version: 6.4.1.0\"\n        },\n        {\n            \"timestamp\": \"2024-04-12_11:11:58.157\",\n            \"logLevel\": \"Debug\",\n            \"source\": \"Table\",\n            \"message\": \"Attempting to load Theobald.Extractors.Table.TableExtractionDefinition information for extraction KNA1\"\n        },\n\n        ...\n        {\n            \"timestamp\": \"2024-04-12_11:12:01.728\",\n            \"logLevel\": \"Debug\",\n            \"source\": \"Table\",\n            \"message\": \"Wrapping up extractor.\"\n        },\n        {\n            \"timestamp\": \"2024-04-12_11:12:01.730\",\n            \"logLevel\": \"Info\",\n            \"source\": \"Table\",\n            \"message\": \"Extraction finished with status FinishedNoErrors.\"\n        },\n        {\n            \"timestamp\": \"2024-04-12_11:12:01.730\",\n            \"logLevel\": \"Debug\",\n            \"source\": \"Table\",\n            \"message\": \"Writing run information.\"\n        },\n        {\n            \"timestamp\": \"2024-04-12_11:12:01.736\",\n            \"logLevel\": \"Info\",\n            \"source\": \"Table\",\n            \"message\": \"Extraction run information was updated.\"\n        }\n    ]\n}\n</code></pre>"},{"location":"web-api/#get-server-logs","title":"Get Server Logs","text":"<pre><code>[protocol]://[host]:[port]/logs/web\n</code></pre> <p>Returns a list of timestamps that correspond to server logs.</p>"},{"location":"web-api/#parameters-options-for-protocolhostportlogsweb","title":"Parameters &amp; Options for <code>[protocol]://[host]:[port]/logs/web</code>","text":"Parameter Description <code>?min=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns the timestamps of server logs after the specified date and time. <code>?max=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns the timestamps of server logs before the specified date and time. <code>/[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns the server log entries with the specified timestamp. <p>Note</p> <p>For information on how to interpret logs, see Logs.</p>"},{"location":"web-api/#examples_3","title":"Examples","text":"http://sherri.theobald.local:8064/logs/web?min=2024-02-05_12:39:29.022 <pre><code>{\n    \"logs\": \n    [\n        \"2024-06-18_10:19:05.500\",\n        \"2024-06-20_12:33:11.346\",\n        \"2024-06-20_12:59:16.603\",\n        \"2024-06-20_13:00:47.833\",\n        \"2024-06-20_13:01:50.974\",\n        \"2024-06-20_13:02:10.364\",\n        \"2024-06-20_13:02:17.521\",\n        \"2024-06-20_13:03:08.071\",\n        \"2024-06-20_13:03:08.819\",\n        \"2024-06-20_13:03:59.725\",\n        \"2024-06-20_13:15:45.630\",\n        \"2024-06-20_13:21:43.620\"\n    ]\n}\n</code></pre> http://sherri.theobald.local:8064/logs/web/2024-06-20_13:21:43.620 <pre><code>{\n    \"logEntries\": \n    [\n        {\n            \"timestamp\": \"2024-06-20_13:21:43.707\",\n            \"logLevel\": \"Info\",\n            \"source\": \"WebServerHandler\",\n            \"message\": \"Client [fe80::d3ac:77ba:ce0f:83b1%8]:56863\"\n        },\n        {\n            \"timestamp\": \"2024-06-20_13:21:43.817\",\n            \"logLevel\": \"Debug\",\n            \"source\": \"HttpServer\",\n            \"message\": \"Reading...\"\n        },\n        {\n            \"timestamp\": \"2024-06-20_13:21:43.852\",\n            \"logLevel\": \"Info\",\n            \"source\": \"HttpServer\",\n            \"message\": \"Processing /logs/extractions/S4HANA,KNA1.\"\n        },\n\n        ...\n\n    ]\n}\n</code></pre>"},{"location":"web-api/#get-software-version","title":"Get Software Version","text":"<pre><code>[protocol]://[host]:[port]/version\n</code></pre> <p>Returns the software version of the Xtract Universal server installation in JSON format. </p>"},{"location":"web-api/#example_2","title":"Example","text":"http://sherri.theobald.local:8064/version <pre><code>{\n  \"version\": \"6.8.1.2\"\n}\n</code></pre>"},{"location":"web-api/#get-destination-details","title":"Get Destination Details","text":"<pre><code>[protocol]://[host]:[port]/destinations\n</code></pre> <p>Returns a list of all defined destinations. For a list of extractions with a specific destination, see Get Extraction Details.</p> <p>The result contains the following elements:</p> Item Description name name of the target connection type connection type host host name, if applicable port port name, if applicable database database name, if applicable user user name in the connection, if applicable schema schema name, if applicable directory directory name, if applicable"},{"location":"web-api/#example_3","title":"Example","text":"http://sherri.theobald.local:8064/destinations <pre><code>Name,Type,Host,Port,Database,User,Schema,Directory\ncsv,FileCSV,,,,,,\"C:\\Users\\alice\\Documents\\csv\\\"\nhttp-csv,CSV,,,,,,\nhttp-json,HTTPJSON,,,,,,\njson,FileJSON,,,,,,\"C:\\Users\\alice\\Documents\\json\"\nsql-server,SQLServer,dbtest-ws2019.theobald.local,1433,alice,THEOBALD\\alice,,\ntableau2022,Tableau,,,,,,\"C:\\Users\\alice\\Documents\\csv\"\n</code></pre>"},{"location":"web-api/#get-extraction-details","title":"Get Extraction Details","text":"<pre><code>[protocol]://[host]:[port]/config/extractions\n</code></pre> <p>Returns a list of all defined extractions in JSON format. </p>"},{"location":"web-api/#response_3","title":"Response","text":"<p>The result contains the following elements:</p> Item Description name name of the extraction (if the extraction is located inside an extraction group, the name of the extraction group is part of the extraction name, e.g., the extraction \"KNA1\" in the extraction group \"S4HANA\" becomes <code>S4HANA,KNA1</code>) type extraction type sapObject name of the extracted SAP object source name of the source connection destination name of the target environment latestRun contains rowCount, duration, state and startedAt of the latest extraction run rowCount number of the last extracted data records duration duration of the last execution state status of the extraction (Running, FinishedNoErrors, FinishedErrors) startedAt timestamp of the last execution created contains machine, timestamp and user of when the extraction was created machine machine on which the extraction was created timestamp timestamp of the creation user user that created the extraction lastChange contains machine, timestamp and user of when the extraction was last changed machine machine on which the extraction was last changed timestamp timestamp of the last change user user that last changed the extraction"},{"location":"web-api/#parameters-options-for-protocolhostportconfigextractions","title":"Parameters &amp; Options for <code>[protocol]://[host]:[port]/config/extractions</code>","text":"Parameter Description <code>/[extraction_name]/parameters</code> Returns a list of runtime parameters used in the specified extraction. Every extraction has a set of Extraction, Source and Custom extraction parameters. The parameters are available in the \"Run Extraction\" window. <code>/[extraction_name]/result-columns</code> Returns the result columns of an extraction. <code>?destinationType=[destination]</code> Returns a list of extractions that write into a specific destination. <p>Note</p> <p>Data fields that contain dates have the data type ConvertedDate if the option Date Conversion in the Destination Settings is active.  If inactive, the data type is StringLengthMax with a length of 8 (Date).</p>"},{"location":"web-api/#examples_4","title":"Examples","text":"http://sherri.theobald.local:8064/config/extractions/ <pre><code>{\n    \"extractions\": \n    [\n        {\n            \"name\": [\n                \"0MAT\"\n            ],\n            \"type\": \"DeltaQ\",\n            \"technicalName\": \"0MATERIAL_ATTR\",\n            \"source\": \"saperp\",\n            \"destination\": \"GoogleCloudStorage\",\n            \"latestRun\": {\n                \"rowCount\": 20275,\n                \"duration\": \"PT00H00M13.383S\",\n                \"state\": \"FinishedNoErrors\",\n                \"startedAt\": \"2023-08-17_11:24:07.770\"\n            },\n            \"created\": {\n                \"machine\": \"TODD\",\n                \"timestamp\": \"20221005T080618.544Z\",\n                \"user\": \"THEOBALD\\\\steffan\"\n            },\n            \"lastChange\": {\n                \"machine\": \"SHERRI\",\n                \"timestamp\": \"20240129T131530.701Z\",\n                \"user\": \"THEOBALD\\\\alice\"\n            }\n        },\n        {\n            \"name\": [\n                \"0MATERIAL\"\n            ],\n            \"type\": \"DeltaQ\",\n            \"technicalName\": \"0COSTCENTER_0101_HIER\",\n            \"source\": \"saperp\",\n            \"destination\": \"csv\",\n            \"latestRun\": {\n                \"rowCount\": 200,\n                \"duration\": \"PT00H00M00.114S\",\n                \"state\": \"FinishedNoErrors\",\n                \"startedAt\": \"2023-08-17_11:31:44.029\"\n            },\n            \"created\": {\n                \"machine\": \"SHERRI\",\n                \"timestamp\": \"20230815T114651.045Z\",\n                \"user\": \"THEOBALD\\\\alice\"\n            },\n            \"lastChange\": {\n                \"machine\": \"SHERRI\",\n                \"timestamp\": \"20230817T113328.786Z\",\n                \"user\": \"THEOBALD\\\\alice\"\n            }\n        }\n    ]\n}\n</code></pre> http://sherri.theobald.local:8064/config/extractions/?destinationType=sqlserver <pre><code>{\n    \"extractions\": \n    [\n        {\n            \"name\": \"BW,2LIS\",\n            \"type\": \"ODP\",\n            \"sapObject\": \"2LIS_11_VAITM\",\n            \"source\": \"bw2\",\n            \"destination\": \"sqlserver\",\n            \"latestRun\": {\n                \"rowCount\": 59058,\n                \"duration\": \"PT00H00M30.593S\",\n                \"state\": \"FinishedNoErrors\",\n                \"webServerLog\": \"2024-05-22_06:44:30.412\",\n                \"startedAt\": \"2024-05-22_06:44:30.855\"\n            },\n            \"created\": {\n                \"machine\": \"SHERRI\",\n                \"timestamp\": \"20240522T095026.285Z\",\n                \"user\": \"THEOBALD\\\\schipka\"\n            },\n            \"lastChange\": {\n                \"machine\": \"SHERRI\",\n                \"timestamp\": \"20240529T102008.256Z\",\n                \"user\": \"THEOBALD\\\\schipka\"\n            }\n        }\n    ]\n}\n</code></pre> http://sherri.theobald.local:8064/config/extractions/KNA1/parameters <pre><code>{\n    \"extraction\": \n    [\n        {\n            \"name\": \"ignoreCache\",\n            \"description\": \"Ignore the result cache\",\n            \"type\": \"Flag\",\n            \"default\": \"False\"\n        },\n        {\n            \"name\": \"preview\",\n            \"description\": \"Enable/disable preview mode\",\n            \"type\": \"Flag\",\n            \"default\": \"False\"\n        },\n        {\n            \"name\": \"source\",\n            \"description\": \"Sets the name of the source\",\n            \"type\": \"Text\",\n            \"default\": \"ec5\"\n        },\n        {\n            \"name\": \"destination\",\n            \"description\": \"Sets the name of the destination\",\n            \"type\": \"Text\",\n            \"default\": \"csv\"\n        },\n        {\n            \"name\": \"rows\",\n            \"description\": \"Maximum number of rows\",\n            \"type\": \"Number\",\n            \"default\": \"0\"\n        },\n        {\n            \"name\": \"whereClause\",\n            \"description\": \"Where Clause\",\n            \"type\": \"Text\",\n            \"default\": null\n        },\n        {\n            \"name\": \"packageSize\",\n            \"description\": \"Package Size\",\n            \"type\": \"Number\",\n            \"default\": \"50000\"\n        }\n    ],\n    \"source\": \n    [\n        {\n            \"name\": \"lang\",\n            \"description\": \"Logon Language\",\n            \"type\": \"Text\",\n            \"default\": \"EN\"\n        }\n    ]\n}\n</code></pre> http://sherri.theobald.local:8064/config/extractions/KNA1/result-columns <pre><code>{\n    \"columns\": \n    [\n        {\n            \"name\": \"KUNNR\",\n            \"description\": \"Customer Number\",\n            \"type\": \"StringLengthMax\",\n            \"length\": 10,\n            \"isPrimaryKey\": true,\n            \"isEncrypted\": false,\n            \"referenceField\": \"\",\n            \"referenceTable\": \"\"\n        },\n        {\n            \"name\": \"LAND1\",\n            \"description\": \"Country Key\",\n            \"type\": \"StringLengthMax\",\n            \"length\": 3,\n            \"isPrimaryKey\": false,\n            \"isEncrypted\": false,\n            \"referenceField\": \"\",\n            \"referenceTable\": \"\"\n        },\n        {\n            \"name\": \"NAME1\",\n            \"description\": \"Name 1\",\n            \"type\": \"StringLengthMax\",\n            \"length\": 35,\n            \"isPrimaryKey\": false,\n            \"isEncrypted\": false,\n            \"referenceField\": \"\",\n            \"referenceTable\": \"\"\n        },\n        {\n            \"name\": \"ORT01\",\n            \"description\": \"City\",\n            \"type\": \"StringLengthMax\",\n            \"length\": 35,\n            \"isPrimaryKey\": false,\n            \"isEncrypted\": false,\n            \"referenceField\": \"\",\n            \"referenceTable\": \"\"\n        },\n        {\n            \"name\": \"Mean_UMSAT\",\n            \"description\": \"Annual sales\",\n            \"type\": \"Double\",\n            \"isPrimaryKey\": false,\n            \"isEncrypted\": false,\n            \"referenceField\": \"UWAER\",\n            \"referenceTable\": \"KNA1\"\n        }\n    ]\n}\n</code></pre>"},{"location":"documentation/about-this-documentation/","title":"About this Documentation","text":"<p>The user documentation of Theobald Software is designed to introduce readers to the main functions of Xtract Universal.</p> <p>Theobald Software's claim is to keep the user documentation up to date according to the latest product version.  Information concerning the older version is removed gradually from the documentation content.  It is generally recommended to always install the latest version of the product.</p> <p>Theobald Software's claim is to always update all the used screenshots of other software vendors, nevertheless this cannot be guaranteed. </p> <p>\u00a92024 Theobald Software GmbH. All rights reserved.</p>","boost":0.5},{"location":"documentation/about-this-documentation/#target-group-and-audience","title":"Target group and audience","text":"<p>This documentation is created for all users of Xtract Universal.  The user documentation offers an overview of the interface, of the navigation and of the basic information to the users who never or rarely worked with the product.  Experienced users can find more detailed information on more complex topics and use the user documentation for reference.</p> <p>Reading the \"Getting Started\" section is the prerequisite for working with the product.  The documentation can also be useful during the evaluation phase. </p>","boost":0.5},{"location":"documentation/about-this-documentation/#typographical-conventions","title":"Typographical conventions","text":"Convention Used for marking: Example bold &amp; square brackets Buttons [Edit] bold URL buttons Subscriptions bold Fields within a window, tab names Name italics Input values MATNR italics Drop-down menu options TextAndCode","boost":0.5},{"location":"documentation/about-this-documentation/#notes-and-warning-messages","title":"Notes and warning messages","text":"<p>There are three main types of warning messages with the corresponding colors.</p> Signal word Color Severity and meaning Note Blue Additional information Warning Yellow Information that is important for executing an error free procedure Tip Green Tips Recommendation Green Theobald Software recommendations and best practices advice <p>Warning Messages comply with EN 82079 and formulated according the SAFE-method that is derived from German. The SAFE method is a procedure for systematically designing safety instructions. The severity of the danger as well as the source of the danger. </p> <p>\"SAFE\" stands for:</p> <p>Schwere der Gefahr (Signalwort) Art und Quelle der Gefahr Folgen bei Missachtung der Gefahr Entkommen (Ma\u00dfnahmen zur Abwehr der Gefahr)</p> <p>Translation:  Severity of the danger (Signal word) Type and source of the danger Consequences of disregarding the danger Escape (measures to avert the danger)</p> <p>Example:</p> <p>Warning</p> <p>RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.</p> <p>Note</p> <p>The corresponding SQL command is generated dynamically and executed on the SAP server.</p>","boost":0.5},{"location":"documentation/designer/","title":"Start the Designer","text":"<p>This page shows how to connect the Xtract Universal Designer with a Xtract Universal Server.</p>","tags":["login"]},{"location":"documentation/designer/#connect-the-designer-to-a-server","title":"Connect the Designer to a Server","text":"<p>When starting the Xtract Universal Designer, the window \"Connect to Xtract Universal Server\" opens. Enter connection and user details to connect the Designer to a Xtract Universal Server.</p> Designer and Server run on the same machineDesigner and Server run on different machines <ol> <li>Launch the Xtract Universal Designer.</li> <li>When the Xtract Universal Server is a local server, the server address is localhost. The default port (8064) may vary depending on the configuration.  </li> <li>If the service is not running on default port 8064, specify the port by adding :[port] after the host name. The default port can be configured in the server settings.</li> <li>Select an authentication method. Once logged in, you can activate or deactivate methods of authentication, see Access Management. </li> <li>Click [ plug2 icon plug2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com).  Connect] to connect the Designer to the Server. The main window of the Designer opens.</li> </ol> <ol> <li>Launch the Xtract Universal Designer.</li> <li>When the Xtract Universal Designer and the Xtract Universal Server run on different machines, enter the host name of the Xtract Universal Server.  Make sure the port is not blocked by your firewall. </li> <li>If the service does not run on default port 8064, specify the port by adding :[port] after the host name. The default port can be configured in the server settings.</li> <li>Select an authentication method. Once logged in, you can activate or deactivate methods of authentication, see Access Management. </li> <li>Click [ plug2 icon plug2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com).  Connect] to connect the Designer to the Server. The main window of the Designer opens.</li> </ol> <p>Xtract Universal offers different methods of authenticating and securing the connection between the Xtract Universal Designer and the Xtract Universal Server.  The default authentication methods are authentication via Windows credentials (current user) and Anonymous (no encryption).</p>","tags":["login"]},{"location":"documentation/designer/#main-window-of-the-designer","title":"Main Window of the Designer","text":"<p>The Designer features different functionalities to design and configure extractions.</p> <p>Tip</p> <p>Press F1 to open the documentation page for particular windows.</p>","tags":["login"]},{"location":"documentation/designer/#main-menu-bar","title":"Main Menu Bar","text":"DesignerExtractionServerSecurityHelp Icon Menu Item Description Details nav_undo icon nav_undo icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Reset Preferences Reset connection settings to server Connect the Designer to a Server link_broken icon link_broken icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Disconnect Log off the server - door_exit icon door_exit icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Exit Close the Designer - Icon Menu Item Description Details plus icon plus icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). New Create a new extraction Create an Extraction Edit Edit an existing extraction - Delete Delete an existing extraction - copy icon copy icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Clone Clone an existing extraction - key2 icon key2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Add/Remove Keywords Define keywords of selected extractions General Settings graph_from icon graph_from icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Source Select an existing SAP source system SAP Connection touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination Select a destination Destinations history2 icon history2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Log Open extraction log Extraction Logs Run Run a selected extraction Run an Extraction selection_delete icon selection_delete icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Abort Abort a selected extraction - Clear Result Cache Clear extraction cache Cache results arrow_circle2 icon arrow_circle2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Refresh Refresh extraction status - Filter Define filters or keywords Define Keywords Icon Menu Item Description Details history2 icon history2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Logs Open server logs Server Logs window_gear icon window_gear icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Settings Open server settings Server Settings graph_from icon graph_from icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Manage Sources Edit connection to source system SAP Connection touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Manage Destinations Edit or delete destinations Destinations. Icon Menu Item Description Details key2 icon key2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Set User Password Set or change user password User Managements user_monitor icon user_monitor icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Manage Users Manage user groups User Groups Icon Menu Item Description Details Online Help (EN) Open the HelpCenter  -  mail_forward icon mail_forward icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Submit Support Ticket Open the Ticket Portal Support Portal arrow_circle2 icon arrow_circle2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Download latest version Link to My Theobald Software Customer Portal Info Product information - history icon history icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Version History List of recent software changes Changelog","tags":["login"]},{"location":"documentation/designer/#gui-buttons","title":"GUI Buttons","text":"Icon Menu Item Description Details plus icon plus icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). New Create a new extraction Create an Extraction Edit Edit existing extraction - Delete Delete existing extraction - copy icon copy icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Clone Clone existing extraction - arrow_circle2 icon arrow_circle2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Refresh Update of the extraction status - Search Filter extraction names List of Extractions graph_from icon graph_from icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Source Select existing SAP source system SAP Connection touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination Select destination Destinations history2 icon history2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Log Open extraction log Extraction Logs Run Run a selected extraction Run an Extraction","tags":["login"]},{"location":"documentation/designer/#extraction-groups","title":"Extraction Groups","text":"<p>Extractions can be organized in groups, see Organize Extractions. The treeview in the main window of the Designer displays all extraction groups and reflects the folder structure in the following directory:  <code>C:\\Program Files\\XtractUniversal\\config\\extractions</code>.</p> <p>Select a group to display the list of extractions that are located in the corresponding directory.</p>","tags":["login"]},{"location":"documentation/designer/#list-of-extractions","title":"List of Extractions","text":"<p>List of all extractions in the currently selected extraction group divided by name, type, modification and creation date etc. </p> <p>Tip</p> <p>Use the search bar above the list to filter extractions. Wildcards (*) are not supported.   More extensive filter options are available using the GUI Buttons .</p> <p>Tip</p> <p>To adjust the settings of multiple extractions at once, select multiple extractions using Ctrl+Left Button. You can now adjust the [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com).  Destination] and [ graph_from icon graph_from icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com).  Source] settings for all selected extractions, see GUI Buttons.</p> <p>Double Click: Double-clicking an extraction works as a shortcut to the [  Edit] button.  Right-click: Right-clicking an extraction in the list opens the following menu:</p> Icon Menu Item Description Details Edit Edit an existing extraction - Delete Delete an existing extraction - copy icon copy icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Clone Clone an existing extraction - key2 icon key2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Add/Remove Keywords Define keywords of selected extractions General Settings history2 icon history2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Log Open extraction log Extraction Logs graph_from icon graph_from icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Source settings Select an existing SAP source system - touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination settings Select a destination Destinations Run Run a selected extraction Run an Extraction window_environment icon window_environment icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Run in browser Run a selected extraction in the default web browser - console icon console icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Run in xu.exe Run a selected extraction in the command-line tool xu.exe Execute Extractions with xu.exe selection_delete icon selection_delete icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Abort Abort a selected extraction -","tags":["login"]},{"location":"documentation/designer/#status-bar","title":"Status Bar","text":"<p>The status bar contains information about the following: </p> <ul> <li>Number of defined extractions in the selected folder, the overall number of extractions and the number of licensed extractions</li> <li>Login username</li> <li>Server authentication method</li> <li>Designer access rights</li> <li>License type and validity period</li> <li>Connected XU server version</li> <li>Connected XU client version</li> </ul>","tags":["login"]},{"location":"documentation/introduction/","title":"Introduction to Xtract Universal","text":"<p> This section contains user documentation for Xtract Universal. </p> <p>Find more information on how to use and navigate through the user documentation in the section \"About this Documentation\". </p>"},{"location":"documentation/introduction/#about-xtract-universal","title":"About Xtract Universal","text":"<p>Xtract Universal is an SAP Connector for data extraction from SAP to various target environments.  The extracted data can than be further processed e.g., in the context of business intelligence, data integration and data analytics.</p> <p></p>"},{"location":"documentation/introduction/#about-extractions","title":"About Extractions","text":"<p>The main entities in Xtract Universal are called \"extractions\".  An extraction is a combination of the following components:</p> <ul> <li>A defined extraction type, e.g., Table, Query, Report, etc.</li> <li>A connection to a source system, e.g., SAP S/4HANA, SAP BW, etc.</li> <li>A connection to a destination / target environment, e.g., SQL Server, Azure, etc. </li> </ul> <p>The graphic on the right depicts a practical example of an extraction and its components.</p> <p>For more information on how to create and maintain extractions in the Xtract Universal Designer, see create extractions.</p>"},{"location":"documentation/introduction/#software-architecture","title":"Software Architecture","text":"<p>Xtract Universal consists of two components:</p> <ul> <li>A Designer to design and configure extractions. When designing extractions in the Designer, the user can create and modify extractions, sources, destinations and alter the server settings.</li> <li>A Server to execute extractions. During the execution phase, the user can execute the extractions that were designed in the Designer.  The execution of the extractions takes place on the server.</li> </ul> <p></p> <p>Depending on the target environment, an extraction can be executed by the Xtract Universal command line tool - xu.exe / xu.elf or by the consuming destination, see Pull and Push Destinations.</p>"},{"location":"documentation/introduction/#extraction-types","title":"Extraction Types","text":"<p>Xtract Universal offers the following extractions types to cover a wide range of data extraction scenarios.</p> <p>BAPI</p><p>Execute BAPIs and Function Modules.  ECC S4/HANA BW BW/4HANA</p> <p>BWCube</p><p>Extract data from SAP BW InfoCubes and BEx Queries.  BW BW/4HANA</p> <p>BW Hierarchy</p><p>Extract Hierarchies from an SAP BW / BI system.  BW BW/4HANA</p> <p>DeltaQ</p><p>Extract data from DataSources (OLTP) and extractors from ERP and ECC systems.  ECC S4/HANA BW BW/4HANA</p> <p>ODP</p><p>Extract data via the SAP Operational Data Provisioning (ODP) framework.  ECC S4/HANA BW BW/4HANA</p> <p>OHS</p><p>Extract data from InfoSpokes and OHS destinations.  BW BW/4HANA</p> <p>Query</p><p>Extract data from ERP queries. BEx queries are covered by BWCube.  ECC S4/HANA</p> <p>Report</p><p>Extract data from SAP ABAP reports.  ECC S4/HANA</p> <p>Table</p><p>Extract data from SAP tables and views.  ECC S4/HANA BW BW/4HANA</p> <p>Table CDC</p><p>Extract delta data from SAP tables and views.  ECC S4/HANA BW BW/4HANA</p>"},{"location":"documentation/logs/","title":"Logs","text":"<p>Xtract Universal logs all steps performed on a system in log files.  This page shows how to access server and extraction logs in the Designer. Logs can also be queried using the Xtract Universal Web API.</p>"},{"location":"documentation/logs/#log-levels","title":"Log Levels","text":"<p>Each log entry is assigned one of the following message types:</p> Type of log entry Description E - Errors Error messages issued during the extraction process. I - Information Status messages, about processes that do not lead to an error. W - Warnings Information about problems that do not lead to an extraction error, e.g., authentication errors. D - Debug Details Detailed information that helps finding error cause."},{"location":"documentation/logs/#access-server-logs","title":"Access Server Logs","text":"<p>To open the server logs, navigate to Server &gt; Log (Web Worker) in the main window of the Designer. </p> <p>The logs are created per TCP connection.  To switch between logs, use the list of timestamps on the left. Server logs are automatically deleted after a defined period of time, see server settings.</p> <p>For information on how to query server logs using the web API, see Web API - Get Server Logs.</p> <p></p>"},{"location":"documentation/logs/#access-extraction-logs","title":"Access Extraction Logs","text":"<p>To open the extraction logs, select an extraction from the list of extractions and click [ history2 icon history2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com).  Logs] in the main window of the Designer. </p> <p>The timestamp for each extraction process is displayed in the left part of the \"View Extraction Log\" window. Extraction logs are automatically deleted after a defined period of time, see server settings.</p> <p>For information on how to query extraction logs using the web API, see Web API - Get Extraction Logs.</p> <ul> <li>To filter the execution date of the logs, enter a time period in  .</li> <li>To filter log levels, use the checkboxes underneath the log display  .</li> <li>To copy the current log to the clipboard, click [ copy icon copy icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). ] . </li> </ul> <p> </p>"},{"location":"documentation/logs/#activate-tracing","title":"Activate Tracing","text":"<p>The RFC communication with SAP can be recorded by tracing for troubleshooting purposes. Standard logging is always active and is independent of the tracing setting. </p> <ol> <li>Open the SAP connection settings of the source that is assigned to the failing extraction.</li> <li>In the tab RFC Options, enter the path to an existing directory in the field Trace directory or create a new directory for the trace files. </li> <li>Reproduce the error. XML files with the tracing information are created in the specified directory.</li> <li>Compress the folder to a zip file (\"Trace.zip\") and send the zip file to the Theobald support team.</li> <li>Reopen the SAP connection settings and delete the path in the field Trace directory.</li> </ol> <p>Warning</p> <p>Increase of used hard drive memory.  A big amount of information is collected when debug logging is activated. This can decrease the capacity of your hard drives dramatically. Activate the debug logging only when necessary e.g., upon request of the support team.</p>"},{"location":"documentation/logs/#generated-log-files","title":"Generated Log Files","text":"<p>The log files are stored in the installation directory of Xtract Universal, e.g., <code>C:\\Program Files\\XtractUniversal\\logs</code>. The following log files are created:</p> Type File Name Description Location path Server ServiceLog.txt Contains the activities of XtractService.exe. <code>C:\\Program Files\\XtractUniversal\\logs</code> Server WebServer-Log: yyyyMMddTHHmmss.fffZ.log, e.g., 20201013T055455.465Z.log The name contains the timestamp in UTC. A new file is created when the server is started, additionally a new log file is created every 24 hours. Theobald.XU.Web.Listener.exe is the corresponding process. <code>C:\\Program Files\\XtractUniversal\\logs\\servers\\web\\listener</code> Server WebWorker-Logs: yyyyMMddTHHmmss.fffZ.log, e.g., 20201013T055455.465Z.log The name contains the timestamp in UTC. A new file is created when a TCP connection is accepted.Theobald.XU.Web.Worker.exe is the corresponding process. <code>C:\\Program Files\\XtractUniversal\\logs\\servers\\web\\worker</code> Extraction Extraction logs: yyyyMMddTHHmmss.fffZ.log, e.g., 20201013T055455.465Z.log The name contains the timestamp in UTC. A new file is created to start an extraction. Theobald.XU.Web.Worker.exe is the corresponding process. <code>C:\\Program Files\\XtractUniversal\\logs\\extractions\\[Name_der_Extaktion]</code> <p>For more information on the server processes, see Server Tasks.</p> <p>Tip</p> <p>To redirect logs to another location, symlink the logs folder of the installation directory to a custom directory. Example for PowerShell: <pre><code>New-Item -ItemType SymbolicLink -Target \"E:\\log\" -Path \"C:\\Program Files\\XtractUniversal\\logs\"\n</code></pre></p>"},{"location":"documentation/logs/#read-extraction-logs","title":"Read Extraction Logs","text":"<p>Read the logs written in understandable language to better understand the procedures of Xtract Universal.  Ihe depicted example log belongs to an extraction that writes data into an SQL destination:</p> <p>  General technical information is displayed.</p> <p>  The SQL destination is prepared for receiving data.</p> <p>  The license check is performed including entity check and other relevant information.</p> <p> Connection to SAP is established.</p> <p> Runtime parameters are logged.</p> <p> Data from SAP is requested.</p> <p> Package no. 1 is received from SAP.</p> <p> Package no. 1 is written to the SQL server.</p> <p> Extraction is completed.</p> <p></p>"},{"location":"documentation/organize-extractions/","title":"Organize Extractions","text":"<p>This page shows how to organize extractions in groups to improve maintenance.</p>"},{"location":"documentation/organize-extractions/#group-extractions","title":"Group Extractions","text":"<p>Follow the steps below to create a new group and add extractions to the group.</p> <ol> <li>In the tree view of the Designer, right-click the extractions folder. The context menu opens. </li> <li>Click New subfolder. The window \"Create new subdirectory\" opens. </li> <li>Enter a unique name for the extraction group and click [OK]. The new subfolder is displayed in the tree view. Nested groups are supported.</li> <li>To add extractions to the subfolder, open the list of extractions in the extractions folder (or any other subfolder) and select all relevant extractions. Use [CTRL]+[left mouse button] to select multiple extractions.</li> <li>Drag and drop extractions from the list of extractions into the new subfolder.  </li> </ol> <p>The treeview in the main window of the Designer reflects the folder structure in the directory <code>C:\\Program Files\\XtractUniversal\\config\\extractions</code>.</p> <p>Tip</p> <p>You can assign an SAP source system or a destination to all extractions within an extraction group by using [Ctrl]+[A] to select all extractions in the group. To assign an SAP source system, use the GUI button [Source]. To assign a destination, use the GUI button [Destination].</p>"},{"location":"documentation/organize-extractions/#rename-or-delete-groups","title":"Rename or Delete Groups","text":"<p>Right-click a group to open the context menu. </p> <ul> <li>Click Rename to rename the group.</li> <li>Click Delete to delete the group and its content.</li> </ul>"},{"location":"documentation/organize-extractions/#look-up-extractions","title":"Look Up Extractions","text":"Search within an Extraction GroupSearch in all Extraction Groups <p>Use the search bar above the extraction list to filter the displayed extractions. Wildcards (*) are not supported.</p> <p></p> <p>Follow the steps below to look up an extraction in all extraction groups:</p> <ol> <li>In the main window of the Designer, click [Search]. The window \"Search Extractions\" opens. </li> <li>Enter the name of a group, extraction, extraction type, source system, destination or a keyword in the search bar. </li> <li>Click  . Search results are displayed.</li> <li>Select an extraction from search results.</li> <li>Click [Go]. </li> </ol> <p>The corresponding extraction group opens and the extraction is is selected.</p>"},{"location":"documentation/access-restrictions/","title":"Access Management","text":"<p>This section contains information on how to set up access restrictions to the Designer and the Server.</p> <ul> <li>Restrictions to the Designer affect the creation or modification of destinations, sources and extractions</li> <li>Restrictions to the Server affect the execution of extractions</li> </ul> <p>There are two types of users and user groups, access can be restricted to:</p> <ul> <li>Windows AD users (Kerberos authentication)</li> <li>Custom users (Basic authentication)</li> </ul>","tags":["security"],"boost":2},{"location":"documentation/access-restrictions/#authentication-between-designer-and-server","title":"Authentication Between Designer and Server","text":"<p>The connection between the Designer and the Server can be established using different authentication and encryption methods. The authentication method guarantees the verification of the identity of the logged in user.</p> <p></p> <p>The following combinations of transport encryption and authentication are available:</p> <p>Note</p> <p>To use Kerberos transport encryption or authenticate an Active Directory (AD) user, a Kerberos Target Principal Name (TPN) is required.  TPN can either be a User Principal Name (UPN) or a Service Principal Name (SPN).  For more information on TPN, see Knowledge Base Article: Target Principal Field (TPN).</p> Authentication Method Description TPN required Windows credentials (current user) The AD user, who runs the Designer authenticates themselves towards the Server via Kerberos. All data exchanged between Designer &amp; Server is encrypted using Kerberos. Windows credentials (different users) The AD user, whose user name and password are entered in the login window, authenticates themselves to the XU server via Kerberos. All data exchanged between Designer &amp; Server is encrypted using Kerberos. Custom credentials (TLS encryption) The custom user, whose user name and password are entered in the login window, authenticates themselves to the Server. All data exchanged between Designer &amp; Server is encrypted via TLS. To use TLS transport encryption, an X.509 server certificate is required for the Service (can be stored in the server settings. In the login window, the DNS hostname of the server for which the certificate is issued needs to be entered into the Server field. Custom credentials (Kerberos encryption) The custom user, whose user name and password are entered in the login window, authenticates themselves to the Server. All data exchanged between Designer &amp; Server is encrypted using Kerberos. Anonymous (no encryption) There is no authentication. The data exchanged between Designer &amp; Server is transferred in plain text without transport encryption.","tags":["security"],"boost":2},{"location":"documentation/access-restrictions/#activate-deactivate-authentication","title":"Activate / Deactivate Authentication","text":"<p>Follow the steps below to activate or deactivate authentication methods:</p> <ol> <li>Open Server &gt; Settings from the main window of the Designer. </li> <li>In the tab Configuration Server, activate or deactivate your authentication methods. </li> <li>Click [OK] to confirm your input. If prompted, restart the server.</li> </ol> <p>When starting the Designer, only the activated authentication methods are available for the server connection.</p>","tags":["security"],"boost":2},{"location":"documentation/access-restrictions/install-x.509-certificate/","title":"Install an X.509 Certificate","text":"<p>The installation of an X.509 certificate is required for transport encryption and authentication. There are two main approaches for creating an X.509 certificate:</p> <ul> <li>Certificate released by an (internal) certification authority (CA) </li> <li>Self-signed certificate</li> </ul> <p>Note</p> <p>On test environments you can use a self-signed certificate. For production environment it is recommended to use a certificate released by an (internal) certificate authority (CA). </p>","boost":2},{"location":"documentation/access-restrictions/install-x.509-certificate/#create-the-x509-certificate","title":"Create the X.509 Certificate","text":"<p>Make sure to have a TLS certificate issued by your IT network team considering the following points:</p> <ul> <li>The certificate property \u201cSubject Alternative Name\u201d contains the DNS name of the server on which the Windows service (e.g. Xtract Universal Service or Board Connector Service) is running. </li> <li>Place the certificate in the Windows Certificate Store on the machine, on which the Windows service is running.</li> <li>The certificate common name (CN) attribute contains the DNS name of the server. </li> </ul> <p>Tip</p> <p>To display the Common Name (CN) of the certificate, double-click the certificate in the Cetrificate Manager and navigate to the Details tab.</p>","boost":2},{"location":"documentation/access-restrictions/install-x.509-certificate/#integrate-the-x509-certificate","title":"Integrate the X.509 Certificate","text":"<ol> <li>Import the certificate to the Windows Certificate Store using Microsoft Management Console (mmc). In the example shown, the server name is \"TODD\": </li> <li>Open Server &gt; Settings from the main window of the Designer.  </li> <li>In the tab Web Server, click [Select X.509 certificate]. The window \"Edit certificate location\" opens.</li> <li>Select the X.509 certificate created for your machine under Local Machine &gt; Personal. </li> <li>Click [OK] to confirm your input. If prompted, restart the server.</li> </ol>","boost":2},{"location":"documentation/access-restrictions/install-x.509-certificate/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base Article: Enable Secure Network Communication (SNC) via X.509 certificate</li> <li>Change Service Account</li> </ul>","boost":2},{"location":"documentation/access-restrictions/restrict-designer-access/","title":"Designer Access","text":"<p>This page shows how to restrict access to Xtract Universal's Designer to Windows AD users or custom users. Access restrictions on the Designer ensure that only dedicated users can create or edit destinations, sources and extractions.  Access restrictions can be performed on several levels. </p>","boost":2},{"location":"documentation/access-restrictions/restrict-designer-access/#restrict-access-to-the-designer","title":"Restrict Access to the Designer","text":"<p>Follow the steps below to restrict the access to the Designer to specific users or user groups. By default, the access restrictions also apply to extractions and sources.</p> <ol> <li>Open Server &gt; Settings from the main window of the Designer. </li> <li>In the tab Configuration Server, activate the option Restrict Designer access to the following users/groups. </li> <li>Activate or deactivate authentication methods for the users or user groups.</li> <li>Click [Add] to add existing Windows AD users or custom users.</li> <li>Assign access rights to the users or user groups.</li> </ol> <p>After access restrictions are set up, only designated users can connect to the Xtract Universal server when starting the Designer.</p>","boost":2},{"location":"documentation/access-restrictions/restrict-designer-access/#restriction-levels","title":"Restriction Levels","text":"<p>A user group can have one of the following access rights.  These rights only concern actions (read, create, modify) that can be performed within the Designer. </p> Restriction Level Description Read Members of this group have read access, but cannot edit extractions. Modify Members of this group have the same access rights as users with \"read\" rights. Furthermore, users with \"modify\" rights can edit existing extractions, but cannot create or clone extractions. Create Members of this group have identical access rights as the users with \"modify\" rights. In addition, users with \"create\" rights can create and clone extractions, but cannot perform any admin activities. Admin Members of this group have all rights, no restrictions and can perform admin tasks. Admin activities include changing server settings, accessing server logs, or editing users and connections (SAP and target environments). Access restrictions on extractions or the source system are ignored.","boost":2},{"location":"documentation/access-restrictions/restrict-designer-access/#restrict-access-to-extractions","title":"Restrict Access to Extractions","text":"<p>Access control can be performed at extraction level.  By default, the Designer access restrictions also apply to extractions and sources.</p> <ol> <li>Double-click an extraction. The main window of the extraction type opens.</li> <li>Click [General Settings]. The window \"General Settings\" opens. </li> <li>In the tab Security, add or remove existing users and assign access rights to the users. </li> </ol> <p>The access control on the extraction level overrides the settings at server level.</p>","boost":2},{"location":"documentation/access-restrictions/restrict-designer-access/#restrict-access-to-sources","title":"Restrict Access to Sources","text":"<p>Access control can be performed at the SAP source level.  By default, the Designer access restrictions also apply to extractions and sources.</p> <ol> <li>Open Server &gt; Manage Sources from the main window. The window \"Manage Sources\" opens. </li> <li>Click [ ] to edit an existing connection. The window \"Change Source\" opens.</li> <li>In the tab Access Control, add or remove existing users and assign access rights to the users. </li> </ol> <p>The access control on the source level overrides the settings at server level.</p>","boost":2},{"location":"documentation/access-restrictions/restrict-server-access/","title":"Server Access","text":"<p>This page shows how to restrict access to Xtract Universal's built in web server to Windows AD users or custom users. Access restrictions on the web server ensure that only dedicated users can execute extractions.  Windows AD credentials or credentials of a custom user must be submitted when running an extraction.</p>","boost":2},{"location":"documentation/access-restrictions/restrict-server-access/#activate-tls-encryption","title":"Activate TLS Encryption","text":"<p>Access restrictions require users to access the web server through an https connection (TLS encryption).  This requires the installation of an X.509 certificate. If the certificate is not listed in the Windows certificate store, install the X.509 certificate.</p> <ol> <li>Open Server &gt; Settings from the main window of the Designer. </li> <li> <p>In the tab Web Server tab, select one of the following protocols:</p> <ul> <li>HTTPS - Restricted to AD users with Designer read access</li> <li>HTTPS - Restricted to custom users with Designer read access </li> </ul> <p></p> </li> <li> <p>Click [Select X.509 certificate]. The \"Edit certificate location\" window opens.</p> </li> <li>Select the X.509 certificate created for your machine under Local Machine &gt; Personal. </li> <li>Click [OK] to confirm your input. The window closes.</li> <li>Optional: Change the port number of the HTTPS port.</li> <li>Click [OK] to confirm your input. If prompted, restart the server.</li> </ol>","boost":2},{"location":"documentation/access-restrictions/restrict-server-access/#restrict-access-to-windows-ad-users-kerberos-authentication","title":"Restrict Access to Windows AD Users (Kerberos Authentication)","text":"<p>Follow the steps below to limit the execution of extractions to users that pass Windows AD credentials, when calling extractions. The caller must have at least Read access to the Designer.</p> <ol> <li>Assign a Windows service account under which the Xtract Universal service runs, see Change Service Account.</li> <li>Activate TLS encryption.  </li> <li>Open Server &gt; Settings from the main window of the Designer.  </li> <li>In the tab Web Server, select HTTPS - Restricted to AD users with Designer read access.</li> <li>In the tab Configuration Server, add the custom users or groups that are allowed to execute extractions.  For more information, see Designer Access.  </li> <li>Assign at least Read permission to the Windows AD users.</li> <li>Close all windows with [OK]. If prompted, restart the server.</li> </ol> <p>Note</p> <p>This type of authentication uses Kerberos authentication via SPNEGO. NTLM is not supported.</p>","boost":2},{"location":"documentation/access-restrictions/restrict-server-access/#restrict-access-to-custom-users-basic-authentication","title":"Restrict Access to Custom Users (Basic Authentication)","text":"<p>Follow the steps below to limit the execution of extractions to users that pass custom credentials, when calling extractions. The custom user must have at least Read access to the Designer.</p> <ol> <li>Activate TLS encryption.</li> <li>Open Server &gt; Settings from the main window of the Designer.  </li> <li>In the tab Web Server, select HTTPS - Restricted to custom users with Designer read access.</li> <li>In the tab Configuration Server, add the custom users or groups that are allowed to execute an extraction. For more information, see Designer Access.  </li> <li>Assign at least Read permission to the custom users.</li> <li>Close all windows with [OK]. If prompted, restart the server.</li> </ol> <p>Note</p> <p>For information on how to call an extraction with Basic Authentication using the xu.exe, see Basic Authentication via Commandline.</p>","boost":2},{"location":"documentation/access-restrictions/restrict-server-access/#related-links","title":"Related Links","text":"<ul> <li>Wikipedia: SPNEGO</li> <li>Knowledge Base Article: Enable Secure Network Communication (SNC) via X.509 certificate</li> <li>Server Settings</li> </ul>","boost":2},{"location":"documentation/access-restrictions/user-management/","title":"User Management","text":"<p>This page shows how to create custom users and user groups in the Xtract Universal Designer. To restrict access to the Xtract Universal Designer or Server, you can use already existing Windows AD Users and Groups or create your own custom users and groups.</p> <p>Note</p> <p>Windows AD Users and Groups are created outside of the Designer. Be careful to only add Windows AD Security Groups to the Designer. Users that are assigned to Distribution Groups are denied access at logon.</p>","boost":2},{"location":"documentation/access-restrictions/user-management/#create-custom-users","title":"Create Custom Users","text":"<p>Follow the steps below to create custom users:</p> <ol> <li>Open Server &gt; Manage Users from the main window of the Designer. </li> <li>Click [Add] to create a new user. </li> <li>Enter a username and a password for the user.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>At the time of user creation, no rights need to be assigned. For information on how to assign access rights, see Designer Access.</p>","boost":2},{"location":"documentation/access-restrictions/user-management/#assign-custom-users-to-user-groups","title":"Assign Custom Users to User Groups","text":"<p>Follow the steps below to assign custom users to user groups. It is recommended to assign users to groups and grant access to particular actions instead of adding single users and granting them access one by one.</p> <ol> <li>Open Server &gt;  Settings.  </li> <li>In the tab Configuration Server, activate the option Restrict Designer access to the following users/groups. </li> <li>Click [New] to create a custom user group. </li> <li>Click [OK] to close the server settings. If prompted, restart the Designer.</li> <li>Open Server &gt; Manage Users from the main window of the Designer. </li> <li>Select an existing custom user and click [Edit]. The \"Edit user\" window opens.</li> <li>Use the arrows [&lt;] and [&gt;] to assign and remove users to and from groups. </li> <li>Click [OK] to confirm your input.</li> </ol> <p>Custom users can only be assigned to custom user groups.  Windows AD users can only be assigned to Windows AD groups, but not to custom user groups. </p>","boost":2},{"location":"documentation/access-restrictions/user-management/#user-groups-after-migration","title":"User Groups after Migration","text":"<p>As custom users can only be assigned to custom user groups and Windows AD users can only be assigned to Windows AD groups, the created custom user groups may disappear when migrating to a newer product version. This does not affect access management, but the access at user level is resolved. </p> <p>To grant access at group level, the Windows AD users need to be assigned to new Windows AD groups.</p>","boost":2},{"location":"documentation/bapi/","title":"BAPI","text":"<p>This page shows how to use the BAPI extraction type. The BAPI extraction type can be used to parameterize and execute SAP function modules and BAPIs for automation.</p>"},{"location":"documentation/bapi/#about-function-modules-bapis","title":"About Function Modules / BAPIs","text":"<p>Function modules are procedures that encapsulate and reuse global functions in the SAP system.  SAP systems contain several predefined functions modules that can be called from any ABAP program.  A Business Application Programming Interface (BAPI) is a remote function module that can access business data and processes of an SAP system from different systems.</p>"},{"location":"documentation/bapi/#custom-bapis","title":"Custom BAPIs","text":"<p>The use of custom BAPIs (Z function modules) is possible. Issues specific to Z function modules are not included in the scope of support provided by Theobald Software.</p>"},{"location":"documentation/bapi/#prerequisites","title":"Prerequisites","text":"<ul> <li>A connection to an SAP system is available, see SAP Connection.</li> <li>The SAP user has sufficient user rights, see SAP Authority Objects.</li> </ul> <p>Warning</p> <p>Missing Authorization. To use the BAPI extraction type, access to the designated authority objects (RFC) in SAP must be available.  Adjust SAP Authority Objects: BAPI accordingly.</p>"},{"location":"documentation/bapi/#create-a-bapi-extraction","title":"Create a BAPI Extraction","text":"<ol> <li>In the main window of the Designer, click [New] to create a new extraction. The window \"Create Extraction\" opens. </li> <li>Select an SAP Connection from the drop-down menu in Source and enter a unique name for your extraction.</li> <li>Select the extraction type BAPI and click [OK]. The main window of the extraction type opens automatically.</li> </ol> <p>The majority of the functions of the extraction type can be accessed in the main window.</p>"},{"location":"documentation/bapi/#look-up-a-function-module-bapi","title":"Look up a Function Module / BAPI","text":"<ol> <li>In the main window of the extraction type, click [ ] to look up a Function Module / BAPI.  The window \"Function Module Lookup\" opens. </li> <li>In the window \"Function Module Lookup\" enter the name of the function module or BAPI  . Use wildcards (*) if needed. </li> <li>Click [ ] . Search results are displayed.</li> <li>Select a Function Module / BAPI   and click [OK]. </li> </ol> <p>The application returns to the main window of the extraction type.</p>"},{"location":"documentation/bapi/#define-the-bapi-extraction-type","title":"Define the BAPI Extraction Type","text":"<p>The BAPI extraction type offers the following options for defining parameters of a Function Module / BAPI:</p> <ol> <li>Add input parameters (data you want to send to SAP) in Imports, see Import Parameters. You can enter scalar values   or structures  . </li> <li>Add output parameters (data you want to receive from SAP) in Exports, see Export Parameters. Select output by activating the checkbox next to the items. </li> <li>Optional: If available, define input and output parameters in Changings, see Changings Parameters.</li> <li> <p>Add tables to the output of the extraction type or add table parameters to the input of the extraction type in Tables, see Table Parameters.</p> <ul> <li>Click [ ] to check the names and data types of the table fields  .</li> <li>Activate the checkbox in the output column to add items to the output  .</li> <li>Click [ ] to edit the content of the table  .</li> </ul> <p></p> </li> <li> <p>Optional: If available, define which exceptions thrown by the Function Module / BAPI are ignored during runtime, see Exceptions.</p> </li> <li>Check the General Settings before running the extraction.</li> <li>Click [OK] to save the extraction type.</li> </ol> <p>You can now run the extraction, see Execute and Automate Extractions.</p>"},{"location":"documentation/bapi/edit-runtime-parameters/","title":"Runtime Parameters","text":"<p>Runtime parameters are are placeholders for values that are passed at runtime, see Extraction Parameters - Custom.</p>"},{"location":"documentation/bapi/edit-runtime-parameters/#create-runtime-parameters","title":"Create Runtime Parameters","text":"<p>There are two types of runtime parameters:</p> Scalar parametersList parameters <p>Scalar runtime parameters represent a single value.  Follow the steps below to create a scalar runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add Scalar] to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime. </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.</p> Type Description Text Can be used for any type of SAP selection field. Number Can be used for numeric SAP selection fields. Flag Can only be used for SAP selection fields THAT require an \u2018X\u2019 (true) or a blank \u2018\u2018 (false) as input value. </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p> <p>List runtime parameters represent multiple values.  Follow the steps below to create a list runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add List] to define list parameters that contain multiple values separated by commas e.g., 1,10 or \u201c1\u201d, \u201c10\u201d. The placeholders need to be populated with actual values at runtime.  </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p>"},{"location":"documentation/bapi/edit-runtime-parameters/#assign-runtime-parameters","title":"Assign Runtime Parameters","text":"<p>Follow the steps below to assign runtime parameters to input fields of import, changings or table parameters of a Function Module / BAPI:</p> <ol> <li>Navigate to the item you want to parameterize in the Imports, Changings or Tables tab.</li> <li>Click the icon button next to the item to switch from static values ( ) to runtime parameters ( ). If no icon is available, create a runtime parameter.</li> <li>Select a runtime parameter from the dropdown-list. </li> </ol> <p>Pass values during runtime, see Extraction Parameters - Custom.</p>"},{"location":"documentation/bapi/general-settings/","title":"General Settings","text":"<p>This page contains an overview of the settings in the window \"General Settings\". To open the general settings, click General Settings in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/bapi/general-settings/#misc-tab","title":"Misc. Tab","text":"<p>The Misc. tab covers cache settings, column encryption and keywords of an extraction type.</p> <p></p>"},{"location":"documentation/bapi/general-settings/#cache-results","title":"Cache results","text":"<p>The Cache results option is only available in pull destinations, e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times.  To decrease the SAP server load, you can select the Cache results option, this way the pull destination pulls the data from cache and not from the SAP.</p> <p>This increases the performance and limits the impact on the SAP system.  If this behavior is not desired (for example, because the data must be always 100% up to date), the cache option must be explicitly turned off.</p>"},{"location":"documentation/bapi/general-settings/#keywords","title":"Keywords","text":"<p>One or more keywords (tags) can be assigned to an extraction.  Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the \"Search Extractions\" window. To open the \"Search Extractions\" window, click [  Search] in the main window of the Designer. </p> <p>Tip</p> <p>To add keywords to multiple extractions at once, select the extractions in the main window of the Designer. Right-click + Add/Remove keywords opens the window \"Add/Remove Keywords To/From Multiple Extractions\".</p>"},{"location":"documentation/bapi/general-settings/#primary-key-tab","title":"Primary Key Tab","text":"<p>Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.  </p> <p></p> <p>The depicted example shows the SAP object MAKT with its primary key inherited from SAP in the general settings of the Designer.  In this example the primary key consists of MANDT, MATNR and SPRAS. The primary key is also taken over in the destination. </p> <p>Note</p> <p>A defined primary key field in a table is a prerequisite for merging data. </p>"},{"location":"documentation/bapi/general-settings/#generate-surrogate-key-column","title":"Generate Surrogate Key Column","text":"<p>If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs.  The surrogate keys are hash values of type signed 8 byte integer, e.g., <code>#-3008591679982390000</code>. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.</p>"},{"location":"documentation/bapi/general-settings/#security-tab","title":"Security Tab","text":"<p>Restrict user access to the extraction. For more information, see Restrict Designer Access.</p> <p></p>"},{"location":"documentation/bapi/general-settings/#columns-order-tab","title":"Columns Order Tab","text":"<p>The \"Columns Order\" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns.  Index 0 defines the first column in the result set, index 1 the second columns, etc.</p> <p></p> Option Description Enabled When this option is active, the defined column order is applied when running the extraction. Swap Swaps the index of 2 columns. All other columns keep their indexes. Insert Inserts the selected column into the selected index. All other indexes are recalculated. Reset default Restores the original column order."},{"location":"documentation/bapi/input-and-output/","title":"Define Input & Output","text":"<p>This page contains an overview and description of the Import, Export, Changings and Tables parameter in the BAPI extraction type.</p> <p>Each Import, Export, Changings and Tables parameter can have one of the following representations:</p> <ul> <li>A scalar field (e.g., characters, date, time, number, amount etc.).</li> <li>A structure consisting of several components.</li> <li>A table (tabular array of data) consisting of columns (data values of the same type) and rows (data records).</li> </ul>"},{"location":"documentation/bapi/input-and-output/#import-parameters","title":"Import Parameters","text":"<p>Import parameters represent the input values sent from the client to SAP.  In the tab Imports you can define import parameters that can be presented as scalar values   or structures  . </p> <p></p> <p>To use the filtering function, enter text in the headers of the columns Name and Description. </p>"},{"location":"documentation/bapi/input-and-output/#scalar-parameters","title":"Scalar Parameters","text":"<p>Assign single values to an import parameter by using one of the following options:</p> <ul> <li>If the input field is empty, enter a static value.</li> <li>If a checkbox is displayed in the input field, the parameter is predefined in SAP. The value in SAP is displayed in a light gray font. To disable the predefined value, activate the checkbox and leave the field empty or enter a new value.</li> <li>If [ ] is displayed in the input field, enter a static value.  Click [ ] to switch between static values and dynamic values that are set at runtime.</li> <li>If [ ] is displayed in the input field, select a runtime parameter. Click [ ] to switch between dynamic values and static values.</li> </ul> <p>When using runtime parameters, make sure the data type of the input matches the data type in SAP. </p>"},{"location":"documentation/bapi/input-and-output/#structure-parameters","title":"Structure Parameters","text":"<p>When a Function Module / BAPI uses structures as import parameters, you can assign structure elements (i.e. fields) similarly to single scalar fields.  Setting a single value or a parameter for the whole structure is not possible. </p> <ol> <li>Click [ ]. The window \"Edit Structures\" opens.</li> <li>Enter static values ( [ ] icon or no icon) or assign runtime parameters ( [ ] icon). </li> </ol> <p>Tip</p> <p>It is possible to use tables as input parameters, see Tables Parameters.</p>"},{"location":"documentation/bapi/input-and-output/#export-parameters","title":"Export Parameters","text":"<p>Export parameters represent the output values sent from SAP back to the client after the execution of a Function Module. In the tab Export you can select the items you want to add to the output of the extraction type.</p>"},{"location":"documentation/bapi/input-and-output/#add-items-to-output","title":"Add Items to Output","text":"<p>Mark the checkbox in the output column to add an item to the output of the extraction type.</p> <p></p> <p>To use the filtering function, enter text in the headers of the columns Name and Description. </p> <p>Tip</p> <p>It is possible to use tables as output parameters, see Add Tables to Output.</p>"},{"location":"documentation/bapi/input-and-output/#changing-parameters","title":"Changing Parameters","text":"<p>Changing parameters represent parameters that can be used for input and output.  In the tab Changings you can define the changing parameters.</p>"},{"location":"documentation/bapi/input-and-output/#table-parameters","title":"Table Parameters","text":"<p>Table parameters are parameters presented in a table structure consisting of multiple rows. Tables can be used for input and output. In the tab Tables you can define table parameters for importing and exporting data into and from an SAP Function Module or BAPI.</p> <p>Tables represent a structure of multiple rows of the same data type.</p> <p>To use the filtering function, enter text in the headers of the columns Name and Description.</p> <p>Note</p> <p>Only 5 tables are available for parallel exporting.</p> <ul> <li>Click [ ] to check the names and data types of the table fields  .</li> <li>Activate the checkbox in the output column to add items to the output  .</li> <li>Click [ ] to edit tables  .</li> </ul> <p></p>"},{"location":"documentation/bapi/input-and-output/#access-metadata-of-tables","title":"Access Metadata of Tables","text":"<p>Click [ ] to display the metadata of a table. The metadata includes the name and the data type of all fields.   After the function module was edited in SAP, refresh the metadata by clicking Refresh metadata </p>"},{"location":"documentation/bapi/input-and-output/#add-tables-to-output","title":"Add Tables to Output","text":"<p>Mark the checkbox in the output column to add a table to the output. </p>"},{"location":"documentation/bapi/input-and-output/#edit-tables","title":"Edit Tables","text":"<p>You can assign tables elements (i.e. fields) similarly to single scalar fields: </p> <ol> <li>Click [ ]. The window \"Edit Table Contents\" opens. </li> <li>Click [Add] to add new set of parameters.</li> <li>Enter values or runtime parameters. When using runtime parameters, make sure the data type of the input matches the SAP data type. </li> <li>Click [Remove] to delete a row.</li> </ol>"},{"location":"documentation/bapi/input-and-output/#exceptions","title":"Exceptions","text":"<p>Exceptions refer to ABAP exceptions / errors messages of an SAP BAPI. If an exception occurs during runtime, Xtract Universal returns a corresponding error message.</p> <p>By default, all exceptions result in errors when running the BAPI extraction type. To ignore exceptions during runtime, deselect the exceptions in the Exceptions tab.</p> <p></p>"},{"location":"documentation/bwcube/","title":"BW Cube","text":"<p>This page shows how to use the BWCube extraction type. The BWCube extraction type can be used to extract MDX or BICS data directly from BW InfoProviders (e.g., Cubes) or from BW Queries.  The BW Queries can be based on InfoProviders.</p>"},{"location":"documentation/bwcube/#prerequisites","title":"Prerequisites","text":"<ul> <li>A connection to an SAP system is available, see SAP Connection.</li> <li>The SAP user has sufficient user rights, see SAP Authority Objects.</li> <li>To extract a BW Query, the attribute Allow External Access to this Query of the Query must be active in the BEx Query Designer or the BW Modeling Tool, see Troubleshooting Article: Allow external access to BW Queries. </li> </ul> <p>Warning</p> <p>Missing Authorization. To use the BWCube extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust SAP Authority Objects: BW Query / BW Cube accordingly.</p>"},{"location":"documentation/bwcube/#mdx-versus-bics","title":"MDX versus BICS","text":"<p>The BWCube extraction type can extract data using the OLAP BAPI Interface (MDX) or the native BICs interface. The following table shows the differences between the two extractors:</p> MDX BICS Lookup syntax for BEx Queries: <code>[tech. name of InfoPovider]/[tech. name of BEx Query]</code>  Example: <code>0SD_C03/0SD_C03_Q0018</code> Lookup syntax for BEx Queries: <code>[tech. name of BEx Query]</code>  Example: <code>0SD_C03_Q0018</code> Lookup syntax for InfoProviders: <code>$[tech. name of InfoProvoider]</code>  Example: <code>$0SD_C03</code> Lookup syntax for InfoProviders: <code>[tech. name of InfoProvider]</code>  Example: <code>0SD_C03</code> Wildcards in lookup: The BEx-Query setting Allow External Access to BW Queries is required.  Example: <code>*0SD_C03_Q0018</code> instead of <code>0SD_C03/0SD_C03_Q0018</code> Wildcards in lookup: not required and therefore not supported Supported InfoProviders:   InfoCubes, Multiproviders, Composite Providers Supported InfoProviders:  InfoCubes, MuliProviders, Composite Providers, DSOs Column names of Key figures: EnterpriseID Column names of Key figures: Technical name If techn. name is empty: name of the base measure.   If name of the base measure is empty: EnterpriseID. Tip: In case of duplicate names, change the technical name in the BEx Query Designer. Character limit for dimension members: max. 60 characters Character limit for dimension members: -"},{"location":"documentation/bwcube/#create-a-bw-cube-extraction","title":"Create a BW Cube Extraction","text":"<ol> <li>In the main window of the Designer, click [New] to create a new extraction. The window \"Create Extraction\" opens. </li> <li>Select an SAP Connection from the drop-down menu in Source and enter a unique name for your extraction.</li> <li>Select the extraction type BW Cube and click [OK]. The main window of the extraction type opens automatically.</li> </ol> <p>The majority of the functions of the extraction type can be accessed in the main window.</p>"},{"location":"documentation/bwcube/#look-up-a-bw-cube-or-query","title":"Look Up a BW Cube or Query","text":"<ol> <li>In the main window of the extraction type, click [ ]. The window \u201cCube and Query Lookup\u201d opens. </li> <li> <p>Select the Extractor, Datasource Type and Extraction Settings of the object  . </p> Option Description Extractor defines if data is extracted using the OLAP BAPI Interface (MDX) or the native BICS Interface. BICS can only be used in combination with the NetWeaver RFC protocol. Datasource Type defines if the look up searches for a BEx Query or an InfoProvider. Extraction Settings only available for the MDX extractor. Use Only Structure if your BWCube extraction was created in an older software version, see extraction settings for more information. </li> <li> <p>In the search bar, enter the name of a Query or a BW Cube / InfoProvider  . Use wildcards (*), if needed.</p> </li> <li>Click [ ]. Search results are displayed. Alternatively click [Direct Load] to skip the lookup and load the BW Cube / InfoProvider directly.  [Direct Load] only works if the full name is entered correctly in the search bar.</li> <li>Select a Query or BW Cube / InfoProvider   and click [OK] to confirm.</li> </ol> <p>The application now returns to the main window of the component.</p> <p>Note</p> <p>Click [Refresh Metadata] to renew metadata.  This is necessary if a data source has been adjusted in SAP, another source system has been connected, or the source system has been updated.</p> <p>Warning</p> <p>Invalid action A BW Query does not apprear in the list.  Switch on the attribute Allow External Access to this Query in the BEx Query Designer or the BW Modeling Tool. For additional details see the Troubleshooting Article: Allow external access to BW Queries. </p>"},{"location":"documentation/bwcube/#define-the-bwcube-extraction-type","title":"Define the BWCube  Extraction Type","text":"<p>The BWCube extraction type offers the following options for Query and BW Cube extractions:</p> <ol> <li> <p>In the tree structure of the extraction type, select the measure (key figures) you want to extract  </p> <p>Tip</p> <p>The tree structure represents the metadata of the Query (or InfoProvider).  The first directory contains all the measures (key figures)  .  The following directories correspond to dimensions and often contain additional dimension properties  . </p> </li> <li> <p>Within the key figures directory, click the arrow to display the available units. Select a unit, if needed.</p> </li> <li>In the following directories, select the dimensions and properties you want to extract  .</li> <li>Optional: Right-click on a dimension to add filters to the dimension, see Dimension Filters.</li> <li>Optional: If a BW Query has a defined variable, click [Edit Variables] to edit the variable or provide input values, see Variables.</li> <li>Click [Load live preview] to display a live preview of the data. For every selected dimension or property, a key figure and a unit is displayed in the result. </li> <li>Check the Extraction Settings and the General Settings before running the extraction.</li> <li>Click [OK] to save the extraction type.</li> </ol> <p>You can now run the extraction, see Execute and Automate Extractions.</p>"},{"location":"documentation/bwcube/edit-runtime-parameters/","title":"Runtime Parameters","text":"<p>Runtime parameters are are placeholders for values that are passed at runtime, see Extraction Parameters - Custom. They can be created in context of Query Variables and Dimension Filters.</p>"},{"location":"documentation/bwcube/edit-runtime-parameters/#create-runtime-parameters","title":"Create Runtime Parameters","text":"<p>There are two types of runtime parameters:</p> Scalar parametersList parameters <p>Scalar runtime parameters represent a single value.  Follow the steps below to create a scalar runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add Scalar] to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime. </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.</p> Type Description Text Can be used for any type of SAP selection field. Number Can be used for numeric SAP selection fields. Flag Can only be used for SAP selection fields THAT require an \u2018X\u2019 (true) or a blank \u2018\u2018 (false) as input value. </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p> <p>List runtime parameters represent multiple values.  Follow the steps below to create a list runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add List] to define list parameters that contain multiple values separated by commas e.g., 1,10 or \u201c1\u201d, \u201c10\u201d. The placeholders need to be populated with actual values at runtime.  </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p>"},{"location":"documentation/bwcube/edit-runtime-parameters/#assign-runtime-parameters","title":"Assign Runtime Parameters","text":"<p>Follow the steps below to assign the runtime parameters to variables or dimension filters.</p> VariablesDimension Filters <ol> <li>In the main window of the extraction type, click [Edit Variables]. The window \"Edit Selections\" opens.</li> <li>Add a filter to the variable, see Variables.</li> <li>Click the icon button next to the input field to switch between static values ( ) and runtime parameters ( ). If no icon button is available, create a runtime parameter. </li> <li>Select a runtime parameter from the dropdown-list.</li> <li>Click [OK] to confirm the input.</li> </ol> <ol> <li>In the main window of the extraction type, right-click the dimension you want to parameterize.</li> <li>Click Edit Filters. The window \"Edit Selections\" opens. </li> <li>Add a filter for the dimension, see Dimension Filter.</li> <li>Click the icon button next to the input field to switch between static values ( ) and runtime parameters ( ). If no icon button is available, create a runtime parameter. </li> <li>Select a runtime parameter from the dropdown-list.</li> <li>Click [OK] to confirm the input.</li> </ol> <p>Pass values during runtime, see Extraction Parameters - Custom.</p>"},{"location":"documentation/bwcube/general-settings/","title":"General Settings","text":"<p>This page contains an overview of the settings in the window \"General Settings\". To open the general settings, click General Settings in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/bwcube/general-settings/#misc-tab","title":"Misc. Tab","text":"<p>The Misc. tab covers cache settings, column encryption and keywords of an extraction type.</p> <p></p>"},{"location":"documentation/bwcube/general-settings/#cache-results","title":"Cache results","text":"<p>The Cache results option is only available in pull destinations, e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times.  To decrease the SAP server load, you can select the Cache results option, this way the pull destination pulls the data from cache and not from the SAP.</p> <p>This increases the performance and limits the impact on the SAP system.  If this behavior is not desired (for example, because the data must be always 100% up to date), the cache option must be explicitly turned off.</p>"},{"location":"documentation/bwcube/general-settings/#keywords","title":"Keywords","text":"<p>One or more keywords (tags) can be assigned to an extraction.  Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the \"Search Extractions\" window. To open the \"Search Extractions\" window, click [  Search] in the main window of the Designer. </p> <p>Tip</p> <p>To add keywords to multiple extractions at once, select the extractions in the main window of the Designer. Right-click + Add/Remove keywords opens the window \"Add/Remove Keywords To/From Multiple Extractions\".</p>"},{"location":"documentation/bwcube/general-settings/#primary-key-tab","title":"Primary Key Tab","text":"<p>Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.  </p> <p></p> <p>The depicted example shows the SAP object MAKT with its primary key inherited from SAP in the general settings of the Designer.  In this example the primary key consists of MANDT, MATNR and SPRAS. The primary key is also taken over in the destination. </p> <p>Note</p> <p>A defined primary key field in a table is a prerequisite for merging data. </p>"},{"location":"documentation/bwcube/general-settings/#generate-surrogate-key-column","title":"Generate Surrogate Key Column","text":"<p>If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs.  The surrogate keys are hash values of type signed 8 byte integer, e.g., <code>#-3008591679982390000</code>. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.</p>"},{"location":"documentation/bwcube/general-settings/#security-tab","title":"Security Tab","text":"<p>Restrict user access to the extraction. For more information, see Restrict Designer Access.</p> <p></p>"},{"location":"documentation/bwcube/general-settings/#columns-order-tab","title":"Columns Order Tab","text":"<p>The \"Columns Order\" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns.  Index 0 defines the first column in the result set, index 1 the second columns, etc.</p> <p></p> Option Description Enabled When this option is active, the defined column order is applied when running the extraction. Swap Swaps the index of 2 columns. All other columns keep their indexes. Insert Inserts the selected column into the selected index. All other indexes are recalculated. Reset default Restores the original column order."},{"location":"documentation/bwcube/settings/","title":"Extraction Settings","text":"<p>This page contains an overview of the extraction settings in the BW Cube extraction type. To open the extraction settings, click Extraction Settings in the main window of the extraction type. </p> <p></p>"},{"location":"documentation/bwcube/settings/#extraction-settings","title":"Extraction Settings","text":""},{"location":"documentation/bwcube/settings/#package-size","title":"Package Size","text":"<p>The extracted data is be split into packages of the defined size. The default value is 50000 lines. A package size between 20000 and 50000 is advisable for large amounts of data. 0 means no packaging.  Not using packaging can lead to an RFC timeout for large data extracts.</p> <p>Warning</p> <p>RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.</p> <p>This option is not supported when using BICS mode.</p>"},{"location":"documentation/bwcube/settings/#row-limit","title":"Row Limit","text":"<p>Specifies the maximum number of extracted records. 0 extracts all data. You can use this option to perform tests with a small amount of data by entering a row limit of e.g., 1000.</p> <p>This option is not supported when using BICS mode.</p>"},{"location":"documentation/bwcube/settings/#member-display","title":"Member Display","text":"<p>This setting is only available for the BICS extractor.</p> <ul> <li>Key: Extracts the key of the dimension member.</li> <li>Text (default): Extracts the text of the dimension member.</li> <li>Text and Key: Adds an additional column for every dimension with the suffix <code>.KEY</code>.  The original column contains the text of the dimension member, the column with the <code>.KEY</code> suffix contains the key of the dimension member.</li> </ul>"},{"location":"documentation/bwcube/settings/#formatted-values","title":"Formatted Values","text":"<p>If this option is active, the values of key figures are formatted as defined in the Query Designer, e.g., scaling factor and number of decimal places.</p>"},{"location":"documentation/bwcube/settings/#only-structure","title":"Only Structure","text":"<p>Activate Only Structure if your BW Cube extraction was created in an old version of the BW Cube component. The method for the metadata retrieval has changed, which affects especially BEx Queries with two structures. This option can only be activated and deactivated in the Look Up window of the component.</p>"},{"location":"documentation/bwcube/settings/#automatic-slicing-dimension","title":"Automatic Slicing Dimension","text":"<p>This option sets a dimension for an automatic slicing.  Slicing is the act of picking a subset of a cube by choosing a single value for one of its dimensions.  Automatic slicing means that a loop is executed for each single value of the chosen slicing dimension (characteristic) to extract the result from BW. </p> <p>Automatic Slicing Dimension allows the extraction of a large amount of data (millions of records) from BW.</p>"},{"location":"documentation/bwcube/settings/#dimension-on-columns","title":"Dimension on Columns","text":"<p>The following extraction setting is only available for MDX extractions.  It appears in the extraction settings window after retrieving the metadata of an MDX extraction.</p> <p>Dimension on Columns allows selecting another dimension/structure than the measure (key figures) dimension on the column axis.  This swaps the measures and the selected dimension: the measures are placed on rows, the selected dimension is placed on columns.</p> <p>Note that:</p> <ul> <li>your BEx Query must contain a dimension structure.</li> <li>dimension filters on columns are not applied.</li> <li>the selected properties for the new column dimension are ignored.</li> <li>unit columns are only supported when key figures is selected for the columns.</li> <li>up to 1000 members will be loaded when confirming the extraction settings window. These members will be the column names.</li> </ul> <p>Recommendation</p> <p>We recommend only using structures on columns.</p> <p>Example:</p> <ul> <li>Output for Dimension on Columns = Key Figures </li> <li>Output for Dimension on Columns = Cal. Year/Quarter [0CALQUARTER] </li> </ul>"},{"location":"documentation/bwcube/settings/#experimental","title":"Experimental","text":"<p>The following settings are only available for BICS extractions.  They appear in the extraction settings window after retrieving the metadata of a BICS extraction.</p> <p></p>"},{"location":"documentation/bwcube/settings/#create-bics-compatibility-report","title":"Create BICS Compatibility Report","text":"<p>When encountering an error using BICS, click [Create BICS Compatibility Report] to run a number of tests to help us analyze why and where the error occured. The test results are automatically stored in a .zip file. If you have multiple different SAP systems, perform this test on each of them.  Send the resulting .zip files to the Theobald Support team.</p>"},{"location":"documentation/bwcube/variables-and-filters/","title":"Variables and Filters","text":"<p>This page shows how to use variables and dimension filters to filter the data that is extracted with the BWCube extraction type. There are two options for filtering:</p> <ul> <li>Variables are usually defined in SAP to create filter options for BW Queries. They can be edited in the BWCube extraction type.</li> <li>Dimension Filters are defined in the BWCube extraction type to filter data from InfoProviders or BW Queries that do not have variables.</li> </ul> <p>Both filter options allow the use of runtime parameters.</p>"},{"location":"documentation/bwcube/variables-and-filters/#edit-variables","title":"Edit Variables","text":"<p>BW queries often have defined variables to create filter options.  Depending on the type of BEx variable (single value, multiple value, interval or complex selection) input fields of the variables are enabled or disabled.</p> <ol> <li>Look up a BEx Query with defined variables  . </li> <li>Click [Edit Variables] . The window \"Edit variables for [name of the query]\" opens.</li> <li>Select a field that uses variables from the drop-down menu  . </li> <li>Add one or more of the following filter types:<ul> <li>Click [Single] to compare the data to a single specified value.</li> <li>Click [Range] to check if the data is (not) within a specified range of values.</li> <li>Click [List] to check if the data is (not) part of a specified list of values. </li> </ul> </li> <li>In the column Sign , select Include to add the filtered data to the output or select Exclude to remove the filtered data from the output.</li> <li> <p>Select an operator in the column Option . The operator filters data according to the table below.</p> Operator Description (not) equal to True if data is (not) equal to the content of operand 1. at least True if data is greater than or equal to the content of operand 1. more than True if data is greater than the content of operand 1. at most True if data is less than or equal to the content of operand 1. less than True if data is less than the content of operand 1. (not) between True if data values do (not) lie between the values of operand 1 and operand 2. element of True if data values are part of operand 1. This option is only available for type List. </li> <li> <p>In the column Value, enter values directly into the input fields Low and High, assign existing  runtime parameters  or look up pre-defined values in SAP.</p> <ul> <li>  - Static values: Enter values directly into the Low and High input fields. </li> <li>  - Runtime parameters: Select an existing runtime parameter from the drop-down list.</li> <li>  - Pre-defined values: If available, select pre-defined values from SAP: </li> </ul> </li> <li>Click [OK] to confirm your input.</li> <li>Click [Load live preview] in the main window of the component to check the result of the filter.  If runtime parameters are defined, you are prompted to populate the parameters with actual values.</li> </ol>"},{"location":"documentation/bwcube/variables-and-filters/#set-dimension-filters","title":"Set Dimension Filters","text":"<p>Each dimension of a BEx Query or an InfoCube offers the possibility to set a filter to execute the MDX statement in BW using the selected filter values. Follow the steps below to create dimension filters in the BWCube extraction type.</p> <ol> <li>In the tree structure of the extraction type, right-click a dimension to open the context menu. </li> <li>Click [Edit Filter]. The window \"Edit Selections\" opens. </li> <li>Add one or more of the following filter types:<ul> <li>Click [Single] to compare the data to a single specified value.</li> <li>Click [Range] to check if the data is (not) within a specified range of values.</li> <li>Click [List] to check if the data is (not) part of a specified list of values. </li> </ul> </li> <li>In the column Sign , select Include to add the filtered data to the output or select Exclude to remove the filtered data from the output. </li> <li> <p>In the column Option, select an operator  . The operator filters data according to the table below.</p> Operator Description (not) equal to True if data is (not) equal to the content of operand 1. at least True if data is greater than or equal to the content of operand 1. more than True if data is greater than the content of operand 1. at most True if data is less than or equal to the content of operand 1. less than True if data is less than the content of operand 1. (not) between True if data values do (not) lie between the values of operand 1 and operand 2. element of True if data values are part of operand 1. This option is only available for type List. </li> <li> <p>In the column Value, enter values directly into the input fields Low and High, assign existing  runtime parameters  or look up pre-defined values in SAP.</p> <ul> <li>  - Static values: Enter values directly into the Low and High input fields. </li> <li>  - Runtime parameters: Select an existing runtime parameter from the drop-down list.</li> <li>  - Pre-defined values: If available, select pre-defined values from SAP: </li> </ul> </li> <li>Click [OK] to confirm your input. </li> <li>Click [Load live preview] in the main window of the extraction type to check the result of the filter. If runtime parameters are defined, you are prompted to populate the parameters with actual values.</li> </ol> <p>When filters are applied, the  symbol is displayed in the treeview of the dimensions.</p>"},{"location":"documentation/deltaq/","title":"DeltaQ","text":"<p>This page shows how to use the DeltaQ extraction type. The DeltaQ extraction type can be used to extract delta data from SAP DataSources.  This means that only recently added or changed data is extracted, instead of a full load. For more information on the delta process, see Initialize a Delta Process.</p> <p>Warning</p> <p>Newer component available. DeltaQ is an old extraction type. If possible, use the newer extraction type ODP.</p>"},{"location":"documentation/deltaq/#prerequisites","title":"Prerequisites","text":"<ul> <li>A connection to an SAP system is available, see SAP Connection.</li> <li>The SAP user has sufficient user rights, see SAP Authority Objects.</li> <li>Configure your SAP system to make DataSources accessible, see SAP Customization for DeltaQ.</li> <li>To use DataSources in Xtract Universal, make sure that the DataSources are activated in SAP.</li> </ul> <p>Warning</p> <p>Missing Authorization. To use the DeltaQ extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust SAP Authority Objects: DeltaQ accordingly.</p>"},{"location":"documentation/deltaq/#sap-transactions-for-working-with-deltaq","title":"SAP Transactions for Working with DeltaQ","text":"<p>The following SAP transaction can support you when working with DataSources:</p> Initial SetupError AnalysisOther Transactions <ul> <li>SBIW - SAP DataSources start page</li> <li>RSA3 - Extractor Checker </li> <li>RSA5 - Install DataSources and Hierarchies from Business Content  </li> <li>RSA6 - Postprocess DataSources and Hierarchies </li> <li>SM59 - Configuration of RFC Connections </li> </ul> <ul> <li>SMQS - qRFC Monitor (QOUT Scheduler): Number of Max.Conn. should be at least 10</li> <li>SM37 - Background jobs</li> <li>SM58 - Transactional RFC</li> <li>SM50 - Process overview</li> <li>SMGW - Gateway Monitor</li> </ul> <ul> <li>RSA7 - Delta Queue Maintenance</li> <li>SMQ1 - qRFC Monitor (outbound queue)</li> <li>WE02 - IDoc list</li> <li>WE20 - Partner profiles</li> </ul>"},{"location":"documentation/deltaq/#create-a-deltaq-extraction","title":"Create a DeltaQ Extraction","text":"<ol> <li>In the main window of the Designer, click [New] to create a new extraction. The window \"Create Extraction\" opens. </li> <li>Select an SAP Connection from the drop-down menu in Source and enter a unique name for your extraction.</li> <li>Select the extraction type DeltaQ and click [OK]. The main window of the extraction type opens automatically.</li> </ol> <p>The majority of the functions of the extraction type can be accessed in the main window.</p>"},{"location":"documentation/deltaq/#look-up-extractors-or-hierarchies","title":"Look Up Extractors or Hierarchies","text":"Look Up ExtractorsLook Up Hierarchies <ol> <li>In the main window of the extraction type, click [ ]. The window \u201cLook Up OLTP Source\u201d opens. </li> <li>In the field Name, enter the name of a DataSource  . Use wildcards ( * ), if needed. </li> <li>Click [ ]. Search results are displayed.</li> <li>Select an extractor   and click [OK] to confirm.</li> </ol> <ol> <li>In the main window of the extraction type, click [ ]. The window \u201cLook Up OLTP Source\u201d opens </li> <li>In the field Name, enter the name of a Hierarchy  . Use wildcards ( * ), if needed. Alternatively, enter the description of a Hierarchy in the field Description.  </li> <li>Click [ ].  Search results are displayed.</li> <li>Select an extractor of type HIER  and click [OK] to confirm. All available Hierarchies are fetched and the window \"Look Up DeltaQ Hierarchies\" opens.</li> <li>Select a Hierarchy   and click [OK] to confirm.  </li> </ol> <p>The application now returns to the main window of the extraction type.</p>"},{"location":"documentation/deltaq/#define-the-deltaq-extraction-type","title":"Define the DeltaQ Extraction Type","text":"<p>The DeltaQ extraction type offers the following options for DataSource extractions:</p> DeltaQ with ExtractorsDeltaQ with Hierarchies <ol> <li>Navigate to Gateway and click [ ]  to look up an RFC destination.  For more information, see DeltaQ Customizing: Gateway. </li> <li>Navigate to Logical Destination and click [ ]  to look up a logical RFC target system.  For more information, see DeltaQ Customizing: Logical Destination.</li> <li>Click Customizing Check  to validate the DeltaQ Customizing on the SAP system. Make sure that all check marks are green. For more information, see DeltaQ Customizing: Customizing Check. </li> <li>Select an Update Mode, e.g., to initialize delta extractions.</li> <li>Select the items you want to extract. </li> <li>Optional: click the Edit option next to an item to filter the data.</li> <li>Click [Run] to testrun the extraction and validate your settings.</li> <li> <p>Click [Activate] to activate the extraction in SAP. After a successful activation, a corresponding status message opens: </p> <p>Note</p> <p>The activation is only required for the update modes Delta, Full or Init. Do not activate the extraction for the Delta Update mode. </p> </li> <li> <p>Check the Extraction Settings and the General Settings before running the extraction.</p> </li> <li>Click [OK] to save the extraction type.</li> </ol> <ol> <li>Navigate to Gateway and click [ ]  to look up an RFC destination.  For more information, see DeltaQ Customizing: Gateway. </li> <li>Navigate to Logical Destination and click [ ]  to look up a logical RFC target system.  For more information, see DeltaQ Customizing: Logical Destination.</li> <li>Click Customizing Check  to validate the DeltaQ Customizing on the SAP system. Make sure that all check marks are green. For more information, see DeltaQ Customizing: Customizing Check. </li> <li>Select an Update Mode, e.g., to initialize delta extractions.</li> <li>Select the items you want to extract. </li> <li>Optional: click the Edit option next to an item to filter the data.</li> <li>Optional: click Extraction Settings to set the language and output format of the Hierarchy, see extraction settings. </li> <li>Check the Extraction Settings and the General Settings before running the extraction.</li> <li>Click [Run] to testrun the extraction and validate your settings.</li> <li>Click [OK] to save the extraction type.</li> </ol> <p>Tip</p> <p>Unlike attributes and transactions, Hierarchies do not have to be activated.</p> <p>You can now run the extraction, see Execute and Automate Extractions.</p>"},{"location":"documentation/deltaq/#execute-deltaq-in-parallel","title":"Execute DeltaQ in Parallel","text":"<p>When extracting multiple DataSources in parallel, it is recommended to use separate RFC destinations, e.g., XTRACT01, XTRACT02, etc. Parallel execution of DataSources with an identical RFC destination is possible, but not recommended.</p>"},{"location":"documentation/deltaq/#related-links","title":"Related Links","text":"<ul> <li>DeltaQ Troubleshooting Guide. </li> <li>SAP Help: Activate the BI Content DataSource</li> <li>Knowledge Base Article: Create Generic DataSource using Function Module and Timestamps</li> </ul>"},{"location":"documentation/deltaq/deltaq-customization/","title":"Customizing Check","text":"<p>This page shows how to reference the SAP RFC destination in the DeltaQ extraction type.</p> <p>Using the DeltaQ extraction type requires customization in SAP, see Customizing for DeltaQ. After an RFC Destination is set up in SAP, the RFC destination and the RFC target system must be entered in the DeltaQ extraction type.</p>"},{"location":"documentation/deltaq/deltaq-customization/#settings-in-the-deltaq-extraction-type","title":"Settings in the DeltaQ Extraction Type","text":""},{"location":"documentation/deltaq/deltaq-customization/#gateway","title":"Gateway","text":"<p>Click [ ] to look up an RFC destination or enter the data of your RFC destination manually:</p> Input Field Description Host The name (or IP address) of your SAP system. Make sure that the Gateway host is the same as in your SAP Connection. Service The gateway service is generally sapgwNN, where NN is the instance number of your SAP system, e.g., a number between 00 and 99. NN must have the same value as the System No field in your SAP Connection or the instance number in your SAP logon. Program ID The name of the registered RFC server. Make sure that the registration of the Program ID and the host is whitelisted in the reginfo ACL on the SAP Gateway, see SAP Blog: RFC Gateway Security."},{"location":"documentation/deltaq/deltaq-customization/#logical-destination","title":"Logical Destination","text":"<p>Click [ ] to look up a logical RFC target system or enter the name of the RFC target system manually.</p>"},{"location":"documentation/deltaq/deltaq-customization/#customizing-check","title":"Customizing Check","text":"<p>In the main window of the component click Customizing Check to validate the DeltaQ customizing on the SAP system. Make sure that all check marks are green. </p> <p></p>"},{"location":"documentation/deltaq/edit-runtime-parameters/","title":"Runtime Parameters","text":"<p>Runtime parameters are are placeholders for values that are passed at runtime, see Extraction Parameters - Custom. They can be created in context of Selections.</p>"},{"location":"documentation/deltaq/edit-runtime-parameters/#create-runtime-parameters","title":"Create Runtime Parameters","text":"<p>There are two types of runtime parameters:</p> Scalar parametersList parameters <p>Scalar runtime parameters represent a single value.  Follow the steps below to create a scalar runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add Scalar] to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime. </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.</p> Type Description Text Can be used for any type of SAP selection field. Number Can be used for numeric SAP selection fields. Flag Can only be used for SAP selection fields THAT require an \u2018X\u2019 (true) or a blank \u2018\u2018 (false) as input value. </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p> <p>List runtime parameters represent multiple values.  Follow the steps below to create a list runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add List] to define list parameters that contain multiple values separated by commas e.g., 1,10 or \u201c1\u201d, \u201c10\u201d. The placeholders need to be populated with actual values at runtime.  </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p>"},{"location":"documentation/deltaq/edit-runtime-parameters/#assign-runtime-parameters","title":"Assign Runtime Parameters","text":"<p>Follow the steps below to assign the runtime parameters to selections.</p> <ol> <li>In the main window of the extraction type, click the [Edit] button next to the selection you want to parameterize.  The window \"Edit Selections\" opens.</li> <li>Add a filter to the selection, see Edit Selections. </li> <li>Click the icon button next to the input field to switch between static values ( ) and runtime parameters ( ). If no icon button is available, create a runtime parameter. </li> <li>Select a runtime parameter from the dropdown-list.</li> <li>Click [OK] to confirm the input.</li> </ol> <p>Pass values during runtime, see Extraction Parameters - Custom.</p>"},{"location":"documentation/deltaq/general-settings/","title":"General Settings","text":"<p>This page contains an overview of the settings in the window \"General Settings\". To open the general settings, click General Settings in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/deltaq/general-settings/#misc-tab","title":"Misc. Tab","text":"<p>The Misc. tab covers cache settings, column encryption and keywords of an extraction type.</p> <p></p>"},{"location":"documentation/deltaq/general-settings/#cache-results","title":"Cache results","text":"<p>The Cache results option is only available in pull destinations, e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times.  To decrease the SAP server load, you can select the Cache results option, this way the pull destination pulls the data from cache and not from the SAP.</p> <p>This increases the performance and limits the impact on the SAP system.  If this behavior is not desired (for example, because the data must be always 100% up to date), the cache option must be explicitly turned off.</p>"},{"location":"documentation/deltaq/general-settings/#keywords","title":"Keywords","text":"<p>One or more keywords (tags) can be assigned to an extraction.  Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the \"Search Extractions\" window. To open the \"Search Extractions\" window, click [  Search] in the main window of the Designer. </p> <p>Tip</p> <p>To add keywords to multiple extractions at once, select the extractions in the main window of the Designer. Right-click + Add/Remove keywords opens the window \"Add/Remove Keywords To/From Multiple Extractions\".</p>"},{"location":"documentation/deltaq/general-settings/#primary-key-tab","title":"Primary Key Tab","text":"<p>Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.  </p> <p></p> <p>The depicted example shows the SAP object MAKT with its primary key inherited from SAP in the general settings of the Designer.  In this example the primary key consists of MANDT, MATNR and SPRAS. The primary key is also taken over in the destination. </p> <p>Note</p> <p>A defined primary key field in a table is a prerequisite for merging data. </p>"},{"location":"documentation/deltaq/general-settings/#generate-surrogate-key-column","title":"Generate Surrogate Key Column","text":"<p>If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs.  The surrogate keys are hash values of type signed 8 byte integer, e.g., <code>#-3008591679982390000</code>. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.</p>"},{"location":"documentation/deltaq/general-settings/#security-tab","title":"Security Tab","text":"<p>Restrict user access to the extraction. For more information, see Restrict Designer Access.</p> <p></p>"},{"location":"documentation/deltaq/general-settings/#columns-order-tab","title":"Columns Order Tab","text":"<p>The \"Columns Order\" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns.  Index 0 defines the first column in the result set, index 1 the second columns, etc.</p> <p></p> Option Description Enabled When this option is active, the defined column order is applied when running the extraction. Swap Swaps the index of 2 columns. All other columns keep their indexes. Insert Inserts the selected column into the selected index. All other indexes are recalculated. Reset default Restores the original column order."},{"location":"documentation/deltaq/selections/","title":"Selections","text":"<p>This page shows how to filter the data that is extracted by the DeltaQ extraction type. Selections limit the result set of the DeltaQ extraction type to extract only records that match the selection.</p>"},{"location":"documentation/deltaq/selections/#edit-selections","title":"Edit Selections","text":"<p>Note</p> <p>Do not define selections when using the update mode Delta Update, because the selections of the Init mode are automatically applied.</p> <p>Follow the steps below to edit selection fields and filter data:</p> <ol> <li>In the subsection OLTP Fields, click Edit next to the field you want to edit. The window \u201cEdit selection\u201d opens. </li> <li>Add one or more of the following filter types:<ul> <li>Click [Single] to compare the data to a single specified value.</li> <li>Click [Range] to check if the data is (not) within a specified range of values.</li> <li>Click [List] to check if the data is (not) part of a specified list of values. </li> </ul> </li> <li>In the column Sign , select Include to add the filtered data to the output or select Exclude to remove the filtered data from the output. </li> <li> <p>In the column Option , select an operator. The operator filters data according to the table below.</p> Operator Description (not) like pattern True if data values do (not) contain to the content of operand 1. (not) equal to True if data is (not) equal to the content of operand 1. at least True if data is greater than or equal to the content of operand 1. more than True if data is greater than the content of operand 1. at most True if data is less than or equal to the content of operand 1. less than True if data is less than the content of operand 1. (not) between True if data values do (not) lie between the values of operand 1 and operand 2. element of True if data values are part of operand 1. This option is only available for type List. </li> <li> <p>In the column Value, enter values directly into the input fields Low and High or assign existing  runtime parameters  to the selection fields  .</p> <p>Note</p> <p>When runtime parameters are available, you can use the icon button inside the input field to switch between static values ( ) and runtime parameters ( ).</p> </li> <li> <p>Click [OK] to confirm your input. </p> </li> <li>Click [Load live preview] in the main window of the extraction type to check the result of your selection.  If runtime parameters are defined, you are prompted to populate the parameters with actual values.</li> </ol>"},{"location":"documentation/deltaq/selections/#script-expressions-for-deltaq","title":"Script Expressions for DeltaQ","text":"<p>Script expressions are usually used to determine a dynamic date based on the current date. </p> Input Description <pre>#{ DateTime.Now.ToString(\"yyyyMMdd\") }#</pre> Current date in SAP format (yyyyMMdd) <pre>#{ String.Concat(DateTime.Now.Year.ToString(), \"0101\") }#</pre> Current year concatenated with \"0101\" (yyyy0101) <pre>#{ String.Concat(DateTime.Now.ToString(\"yyyy\"), \"0101\") }#</pre> Current year concatenated with \"0101\" (yyyy0101) <pre>#{ String.Concat(DateTime.Now.ToString(\"yyyyMMdd\").Substring(0,4), \"0101\") }#</pre> Current year concatenated with \"0101\" (yyyy0101) <p>For more information on script expression, see Script Expressions.</p>"},{"location":"documentation/deltaq/selections/#data-format","title":"Data Format","text":"<p>Use the following internal SAP representation for input:</p> <ul> <li>Date: The date 01.01.1999 has the internal representation 19990101 (YYYYMMDD).</li> <li>Year period: The year period 001.1999 has the internal representation 1999001 (YYYYPPP).</li> <li>Numbers: Numbers must contain the leading zeros, e.g., customer number 1000 has the internal representation 0000001000.</li> </ul> <p>Warning</p> <p>Values accept only the internal SAP representation. Input that does not use the internal SAP representation results in error messages.  Use the internal SAP representation.  Example:  <pre><code>ERPConnect.ABAPProgramException: RfcInvoke failed(RFC_ABAP_MESSAGE): Enter date in the format \\_.\\_.\\_\n</code></pre></p>"},{"location":"documentation/deltaq/settings/","title":"Extraction Settings","text":"<p>This page contains an overview of the extraction settings in the DeltaQ extraction type. To open the extraction settings, click Extraction Settings in the main window of the extraction type. </p> <p></p> <p>The DeltaQ settings consist of the following tabs: </p> <ul> <li> <p>Base</p> </li> <li> <p>Hierarchy</p> </li> </ul>"},{"location":"documentation/deltaq/settings/#base-settings","title":"Base Settings","text":""},{"location":"documentation/deltaq/settings/#transfer-mode","title":"Transfer Mode","text":"<p>The raw data packages can be sent by SAP via tRFC call or Data-IDoc. tRFC is is the default setting.  Switch to IDoc to monitor the raw data packages in the transaction WE02 (IDoc-Monitoring) for debugging reasons. </p>"},{"location":"documentation/deltaq/settings/#misc-settings","title":"Misc. Settings","text":""},{"location":"documentation/deltaq/settings/#automatic-synchronization","title":"Automatic Synchronization","text":"<p>Option to prevent manual changes in the transactional system when switching from test environment to production environment.  Example: To use DeltaQ extractions in the production environment, the data source has to be enabled in the production environment.  If Automatic Synchronization is active, the activation is performed automatically and the timestamp of the data source is changed to be consistent with the settings of the SAP system. </p> <p>Note</p> <p>If the data source is modified in the SAP system, manually activate the data source in the  DeltaQ component, even when Automatic Synchronization is active.  Otherwise data load will fail. This behavior belongs to the SAP design, see SAP Help: Replication of DataSources.</p>"},{"location":"documentation/deltaq/settings/#add-serialization-info-to-output","title":"Add Serialization Info to Output","text":"<p>Adds the columns DataPackageID and RowCounter to the output. Example: the following columns that are a composite key of the SAP records are included in the output:</p> <ul> <li>RequestID</li> <li>DataPackageID </li> <li>RowCounter</li> </ul> <p>Note</p> <p>Newer data has a higher PackageID. In the same package newer data has a higher RowCounter.</p>"},{"location":"documentation/deltaq/settings/#accept-gaps-in-datapackage-id","title":"Accept Gaps in DataPackage ID","text":"<p>At the end of each extraction the DeltaQ component performs a consistency check.  The extraction is considered consistent if all data packages arrive correctly.  Example: When using a filter function in the user exit of an OLTP source, certain data packages are not sent.  In this case the filter function is an inconsistency. If Accept Gaps in DataPackage ID is active, gaps in the package numbering are not considered inconsistencies.  Only use this option when a filter function exists in the user exit.</p>"},{"location":"documentation/deltaq/settings/#timeout-sec","title":"Timeout (sec)","text":"<p>Enter a time period (in seconds). The timeout applies when an extraction finishes on the SAP side, but not all tRFC calls have been received. </p>"},{"location":"documentation/deltaq/settings/#hierarchy-settings","title":"Hierarchy Settings","text":"<p>The following settings only apply to Hierarchy extractions.</p> <p></p>"},{"location":"documentation/deltaq/settings/#extraction","title":"Extraction","text":""},{"location":"documentation/deltaq/settings/#language","title":"Language","text":"<p>Enter the language of the Hierarchy, e.g., \u2018E\u2019 or \u2018D\u2019.</p>"},{"location":"documentation/deltaq/settings/#hierarchy-name","title":"Hierarchy Name","text":"<p>Enter the name of the Hierarchy.</p>"},{"location":"documentation/deltaq/settings/#hierarchy-class","title":"Hierarchy Class","text":"<p>Enter the class of the Hierarchy.</p>"},{"location":"documentation/deltaq/settings/#representation","title":"Representation","text":"<ul> <li>ParentChild: The Hierarchy is represented in the SAP parent-child format, see Output Formats: ParentChild. Example: </li> <li>Natural: The SAP parent-child Hierarchy is transformed into a regular hierarchy, see Output Formats: Natural. Example: </li> <li>ParentChildWithNodeNames: The Hierarchy is represented in a reduced SAP parent-child format that only includes single nodes and their parent, see Output Formats: ParentChildWithNodeNames. Example: </li> </ul>"},{"location":"documentation/deltaq/settings/#natural-representation","title":"Natural Representation","text":"<p>Note</p> <p>The subsection Natural Settings is only active, when the Representation is set to Natural.</p>"},{"location":"documentation/deltaq/settings/#level-count","title":"Level Count","text":"<p>Defines the maximum number of levels. The following example shows a Hierarchy with four levels.  </p>"},{"location":"documentation/deltaq/settings/#fill-empty-levels","title":"Fill empty levels","text":"<p>Copies the bottom element of the Hierarchy until the last level. The following example depicts the previously shown Hierarchy with the activated Repeat Leaves option. </p>"},{"location":"documentation/deltaq/settings/#description-texts-for-levels","title":"Description texts for levels","text":"<p>Sets the output field LevelTextN for each field LevelN containing the text based on the system language settings. </p>"},{"location":"documentation/deltaq/settings/#leaves-only","title":"Leaves only","text":"<p>Returns only the leaves as data records. </p>"},{"location":"documentation/deltaq/settings/#maintenance","title":"Maintenance","text":"<p>Click [Maintenance] to open a list of Init requests of the DataSource (SAP transaction RSA7).</p> <p></p> <p>Select an Init request and click [ ] to delete it. This is necessary when re-initializing a delta process.</p>"},{"location":"documentation/deltaq/update-mode/","title":"Update Mode","text":"<p>The DeltaQ extraction type is primarily used for delta extractions. This means that only recently added or changed data is extracted, instead of a full load. The data that is extracted is defined by the Update Mode setting in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/deltaq/update-mode/#update-modes","title":"Update Modes","text":"<p>The DeltaQ extraction type offers the following update modes:</p> Mode Parameter Value Description Full Full Extracts all data that match the set selection criteria. Delta Update Delta Only extracts data added or changed on the SAP system since the last delta request. Initialize a delta process before running a delta update. To prevent errors, aborts and gaps in your data during a delta run, run the next extraction in the update mode Repeat. Delta Initialization Init Extracts all data as full load and initializes the delta process. When re-initializing a delta process, first delete any existing Inits using the [Maintenance] button in the extraction settings menu. Repeat Repeat Repeats the last delta run and updates and any delta data accumulated since the last run. Deletes any data from the last (unsuccessful) delta update before running a repeat. You can run a repeat multiple times. Delta Init (without data) InitNoData Initializes the delta process without extracting any data from the SAP DataSources. The result of the Delta Init on the SAP side. When re-initializing a delta process, first delete any existing Inits using the [Maintenance] button in the extraction settings menu. Non-cumulative init InitNoncumulative Relevant for DataSources like 2LIS_03_BX. Activate (don't extract) Activate Activates a DataSource similar to the [Activate] button, but it is more practical when activating DataSources in a batch. No data is extracted. <p>Tip</p> <p>Update mode can be set dynamically at runtime using the parameters values to overwrite the extraction parameter updateType, see Run an Extraction.</p> <p>For more information about when to use which update mode, see Initialize a Delta Process</p>"},{"location":"documentation/deltaq/update-mode/#initialize-a-delta-process","title":"Initialize a Delta Process","text":"<p>To start a delta process with the DeltaQ extraction type, the delta must be initialized first.  The following delta process shows when to use which Update Modes. </p>"},{"location":"documentation/deltaq/update-mode/#step-1-delta-initialization-c","title":"Step 1: Delta Initialization [C]","text":"<p>This mode requests all data that meets the selection criteria.</p> <p>Set Update Mode to Delta Initialization and run the extraction. The initialization selections are copied to load the delta records. You can now use Delta Update.</p> <p>Note</p> <p>When re-initializing a delta process, first delete any existing Inits (initialization requests) by clicking [Maintenance] in the extraction settings.</p>"},{"location":"documentation/deltaq/update-mode/#alternative-step-1-delta-init-without-data-s","title":"Alternative Step 1: Delta Init (without data) [S]","text":"<p>This mode is similar to Delta Initialization, but no data is extracted from the SAP DataSource.</p> <p>Set Update Mode to Delta Init (without data) and run the extraction. You can now use Delta Update.</p> <p>Note</p> <p>When re-initializing a delta process, first delete any existing Inits (initialization requests) by clicking [Maintenance] in the extraction settings.</p>"},{"location":"documentation/deltaq/update-mode/#step-2-delta-update-d","title":"Step 2: Delta Update [D]","text":"<p>Delta Update only extracts data added or changed on the SAP system since the last delta request.</p> <p>Run the extraction with Delta Initialization or Delta Init (without data) once before setting the Update Mode to Delta Update.</p> <p>Note</p> <p>To prevent errors, aborts and gaps run the next extraction in the update mode Repeat.</p>"},{"location":"documentation/deltaq/update-mode/#optional-repeat-r","title":"Optional: Repeat [R]","text":"<p>This mode repeats the last delta run and updates all data accumulated since the last run.  If the last run was unsuccessful, all data from the last delta update are deleted before a new run is started. A Repeat can be started several times.</p> <p>Many DataSources have the field ROCANCEL. This field defines if records are added or overwritten depending on the delta process type of the DataSource.  It defines how a record is updated in the delta process. In an ABR mode: </p> <ul> <li>blank returns an after image</li> <li>'X'  returns a before image</li> <li>'D' deletes the record</li> <li>'R' returns a reverse image</li> </ul>"},{"location":"documentation/deltaq/update-mode/#optional-delta-queue-rsa7","title":"Optional: Delta Queue - RSA7","text":"<p>Once delta is activated, you can view the queued datasets in the Delta queue in SAP transaction RSA7. If there is no new data to be transferred, a corresponding protocol message is displayed and the data pipeline is empty.</p> <p>Note</p> <p>Before initiating the next update, make sure that a delta update has been executed successfully. Running a new delta update removes the last one. </p>"},{"location":"documentation/deltaq/update-mode/#related-links","title":"Related Links","text":"<ul> <li>SAP Help: Delta Transfer to BI</li> <li>SAP Help: Delta Process</li> </ul>"},{"location":"documentation/destinations/","title":"Destinations","text":"<p>Xtract Universal allows you to extract data from SAP systems and load it to different destination environments. There are two types of destinations, depending on where the extraction process is started:  </p> <ul> <li> <p> Pull Destinations</p> <p>Extractions with pull destinations provide data on request. The extraction process is started by the destination environment.  When a consumer requests the data, Xtract Universal translates the request into a query for the underlying SAP system, retrieves the data directly from the source system and delivers it to the consumer.</p> </li> <li> <p> Push Destinations</p> <p>Extractions with push destinations provide data proactively. The extraction process is started in Xtract Universal, e.g. via a scheduled extraction. An extraction with push destinations extracts data from the SAP source systems and loads them into the destination, where the data can be processed further.</p> </li> </ul>"},{"location":"documentation/destinations/#manage-destinations","title":"Manage Destinations","text":"<p>To open a list of all existing destinations, navigate to Server &gt; Destinations. The window \"Manage Destinations\" opens. Here, you can edit, delete and add new destinations. </p> <p>Note</p> <p>The http-csv destination is a default destination and cannot be deleted.</p>"},{"location":"documentation/destinations/#databases-data-warehouses","title":"Databases / Data Warehouses","text":"<p>Amazon Redshift push-destination</p> <p>Azure Synapse Analytics push-destination</p> <p>Azure SQL Database push-destination</p> <p>EXASolution push-destination</p> <p>IBM Db2 push-destination</p> <p>MySQL push-destination</p> <p>Oracle push-destination</p> <p>PostgreSQL push-destination</p> <p>SAP HANA push-destination</p> <p>Snowflake push-destination</p> <p>SQL Server push-destination</p>"},{"location":"documentation/destinations/#business-intelligence-analytics-etl","title":"Business Intelligence / Analytics / ETL","text":"<p>Alteryx pull-destination</p> <p>KNIME pull-destination</p> <p>Power BI Connector pull-destination</p> <p>Power BI Report Server pull-destination</p> <p>QlikSense and QlikView pull-destination</p> <p>SQL Server Reporting Services pull-destination</p> <p>Tableau push-destination</p>"},{"location":"documentation/destinations/#business-systems","title":"Business Systems","text":"<p>Salesforce push-destination</p> <p>SharePoint push-destination</p>"},{"location":"documentation/destinations/#cloud-storage","title":"Cloud Storage","text":"<p>Amazon S3 push-destination</p> <p>Azure Blob Storagepush-destination</p> <p>Azure Data Lake Storage push-destination</p> <p>Google Cloud Storage push-destination</p> <p>Huawei Cloud OBS push-destination</p>"},{"location":"documentation/destinations/#generic-destinations","title":"Generic Destinations","text":"<p>CSV Web Service pull-destination</p> <p>JSON Web Service pull-destination</p> <p>CSV Flat-Filespush-destination</p> <p>JSON Flat-Filespush-destination</p> <p>Parquet Flat-Filespush-destination</p>"},{"location":"documentation/destinations/Power-BI-Connector/","title":"Power BI Connector","text":"<p>This page shows how to set up and use the Power BI Connector destination.  The Power BI Connector destination loads data to Power BI</p> <p>The destination offers two ways to connect Power BI with Xtract Universal:</p> <ul> <li>Power BI custom connector</li> <li>Generic Power Query M-script</li> </ul> <p></p>"},{"location":"documentation/destinations/Power-BI-Connector/#supported-power-bi-environments","title":"Supported Power BI Environments","text":"<p>Xtract Universal supports the following environments:</p> <ul> <li>Power BI Desktop via Power BI Custom Connector</li> <li>Power BI service via Power BI on-premises data gateway</li> <li>Power BI Report Server via Power Query M-script</li> </ul> Custom Connector M-script Power BI Desktop Power BI Service Power BI Report Server"},{"location":"documentation/destinations/Power-BI-Connector/#create-a-new-power-bi-connector-destination","title":"Create a new Power BI Connector Destination","text":"<p>Follow the steps below to add a new Power BI Connector destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Power BI Connector from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/Power-BI-Connector/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination. To use the Power BI Connector destination, no further settings are necessary.</p> <p></p>"},{"location":"documentation/destinations/Power-BI-Connector/#assign-the-power-bi-connector-destination-to-an-extraction","title":"Assign the Power BI Connector Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/Power-BI-Connector/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/Power-BI-Connector/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/Power-BI-Connector/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/Power-BI-Connector/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/Power-BI-Connector/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/Power-BI-Connector/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/Power-BI-Connector/#power-bi-custom-connector","title":"Power BI Custom Connector","text":"<p>The Power BI Connector destination works in combination with a custom extension file for Power BI.  This extension file XtractUniversalExtension.pqx is located in the powerbi folder of the Xtract Universal installation directory, e.g., <code>C:\\Program Files\\XtractUniversal\\powerbi\\</code>.  The Power BI Custom Connector was developed following Microsoft's guidelines for custom connectors in Power BI. </p>"},{"location":"documentation/destinations/Power-BI-Connector/#install-the-power-bi-custom-connector","title":"Install the Power BI Custom Connector","text":"<p>Follow the steps below to install the Power BI Custom Connector.</p> Automatic SetupManual Setup for Power BI Desktop <ol> <li>Open the powerbi folder located in the Xtract Univesal installation directory, e.g., <code>C:\\Program Files\\XtractUniversal</code>.</li> <li>Run the install-connector PowerShell script.  This script copies the XtractUniversalExtension.pqx file to the <code>[Documents]\\Power BI Desktop\\Custom Connectors</code> folder.  If the folder does not exist, the script creates the folder.</li> <li>Run the trust-connector PowerShell script.  This script modifies the registry at <code>HKLM:\\SOFTWARE\\Policies\\Microsoft\\Power BI Desktop\\</code>.  The script adds or modifies the registry value TrustedCertificateThumbprints with the thumbprint of the XtractUniversalExtension.pqx file.  This procedure follows Microsoft's recommendations for trusting third-party connectors.</li> </ol> <ol> <li>Check if the directory <code>[Documents]\\Power BI Desktop\\Custom Connectors</code> exists.</li> <li>Create this directory if it doesn't exist.</li> <li>Open the powerbi folder located in the Xtract Univesal installation directory, e.g., <code>C:\\Program Files\\XtractUniversal\\powerbi\\</code>.</li> <li>Copy the XtractUniversalExtension.pqx file from the powerbi folder to <code>[Documents]\\Power BI Desktop\\Custom Connectors</code>.</li> <li>Restart Power BI.</li> <li>In Power BI Desktop, navigate to File &gt; Options and settings &gt; Options &gt; Security.</li> <li>In the subsection Data Extensions, activate the option (Not Recommended) Allow any extension to load without validation or warning.  </li> <li>Click [OK].</li> <li>Restart Power BI.</li> </ol> <p>The Xtract Universal data source is now available within Power BI. </p>"},{"location":"documentation/destinations/Power-BI-Connector/#power-bi-settings","title":"Power BI Settings","text":"<p>Adjust the following settings in Power BI:</p> <ol> <li>In Power BI, navigate to File &gt; Options and settings &gt; Options to open the option menu.</li> <li>Activate the option (Not Recommended) Allow any extension to load without validation or warning in the tab Security. </li> <li> <p>Optional: Increase the Power BI cache in the tab Load Data to prevent multiple calls from Power BI to Xtract Universal when extracting large data volumes.</p> <p>Note</p> <p>Multiple calls to Xtract Universal result in in multiple entries for the same extraction in the Xtract Universal extraction log.</p> <p></p> </li> </ol>"},{"location":"documentation/destinations/Power-BI-Connector/#connect-power-bi-with-xtract-universal","title":"Connect Power BI with Xtract Universal","text":"<p>Follow the steps below to connect Power BI with Xtract Universal</p> <ol> <li>In Power BI, select Xtract Universal from the Get Data menu.</li> <li>Click the [Connect] button.</li> <li> <p>Enter the URL of the Xtract Universal web server <code>&lt;Protocol&gt;://&lt;HOST or IP address&gt;:&lt;Port&gt;/</code>. </p> <p>When prompted for Anonymous, Basic or Windows authentication, follow the steps as outlined in Single Sign On and SAP authentication. The \"Navigator\" window lists all extractions that are assigned to a Power BI Connector destination in Xtract Universal. </p> </li> <li> <p>Select an extraction from the list. The preview data shows the actual SAP column headers and preview data (real data or dummy data, depending on the extraction type).</p> </li> <li>Click the [Load] button. This triggers an extraction in Xtract Universal and writes the extracted data to Power BI. </li> </ol> <p>The data is now available for further processing.</p>"},{"location":"documentation/destinations/Power-BI-Connector/#tutorial","title":"Tutorial","text":"<p>The following YouTube tutorial shows how to install the Power BI Connector and how to use Xtract Universal with Power BI:</p>"},{"location":"documentation/destinations/Power-BI-Connector/#parameterizing-in-power-bi-custom-connector","title":"Parameterizing in Power BI Custom Connector","text":"<p>When using the Power BI Custom Connector, Xtract Universal custom parameters can be populated when setting up the connection in Power BI.</p> <p>A list of an extraction's custom parameters is displayed under the Custom tab in the \"Run Extraction\" window.  In the depicted example the list contains one entry BUKRS_low. </p> <p></p> <p>The list of Xtract Universal custom parameters is exposed in Power BI Desktop when creating a report based on the selected extraction.  The exposed Xtract Universal parameters can be filled with values from within Power BI Desktop.  </p>"},{"location":"documentation/destinations/Power-BI-Connector/#power-query-m-script","title":"Power Query M-script","text":"<p>Instead of the Power BI Custom Connector, you can also use the Custom Power Query M-script to connect Power BI to Xtract Universal.  The Power BI Query M-script is located inside the Xtract Universal installation directory, e.g., <code>C:\\Program Files\\XtractUniversal\\powerbi\\loading_script.txt</code>.</p> <p>Warning</p> <p>Use of the Custom Connector &amp; Query M-script Power Query M-script and Power BI Custom Connector do not belong together. Use either the Power Query M-script or the Power BI Custom Connector.</p>"},{"location":"documentation/destinations/Power-BI-Connector/#set-up-the-power-query-m-script-in-power-bi","title":"Set up the Power Query M-script in Power BI","text":"<p>Note</p> <p>Only use the extractions with the Power BI Connector destination assigned to them.</p> <p>Follow the steps below to set up the Power Query M-script in Power BI to connect with Xtract Universal:</p> <ol> <li>Create a new Power BI report using Home &gt; Get Data &gt; Blank Query as a data source.</li> <li>Open the [Advanced Editor]. </li> <li>Open the Xtract Universal loading_script.txt in any text editor.</li> <li>Copy the content of loading_script.txt into the Advanced Editor in Power BI.</li> <li>Change the values for ExtractionName and the ServerURL to match the names of your Xtract Universal extraction and web server . </li> <li>Within the Advanced Editor, click [Done]  to confirm the script.</li> <li>Click [Close &amp; Apply].</li> <li>In Power BI, navigate to File &gt; Options and settings &gt; Options to open the option menu.</li> <li>Activate the option (Not Recommended) Allow any extension to load without validation or warning in the tab Security. </li> <li> <p>Optional: Increase the Power BI cache in the tab Load Data to prevent multiple calls from Power BI to Xtract Universal when extracting large data volumes.</p> <p>Note</p> <p>Multiple calls to Xtract Universal result in in multiple entries for the same extraction in the Xtract Universal extraction log.</p> <p></p> </li> </ol>"},{"location":"documentation/destinations/Power-BI-Connector/#parameterizing-in-power-query-m-script","title":"Parameterizing in Power Query M-Script","text":"<ol> <li>Open the Power Query M-script.</li> <li> <p>Navigate to Parameters and replace the values with actual values or with parameters defined in Power BI.</p> <pre><code>    // Record containing run parameters with corresponding values, can be empty\n    // Usage: &lt;XU parameter name&gt;= &lt;value or Power BI parameter&gt;\n    // MUST NOT use \"name\" as a record field here\n    Parameters = [ /*rows= \"300\", myparameter= SomePowerBIParameter*/ ],\n</code></pre> </li> </ol> Replace with actual valuesReplace with Power BI parameters <p></p> <p> </p>"},{"location":"documentation/destinations/Power-BI-Connector/#single-sign-on-and-sap-authentication","title":"Single Sign On and SAP Authentication","text":"<p>When setting up the Xtract Universal data source in Power BI for the first time, you are prompted for one of the following authorization methods. Select an authorization method according to your landscape:</p> <ul> <li>Anonymous: Select this option if the Xtract Universal server settings don't require any authentication for running an extraction.</li> <li>Basic: Select this option if the Require SAP Credentials to be explicitly supplied for execution checkbox is marked in the SAP Source Settings in Xtract Universal. Enter your SAP credentials in the respective input fields.</li> <li>Windows: Select this option if you want to use SSO or if you have restricted access to extractions in the Xtract Universal server settings. Enter \\\\\\ in the user field and your Windows password in the Password field. <p>Xtract Universal and the Power BI Connector destination support single sign on (SSO) to SAP.  If SSO is set up correctly, the Windows credentials of the executing Power BI user are mapped to this user's SAP credentials.  This leverages the user's SAP authorizations and Power BI will only show data that matches the user's SAP authorizations.</p> <p></p>"},{"location":"documentation/destinations/Power-BI-Connector/#related-links","title":"Related Links","text":"<ul> <li>Connect Xtract Universal to Power BI Service</li> <li>Microsoft Documentation: What is an on-premises data gateway?</li> <li>Microsoft Documentation: Use custom data connectors with the on-premises data gateway</li> <li>Microsoft Documentation: Configure scheduled refresh</li> <li>Microsoft Documentation: Parameters in Power BI Desktop</li> </ul>"},{"location":"documentation/destinations/alteryx/","title":"Alteryx","text":"<p>This page shows how to set up and use the Alteryx destination.  The Alteryx destination enables users to load SAP data from Alteryx.</p>"},{"location":"documentation/destinations/alteryx/#requirements","title":"Requirements","text":"<p>To use the Alteryx destination, the Xtract Universal Alteryx plugin for the Alteryx Designer must be installed. </p> Installation using the Xtract Universal SetupManual installation (for a separate Alteryx system) <p>The Xtract Universal setup installs the Xtract Universal Alteryx plugin if there is a valid Alteryx installation on the current system. If you install Alteryx after installing Xtract Universal, run the Xtract Universal setup again. The Xtract Universal setup creates the following entries and extensions in the installation directory of that Alteryx installation:</p> <ul> <li><code>Alteryx\\Settings\\AdditionalPlugins\\XtractUniversal.ini</code></li> <li><code>Alteryx\\bin\\RuntimeData\\icons\\categories\\XtractUniversal.png</code></li> <li><code>Alteryx\\bin\\RuntimeData\\DefaultSettings.xml</code></li> </ul> <p>Note</p> <p>Run the Xtract Universal setup on every machine that needs the Xtract Universal Alteryx plugin.  If there is more than one Alteryx installation on your system, the Xtract Universal setup only detects one installation.  In this case, a manual installation is required.</p> <ol> <li>Copy the Alteryx folder <code>C:\\Program Files\\XtractUniversal\\alteryx</code> from your local Xtract Universal installation directory to any directory on the server you want to install the plug-in. </li> <li>Run the <code>C:\\Program Files\\XtractUniversal\\alteryx\\AlteryxPluginSetup.exe</code> from a Windows command shell. </li> </ol> <p>Note</p> <p>The following commands are supported followed by a parameter pointing to the Alteryx installation directory: </p> <ul> <li>/i (for install) e.g., <code>C:\\Program Files\\XtractUniversal\\alteryx&gt;AlteryxPluginSetup /i \"C:\\Users\\mywindowsuser\\AppData\\Local\\Alteryx\"</code></li> <li>/u (for uninstall)</li> </ul> <p>When encountering issues during or after the installation of the plugin (e.g., the plugin is not showing in Alteryx), send the setup.log file located in <code>C:\\Program Files\\XtractUniversal\\alteryx\\setup.log</code> to the Theobald Support.</p>"},{"location":"documentation/destinations/alteryx/#create-a-new-alteryx-destination","title":"Create a new Alteryx Destination","text":"<p>Follow the steps below to add a new Alteryx destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Alteryx from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/alteryx/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination. To use the Alteryx destination, no further settings are necessary.</p> <p></p>"},{"location":"documentation/destinations/alteryx/#assign-the-alteryx-destination-to-an-extraction","title":"Assign the Alteryx Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/alteryx/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/alteryx/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/alteryx/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/alteryx/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/alteryx/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/alteryx/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/alteryx/#use-xtract-universal-in-alteryx","title":"Use Xtract Universal in Alteryx","text":"<p>To use Xtract Universal extractions in the Alteryx Designer, add the the Xtract Universal tool to your Alteryx workflow. The specify the Connection settings and the Parameters settings of the Xtract Universal tool.</p> <p></p>"},{"location":"documentation/destinations/alteryx/#connection","title":"Connection","text":"<p>Connect to the Xtract Universal Server and select the extraction you want to to execute in Alteryx.</p> <p>Note</p> <p>Make sure the Xtract Universal server is running.</p>"},{"location":"documentation/destinations/alteryx/#server","title":"Server","text":"<p>Enter the name, IP or domain and port to access the server within the network, see Connect to an Xtract Universal Server. Format: <code>[Server]:[Port]</code>.</p>"},{"location":"documentation/destinations/alteryx/#extraction","title":"Extraction","text":"<p>Select an extraction from the drop-down menu.  Only extractions with that have the Alteryx destination assigned to them are displayed. </p>"},{"location":"documentation/destinations/alteryx/#send-sap-credentials","title":"Send SAP credentials","text":"<p>Activate the checkbox Send SAP credentials, if the option Require SAP Credentials to be explicitly supplied for execution is active in the SAP source settings in Xtract Universal. The setting Send SAP credentials can be useful in self service scenarios.  When each extraction needs to be executed using an individual user's SAP credentials instead of the globally defined credentials.</p>"},{"location":"documentation/destinations/alteryx/#authenticate-using-current-windows-user","title":"Authenticate using current Windows user","text":"<p>Activate the checkbox Authenticate using current Windows user to use the Windows user that runs Alteryx for authentication.</p>"},{"location":"documentation/destinations/alteryx/#parameters","title":"Parameters","text":"<p>In the tab Parameters tab, the Xtract Universal tool loads available parameters for the specified extraction.  The depicted examples show how to override custom parameters created in Xtract Universal.</p> Override Custom Parameters with Static ValuesOverride Custom Parameters with Dynamic Values <p>In the following example, an extraction of SAP customers contains the custom parameter city.  The parameter is available in the tab Custom Defined Parameters and can be overwritten with a static value. To override the parameter city, activate the checkbox Override and enter a new value, e.g., \"Stuttgart\".</p> <p></p> <p>The Xtract Universal tool can receive inputs, e.g., via the Input Data tool. The data input can be used to dynamically override parameters in Xtract Universal.  In the following example, an extraction of SAP customers contains the custom parameter city  The parameter is available in the tab Custom Defined Parameters and can be overwritten with a dynamic input value.</p> <p>To override the parameter city, activate the checkbox Override and the checkbox Map.  Select an item out of the drop-down list in the field value.</p> <p></p> <p>Tip</p> <p>If the connection to the specified Xtract Universal server is not established and no errors are shown, the Xtract Unversal tool offers a tooltip in the following format:  <code>[Extraction] @ [Server]</code>.</p> <p></p>"},{"location":"documentation/destinations/amazon-aws-s3/","title":"Amazon S3","text":"<p>This page shows how to set up and use the Amazon S3 destination.  The Amazon S3 destination loads data to the Amazon S3 cloud storage. For more information on Amazon S3, see AWS Documentation: Getting Started with Amazon S3.</p> <p>Tip</p> <p>You can install Xtract Universal in an Amazon Elastic Compute (EC2) instance.  Check Amazon EC2: Getting Started with Amazon EC2 to deploy an instance where you can install Xtract Universal in your AWS Account.  Make sure the instance is deployed in the same region as your SAP solution to reduce latency and optimize performance.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#requirements","title":"Requirements","text":"<ul> <li>An Amazon Web Services (AWS) Account.</li> <li>Access Keys (consisting of \"access key ID\" and \"secret access key\") of your AWS user or an IAM role attached to the EC2 instance that runs Xtract Universal, see AWS Documentation: Using an IAM role to grant permissions to applications running on Amazon EC2 instances. For more information on the IAM role, see AWS Documentation: Security best practices in IAM.</li> <li>An S3 bucket, in which you can upload data.</li> <li>Sufficient permissions for list, read and write activities on S3. You must grant these rights in the user policy, but you can limit them to certain buckets.  In the following example, the set permissions have been tested in a test environment:  </li> </ul> <p>Note</p> <p>Xtract Universal uses so called Multipart upload for uploading data to S3.  Data extracted from SAP is uploaded to S3 not as one big chunk of data but in smaller parts.  These parts are buffered on the S3 side. If the extraction is successful, those parts are assembled by S3 into one file.  While the extraction is still running this file is not visible on S3.</p> <p>Recommendation</p> <p>It's recommended you enable S3 versioning or perform data backups regularly, see Amazon AWS: Getting Started - Backup &amp; Restore with AWS.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#extraction-failed","title":"Extraction Failed","text":"<p>In case the extraction fails, the already uploaded parts are deleted from S3.  In case of an \"uncontrolled\" extraction failure, for example due to network issues, Xtract Universal will not be able to delete those uploaded parts from S3.</p> <p>Therefore it is recommended to change the settings on S3 in a way that will trigger the automatic deletion of unused multiparts, e.g. after a day.  You can find this setting by selecting a bucket and opening the \"Management\" tab.  Select \"Lifecycle\" and \"Add lifecycle rule\" and create a rule for deleting unused multiparts.</p> <p></p>"},{"location":"documentation/destinations/amazon-aws-s3/#create-a-new-amazon-s3-destination","title":"Create a new Amazon S3 Destination","text":"<p>Follow the steps below to add a new Amazon S3 destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Amazon S3 from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#s3-settings","title":"S3 Settings","text":""},{"location":"documentation/destinations/amazon-aws-s3/#connection","title":"Connection","text":""},{"location":"documentation/destinations/amazon-aws-s3/#inherit-credentials-from-iam-role","title":"Inherit Credentials from IAM role","text":"<p>The credentials and permissions of the IAM role assigned to the EC2 instance, on which Xtract Universal is running will be used for authentication.  For more information on the IAM role, see AWS Documentation: Security best practices in IAM.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#access-key-id-and-secret-key","title":"Access key ID and Secret key","text":"<p>Preferable authentication method towards Amazon AWS. Determine the values via AWS Identity and Access Management (IAM). More information is available in the official AWS documentation.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#connect","title":"Connect","text":"<p>After entering Access key ID and Secret key, click [Connect]. After successfully connecting, select bucket name and region.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#bucket","title":"Bucket","text":""},{"location":"documentation/destinations/amazon-aws-s3/#bucket-name-and-region","title":"Bucket name and Region","text":"<p>Select a bucket and a region of the bucket's location. The SAP data is extracted into the selected bucket.</p> <p>Note</p> <p>The drop-down menus list all available buckets and regions, make sure to select the correct combination of bucket &amp; region. Validate the connectivity to the selected bucket by clicking [Test Connection).</p>"},{"location":"documentation/destinations/amazon-aws-s3/#test-connection","title":"Test Connection","text":"<p>Validates the right combination of bucket and region. Insures bucket's accessibility from Xtract Universal using the entered access keys.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#server-side-encryption","title":"Server-side encryption","text":"<p>Choose how to encrypt data after uploading them to S3.</p> <p>Note</p> <p>The setting \"Server-side encryption\" does not relate to transport encryption between Xtract Universal and S3.  By default, the channel for sending data to S3 is always encrypted. </p>"},{"location":"documentation/destinations/amazon-aws-s3/#none","title":"None","text":"<p>Server-sided encryption of data not active.  For more information, see AWS Documentation: Protecting data with server-side encryption.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#sse-s3","title":"SSE-S3","text":"<p>Encrypts data using the by default available S3 user account encryption key, see S3 Managed Encryption Keys.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#sse-kms-und-key-id","title":"SSE-KMS und Key ID*","text":"<p>Encryption using a custom encryption key created on AWS, see AWS Key Management Services.  The key can be created on the AWS website</p>"},{"location":"documentation/destinations/amazon-aws-s3/#misc","title":"Misc","text":"<p>All settings in Misc are optional.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#folder-path","title":"Folder Path","text":"<p>Enter the directory to upload files into, see Destination Settings &gt; Folder name.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#file-owner","title":"File Owner","text":"<p>If you upload files as an AWS user of an Account A to an Account B, you can select the option \"Bucket Owner\". Without a declared owner, uploaded files cannot be opened directly.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#file-format","title":"File Format","text":"<p>Select the required file format. You can choose between CSV, JSON and Parquet.</p> <p></p>"},{"location":"documentation/destinations/amazon-aws-s3/#cvs-settings","title":"CVS Settings","text":"<p>The settings for file type CSV correspond to the settings of the Flat File CSV destination:</p> <ul> <li>CSV Settings</li> <li>Convert / Encoding</li> </ul>"},{"location":"documentation/destinations/amazon-aws-s3/#parquet-settings","title":"Parquet Settings","text":"<p>The settings for file type Parquet correspond to the settings of the Flat File Parquet destination:</p> <ul> <li>Compatibility Mode</li> </ul>"},{"location":"documentation/destinations/amazon-aws-s3/#connection-retry","title":"Connection Retry","text":"<p>Connection retry is a built-in function of the Amazon S3 destination.  The retry function is activated by default.</p> <p>Connection retry is a functionality that prevents extractions from failing in case of transient connection interruptions to Amazon S3.  Xtract Universal follows an exponential retry strategy. The selected exponential strategy results in seven retry attempts and an overall timespan of 140 seconds.  If a connection is not established during the timespan of 140 seconds, the extraction fails.</p> <p>For more general information about retry strategies in an AWS S3 environment go to the official AWS Help.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#assign-the-amazon-s3-destination-to-an-extraction","title":"Assign the Amazon S3 Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/amazon-aws-s3/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/amazon-aws-s3/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p> <p>Note</p> <p>If the name of an object does not begin with a letter, it will be prefixed with an \u2018x\u2019, e.g. an object by the name <code>_namespace_tabname.csv</code> will be renamed <code>x_namespace_tabname.csv</code> when uploaded to the destination. This is to ensure that all uploaded objects are compatible with Azure Data Factory, Hadoop and Spark, which require object names to begin with a letter or give special meaning to objects whose names start with certain non-alphabetic characters. </p>"},{"location":"documentation/destinations/amazon-aws-s3/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/amazon-aws-s3/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/amazon-aws-s3/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#folder-name","title":"Folder name","text":"<p>To write extraction data to a location within a specific folder in an Amazon S3 bucket, enter a folder name without slashes. Subfolders are supported and can be defined using the following syntax:  <code>[folder]/[subfolder_1]/[subfolder_2]/\u2026</code></p>"},{"location":"documentation/destinations/amazon-aws-s3/#use-script-expressions-as-dynamic-folder-paths","title":"Use Script Expressions as Dynamic Folder Paths","text":"<p>Script expressions can be used to generate a dynamic folder path. This allows generating folder paths that are composed of an extraction's properties, e.g., extraction name, SAP source object. The described scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example: /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFields]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction. <pre>#{Extraction.Fields[\"[0D_NW_CODE]\"].Selections[0].Value}#</pre> Only for BWCube extractions (MDX mode): returns the input value of a defined selection. <pre>#{Extraction.Fields[\"[0D_NW_CHANN]\"].RangeSelections[0].LowerValue}#</pre> Only for BWCube extractions (MDX mode): returns the lower input value of a defined selection range. <pre>#{Extraction.Fields[\"[0D_NW_CHANN]\"].RangeSelections[0].UpperValue}#</pre> Only for BWCube extractions (MDX mode): returns the upper input value of a defined selection range. <pre>#{Extraction.Fields[\"0D_NW_CODE\"].Selections[0].Value}#</pre> Only for BWCube extractions (BICS mode): returns the input value of a defined selection. <pre>#{Extraction.Fields[\"0D_NW_CHANN\"].RangeSelections[0].LowerValue}#</pre> Only for BWCube extractions (BICS mode): returns the lower input value of a defined selection range. <pre>#{Extraction.Fields[\"0D_NW_CHANN\"].RangeSelections[0].UpperValue}#</pre> Only for BWCube extractions (BICS mode): returns the upper input value of a defined selection range."},{"location":"documentation/destinations/amazon-aws-s3/#compression","title":"Compression","text":"<p>Compression is only available for the csv file format, see Destination Details: File Format.</p> Option Description None The data is transferred uncompressed and stored as a csv file. GZip The data is transferred compressed and stored as a gz file."},{"location":"documentation/destinations/amazon-aws-s3/#file-splitting","title":"File Splitting","text":"<p>Writes extraction data of a single extraction to multiple files.  Each filename is appended by _part[nnn]. </p>"},{"location":"documentation/destinations/amazon-aws-s3/#max-file-size","title":"Max. file size","text":"<p>The value set in Max. file size determines the maximum size of each file. </p> <p>Note</p> <p>The option Max. file size does not apply to gzip files.  The size of a gzipped file cannot be determined in advance.</p>"},{"location":"documentation/destinations/amazon-aws-s3/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base Article: Run Xtract Universal in a VM on AWS EC2</li> <li>Amazon AWS: Getting Started - Backup &amp; Restore with AWS</li> <li>Amazon S3: Getting Started with Amazon S3</li> <li>Amazon EC2: Getting Started with Amazon EC2</li> <li>Amazon Documentation: Amazon EC2 security groups for Linux instances</li> <li>Amazon Documentation: Security best practices in IAM</li> <li>Amazon Documentation: Overview of managing access</li> </ul>"},{"location":"documentation/destinations/amazon-redshift/","title":"Amazon Redshift","text":"<p>This page shows how to set up and use the Amazon Redshift destination.  The Amazon Redshift destination loads data to the Amazon Redshift database. For more information on Amazon Redshift, see Getting Started with Amazon Redshift.</p>"},{"location":"documentation/destinations/amazon-redshift/#requirements","title":"Requirements","text":"<ul> <li>Run an instance with valid credentials.</li> <li>Your client computer has to be authorized to access the cluster.</li> <li>For establishing a connection to Amazon Redshift, a suitable database driver is required.</li> <li>Download and install the x64 Redshift ODBC driver version 2.x from the official website. If the driver is missing, the connection test fails. </li> </ul> <p>Note</p> <p>Prior to Xtract Universal version 2.102.0 you have to install the Mono.Security.dll assembly instead of above mentioned ODBC driver. You can download the complete Mono package from the official project site.  Make sure to install the Mono.Security assembly, compiled on .NET 2.0. by copying the Mono.Security.dll file into your Xtract Universal installation directory.</p>"},{"location":"documentation/destinations/amazon-redshift/#create-a-new-amazon-redshift-destination","title":"Create a new Amazon Redshift Destination","text":"<p>Follow the steps below to add a new Amazon Redshift destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Amazon Redshift from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/amazon-redshift/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/amazon-redshift/#endpoint","title":"Endpoint","text":"<p>Enter the server address of the Amazon Redshift System. (Found in the Redshift Management Console)</p>"},{"location":"documentation/destinations/amazon-redshift/#port","title":"Port","text":"<p>Enter the port number for the connection.</p>"},{"location":"documentation/destinations/amazon-redshift/#username-password","title":"Username / Password","text":"<p>Enter the username and password of the database user.</p>"},{"location":"documentation/destinations/amazon-redshift/#database","title":"Database","text":"<p>Enter the name of the database you want to write to.</p>"},{"location":"documentation/destinations/amazon-redshift/#test-connection","title":"Test Connection","text":"<p>Checks the database connection.</p>"},{"location":"documentation/destinations/amazon-redshift/#assign-the-amazon-redshift-destination-to-an-extraction","title":"Assign the Amazon Redshift Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/amazon-redshift/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/amazon-redshift/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/amazon-redshift/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/amazon-redshift/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/amazon-redshift/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/amazon-redshift/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/amazon-redshift/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/amazon-redshift/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/amazon-redshift/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/amazon-redshift/#preparation","title":"Preparation","text":"<p>Defines the action on the target database before the data is inserted into the target table.</p> Option Description Drop &amp; Create Remove table if available and create new table (default). Truncate Or Create Empty table if available, otherwise create. Create If Not Exists Create table if not available. Prepare Merge Prepares the merge process and creates e.g. a temporary staging table, see Merge Data. None No action. Custom SQL Here you can define your own script, see Custom SQL Statements. <p>To only create the table in the first step and not insert any data, you have two options:</p> <ol> <li>Copy the SQL statement and execute it directly on the target data database.</li> <li>Select the None option for Row Processing and execute the extraction.</li> </ol> <p>Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.</p>"},{"location":"documentation/destinations/amazon-redshift/#row-processing","title":"Row Processing","text":"<p>Defines how the data is inserted into the target table.</p> Option Description Insert Insert records (default). Fill merge staging table Insert records into the staging table. None No action. Custom SQL Define your own script, see Custom SQL Statements. Merge (deprecated) This option is obsolete, see Merge Data. Use the Fill merge staging table option."},{"location":"documentation/destinations/amazon-redshift/#finalization","title":"Finalization","text":"<p>Defines the action on the target database after the data has been successfully inserted into the target table.</p> Option Description Finalize Merge Closes the merge process and deletes the temporary staging table, for example. None No action (default). Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/amazon-redshift/#transaction-style","title":"Transaction style","text":"<p>Note</p> <p>The available options for Transaction Style vary depending on the destination.</p>"},{"location":"documentation/destinations/amazon-redshift/#one-transaction","title":"One Transaction","text":"<p>Preparation, Row Processing and Finalization are all performed in a single transaction.</p> <ul> <li>Advantage: clean rollback of all changes.</li> <li>Disadvantage: possibly extensive locking during the entire extraction period. </li> </ul> <p>Recommendation</p> <p>Only use One Transaction in combination with DML commands, e.g., \"truncate table\" and \"insert.  Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command.  Example: If a table is created in the preparation step, the opened \"OneTransaction\" is committed and a rollback in the next steps is not performed correctly.</p>"},{"location":"documentation/destinations/amazon-redshift/#three-transactions","title":"Three Transactions","text":"<p>Preparation, Row Processing and Finalization are each executed in a separate transaction.</p> <ul> <li>Advantage: clean rollback of the individual sections, possibly shorter locking phases than with One Transaction (e.g. with DDL in Preparation, the entire DB is only locked during preparation and not for the entire extraction duration). </li> <li>Disadvantage: no rollback of previous step possible (error in Row Processing only rolls back changes from Row Processing, but not Preparation). </li> </ul>"},{"location":"documentation/destinations/amazon-redshift/#rowprocessingonly","title":"RowProcessingOnly","text":"<p>Only Row Processing is executed in a transaction. Preparation and Finalization without an explicit transaction (implicit commits).</p> <ul> <li>Advantage: DDL in 'Preparation and Finalization* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)</li> <li>Disadvantage: no rollback of Preparation/Finalization.</li> </ul>"},{"location":"documentation/destinations/amazon-redshift/#no-transaction","title":"No Transaction","text":"<p>No explicit transactions.</p> <ul> <li>Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.</li> <li>Disadvantage: no rollback </li> </ul>"},{"location":"documentation/destinations/amazon-redshift/#merge-data","title":"Merge Data","text":"<p>The following example depicts the update of the existing data records in a database by running an extraction to merge data.  In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP. </p> <p>With a merge command, the updated value is written to the destination database table.  The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.</p> <p></p> <p>Tip</p> <p>Alternatively to merging, the data can be also updated by means of full load.  The full load method is less efficient.</p>"},{"location":"documentation/destinations/amazon-redshift/#prerequisites","title":"Prerequisites","text":"<p>You need a table with existing SAP data, in which new data can be merged. Ideally, the table with existing data is created in the initial load with the corresponding Preparation option and filled with data with the Row Processing option Insert.</p> <p>After the table is created, open SAP and change a field value in the SAP table that is used for the data merge. </p> <p>Warning</p> <p>Faulty merge.  Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the General Settings of the extraction type to execute the merge command.</p>"},{"location":"documentation/destinations/amazon-redshift/#merge-command","title":"Merge Command","text":"<p>The merge process is performed using a staging table and takes place in three steps:</p> <ol> <li>A temporary table is created.</li> <li>The data is inserted in the temporary table.</li> <li>The temporary table is merged with the target table and then the temporary table is deleted.</li> </ol> <p>Follow the steps below to set up the merge process in Xtract Universal:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions. </li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</li> <li>Make sure to assign Amazon Redshift destination to the extraction. </li> <li>Apply the following destination settings: </li> <li>Click [OK] and run the extraction.</li> </ol> <p>More information about the updated fields can be found in the SQL statement. It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update. Fields that do not appear in the SQL statement are not affected by changes. </p>"},{"location":"documentation/destinations/amazon-redshift/#custom-sql-statements","title":"Custom SQL Statements","text":"<p>The Amazon Redshift destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the Amazon Redshift destination:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination], the window \"Destination Settings\" opens.</li> <li>Make sure to assign the Amazon Redshift destination to the extraction.</li> <li> <p>Select the option Custom SQL from the drop-down list   in one of the following sections:</p> <ul> <li>Preparation</li> <li>Row Processing</li> <li>Finalization</li> </ul> <p></p> </li> <li> <p>Click [Edit SQL] . The window \"Edit SQL\" opens.</p> </li> <li>Enter your custom SQL statement and click [OK] to confirm your input.</li> </ol>"},{"location":"documentation/destinations/amazon-redshift/#use-templates","title":"Use Templates","text":"<p>Existing SQL commands can be used as templates. You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:</p> <ul> <li>Preparation, e.g., in Drop &amp; Create or Create if Not Exists</li> <li>Row Processing, e.g., in Insert or Merge</li> <li>Finalization</li> </ul> <p>Follow the steps below to generate a Custom SQL command from a template:</p> <ol> <li>In one of the staging steps, select the Custom SQL option from the drop-down list  .</li> <li>Click [Edit SQL] . The dialogue \"Edit SQL\" opens. </li> <li>Navigate to the drop-down menu and select an existing command  . </li> <li>Click [Generate Statement]. A new statement is generated. </li> <li>Click [Copy] to copy the statement to the clipboard.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>Check out the Microsoft SQL Server example for details on predefined expressions.</p> <p>Note</p> <p>The custom SQL code is used for SQL Server destinations.  A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.</p>"},{"location":"documentation/destinations/amazon-redshift/#use-script-expressions","title":"Use Script Expressions","text":"<p>You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported: </p> Input Description <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.TableName }#</pre> Name of the database table extracted data is written to. <pre>#{Extraction.RowsCount }#</pre> Count of the extracted rows. <pre>#{Extraction.RunState}#</pre> Status of the extraction (Running, FinishedNoErrors, FinishedErrors). <pre>#{(int)Extraction.RunState}#</pre> Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors). <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <p>For more information, see Script Expressions.</p> Example: Verify the existence of a table in a database using 'ExistsTable'<pre><code>#{\n   iif\n   (\n      ExistsTable(\"MAKT\"),\n      \"TRUNCATE TABLE \\\"MAKT\\\";\",\n      \"\n         CREATE TABLE \\\"MAKT\\\"(\n            \\\"MATNR\\\" VARCHAR(18),\n            \\\"SPRAS\\\" VARCHAR(2),\n            \\\"MAKTX\\\" VARCHAR(40));\n      \"\n   )\n}#\n</code></pre>"},{"location":"documentation/destinations/amazon-redshift/#create-a-status-overview","title":"Create a Status Overview","text":"<p>The table \"ExtractionStatistics\" provides an overview and status of the executed Xtract Universal extractions. To create the \"ExtractionStatistics\" table, create an SQL table according to the following example:</p> Create ExtractionStatistics<pre><code>CREATE TABLE [dbo].[ExtractionStatistics](\n    [TableName] [nchar](50) NULL,\n    [RowsCount] [int] NULL,\n    [Timestamp] [nchar](50) NULL,\n    [RunState] [nchar](50) NULL\n) ON [PRIMARY]\nGO\n</code></pre> <p>The ExtractionStatistics table is filled in the Finalization process step, using the following SQL statement:</p> Fill ExtractionStatistics<pre><code>INSERT INTO [ExtractionStatistics]\n(\n     [TableName], \n     [RowsCount], \n     [Timestamp],\n     [RunState]\n)\nVALUES\n(\n     '#{Extraction.TableName}#', \n     '#{Extraction.RowsCount}#', \n     '#{Extraction.Timestamp}#',\n     '#{Extraction.RunState}#'\n);\n</code></pre>"},{"location":"documentation/destinations/azure-storage/","title":"Azure Storage","text":"<p>This page shows how to set up and use the Azure Storage destination.  The Azure Storage destination loads data to a cloud based Azure Storage. </p>"},{"location":"documentation/destinations/azure-storage/#requirements","title":"Requirements","text":"<p>The Azure Storage (Blob / Data Lake) destination supports the following Azure storage account types:</p> <ul> <li>General-purpose V2 (including Azure Data Lake Storage Gen2)</li> <li>General-purpose V1</li> <li>BlockBlobStorage</li> <li>BlobStorage</li> </ul> <p>To use the Azure Storage (Blob / Data Lake) destination you need one of the above Azure storage accounts.  For more information, see Microsoft Azure storage documentation.</p>"},{"location":"documentation/destinations/azure-storage/#create-a-new-azure-storage-destination","title":"Create a new Azure Storage Destination","text":"<p>Follow the steps below to add a new Azure Storage destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Azure Storage from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/azure-storage/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p>"},{"location":"documentation/destinations/azure-storage/#azure-storage","title":"Azure Storage","text":""},{"location":"documentation/destinations/azure-storage/#connection-type","title":"Connection Type","text":"<p>The subsection Connection Type offers two different methods for authenticating and authorizing access to an Azure Storage account:</p> <ul> <li> <p>Access Key</p> <p>This method of authentication authorizes access to the complete storage account.  For more information, see Microsoft documentation. </p> </li> <li> <p>Azure Active Directory</p> <p>Authentication via Azure Active Directory uses OAuth 2.0 and Azure AD for authentication.  With this option access rights can be granted on storage account or container level.  For more information, see Knowledge Base Article: Authentication via Azure Active Directory for Azure Storage.</p> </li> </ul>"},{"location":"documentation/destinations/azure-storage/#access-key-parameters","title":"Access Key Parameters","text":"<p>The input fields in the subsection Access key parameters vary depending on the selected authentication method.</p> Authentication via Access KeyAuthentication via Azure Active Directory Input Field Description Storage account Enter your storage account name. Do not enter the full URL. Access key Enter the access key of the Azure Storage account. <p>You can copy the storage account name and access key from the Azure Portal. </p> <p>Note</p> <p>Make sure to correctly set up authentication via Azure Active Directory in Azure, see Knowledge Base Article: Authentication via Azure Active Directory for Azure Storage.</p> Input Field Description Storage account Enter your storage account name. Tenant ID Enter the ID of the Azure AD tenant. Client ID Enter the ID of the registered app. <p>You copy the tenant ID and client ID from the Azure portal. </p>"},{"location":"documentation/destinations/azure-storage/#connect","title":"Connect","text":"<p>Click [Connect] to establish a connection to the storage account.  If the connection is successful, a \"Connection successful\" info window opens.</p>"},{"location":"documentation/destinations/azure-storage/#connect_1","title":"Connect","text":"<p>Click [Connect] to establish a connection to the storage account.  A browser window pops up, where you have to sign in using your Azure AD credentials. The \"Permissions requested\" window lists the requested permissions, see Knowledge Base Article: Authentication via Azure Active Directory.  Click [Accept]. If the connection is successful, a \"Connection successful\" info window opens. </p> <p></p>"},{"location":"documentation/destinations/azure-storage/#container","title":"Container","text":"<p>This subsection is activated after a connection to the storage account was successfully established.</p> <ul> <li>When using Access Key authentication, choose a Blob container from the drop-down menu.</li> <li>When using Azure Active Directory authentication, enter the name of the Blob container manually.</li> </ul>"},{"location":"documentation/destinations/azure-storage/#test-connection","title":"Test connection","text":"<p>Click [Test Connection] to check if the storage container can be accessed.  If the connection is successful, a \"Connection to container &lt;name of container&gt; successful\" info window opens. </p> <p>The Azure Storage (Blob / Data Lake) destination can now be used.</p>"},{"location":"documentation/destinations/azure-storage/#misc","title":"Misc","text":"<p>Note</p> <p>The settings in Misc can only be used in combination with a Blob container. </p>"},{"location":"documentation/destinations/azure-storage/#folder-path","title":"Folder path","text":"<p>Option to create a folder structure within the container for saving files. Script expressions are supported, see Destination Settings &gt; Folder.</p> <p>For creating a single folder, enter a folder name without slashes: <code>[folder]</code>  Subfolders are supported and can be defined using the following syntax: <code>[folder]/[subfolder_1]/[subfolder_2]/[..]</code></p>"},{"location":"documentation/destinations/azure-storage/#file-format","title":"File Format","text":"<p>Select the required file format. You can choose between Parquet and CSV.</p> <p></p>"},{"location":"documentation/destinations/azure-storage/#cvs-settings","title":"CVS Settings","text":"<p>The settings for file type CSV correspond to the settings of the Flat File CSV destination:</p> <ul> <li>CSV Settings</li> <li>Convert / Encoding</li> </ul>"},{"location":"documentation/destinations/azure-storage/#parquet-settings","title":"Parquet Settings","text":"<p>The settings for file type Parquet correspond to the settings of the Flat File Parquet destination:</p> <ul> <li>Compatibility Mode</li> </ul>"},{"location":"documentation/destinations/azure-storage/#connection-retry-and-rollback","title":"Connection Retry and Rollback","text":"<p>Connection retry is a built-in function of the Azure Storage destination.  The retry function is activated by default.</p> <p>Connection retry is a functionality that prevents extractions from failing in case of transient connection interruptions to Azure Storage.  Xtract Universal follows an exponential retry strategy. The selected exponential strategy results in seven retry attempts and an overall timespan of 140 seconds.  If a connection is not established during the timespan of 140 seconds, the extraction fails.</p> <p>The retry function is implemented according to Microsoft Guidelines. The retry logic is based on WebExceptionStatus. </p> <p>Rollback covers scenarios where extractions do not fail due to connection failures to Azure but due to an error when connecting to SAP. In those cases Xtract Universal tries to remove any files from Azure storage that were created in the course of the extraction.</p>"},{"location":"documentation/destinations/azure-storage/#assign-the-azure-storage-destination-to-an-extraction","title":"Assign the Azure Storage Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/azure-storage/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/azure-storage/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/azure-storage/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p> <p>Note</p> <p>If the name of an object does not begin with a letter, it will be prefixed with an \u2018x\u2019, e.g. an object by the name <code>_namespace_tabname.csv</code> will be renamed <code>x_namespace_tabname.csv</code> when uploaded to the destination. This is to ensure that all uploaded objects are compatible with Azure Data Factory, Hadoop and Spark, which require object names to begin with a letter or give special meaning to objects whose names start with certain non-alphabetic characters. </p>"},{"location":"documentation/destinations/azure-storage/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/azure-storage/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/azure-storage/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/azure-storage/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/azure-storage/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/azure-storage/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/azure-storage/#blob-type","title":"Blob Type","text":""},{"location":"documentation/destinations/azure-storage/#append-blob","title":"Append Blob","text":"<p>Creates an Append Blob.</p>"},{"location":"documentation/destinations/azure-storage/#block-blob","title":"Block Blob","text":"<p>Creates a Block Blob.</p> <p>Note</p> <p>For both file types an MD5 hash is created upon upload to Azure storage.</p>"},{"location":"documentation/destinations/azure-storage/#folder","title":"Folder","text":"<p>Option to create a folder structure within the container for saving files, see Destination Details &gt; Folder Path. </p> <p>For creating a single folder, enter a folder name without slashes: <code>[folder]</code>  Subfolders are supported and can be defined using the following syntax: <code>[folder]/[subfolder_1]/[subfolder_2]/[..]</code></p>"},{"location":"documentation/destinations/azure-storage/#use-script-expressions-as-dynamic-folder-paths","title":"Use Script Expressions as Dynamic Folder Paths","text":"<p>Script expressions can be used to generate a dynamic folder path. This allows generating folder paths that are composed of an extraction's properties, e.g., extraction name, SAP source object. The described scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example: /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFields]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction. <pre>#{Extraction.Fields[\"[0D_NW_CODE]\"].Selections[0].Value}#</pre> Only for BWCube extractions (MDX mode): returns the input value of a defined selection. <pre>#{Extraction.Fields[\"[0D_NW_CHANN]\"].RangeSelections[0].LowerValue}#</pre> Only for BWCube extractions (MDX mode): returns the lower input value of a defined selection range. <pre>#{Extraction.Fields[\"[0D_NW_CHANN]\"].RangeSelections[0].UpperValue}#</pre> Only for BWCube extractions (MDX mode): returns the upper input value of a defined selection range. <pre>#{Extraction.Fields[\"0D_NW_CODE\"].Selections[0].Value}#</pre> Only for BWCube extractions (BICS mode): returns the input value of a defined selection. <pre>#{Extraction.Fields[\"0D_NW_CHANN\"].RangeSelections[0].LowerValue}#</pre> Only for BWCube extractions (BICS mode): returns the lower input value of a defined selection range. <pre>#{Extraction.Fields[\"0D_NW_CHANN\"].RangeSelections[0].UpperValue}#</pre> Only for BWCube extractions (BICS mode): returns the upper input value of a defined selection range."},{"location":"documentation/destinations/azure-storage/#compression","title":"Compression","text":"<p>Compression is only available for the csv file format, see Destination Details: File Format.</p>"},{"location":"documentation/destinations/azure-storage/#gzip","title":"GZip","text":"<p>The data is transferred compressed and stored as a gz file. </p>"},{"location":"documentation/destinations/azure-storage/#file-splitting","title":"File Splitting","text":"<p>Writes extraction data of a single extraction to multiple files.  Each filename is appended by _part[nnn]. </p>"},{"location":"documentation/destinations/azure-storage/#max-file-size","title":"Max. file size","text":"<p>The value set in Max. file size determines the maximum size of each file. </p> <p>Note</p> <p>The option Max. file size does not apply to gzip files.  The size of a gzipped file cannot be determined in advance.</p>"},{"location":"documentation/destinations/azure-storage/#common-data-model","title":"Common Data Model","text":"<p>If this option is enabled, a Common Data Model JSON file is generated and written to the destination alongside the extracted data. The CDM file can be used to automate data transformation in Azure.</p> <p>For more information on Common Data Models, see Microsoft Documentation: Common Data Model.</p> <p>Note</p> <p>This option is still in preview mode.</p>"},{"location":"documentation/destinations/azure-storage/#entity-name","title":"Entity name","text":"<p>Enter a name for the generated .cdm.json file.</p>"},{"location":"documentation/destinations/azure-storage/#manifest","title":"Manifest","text":"<p>Enter the name of your manifest file.</p>"},{"location":"documentation/destinations/azure-storage/#tutorial","title":"Tutorial","text":"<p>The following YouTube tutorial shows how to set up Xtract Universal with the Azure Storage destination:</p>"},{"location":"documentation/destinations/azure-storage/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base Article: Authentication via Azure Active Directory for Azure Storage</li> <li>Integration via Azure Data Factory</li> </ul>"},{"location":"documentation/destinations/azure-synapse-analytics/","title":"Azure Synapse Analytics (SQL pool)","text":"<p>This page shows how to set up and use the Azure Synapse Analytics (SQL pool) destination.  The Azure Synapse Analytics (SQL pool) destination loads data to an Azure Synapse SQL Pool.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#requirements","title":"Requirements","text":"<p>To use the Azure Synapse Analytics SQL Pool destination, you need:</p> <ul> <li>An Azure Analytics SQL database.</li> <li>Azure portal firewall rules that grant access for the IP addresses Xtract Universal is running on.</li> </ul>"},{"location":"documentation/destinations/azure-synapse-analytics/#create-a-new-azure-synapse-analytics-sql-pool-destination","title":"Create a new Azure Synapse Analytics (SQL pool) Destination","text":"<p>Follow the steps below to add a new Azure Synapse Analytics (SQL pool) destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Azure Synapse Analytics (SQL pool) from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/azure-synapse-analytics/#server-name","title":"Server Name","text":"<p>Enter the name of the Azure Servers in the following format: <code>[servername].database.windows.net</code></p>"},{"location":"documentation/destinations/azure-synapse-analytics/#user-name","title":"User Name","text":"<p>Enter the user name.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#password","title":"Password","text":"<p>Enter the password.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#database-name","title":"Database Name","text":"<p>Enter the name of the Azure Synapse SQl Pool.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#test-connection","title":"Test Connection","text":"<p>Check the database connection.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#assign-the-azure-synapse-analytics-sql-pool-destination-to-an-extraction","title":"Assign the Azure Synapse Analytics (SQL pool) Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/azure-synapse-analytics/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/azure-synapse-analytics/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/azure-synapse-analytics/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/azure-synapse-analytics/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#preparation","title":"Preparation","text":"<p>Defines the action on the target database before the data is inserted into the target table.</p> Option Description Drop &amp; Create Remove table if available and create new table (default). Truncate Or Create Empty table if available, otherwise create. Create If Not Exists Create table if not available. Prepare Merge Prepares the merge process and creates e.g. a temporary staging table, see Merge Data. None No action. Custom SQL Here you can define your own script, see Custom SQL Statements. <p>To only create the table in the first step and not insert any data, you have two options:</p> <ol> <li>Copy the SQL statement and execute it directly on the target data database.</li> <li>Select the None option for Row Processing and execute the extraction.</li> </ol> <p>Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#row-processing","title":"Row Processing","text":"<p>Defines how the data is inserted into the target table.</p> Option Description Insert Insert records (default). Fill merge staging table Insert records into the staging table. None No action. Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/azure-synapse-analytics/#finalization","title":"Finalization","text":"<p>Defines the action on the target database after the data has been successfully inserted into the target table.</p> Option Description Finalize Merge Closes the merge process and deletes the temporary staging table, for example. None No action (default). Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/azure-synapse-analytics/#debugging","title":"Debugging","text":"<p>Warning</p> <p>Performance decrease! The performance decreases when bulk insert is disabled. Disable the bulk insert only when necessary, e.g., upon request of the support team.</p> <p>By activating the checkbox Disable bulk operations, the default bulk insert is deactivated when writing to a database.</p> <p>This option enables detailed error analysis, if certain data rows cannot be persisted on the database. Possible causes for the incorrect behavior are incorrect values with regard to the stored data type.</p> <p>Debugging needs to be deactivated again, after the successful error analysis, otherwise the performance of the database write processes remains low. </p> <p>Note</p> <p>Bulk operations are not supported when using Custom SQL statements, e.g., in Row Processing.  Bulk operations lead to performance decrease.  To increase performance when using Custom SQL statements, it is recommended to perform the custom processing in the Finalization step.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#transaction-style","title":"Transaction style","text":"<p>Note</p> <p>The available options for Transaction Style vary depending on the destination.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#rowprocessingonly","title":"RowProcessingOnly","text":"<p>Only Row Processing is executed in a transaction. Preparation and Finalization without an explicit transaction (implicit commits).</p> <ul> <li>Advantage: DDL in 'Preparation and Finalization* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)</li> <li>Disadvantage: no rollback of Preparation/Finalization.</li> </ul>"},{"location":"documentation/destinations/azure-synapse-analytics/#no-transaction","title":"No Transaction","text":"<p>No explicit transactions.</p> <ul> <li>Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.</li> <li>Disadvantage: no rollback </li> </ul>"},{"location":"documentation/destinations/azure-synapse-analytics/#merge-data","title":"Merge Data","text":"<p>The following example depicts the update of the existing data records in a database by running an extraction to merge data.  In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP. </p> <p>With a merge command, the updated value is written to the destination database table.  The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.</p> <p></p> <p>Tip</p> <p>Alternatively to merging, the data can be also updated by means of full load.  The full load method is less efficient.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#prerequisites","title":"Prerequisites","text":"<p>You need a table with existing SAP data, in which new data can be merged. Ideally, the table with existing data is created in the initial load with the corresponding Preparation option and filled with data with the Row Processing option Insert.</p> <p>After the table is created, open SAP and change a field value in the SAP table that is used for the data merge. </p> <p>Warning</p> <p>Faulty merge.  Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the General Settings of the extraction type to execute the merge command.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#merge-command","title":"Merge Command","text":"<p>The merge process is performed using a staging table and takes place in three steps:</p> <ol> <li>A temporary table is created.</li> <li>The data is inserted in the temporary table.</li> <li>The temporary table is merged with the target table and then the temporary table is deleted.</li> </ol> <p>Follow the steps below to set up the merge process in Xtract Universal:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions. </li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</li> <li>Make sure to assign Azure Synapse Analytics (SQL pool) destination to the extraction. </li> <li>Apply the following destination settings: </li> <li>Click [OK] and run the extraction.</li> </ol> <p>More information about the updated fields can be found in the SQL statement. It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update. Fields that do not appear in the SQL statement are not affected by changes. </p>"},{"location":"documentation/destinations/azure-synapse-analytics/#custom-sql-statements","title":"Custom SQL Statements","text":"<p>The Azure Synapse Analytics (SQL pool) destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the Azure Synapse Analytics (SQL pool) destination:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination], the window \"Destination Settings\" opens.</li> <li>Make sure to assign the Azure Synapse Analytics (SQL pool) destination to the extraction.</li> <li> <p>Select the option Custom SQL from the drop-down list   in one of the following sections:</p> <ul> <li>Preparation</li> <li>Row Processing</li> <li>Finalization</li> </ul> <p></p> </li> <li> <p>Click [Edit SQL] . The window \"Edit SQL\" opens.</p> </li> <li>Enter your custom SQL statement and click [OK] to confirm your input.</li> </ol>"},{"location":"documentation/destinations/azure-synapse-analytics/#use-templates","title":"Use Templates","text":"<p>Existing SQL commands can be used as templates. You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:</p> <ul> <li>Preparation, e.g., in Drop &amp; Create or Create if Not Exists</li> <li>Row Processing, e.g., in Insert or Merge</li> <li>Finalization</li> </ul> <p>Follow the steps below to generate a Custom SQL command from a template:</p> <ol> <li>In one of the staging steps, select the Custom SQL option from the drop-down list  .</li> <li>Click [Edit SQL] . The dialogue \"Edit SQL\" opens. </li> <li>Navigate to the drop-down menu and select an existing command  . </li> <li>Click [Generate Statement]. A new statement is generated. </li> <li>Click [Copy] to copy the statement to the clipboard.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>Check out the Microsoft SQL Server example for details on predefined expressions.</p> <p>Note</p> <p>The custom SQL code is used for SQL Server destinations.  A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.</p>"},{"location":"documentation/destinations/azure-synapse-analytics/#use-script-expressions","title":"Use Script Expressions","text":"<p>You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported: </p> Input Description <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.TableName }#</pre> Name of the database table extracted data is written to. <pre>#{Extraction.RowsCount }#</pre> Count of the extracted rows. <pre>#{Extraction.RunState}#</pre> Status of the extraction (Running, FinishedNoErrors, FinishedErrors). <pre>#{(int)Extraction.RunState}#</pre> Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors). <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <p>For more information, see Script Expressions.</p> Example: Verify the existence of a table in a database using 'ExistsTable'<pre><code>#{\n   iif\n   (\n      ExistsTable(\"MAKT\"),\n      \"TRUNCATE TABLE \\\"MAKT\\\";\",\n      \"\n         CREATE TABLE \\\"MAKT\\\"(\n            \\\"MATNR\\\" VARCHAR(18),\n            \\\"SPRAS\\\" VARCHAR(2),\n            \\\"MAKTX\\\" VARCHAR(40));\n      \"\n   )\n}#\n</code></pre>"},{"location":"documentation/destinations/azure-synapse-analytics/#create-a-status-overview","title":"Create a Status Overview","text":"<p>The table \"ExtractionStatistics\" provides an overview and status of the executed Xtract Universal extractions. To create the \"ExtractionStatistics\" table, create an SQL table according to the following example:</p> Create ExtractionStatistics<pre><code>CREATE TABLE [dbo].[ExtractionStatistics](\n    [TableName] [nchar](50) NULL,\n    [RowsCount] [int] NULL,\n    [Timestamp] [nchar](50) NULL,\n    [RunState] [nchar](50) NULL\n) ON [PRIMARY]\nGO\n</code></pre> <p>The ExtractionStatistics table is filled in the Finalization process step, using the following SQL statement:</p> Fill ExtractionStatistics<pre><code>INSERT INTO [ExtractionStatistics]\n(\n     [TableName], \n     [RowsCount], \n     [Timestamp],\n     [RunState]\n)\nVALUES\n(\n     '#{Extraction.TableName}#', \n     '#{Extraction.RowsCount}#', \n     '#{Extraction.Timestamp}#',\n     '#{Extraction.RunState}#'\n);\n</code></pre>"},{"location":"documentation/destinations/azure-synapse-analytics/#related-links","title":"Related Links","text":"<ul> <li>Microsoft Documentation: Microsoft Azure Synapse Analytics</li> <li>Integration via Azure Data Factory</li> </ul>"},{"location":"documentation/destinations/csv-flat-file/","title":"Flat File CSV","text":"<p>This page shows how to set up and use the Flat File CSV destination.  The Flat File CSV destination is a CSV (comma-separated values) flat file that can be written to a local directory or a network drive.  </p>"},{"location":"documentation/destinations/csv-flat-file/#create-a-new-flat-file-csv-destination","title":"Create a new Flat File CSV Destination","text":"<p>Follow the steps below to add a new Flat File CSV destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Flat File CSV from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/csv-flat-file/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/csv-flat-file/#file-output-path","title":"File output path","text":""},{"location":"documentation/destinations/csv-flat-file/#file-output-path_1","title":"File output path","text":"<p>Enter the folder path to save the destination flat files in.  If the entered folder does not exist, a new folder is created.</p> <p>Note</p> <p>To write flat files to a network drive, you need to:</p> <ul> <li>Enter the File output path in UNC format e.g., <code>\\\\Server2\\Share\\Folder1</code>.</li> <li>Run the Xtract Universal service by a user with write permission to the directory. </li> </ul>"},{"location":"documentation/destinations/csv-flat-file/#csv-settings","title":"CSV Settings","text":""},{"location":"documentation/destinations/csv-flat-file/#column-seperator","title":"Column seperator","text":"<p>Defines how two columns in CSV are separated.</p>"},{"location":"documentation/destinations/csv-flat-file/#row-separator","title":"Row separator","text":"<p>Defines how two rows in CSV are separated.</p>"},{"location":"documentation/destinations/csv-flat-file/#quote-symbol","title":"Quote symbol","text":"<p>Defines which character is used to encase field data. A sequence of characters may be used as \"Quote symbol\". Quotation is applied in the following scenarios:</p> <ul> <li>The Column separator is part of the field data.</li> <li>The Quote symbol is part of the field data.</li> <li>The Row separator is part of the field data.</li> <li>The Escape character is part of the field data.</li> </ul>"},{"location":"documentation/destinations/csv-flat-file/#escape-character","title":"Escape character","text":"<p>When Escape character is part of the field data, the respective field containing this character is encased by the \"Quote symbol\". The default escape character is the backslash '\\'. The field may remain empty.</p>"},{"location":"documentation/destinations/csv-flat-file/#column-names-in-first-row","title":"Column names in first row","text":"<p>Defines if the first row contains the column names. This option is set per default.</p>"},{"location":"documentation/destinations/csv-flat-file/#row-separator-after-last-row","title":"Row separator after last row","text":"<p>Defines if the last row contains a row separator. This option is set per default.</p>"},{"location":"documentation/destinations/csv-flat-file/#convert-encoding","title":"Convert / Encoding","text":""},{"location":"documentation/destinations/csv-flat-file/#decimal-separator","title":"Decimal separator","text":"<p>Defines the decimal separator of decimal number for the output. Dot (.) is the default value. </p>"},{"location":"documentation/destinations/csv-flat-file/#date-format","title":"Date format","text":"<p>Defines a customized date format (e.g. YYYY-MM-DD or MM/DD/YYYY) for converting valid SAP dates (YYYYMMDD). Default is YYYY-MM-DD.</p>"},{"location":"documentation/destinations/csv-flat-file/#time-format","title":"Time format","text":"<p>Defines a customized time format (e.g. HH-MM-SS or HH:MM:SS) for converting valid SAP times (HHMMSS). Default is HH:MM:SS.</p>"},{"location":"documentation/destinations/csv-flat-file/#text-encoding","title":"Text Encoding","text":"<p>Defines the text encoding.</p>"},{"location":"documentation/destinations/csv-flat-file/#assign-the-flat-file-csv-destination-to-an-extraction","title":"Assign the Flat File CSV Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/csv-flat-file/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/csv-flat-file/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/csv-flat-file/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/csv-flat-file/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/csv-flat-file/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/csv-flat-file/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/csv-flat-file/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/csv-flat-file/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/csv-flat-file/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/csv-flat-file/#existing-files","title":"Existing files","text":"<p>If a flat file by the same name already exists in the target directory, the following actions can be taken:</p> Option Description Replace file The export process overwrites existing files. Append results The export process appends new data to an already existing file, see Column Mapping. Abort extraction The process is aborted, if the file already exists."},{"location":"documentation/destinations/csv-flat-file/#file-splitting","title":"File Splitting","text":"<p>Writes extraction data of a single extraction to multiple files.  Each filename is appended by _part[nnn]. </p>"},{"location":"documentation/destinations/csv-flat-file/#max-file-size","title":"Max. file size","text":"<p>The value set in Max. file size determines the maximum size of each file. </p> <p>Note</p> <p>The option Max. file size does not apply to gzip files.  The size of a gzipped file cannot be determined in advance.</p>"},{"location":"documentation/destinations/csv-flat-file/#column-mapping","title":"Column Mapping","text":"<p>Activate Column Mapping when appending data to an existing file or entity that has different column names or a different number of columns. This can be the case when extracting data from two or more extractions into the same destination file, where the column names of the extraction and the destination file differ.</p> <p>Note</p> <p>The column names in the extraction and destination must be unique.  If duplicated column names are found, an error message is displayed. The column names must be corrected, before column mapping can be used.</p> <ol> <li>When working with flat files, ensure that:<ol> <li>the XU server and the Designer both have access to the destination file.</li> <li>the output directory and the file name of the extraction match the destination file. </li> <li>the Column Name Style of the extraction and destination file match.</li> </ol> </li> <li>Select the option Append results in the subsection Existing Files.</li> <li> <p>Click [Map] to assign columns. The window \"Column Mapping\" opens.</p> <ul> <li>Destination Columns displays the names of the columns that are available in the destination file or entity.</li> <li>Not Mapped defines whether or not columns are mapped to the destination columns.</li> <li>Source Columns defines which SAP column is mapped to a destination column.</li> </ul> <p></p> </li> <li> <ol> <li>If the column names of the extraction and the names of the destination columns match, click [Auto map by name].</li> <li>If the column names do not match, assign columns manually by selecting the respective SAP column from the dropdown menu under Source Columns.</li> <li>If a column does not have a counterpart or is not supposed to be appended, activate the checkbox under Not Mapped.</li> </ol> </li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction the extracted data is added to the destination file or entity as specified in the column mapping.</p> <p>Tip</p> <p>In case an error message pops up, click [Show more] to see a description of what caused the error.</p>"},{"location":"documentation/destinations/csv-via-http/","title":"HTTP CSV","text":"<p>This page shows how to set up and use the HTTP CSV destination.  The HTTP CSV destination is a generic CSV stream over HTTP.  It is supported by many products, e.g., Layer2, INFONEA and KNIME. </p>"},{"location":"documentation/destinations/csv-via-http/#create-a-new-http-csv-destination","title":"Create a new HTTP CSV Destination","text":"<p>Follow the steps below to add a new HTTP CSV destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type HTTP CSV from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/csv-via-http/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/csv-via-http/#csv-settings","title":"CSV Settings","text":""},{"location":"documentation/destinations/csv-via-http/#column-seperator","title":"Column seperator","text":"<p>Defines how two columns in CSV are separated.</p>"},{"location":"documentation/destinations/csv-via-http/#row-separator","title":"Row separator","text":"<p>Defines how two rows in CSV are separated.</p>"},{"location":"documentation/destinations/csv-via-http/#quote-symbol","title":"Quote symbol","text":"<p>Defines which character is used to encase field data. A sequence of characters may be used as \"Quote symbol\". Quotation is applied in the following scenarios:</p> <ul> <li>The Column separator is part of the field data.</li> <li>The Quote symbol is part of the field data.</li> <li>The Row separator is part of the field data.</li> <li>The Escape character is part of the field data.</li> </ul>"},{"location":"documentation/destinations/csv-via-http/#escape-character","title":"Escape character","text":"<p>When Escape character is part of the field data, the respective field containing this character is encased by the \"Quote symbol\". The default escape character is the backslash '\\'. The field may remain empty.</p>"},{"location":"documentation/destinations/csv-via-http/#column-names-in-first-row","title":"Column names in first row","text":"<p>Defines if the first row contains the column names. This option is set per default.</p>"},{"location":"documentation/destinations/csv-via-http/#row-separator-after-last-row","title":"Row separator after last row","text":"<p>Defines if the last row contains a row separator. This option is set per default.</p>"},{"location":"documentation/destinations/csv-via-http/#convert-encoding","title":"Convert / Encoding","text":""},{"location":"documentation/destinations/csv-via-http/#decimal-separator","title":"Decimal separator","text":"<p>Defines the decimal separator of decimal number for the output. Dot (.) is the default value. </p>"},{"location":"documentation/destinations/csv-via-http/#date-format","title":"Date format","text":"<p>Defines a customized date format (e.g. YYYY-MM-DD or MM/DD/YYYY) for converting valid SAP dates (YYYYMMDD). Default is YYYY-MM-DD.</p>"},{"location":"documentation/destinations/csv-via-http/#time-format","title":"Time format","text":"<p>Defines a customized time format (e.g. HH-MM-SS or HH:MM:SS) for converting valid SAP times (HHMMSS). Default is HH:MM:SS.</p>"},{"location":"documentation/destinations/csv-via-http/#text-encoding","title":"Text Encoding","text":"<p>Defines the text encoding.</p>"},{"location":"documentation/destinations/csv-via-http/#assign-the-http-csv-destination-to-an-extraction","title":"Assign the HTTP CSV Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/csv-via-http/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/csv-via-http/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/csv-via-http/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/csv-via-http/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/csv-via-http/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/csv-via-http/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/exasol/","title":"EXASolution","text":"<p>This page shows how to set up and use the EXASolution destination.  The EXASolution destination loads data to an EXASolution database.</p>"},{"location":"documentation/destinations/exasol/#requirements","title":"Requirements","text":"<p>As of Xtract Universal version 4.2.26.0 the Exasol ADO.Net driver ExaDataProvider is provided with the setup of Xtract Universal.  There are no additional installations needed to use Exasol database destination.</p>"},{"location":"documentation/destinations/exasol/#create-a-new-exasolution-destination","title":"Create a new EXASolution Destination","text":"<p>Follow the steps below to add a new EXASolution destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type EXASolution from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/exasol/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/exasol/#connection-string","title":"Connection string","text":"<p>Enter the name or IP of the DB2 server and the port number. </p>"},{"location":"documentation/destinations/exasol/#user-id-password","title":"User Id / Password","text":"<p>Enter the user ID and password of the database user.</p>"},{"location":"documentation/destinations/exasol/#schema","title":"Schema","text":"<p>Enter the schema of the database.</p>"},{"location":"documentation/destinations/exasol/#test-connection","title":"Test Connection","text":"<p>Checks the database connection.</p>"},{"location":"documentation/destinations/exasol/#assign-the-exasolution-destination-to-an-extraction","title":"Assign the EXASolution Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/exasol/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/exasol/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/exasol/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/exasol/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/exasol/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/exasol/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/exasol/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/exasol/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/exasol/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/exasol/#preparation","title":"Preparation","text":"<p>Defines the action on the target database before the data is inserted into the target table.</p> Option Description Drop &amp; Create Remove table if available and create new table (default). Truncate Or Create Empty table if available, otherwise create. Create If Not Exists Create table if not available. Prepare Merge Prepares the merge process and creates e.g. a temporary staging table, see Merge Data. None No action. Custom SQL Here you can define your own script, see Custom SQL Statements. <p>To only create the table in the first step and not insert any data, you have two options:</p> <ol> <li>Copy the SQL statement and execute it directly on the target data database.</li> <li>Select the None option for Row Processing and execute the extraction.</li> </ol> <p>Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.</p>"},{"location":"documentation/destinations/exasol/#row-processing","title":"Row Processing","text":"<p>Defines how the data is inserted into the target table.</p> Option Description Insert Insert records (default). Fill merge staging table Insert records into the staging table. None No action. Custom SQL Define your own script, see Custom SQL Statements. Merge (deprecated) This option is obsolete, see Merge Data. Use the Fill merge staging table option."},{"location":"documentation/destinations/exasol/#finalization","title":"Finalization","text":"<p>Defines the action on the target database after the data has been successfully inserted into the target table.</p> Option Description Finalize Merge Closes the merge process and deletes the temporary staging table, for example. None No action (default). Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/exasol/#debugging","title":"Debugging","text":"<p>Warning</p> <p>Performance decrease! The performance decreases when bulk insert is disabled. Disable the bulk insert only when necessary, e.g., upon request of the support team.</p> <p>By activating the checkbox Disable bulk operations, the default bulk insert is deactivated when writing to a database.</p> <p>This option enables detailed error analysis, if certain data rows cannot be persisted on the database. Possible causes for the incorrect behavior are incorrect values with regard to the stored data type.</p> <p>Debugging needs to be deactivated again, after the successful error analysis, otherwise the performance of the database write processes remains low. </p> <p>Note</p> <p>Bulk operations are not supported when using Custom SQL statements, e.g., in Row Processing.  Bulk operations lead to performance decrease.  To increase performance when using Custom SQL statements, it is recommended to perform the custom processing in the Finalization step.</p>"},{"location":"documentation/destinations/exasol/#transaction-style","title":"Transaction style","text":"<p>Note</p> <p>The available options for Transaction Style vary depending on the destination.</p>"},{"location":"documentation/destinations/exasol/#one-transaction","title":"One Transaction","text":"<p>Preparation, Row Processing and Finalization are all performed in a single transaction.</p> <ul> <li>Advantage: clean rollback of all changes.</li> <li>Disadvantage: possibly extensive locking during the entire extraction period. </li> </ul> <p>Recommendation</p> <p>Only use One Transaction in combination with DML commands, e.g., \"truncate table\" and \"insert.  Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command.  Example: If a table is created in the preparation step, the opened \"OneTransaction\" is committed and a rollback in the next steps is not performed correctly.</p>"},{"location":"documentation/destinations/exasol/#three-transactions","title":"Three Transactions","text":"<p>Preparation, Row Processing and Finalization are each executed in a separate transaction.</p> <ul> <li>Advantage: clean rollback of the individual sections, possibly shorter locking phases than with One Transaction (e.g. with DDL in Preparation, the entire DB is only locked during preparation and not for the entire extraction duration). </li> <li>Disadvantage: no rollback of previous step possible (error in Row Processing only rolls back changes from Row Processing, but not Preparation). </li> </ul>"},{"location":"documentation/destinations/exasol/#rowprocessingonly","title":"RowProcessingOnly","text":"<p>Only Row Processing is executed in a transaction. Preparation and Finalization without an explicit transaction (implicit commits).</p> <ul> <li>Advantage: DDL in 'Preparation and Finalization* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)</li> <li>Disadvantage: no rollback of Preparation/Finalization.</li> </ul>"},{"location":"documentation/destinations/exasol/#no-transaction","title":"No Transaction","text":"<p>No explicit transactions.</p> <ul> <li>Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.</li> <li>Disadvantage: no rollback </li> </ul>"},{"location":"documentation/destinations/exasol/#merge-data","title":"Merge Data","text":"<p>The following example depicts the update of the existing data records in a database by running an extraction to merge data.  In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP. </p> <p>With a merge command, the updated value is written to the destination database table.  The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.</p> <p></p> <p>Tip</p> <p>Alternatively to merging, the data can be also updated by means of full load.  The full load method is less efficient.</p>"},{"location":"documentation/destinations/exasol/#prerequisites","title":"Prerequisites","text":"<p>You need a table with existing SAP data, in which new data can be merged. Ideally, the table with existing data is created in the initial load with the corresponding Preparation option and filled with data with the Row Processing option Insert.</p> <p>After the table is created, open SAP and change a field value in the SAP table that is used for the data merge. </p> <p>Warning</p> <p>Faulty merge.  Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the General Settings of the extraction type to execute the merge command.</p>"},{"location":"documentation/destinations/exasol/#merge-command","title":"Merge Command","text":"<p>The merge process is performed using a staging table and takes place in three steps:</p> <ol> <li>A temporary table is created.</li> <li>The data is inserted in the temporary table.</li> <li>The temporary table is merged with the target table and then the temporary table is deleted.</li> </ol> <p>Follow the steps below to set up the merge process in Xtract Universal:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions. </li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</li> <li>Make sure to assign EXASolution destination to the extraction. </li> <li>Apply the following destination settings: </li> <li>Click [OK] and run the extraction.</li> </ol> <p>More information about the updated fields can be found in the SQL statement. It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update. Fields that do not appear in the SQL statement are not affected by changes. </p>"},{"location":"documentation/destinations/exasol/#custom-sql-statements","title":"Custom SQL Statements","text":"<p>The EXASolution destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the EXASolution destination:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination], the window \"Destination Settings\" opens.</li> <li>Make sure to assign the EXASolution destination to the extraction.</li> <li> <p>Select the option Custom SQL from the drop-down list   in one of the following sections:</p> <ul> <li>Preparation</li> <li>Row Processing</li> <li>Finalization</li> </ul> <p></p> </li> <li> <p>Click [Edit SQL] . The window \"Edit SQL\" opens.</p> </li> <li>Enter your custom SQL statement and click [OK] to confirm your input.</li> </ol>"},{"location":"documentation/destinations/exasol/#use-templates","title":"Use Templates","text":"<p>Existing SQL commands can be used as templates. You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:</p> <ul> <li>Preparation, e.g., in Drop &amp; Create or Create if Not Exists</li> <li>Row Processing, e.g., in Insert or Merge</li> <li>Finalization</li> </ul> <p>Follow the steps below to generate a Custom SQL command from a template:</p> <ol> <li>In one of the staging steps, select the Custom SQL option from the drop-down list  .</li> <li>Click [Edit SQL] . The dialogue \"Edit SQL\" opens. </li> <li>Navigate to the drop-down menu and select an existing command  . </li> <li>Click [Generate Statement]. A new statement is generated. </li> <li>Click [Copy] to copy the statement to the clipboard.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>Check out the Microsoft SQL Server example for details on predefined expressions.</p> <p>Note</p> <p>The custom SQL code is used for SQL Server destinations.  A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.</p>"},{"location":"documentation/destinations/exasol/#use-script-expressions","title":"Use Script Expressions","text":"<p>You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported: </p> Input Description <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.TableName }#</pre> Name of the database table extracted data is written to. <pre>#{Extraction.RowsCount }#</pre> Count of the extracted rows. <pre>#{Extraction.RunState}#</pre> Status of the extraction (Running, FinishedNoErrors, FinishedErrors). <pre>#{(int)Extraction.RunState}#</pre> Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors). <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <p>For more information, see Script Expressions.</p> Example: Verify the existence of a table in a database using 'ExistsTable'<pre><code>#{\n   iif\n   (\n      ExistsTable(\"MAKT\"),\n      \"TRUNCATE TABLE \\\"MAKT\\\";\",\n      \"\n         CREATE TABLE \\\"MAKT\\\"(\n            \\\"MATNR\\\" VARCHAR(18),\n            \\\"SPRAS\\\" VARCHAR(2),\n            \\\"MAKTX\\\" VARCHAR(40));\n      \"\n   )\n}#\n</code></pre>"},{"location":"documentation/destinations/exasol/#create-a-status-overview","title":"Create a Status Overview","text":"<p>The table \"ExtractionStatistics\" provides an overview and status of the executed Xtract Universal extractions. To create the \"ExtractionStatistics\" table, create an SQL table according to the following example:</p> Create ExtractionStatistics<pre><code>CREATE TABLE [dbo].[ExtractionStatistics](\n    [TableName] [nchar](50) NULL,\n    [RowsCount] [int] NULL,\n    [Timestamp] [nchar](50) NULL,\n    [RunState] [nchar](50) NULL\n) ON [PRIMARY]\nGO\n</code></pre> <p>The ExtractionStatistics table is filled in the Finalization process step, using the following SQL statement:</p> Fill ExtractionStatistics<pre><code>INSERT INTO [ExtractionStatistics]\n(\n     [TableName], \n     [RowsCount], \n     [Timestamp],\n     [RunState]\n)\nVALUES\n(\n     '#{Extraction.TableName}#', \n     '#{Extraction.RowsCount}#', \n     '#{Extraction.Timestamp}#',\n     '#{Extraction.RunState}#'\n);\n</code></pre>"},{"location":"documentation/destinations/google-cloud-storage/","title":"Google Cloud Storage","text":"<p>This page shows how to set up and use the Google Cloud Storage destination.  The Google Cloud Storage destination loads data to a Google Cloud Storage.</p>"},{"location":"documentation/destinations/google-cloud-storage/#about-google-cloud-storage","title":"About Google Cloud Storage","text":"<p>Google Cloud Platform (GCP) is a collection of cloud services provided by Google. Google Cloud Platform is available at cloud.google.com.</p> <p>Google Cloud Storage is one of the Google services used for storing data in the Google infrastructure. For more information see Google Cloud Storage Documentation.</p>"},{"location":"documentation/destinations/google-cloud-storage/#gcp-console","title":"GCP console","text":"<p>The GCP console allows configuring of all resources and services.  To get to the overview dashboard, navigate to the Google Cloud Storage page and click [Console] or [Go to console]. </p> <p>To access all settings and services use the navigation menu on the upper left side.</p> <p></p>"},{"location":"documentation/destinations/google-cloud-storage/#requirements","title":"Requirements","text":"<ul> <li>A Google account</li> <li>A Google Cloud Platform (GCP) subscription (trial version offered)</li> <li>A project (\"My First Project\" is pre-defined)</li> <li>A Google Cloud Storage (GCS) bucket for data extractions</li> </ul>"},{"location":"documentation/destinations/google-cloud-storage/#create-a-new-google-cloud-storage-destination","title":"Create a new Google Cloud Storage Destination","text":"<p>Follow the steps below to add a new Google Cloud Storage destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Google Cloud Storage from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/google-cloud-storage/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p>"},{"location":"documentation/destinations/google-cloud-storage/#gcs-settings","title":"GCS Settings","text":""},{"location":"documentation/destinations/google-cloud-storage/#connection-type","title":"Connection Type","text":"<p>The subsection Connection Type offers two different methods for authenticating and authorizing access to an Azure Storage account:</p> <ul> <li> <p>User Login</p> <p>Logs into Google Cloud Storage using the OAuth client ID authentication.  To enable the OAuth 2.0 protocol, configure an OAuth flow with the required access permissions to Xtract Universal, see Knowledge Base Article: Setting Up OAuth 2.0 for the Google Cloud Storage Destination.</p> </li> <li> <p>Service Account</p> <p>Logs into Google Cloud Storage using the credentials of a service account for authentication. The service account is identified by a RSA key pair.  When creating the keys, the user receives a service account file from Google containing information about the account.</p> </li> </ul> <p>The following input fields vary depending on the selected authentication method.</p> Authentication via User LoginAuthentication via Service Acccount"},{"location":"documentation/destinations/google-cloud-storage/#client-id","title":"Client ID","text":"<p>Enter the client ID created in the OAuth 2.0 setup. </p>"},{"location":"documentation/destinations/google-cloud-storage/#client-secret","title":"Client Secret","text":"<p>Enter the client secret created in the OAuth 2.0 setup. </p>"},{"location":"documentation/destinations/google-cloud-storage/#connect","title":"Connect","text":"<p>Processes the previously created OAuth flow to establish a connection with the storage account. Choose your Google account and grant access to Xtract Universal in all required windows.  When a connection is successful, an \"Authentication succeeded\" message is displayed in the browser.  In Xtract Universal a \"Connection established\" message is displayed in a separate window.   </p> <p>Warning</p> <p>This app isn't verified. This error message occurs if you did not verify the app.  Click [Advanced] and [Go to Xtract Universal (unsafe)] to continue. </p>"},{"location":"documentation/destinations/google-cloud-storage/#key-file","title":"Key file","text":"<p>Enter a path to the service account file, that is created together with the keys.  Make sure that the Xtract Universal service has access to the file. </p>"},{"location":"documentation/destinations/google-cloud-storage/#bucket","title":"Bucket","text":"<p>When using OAuth 2.0 authentication, the \"Bucket\" subsection can only be filled after a connection to the storage account has been established.</p>"},{"location":"documentation/destinations/google-cloud-storage/#project-id","title":"Project ID","text":"<p>The Project ID can be looked up in the GCP dashboard under Project info.</p> <p></p>"},{"location":"documentation/destinations/google-cloud-storage/#bucket-name","title":"Bucket name","text":"<p>When using OAuth 2.0 authentication, click [Get buckets] to display available buckets. A bucket can be created in the navigation menu under Storage &gt; Browser.</p> <p></p> <p>Choose a bucket name, location type and storage class or access control. </p> <p>Under Advanced Settings (optional) you can select the desired encryption method applied to the bucket. Get more details on encryption on the official Google Help Page.     </p>"},{"location":"documentation/destinations/google-cloud-storage/#encryption","title":"Encryption","text":""},{"location":"documentation/destinations/google-cloud-storage/#default","title":"Default","text":"<p>Applies the encryption method specified in your GCS bucket.  Google encrypts all data that is stored on the Google servers by default. In addition you can use the Google Cloud Key Management Service (KMS) to create and apply keys to your buckets.  The KMS can be enabled in the GCP console's navigation menu under Security &gt; Cryptographic Keys.</p>"},{"location":"documentation/destinations/google-cloud-storage/#customer-supplied","title":"Customer-supplied","text":"<p>If you check the Customer-supplied option, you need to provide a valid AES256 Crypto Key (256 bit in length).  The Crypto key is not stored in the GCP and demands the additional effort to be able to to decrypt your data later. </p>"},{"location":"documentation/destinations/google-cloud-storage/#crypto-key","title":"Crypto key","text":"<p>If Customer Supplied is selected as the encryption method, enter the cryptographic key into.</p>"},{"location":"documentation/destinations/google-cloud-storage/#file-format","title":"File Format","text":"<p>Select the required file format. You can choose between CSV, JSON and Parquet.</p> <p></p>"},{"location":"documentation/destinations/google-cloud-storage/#cvs-settings","title":"CVS Settings","text":"<p>The settings for file type CSV correspond to the settings of the Flat File CSV destination:</p> <ul> <li>CSV Settings</li> <li>Convert / Encoding</li> </ul>"},{"location":"documentation/destinations/google-cloud-storage/#json-settings","title":"JSON Settings","text":"<p>To use the JSON file format, no further settings are necessary.</p>"},{"location":"documentation/destinations/google-cloud-storage/#parquet-settings","title":"Parquet Settings","text":"<p>The settings for file type Parquet correspond to the settings of the Flat File Parquet destination:</p> <ul> <li>Compatibility Mode</li> </ul>"},{"location":"documentation/destinations/google-cloud-storage/#connection-retry","title":"Connection Retry","text":"<p>Connection retry is a built-in function of the Google Cloud Storage destination.  The retry function is activated by default.</p> <p>Connection retry is a functionality that prevents extractions from failing in case of transient connection interruptions to Google Cloud Storage. Xtract universal follows an exponential retry strategy. The selected exponential strategy results in eight retry attempts and an overall timespan of 140 seconds. If a connection is not established during the timespan of 140 seconds, the extraction fails.</p> <p>For more general information about retry strategies in a Google Cloud Storage environment go to the official Google Cloud Help.</p>"},{"location":"documentation/destinations/google-cloud-storage/#assign-the-google-cloud-storage-destination-to-an-extraction","title":"Assign the Google Cloud Storage Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/google-cloud-storage/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/google-cloud-storage/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/google-cloud-storage/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p> <p>Note</p> <p>If the name of an object does not begin with a letter, it will be prefixed with an \u2018x\u2019, e.g. an object by the name <code>_namespace_tabname.csv</code> will be renamed <code>x_namespace_tabname.csv</code> when uploaded to the destination. This is to ensure that all uploaded objects are compatible with Azure Data Factory, Hadoop and Spark, which require object names to begin with a letter or give special meaning to objects whose names start with certain non-alphabetic characters. </p>"},{"location":"documentation/destinations/google-cloud-storage/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/google-cloud-storage/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/google-cloud-storage/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/google-cloud-storage/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/google-cloud-storage/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/google-cloud-storage/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/google-cloud-storage/#folder-name","title":"Folder name","text":"<p>To write extraction data to a location within a specific folder in a Google Cloud Storage bucket, enter a folder name without slashes. Subfolders are supported and can be defined using the following syntax:  <code>[folder]/[subfolder_1]/[subfolder_2]/\u2026</code></p>"},{"location":"documentation/destinations/google-cloud-storage/#use-script-expressions-as-dynamic-folder-paths","title":"Use Script Expressions as Dynamic Folder Paths","text":"<p>Script expressions can be used to generate a dynamic folder path. This allows generating folder paths that are composed of an extraction's properties, e.g., extraction name, SAP source object. The described scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example: /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFields]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction. <pre>#{Extraction.Fields[\"[0D_NW_CODE]\"].Selections[0].Value}#</pre> Only for BWCube extractions (MDX mode): returns the input value of a defined selection. <pre>#{Extraction.Fields[\"[0D_NW_CHANN]\"].RangeSelections[0].LowerValue}#</pre> Only for BWCube extractions (MDX mode): returns the lower input value of a defined selection range. <pre>#{Extraction.Fields[\"[0D_NW_CHANN]\"].RangeSelections[0].UpperValue}#</pre> Only for BWCube extractions (MDX mode): returns the upper input value of a defined selection range. <pre>#{Extraction.Fields[\"0D_NW_CODE\"].Selections[0].Value}#</pre> Only for BWCube extractions (BICS mode): returns the input value of a defined selection. <pre>#{Extraction.Fields[\"0D_NW_CHANN\"].RangeSelections[0].LowerValue}#</pre> Only for BWCube extractions (BICS mode): returns the lower input value of a defined selection range. <pre>#{Extraction.Fields[\"0D_NW_CHANN\"].RangeSelections[0].UpperValue}#</pre> Only for BWCube extractions (BICS mode): returns the upper input value of a defined selection range."},{"location":"documentation/destinations/google-cloud-storage/#compression","title":"Compression","text":"<p>Compression is only available for the csv file format, see Destination Details: File Format.</p> Option Description None The data is transferred uncompressed and stored as a csv file. GZip The data is transferred compressed and stored as a gz file."},{"location":"documentation/destinations/google-cloud-storage/#file-splitting","title":"File Splitting","text":"<p>Writes extraction data of a single extraction to multiple files.  Each filename is appended by _part[nnn]. </p>"},{"location":"documentation/destinations/google-cloud-storage/#max-file-size","title":"Max. file size","text":"<p>The value set in Max. file size determines the maximum size of each file. </p> <p>Note</p> <p>The option Max. file size does not apply to gzip files.  The size of a gzipped file cannot be determined in advance.</p>"},{"location":"documentation/destinations/huawei/","title":"Huawei Cloud OBS","text":"<p>This page shows how to set up and use the Huawei Cloud OBS destination.  The Huawei Cloud OBS destination loads data to a Huawei Cloud Object Storage Service (OBS).</p> <p>Warning</p> <p>File Fragments in the Cloud Storage Huawei Cloud OBS destination uses multipart upload. That means that data is uploaded in fragments that are merged into a single file at the end of the extraction.  When an extraction fails due to connection issues, the request to cancel the multipart upload can fail.  Delete the fragments manually, see Huawei Cloud Support: Deleting Fragments Directly. </p>"},{"location":"documentation/destinations/huawei/#create-a-new-huawei-cloud-obs-destination","title":"Create a new Huawei Cloud OBS Destination","text":"<p>Follow the steps below to add a new Huawei Cloud OBS destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Huawei Cloud OBS from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/huawei/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/huawei/#authentication","title":"Authentication","text":""},{"location":"documentation/destinations/huawei/#access-key-id-ak","title":"Access Key ID (AK)","text":"<p>Enter the access key of the Huawei Cloud OBS account.  For more information on how to create access keys, see Huawei Cloud Support: Creating Access Keys (AK and SK)</p>"},{"location":"documentation/destinations/huawei/#secret-access-key-id-sk","title":"Secret Access Key ID (SK)","text":"<p>Enter the secret access key of the Huawei Cloud OBS account. For more information on how to create access keys, see Huawei Cloud Support: Creating Access Keys (AK and SK)</p>"},{"location":"documentation/destinations/huawei/#region","title":"Region","text":"<p>Select the region of the data storage.</p>"},{"location":"documentation/destinations/huawei/#connect","title":"Connect","text":"<p>Click [Connect] to establish a connection to the storage account.  If the connection is successful, \"Connected\" is displayed next to the button.</p>"},{"location":"documentation/destinations/huawei/#bucket","title":"Bucket","text":"<p>This setting only becomes available after a connection to the storage account is established. Select a bucket. The SAP data is extracted into the selected bucket.  Click [ arrow_circle2 icon arrow_circle2 icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). ] to refresh the list of available buckets.</p>"},{"location":"documentation/destinations/huawei/#misc","title":"Misc","text":""},{"location":"documentation/destinations/huawei/#folder-path","title":"Folder path","text":"<p>Option to create a folder structure within the container for saving files. Script expressions are supported, see Destination Settings &gt; Folder.</p> <p>For creating a single folder, enter a folder name without slashes: <code>[folder]</code>  Subfolders are supported and can be defined using the following syntax: <code>[folder]/[subfolder_1]/[subfolder_2]/[..]</code></p>"},{"location":"documentation/destinations/huawei/#file-format","title":"File Format","text":"<p>Select the required file format. You can choose between CSV, JSON and Parquet.</p> <p></p>"},{"location":"documentation/destinations/huawei/#cvs-settings","title":"CVS Settings","text":"<p>The settings for file type CSV correspond to the settings of the Flat File CSV destination:</p> <ul> <li>CSV Settings</li> <li>Convert / Encoding</li> </ul>"},{"location":"documentation/destinations/huawei/#json-settings","title":"JSON Settings","text":"<p>To use the JSON file format, no further settings are necessary.</p>"},{"location":"documentation/destinations/huawei/#parquet-settings","title":"Parquet Settings","text":"<p>The settings for file type Parquet correspond to the settings of the Flat File Parquet destination:</p> <ul> <li>Compatibility Mode</li> </ul>"},{"location":"documentation/destinations/huawei/#connection-retry-and-rollback","title":"Connection Retry and Rollback","text":"<p>Connection retry is a built-in function of the Huawei Cloud OBS destination.  The retry function is activated by default.</p> <p>Connection retry is a functionality that prevents extractions from failing in case of transient connection interruptions to Huawei Cloud OBS.  Xtract Universal follows an exponential retry strategy. The selected exponential strategy results in seven retry attempts and an overall timespan of 140 seconds.  If a connection is not established during the timespan of 140 seconds, the extraction fails.</p> <p>Rollback covers scenarios where extractions do not fail due to connection failures to Huawei but e.g. due to an error when connecting to SAP. In those cases Xtract Universal tries to remove any files from the Huawei Cloud storage that were created in the course of the extraction.</p>"},{"location":"documentation/destinations/huawei/#assign-the-huawei-cloud-obs-destination-to-an-extraction","title":"Assign the Huawei Cloud OBS Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/huawei/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/huawei/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/huawei/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/huawei/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/huawei/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/huawei/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/huawei/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/huawei/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/huawei/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/huawei/#folder","title":"Folder","text":"<p>Option to create a folder structure within the container for saving files, see Destination Details &gt; Folder Path.</p> <p>For creating a single folder, enter a folder name without slashes: <code>[folder]</code>  Subfolders are supported and can be defined using the following syntax: <code>[folder]/[subfolder_1]/[subfolder_2]/[..]</code></p>"},{"location":"documentation/destinations/huawei/#use-script-expressions-as-dynamic-folder-paths","title":"Use Script Expressions as Dynamic Folder Paths","text":"<p>Script expressions can be used to generate a dynamic folder path. This allows generating folder paths that are composed of an extraction's properties, e.g., extraction name, SAP source object. The described scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example: /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFields]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction. <pre>#{Extraction.Fields[\"[0D_NW_CODE]\"].Selections[0].Value}#</pre> Only for BWCube extractions (MDX mode): returns the input value of a defined selection. <pre>#{Extraction.Fields[\"[0D_NW_CHANN]\"].RangeSelections[0].LowerValue}#</pre> Only for BWCube extractions (MDX mode): returns the lower input value of a defined selection range. <pre>#{Extraction.Fields[\"[0D_NW_CHANN]\"].RangeSelections[0].UpperValue}#</pre> Only for BWCube extractions (MDX mode): returns the upper input value of a defined selection range. <pre>#{Extraction.Fields[\"0D_NW_CODE\"].Selections[0].Value}#</pre> Only for BWCube extractions (BICS mode): returns the input value of a defined selection. <pre>#{Extraction.Fields[\"0D_NW_CHANN\"].RangeSelections[0].LowerValue}#</pre> Only for BWCube extractions (BICS mode): returns the lower input value of a defined selection range. <pre>#{Extraction.Fields[\"0D_NW_CHANN\"].RangeSelections[0].UpperValue}#</pre> Only for BWCube extractions (BICS mode): returns the upper input value of a defined selection range."},{"location":"documentation/destinations/huawei/#compression","title":"Compression","text":"<p>Compression is only available for the csv file format, see Destination Details: File Format.</p>"},{"location":"documentation/destinations/huawei/#gzip","title":"GZip","text":"<p>The data is transferred compressed and stored as a gz file. </p>"},{"location":"documentation/destinations/huawei/#file-splitting","title":"File Splitting","text":"<p>Writes extraction data of a single extraction to multiple files.  Each filename is appended by _part[nnn]. </p>"},{"location":"documentation/destinations/huawei/#max-file-size","title":"Max. file size","text":"<p>The value set in Max. file size determines the maximum size of each file. </p> <p>Note</p> <p>The option Max. file size does not apply to gzip files.  The size of a gzipped file cannot be determined in advance.</p>"},{"location":"documentation/destinations/ibm-db2/","title":"IBM Db2","text":"<p>This page shows how to set up and use the IBM Db2 destination.  The IBM Db2 destination loads data to an IBM Db2 destination.</p>"},{"location":"documentation/destinations/ibm-db2/#requirements","title":"Requirements","text":"<p>The appropriate version (32bit for 32bit OS, 64bit for 64bit OS) of the ADO .NET driver must be installed. Select the IBM Data Server Driver Package and then the IBM Data Server Driver Package (Windows AMD64 and Intel EM64T) or  IBM Data Server Driver Package (Windows 32-bit AMD and Intel x86), see IBM Data Server Client Packages - Version 11.5 GA.</p> <p>If a fixed version is available, download the fixed version of the provider from the software vendor's website, see Overview IBM Data Server Client Packages. </p>"},{"location":"documentation/destinations/ibm-db2/#create-a-new-ibm-db2-destination","title":"Create a new IBM Db2 Destination","text":"<p>Follow the steps below to add a new IBM Db2 destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type IBM Db2 from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/ibm-db2/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/ibm-db2/#provider","title":"Provider","text":"<p>Enter the .Net provider for DB2. To install the provider refer to requirements.</p>"},{"location":"documentation/destinations/ibm-db2/#host-name-port","title":"Host Name / Port","text":"<p>Enter the name or IP of the DB2 server and the port number. </p>"},{"location":"documentation/destinations/ibm-db2/#username-password","title":"Username / Password","text":"<p>IBM Db2 authentication user name and password.</p>"},{"location":"documentation/destinations/ibm-db2/#database-name","title":"Database name","text":"<p>Enter the name of the IBM database.</p>"},{"location":"documentation/destinations/ibm-db2/#default-schema","title":"Default schema","text":"<p>Enter the schema of the DB2 database.</p>"},{"location":"documentation/destinations/ibm-db2/#test-connection","title":"Test Connection","text":"<p>Check the database connection.</p>"},{"location":"documentation/destinations/ibm-db2/#assign-the-ibm-db2-destination-to-an-extraction","title":"Assign the IBM Db2 Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/ibm-db2/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/ibm-db2/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/ibm-db2/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/ibm-db2/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/ibm-db2/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/ibm-db2/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/ibm-db2/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/ibm-db2/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/ibm-db2/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/ibm-db2/#preparation","title":"Preparation","text":"<p>Defines the action on the target database before the data is inserted into the target table.</p> Option Description Drop &amp; Create Remove table if available and create new table (default). Truncate Or Create Empty table if available, otherwise create. Create If Not Exists Create table if not available. Prepare Merge Prepares the merge process and creates e.g. a temporary staging table, see Merge Data. None No action. Custom SQL Here you can define your own script, see Custom SQL Statements. <p>To only create the table in the first step and not insert any data, you have two options:</p> <ol> <li>Copy the SQL statement and execute it directly on the target data database.</li> <li>Select the None option for Row Processing and execute the extraction.</li> </ol> <p>Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.</p>"},{"location":"documentation/destinations/ibm-db2/#row-processing","title":"Row Processing","text":"<p>Defines how the data is inserted into the target table.</p> Option Description Insert Insert records (default). Fill merge staging table Insert records into the staging table. None No action. Custom SQL Define your own script, see Custom SQL Statements. Merge (deprecated) This option is obsolete, see Merge Data. Use the Fill merge staging table option."},{"location":"documentation/destinations/ibm-db2/#finalization","title":"Finalization","text":"<p>Defines the action on the target database after the data has been successfully inserted into the target table.</p> Option Description Finalize Merge Closes the merge process and deletes the temporary staging table, for example. None No action (default). Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/ibm-db2/#transaction-style","title":"Transaction style","text":"<p>Note</p> <p>The available options for Transaction Style vary depending on the destination.</p>"},{"location":"documentation/destinations/ibm-db2/#one-transaction","title":"One Transaction","text":"<p>Preparation, Row Processing and Finalization are all performed in a single transaction.</p> <ul> <li>Advantage: clean rollback of all changes.</li> <li>Disadvantage: possibly extensive locking during the entire extraction period. </li> </ul> <p>Recommendation</p> <p>Only use One Transaction in combination with DML commands, e.g., \"truncate table\" and \"insert.  Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command.  Example: If a table is created in the preparation step, the opened \"OneTransaction\" is committed and a rollback in the next steps is not performed correctly.</p>"},{"location":"documentation/destinations/ibm-db2/#three-transactions","title":"Three Transactions","text":"<p>Preparation, Row Processing and Finalization are each executed in a separate transaction.</p> <ul> <li>Advantage: clean rollback of the individual sections, possibly shorter locking phases than with One Transaction (e.g. with DDL in Preparation, the entire DB is only locked during preparation and not for the entire extraction duration). </li> <li>Disadvantage: no rollback of previous step possible (error in Row Processing only rolls back changes from Row Processing, but not Preparation). </li> </ul>"},{"location":"documentation/destinations/ibm-db2/#rowprocessingonly","title":"RowProcessingOnly","text":"<p>Only Row Processing is executed in a transaction. Preparation and Finalization without an explicit transaction (implicit commits).</p> <ul> <li>Advantage: DDL in 'Preparation and Finalization* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)</li> <li>Disadvantage: no rollback of Preparation/Finalization.</li> </ul>"},{"location":"documentation/destinations/ibm-db2/#no-transaction","title":"No Transaction","text":"<p>No explicit transactions.</p> <ul> <li>Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.</li> <li>Disadvantage: no rollback </li> </ul>"},{"location":"documentation/destinations/ibm-db2/#merge-data","title":"Merge Data","text":"<p>The following example depicts the update of the existing data records in a database by running an extraction to merge data.  In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP. </p> <p>With a merge command, the updated value is written to the destination database table.  The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.</p> <p></p> <p>Tip</p> <p>Alternatively to merging, the data can be also updated by means of full load.  The full load method is less efficient.</p>"},{"location":"documentation/destinations/ibm-db2/#prerequisites","title":"Prerequisites","text":"<p>You need a table with existing SAP data, in which new data can be merged. Ideally, the table with existing data is created in the initial load with the corresponding Preparation option and filled with data with the Row Processing option Insert.</p> <p>After the table is created, open SAP and change a field value in the SAP table that is used for the data merge. </p> <p>Warning</p> <p>Faulty merge.  Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the General Settings of the extraction type to execute the merge command.</p>"},{"location":"documentation/destinations/ibm-db2/#merge-command","title":"Merge Command","text":"<p>The merge process is performed using a staging table and takes place in three steps:</p> <ol> <li>A temporary table is created.</li> <li>The data is inserted in the temporary table.</li> <li>The temporary table is merged with the target table and then the temporary table is deleted.</li> </ol> <p>Follow the steps below to set up the merge process in Xtract Universal:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions. </li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</li> <li>Make sure to assign IBM Db2 destination to the extraction. </li> <li>Apply the following destination settings: </li> <li>Click [OK] and run the extraction.</li> </ol> <p>More information about the updated fields can be found in the SQL statement. It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update. Fields that do not appear in the SQL statement are not affected by changes. </p>"},{"location":"documentation/destinations/ibm-db2/#custom-sql-statements","title":"Custom SQL Statements","text":"<p>The IBM Db2 destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the IBM Db2 destination:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination], the window \"Destination Settings\" opens.</li> <li>Make sure to assign the IBM Db2 destination to the extraction.</li> <li> <p>Select the option Custom SQL from the drop-down list   in one of the following sections:</p> <ul> <li>Preparation</li> <li>Row Processing</li> <li>Finalization</li> </ul> <p></p> </li> <li> <p>Click [Edit SQL] . The window \"Edit SQL\" opens.</p> </li> <li>Enter your custom SQL statement and click [OK] to confirm your input.</li> </ol>"},{"location":"documentation/destinations/ibm-db2/#use-templates","title":"Use Templates","text":"<p>Existing SQL commands can be used as templates. You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:</p> <ul> <li>Preparation, e.g., in Drop &amp; Create or Create if Not Exists</li> <li>Row Processing, e.g., in Insert or Merge</li> <li>Finalization</li> </ul> <p>Follow the steps below to generate a Custom SQL command from a template:</p> <ol> <li>In one of the staging steps, select the Custom SQL option from the drop-down list  .</li> <li>Click [Edit SQL] . The dialogue \"Edit SQL\" opens. </li> <li>Navigate to the drop-down menu and select an existing command  . </li> <li>Click [Generate Statement]. A new statement is generated. </li> <li>Click [Copy] to copy the statement to the clipboard.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>Check out the Microsoft SQL Server example for details on predefined expressions.</p> <p>Note</p> <p>The custom SQL code is used for SQL Server destinations.  A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.</p>"},{"location":"documentation/destinations/ibm-db2/#use-script-expressions","title":"Use Script Expressions","text":"<p>You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported: </p> Input Description <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.TableName }#</pre> Name of the database table extracted data is written to. <pre>#{Extraction.RowsCount }#</pre> Count of the extracted rows. <pre>#{Extraction.RunState}#</pre> Status of the extraction (Running, FinishedNoErrors, FinishedErrors). <pre>#{(int)Extraction.RunState}#</pre> Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors). <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <p>For more information, see Script Expressions.</p> Example: Verify the existence of a table in a database using 'ExistsTable'<pre><code>#{\n   iif\n   (\n      ExistsTable(\"MAKT\"),\n      \"TRUNCATE TABLE \\\"MAKT\\\";\",\n      \"\n         CREATE TABLE \\\"MAKT\\\"(\n            \\\"MATNR\\\" VARCHAR(18),\n            \\\"SPRAS\\\" VARCHAR(2),\n            \\\"MAKTX\\\" VARCHAR(40));\n      \"\n   )\n}#\n</code></pre>"},{"location":"documentation/destinations/ibm-db2/#create-a-status-overview","title":"Create a Status Overview","text":"<p>The table \"ExtractionStatistics\" provides an overview and status of the executed Xtract Universal extractions. To create the \"ExtractionStatistics\" table, create an SQL table according to the following example:</p> Create ExtractionStatistics<pre><code>CREATE TABLE [dbo].[ExtractionStatistics](\n    [TableName] [nchar](50) NULL,\n    [RowsCount] [int] NULL,\n    [Timestamp] [nchar](50) NULL,\n    [RunState] [nchar](50) NULL\n) ON [PRIMARY]\nGO\n</code></pre> <p>The ExtractionStatistics table is filled in the Finalization process step, using the following SQL statement:</p> Fill ExtractionStatistics<pre><code>INSERT INTO [ExtractionStatistics]\n(\n     [TableName], \n     [RowsCount], \n     [Timestamp],\n     [RunState]\n)\nVALUES\n(\n     '#{Extraction.TableName}#', \n     '#{Extraction.RowsCount}#', \n     '#{Extraction.Timestamp}#',\n     '#{Extraction.RunState}#'\n);\n</code></pre>"},{"location":"documentation/destinations/json-flat-file/","title":"Flat File JSON","text":"<p>This page shows how to set up and use the Flat File JSON destination.  The Flat File JSON destination is a JSON flat file that can be written to a local directory or a network drive.  </p>"},{"location":"documentation/destinations/json-flat-file/#create-a-new-flat-file-json-destination","title":"Create a new Flat File JSON Destination","text":"<p>Follow the steps below to add a new Flat File JSON destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Flat File JSON from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/json-flat-file/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/json-flat-file/#file-output-path","title":"File output path","text":"<p>Enter the folder path to save the destination flat files in.  If the entered folder does not exist, a new folder is created.</p> <p>Note</p> <p>To write flat files to a network drive, you need to:</p> <ul> <li>Enter the File output path in UNC format e.g., <code>\\\\Server2\\Share\\Folder1</code>.</li> <li>Run the Xtract Universal service by a user with write permission to the directory. </li> </ul>"},{"location":"documentation/destinations/json-flat-file/#assign-the-flat-file-json-destination-to-an-extraction","title":"Assign the Flat File JSON Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/json-flat-file/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/json-flat-file/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/json-flat-file/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/json-flat-file/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/json-flat-file/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/json-flat-file/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/json-flat-file/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/json-flat-file/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/json-flat-file/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/json-flat-file/#existing-files","title":"Existing files","text":"<p>If a flat file by the same name already exists in the target directory, the following actions can be taken:</p> Option Description Replace file The export process overwrites existing files. Abort extraction The process is aborted, if the file already exists."},{"location":"documentation/destinations/json-flat-file/#file-splitting","title":"File Splitting","text":"<p>Writes extraction data of a single extraction to multiple files.  Each filename is appended by _part[nnn]. </p>"},{"location":"documentation/destinations/json-flat-file/#max-file-size","title":"Max. file size","text":"<p>The value set in Max. file size determines the maximum size of each file. </p> <p>Note</p> <p>The option Max. file size does not apply to gzip files.  The size of a gzipped file cannot be determined in advance.</p>"},{"location":"documentation/destinations/json-via-http/","title":"HTTP JSON","text":"<p>This page shows how to set up and use the HTTP JSON destination.  The HTTP JSON destination is a generic JSON stream over HTTP. </p>"},{"location":"documentation/destinations/json-via-http/#create-a-new-http-json-destination","title":"Create a new HTTP JSON Destination","text":"<p>Follow the steps below to add a new HTTP JSON destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type HTTP JSON from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/json-via-http/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination. To use the JSON destination, no further settings are necessary.</p> <p></p>"},{"location":"documentation/destinations/json-via-http/#assign-the-http-json-destination-to-an-extraction","title":"Assign the HTTP JSON Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/json-via-http/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/json-via-http/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/json-via-http/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/json-via-http/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/json-via-http/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/json-via-http/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/knime/","title":"KNIME","text":"<p>This page shows how to set up and use the KNIME destination.  The KNIME destination loads data to KNIME. </p>"},{"location":"documentation/destinations/knime/#create-a-new-knime-destination","title":"Create a new KNIME Destination","text":"<p>Follow the steps below to add a new KNIME destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type KNIME from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/knime/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination. To use the KNIME destination, no further settings are necessary.</p> <p></p>"},{"location":"documentation/destinations/knime/#assign-the-knime-destination-to-an-extraction","title":"Assign the KNIME Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/knime/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/knime/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/knime/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/knime/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/knime/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/knime/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/knime/#knime-integration-via-sap-reader","title":"KNIME Integration via SAP Reader","text":""},{"location":"documentation/destinations/knime/#requirements","title":"Requirements","text":"<p>The following software applications must be installed for integration into KNIME:</p> <ul> <li>Latest version of Xtract Universal, see Download Trial Version</li> <li>Latest version of the KNIME Analytics Platform</li> </ul>"},{"location":"documentation/destinations/knime/#step-by-step-guide","title":"Step by Step Guide","text":"<ol> <li>Create an extraction in Xtract Universal. Select KNIME as the destination for the extraction  . </li> <li>Start the KNIME Analytics Platform. </li> <li>Install the extension SAP Reader (Theobald Software). </li> <li>Drag &amp; Drop the Node / Source 'SAP Reader (Theobald Software)'   onto the KNIME Canvas. </li> <li>Open the SAP Reader Task 'Settings' and enter the URL address of the Xtract Universal Server, e.g. <code>http://localhost:8065/</code> . </li> <li>Click [Fetch Queries]  and select an extraction.</li> <li>Confirm your input with [OK] .</li> <li>Start the extraction via [Execute].  </li> <li>Check the extracted SAP data via [SAP Query Result]. </li> </ol>"},{"location":"documentation/destinations/knime/#related-links","title":"Related Links","text":"<ul> <li>KNIME SAP Reader (Theobald Software)</li> <li>Youtube-Video: Webinar \"SAP Data to Insights with KNIME\"</li> <li>Knowledge Base Article: Dynamic Runtime Parameter within KNIME Workflow</li> </ul>"},{"location":"documentation/destinations/microsoft-sql-server/","title":"Microsoft SQL Server","text":"<p>This page shows how to set up and use the Microsoft SQL Server destination.  The Microsoft SQL Server destination loads data to a Microsoft SQL Server Database or a Microsoft Azure SQL Database destination.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#requirements","title":"Requirements","text":"<p>No driver installation is required since the ADO .NET driver for SQL Server is delivered and installed as a part of the .NET framework.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#create-a-new-microsoft-sql-server-destination","title":"Create a new Microsoft SQL Server Destination","text":"<p>Follow the steps below to add a new Microsoft SQL Server destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Microsoft SQL Server from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/microsoft-sql-server/#server-name","title":"Server Name","text":"<p>Specifies the host address of the SQL Server. Please note the following syntax:</p> Syntax Example [ServerName] <code>dbtest</code> [ServerName],[Port] <code>dbtest,1433</code> [ServerName].[Domain],[Port] <code>dbtest.theobald.software,1433</code> <p>It is only necessary to specify the port if it has been edited outside the SQL standard.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#require-tls-encryption","title":"Require TLS encryption","text":"<p>Client-side enforcement for using TLS encrpytion.  Adds the following parameters to the connection string:</p> <ul> <li>Encrypt = On</li> <li>TrustServerCertificate = Off</li> </ul> <p>For more information, see Microsoft Documentation: Enable Encrypted Connections to the Database Engine</p>"},{"location":"documentation/destinations/microsoft-sql-server/#windows-authentication","title":"Windows Authentication","text":"<p>Uses the service account, under which the XU service is running, for authentication against SQL Server.</p> <p>Note</p> <p>To successfully connect to the database using Windows authentication, make sure to run the XU service under a Windows AD user with access to the database.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#impersonate-authenticated-caller","title":"Impersonate authenticated caller","text":"<p>Uses the Windows AD user, executing the extraction, for authentication against SQL Server using Kerberos authentication. For using this functionality a similar configuration as for Kerberos Single Sign On against SAP is required.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#user-name","title":"User Name","text":"<p>The user name for the SQL Server authentication. </p>"},{"location":"documentation/destinations/microsoft-sql-server/#password","title":"Password","text":"<p>The password for the SQL Server authentication</p>"},{"location":"documentation/destinations/microsoft-sql-server/#database-name","title":"Database Name","text":"<p>Defines the name of the SQL Server database.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#test-connection","title":"Test Connection","text":"<p>Checks the database connection. </p>"},{"location":"documentation/destinations/microsoft-sql-server/#assign-the-microsoft-sql-server-destination-to-an-extraction","title":"Assign the Microsoft SQL Server Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/microsoft-sql-server/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/microsoft-sql-server/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/microsoft-sql-server/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/microsoft-sql-server/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/microsoft-sql-server/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#preparation","title":"Preparation","text":"<p>Defines the action on the target database before the data is inserted into the target table.</p> Option Description Drop &amp; Create Remove table if available and create new table (default). Truncate Or Create Empty table if available, otherwise create. Create If Not Exists Create table if not available. Prepare Merge Prepares the merge process and creates e.g. a temporary staging table, see Merge Data. None No action. Custom SQL Here you can define your own script, see Custom SQL Statements. <p>To only create the table in the first step and not insert any data, you have two options:</p> <ol> <li>Copy the SQL statement and execute it directly on the target data database.</li> <li>Select the None option for Row Processing and execute the extraction.</li> </ol> <p>Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#row-processing","title":"Row Processing","text":"<p>Defines how the data is inserted into the target table.</p> Option Description Insert Insert records (default). Fill merge staging table Insert records into the staging table. None No action. Custom SQL Define your own script, see Custom SQL Statements. Merge (deprecated) This option is obsolete, see Merge Data. Use the Fill merge staging table option."},{"location":"documentation/destinations/microsoft-sql-server/#finalization","title":"Finalization","text":"<p>Defines the action on the target database after the data has been successfully inserted into the target table.</p> Option Description Finalize Merge Closes the merge process and deletes the temporary staging table, for example. None No action (default). Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/microsoft-sql-server/#debugging","title":"Debugging","text":"<p>Warning</p> <p>Performance decrease! The performance decreases when bulk insert is disabled. Disable the bulk insert only when necessary, e.g., upon request of the support team.</p> <p>By activating the checkbox Disable bulk operations, the default bulk insert is deactivated when writing to a database.</p> <p>This option enables detailed error analysis, if certain data rows cannot be persisted on the database. Possible causes for the incorrect behavior are incorrect values with regard to the stored data type.</p> <p>Debugging needs to be deactivated again, after the successful error analysis, otherwise the performance of the database write processes remains low. </p> <p>Note</p> <p>Bulk operations are not supported when using Custom SQL statements, e.g., in Row Processing.  Bulk operations lead to performance decrease.  To increase performance when using Custom SQL statements, it is recommended to perform the custom processing in the Finalization step.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#transaction-style","title":"Transaction style","text":"<p>Note</p> <p>The available options for Transaction Style vary depending on the destination.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#one-transaction","title":"One Transaction","text":"<p>Preparation, Row Processing and Finalization are all performed in a single transaction.</p> <ul> <li>Advantage: clean rollback of all changes.</li> <li>Disadvantage: possibly extensive locking during the entire extraction period. </li> </ul> <p>Recommendation</p> <p>Only use One Transaction in combination with DML commands, e.g., \"truncate table\" and \"insert.  Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command.  Example: If a table is created in the preparation step, the opened \"OneTransaction\" is committed and a rollback in the next steps is not performed correctly.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#three-transactions","title":"Three Transactions","text":"<p>Preparation, Row Processing and Finalization are each executed in a separate transaction.</p> <ul> <li>Advantage: clean rollback of the individual sections, possibly shorter locking phases than with One Transaction (e.g. with DDL in Preparation, the entire DB is only locked during preparation and not for the entire extraction duration). </li> <li>Disadvantage: no rollback of previous step possible (error in Row Processing only rolls back changes from Row Processing, but not Preparation). </li> </ul>"},{"location":"documentation/destinations/microsoft-sql-server/#rowprocessingonly","title":"RowProcessingOnly","text":"<p>Only Row Processing is executed in a transaction. Preparation and Finalization without an explicit transaction (implicit commits).</p> <ul> <li>Advantage: DDL in 'Preparation and Finalization* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)</li> <li>Disadvantage: no rollback of Preparation/Finalization.</li> </ul>"},{"location":"documentation/destinations/microsoft-sql-server/#no-transaction","title":"No Transaction","text":"<p>No explicit transactions.</p> <ul> <li>Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.</li> <li>Disadvantage: no rollback </li> </ul>"},{"location":"documentation/destinations/microsoft-sql-server/#merge-data","title":"Merge Data","text":"<p>The following example depicts the update of the existing data records in a database by running an extraction to merge data.  In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP. </p> <p>With a merge command, the updated value is written to the destination database table.  The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.</p> <p></p> <p>Tip</p> <p>Alternatively to merging, the data can be also updated by means of full load.  The full load method is less efficient.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#prerequisites","title":"Prerequisites","text":"<p>You need a table with existing SAP data, in which new data can be merged. Ideally, the table with existing data is created in the initial load with the corresponding Preparation option and filled with data with the Row Processing option Insert.</p> <p>After the table is created, open SAP and change a field value in the SAP table that is used for the data merge. </p> <p>Warning</p> <p>Faulty merge.  Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the General Settings of the extraction type to execute the merge command.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#merge-command","title":"Merge Command","text":"<p>The merge process is performed using a staging table and takes place in three steps:</p> <ol> <li>A temporary table is created.</li> <li>The data is inserted in the temporary table.</li> <li>The temporary table is merged with the target table and then the temporary table is deleted.</li> </ol> <p>Follow the steps below to set up the merge process in Xtract Universal:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions. </li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</li> <li>Make sure to assign Microsoft SQL Server destination to the extraction. </li> <li>Apply the following destination settings: </li> <li>Click [OK] and run the extraction.</li> </ol> <p>More information about the updated fields can be found in the SQL statement. It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update. Fields that do not appear in the SQL statement are not affected by changes. </p>"},{"location":"documentation/destinations/microsoft-sql-server/#custom-sql-statements","title":"Custom SQL Statements","text":"<p>The Microsoft SQL Server destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the Microsoft SQL Server destination:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination], the window \"Destination Settings\" opens.</li> <li>Make sure to assign the Microsoft SQL Server destination to the extraction.</li> <li> <p>Select the option Custom SQL from the drop-down list   in one of the following sections:</p> <ul> <li>Preparation</li> <li>Row Processing</li> <li>Finalization</li> </ul> <p></p> </li> <li> <p>Click [Edit SQL] . The window \"Edit SQL\" opens.</p> </li> <li>Enter your custom SQL statement and click [OK] to confirm your input.</li> </ol>"},{"location":"documentation/destinations/microsoft-sql-server/#use-templates","title":"Use Templates","text":"<p>Existing SQL commands can be used as templates. You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:</p> <ul> <li>Preparation, e.g., in Drop &amp; Create or Create if Not Exists</li> <li>Row Processing, e.g., in Insert or Merge</li> <li>Finalization</li> </ul> <p>Follow the steps below to generate a Custom SQL command from a template:</p> <ol> <li>In one of the staging steps, select the Custom SQL option from the drop-down list  .</li> <li>Click [Edit SQL] . The dialogue \"Edit SQL\" opens. </li> <li>Navigate to the drop-down menu and select an existing command  . </li> <li>Click [Generate Statement]. A new statement is generated. </li> <li>Click [Copy] to copy the statement to the clipboard.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>Check out the Microsoft SQL Server example for details on predefined expressions.</p> <p>Note</p> <p>The custom SQL code is used for SQL Server destinations.  A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.</p>"},{"location":"documentation/destinations/microsoft-sql-server/#use-script-expressions","title":"Use Script Expressions","text":"<p>You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported: </p> Input Description <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.TableName }#</pre> Name of the database table extracted data is written to. <pre>#{Extraction.RowsCount }#</pre> Count of the extracted rows. <pre>#{Extraction.RunState}#</pre> Status of the extraction (Running, FinishedNoErrors, FinishedErrors). <pre>#{(int)Extraction.RunState}#</pre> Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors). <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <p>For more information, see Script Expressions.</p> Example: Verify the existence of a table in a database using 'ExistsTable'<pre><code>#{\n   iif\n   (\n      ExistsTable(\"MAKT\"),\n      \"TRUNCATE TABLE \\\"MAKT\\\";\",\n      \"\n         CREATE TABLE \\\"MAKT\\\"(\n            \\\"MATNR\\\" VARCHAR(18),\n            \\\"SPRAS\\\" VARCHAR(2),\n            \\\"MAKTX\\\" VARCHAR(40));\n      \"\n   )\n}#\n</code></pre>"},{"location":"documentation/destinations/microsoft-sql-server/#create-a-status-overview","title":"Create a Status Overview","text":"<p>The table \"ExtractionStatistics\" provides an overview and status of the executed Xtract Universal extractions. To create the \"ExtractionStatistics\" table, create an SQL table according to the following example:</p> Create ExtractionStatistics<pre><code>CREATE TABLE [dbo].[ExtractionStatistics](\n    [TableName] [nchar](50) NULL,\n    [RowsCount] [int] NULL,\n    [Timestamp] [nchar](50) NULL,\n    [RunState] [nchar](50) NULL\n) ON [PRIMARY]\nGO\n</code></pre> <p>The ExtractionStatistics table is filled in the Finalization process step, using the following SQL statement:</p> Fill ExtractionStatistics<pre><code>INSERT INTO [ExtractionStatistics]\n(\n     [TableName], \n     [RowsCount], \n     [Timestamp],\n     [RunState]\n)\nVALUES\n(\n     '#{Extraction.TableName}#', \n     '#{Extraction.RowsCount}#', \n     '#{Extraction.Timestamp}#',\n     '#{Extraction.RunState}#'\n);\n</code></pre>"},{"location":"documentation/destinations/microsoft-sql-server/#custom-sql-example","title":"Custom SQL Example","text":"<p>In the depicted example, the table KNA1 is extended by a column with the current timestamp of type DATETIME. The new column is filled dynamically using a .NET-based function. </p> <p>Note</p> <p>The data types that can be used in the SQL statement depend on the SQL Server database version.</p> <ol> <li>In the staging step Preparation, select the option Custom SQL from the drop-down list and click [Edit SQL]. The window \"Edit SQL\" opens.</li> <li>In the drop-down menu, select the option Drop &amp; Create and click [Generate Statement] to use the template for Drop &amp; Create.</li> <li>Add the following line in the generated statement:  <pre><code>[Extraction_Date] NATIONAL CHARACTER VARYING(23)\n</code></pre></li> <li>Click [OK] to confirm your input. </li> <li>In the staging step Row Processing, select the option Insert.  At this point, no data is written from the SAP source system, but <code>NULL</code> values are written to the newly created Extraction_Date column.</li> <li>In the staging step Finalization, the <code>NULL</code> values can be filled by a custom SQL statement.  Select the option Custom SQL from the drop-down list and click [Edit SQL].  The window \"Edit SQL\" opens.</li> <li> <p>Paste the following SQl statement into the editor:</p> <pre><code>UPDATE [dbo].[KNA1] \nSET [Extraction_Date] = '#{Extraction.Timestamp}#' \nWHERE [Extraction_Date] IS NULL;\n</code></pre> <p>The <code>NULL</code> values are filled with the current date of the extraction and written to the SQL target table using the T-SQL command <code>UPDATE</code>. </p> </li> <li> <p>Click [OK] to confirm your input and run the extraction.</p> </li> </ol> <p>Check the existence of the extended column Extraction_Date  in the SQL Server View of table KNA1. </p>"},{"location":"documentation/destinations/microsoft-sql-server/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base Article: Post-Processing Column Name Style</li> <li>Knowledge Base Article: Collation Settings for MSSQL Server Destination</li> <li>Integration via Azure Data Factory</li> </ul>"},{"location":"documentation/destinations/mysql/","title":"MySQL","text":"<p>This page shows how to set up and use the MySQL destination.  The MySQL destination loads data to a MySQL database.</p>"},{"location":"documentation/destinations/mysql/#requirements","title":"Requirements","text":"<p>As of Xtract Universal version 4.2.26.0 the MySQL ADO.Net driver is provided with the setup of Xtract Universal. There are no additional installations needed to use MySQL database destination.</p>"},{"location":"documentation/destinations/mysql/#create-a-new-mysql-destination","title":"Create a new MySQL Destination","text":"<p>Follow the steps below to add a new MySQL destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type MySQL from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/mysql/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/mysql/#assign-the-mysql-destination-to-an-extraction","title":"Assign the MySQL Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/mysql/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/mysql/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/mysql/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/mysql/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/mysql/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/mysql/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/mysql/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/mysql/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/mysql/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/mysql/#preparation","title":"Preparation","text":"<p>Defines the action on the target database before the data is inserted into the target table.</p> Option Description Drop &amp; Create Remove table if available and create new table (default). Truncate Or Create Empty table if available, otherwise create. Create If Not Exists Create table if not available. Prepare Merge Prepares the merge process and creates e.g. a temporary staging table, see Merge Data. None No action. Custom SQL Here you can define your own script, see Custom SQL Statements. <p>To only create the table in the first step and not insert any data, you have two options:</p> <ol> <li>Copy the SQL statement and execute it directly on the target data database.</li> <li>Select the None option for Row Processing and execute the extraction.</li> </ol> <p>Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.</p>"},{"location":"documentation/destinations/mysql/#row-processing","title":"Row Processing","text":"<p>Defines how the data is inserted into the target table.</p> Option Description Insert Insert records (default). Fill merge staging table Insert records into the staging table. None No action. Custom SQL Define your own script, see Custom SQL Statements. Merge (deprecated) This option is obsolete, see Merge Data. Use the Fill merge staging table option."},{"location":"documentation/destinations/mysql/#finalization","title":"Finalization","text":"<p>Defines the action on the target database after the data has been successfully inserted into the target table.</p> Option Description Finalize Merge Closes the merge process and deletes the temporary staging table, for example. None No action (default). Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/mysql/#transaction-style","title":"Transaction style","text":"<p>Note</p> <p>The available options for Transaction Style vary depending on the destination.</p>"},{"location":"documentation/destinations/mysql/#one-transaction","title":"One Transaction","text":"<p>Preparation, Row Processing and Finalization are all performed in a single transaction.</p> <ul> <li>Advantage: clean rollback of all changes.</li> <li>Disadvantage: possibly extensive locking during the entire extraction period. </li> </ul> <p>Recommendation</p> <p>Only use One Transaction in combination with DML commands, e.g., \"truncate table\" and \"insert.  Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command.  Example: If a table is created in the preparation step, the opened \"OneTransaction\" is committed and a rollback in the next steps is not performed correctly.</p>"},{"location":"documentation/destinations/mysql/#three-transactions","title":"Three Transactions","text":"<p>Preparation, Row Processing and Finalization are each executed in a separate transaction.</p> <ul> <li>Advantage: clean rollback of the individual sections, possibly shorter locking phases than with One Transaction (e.g. with DDL in Preparation, the entire DB is only locked during preparation and not for the entire extraction duration). </li> <li>Disadvantage: no rollback of previous step possible (error in Row Processing only rolls back changes from Row Processing, but not Preparation). </li> </ul>"},{"location":"documentation/destinations/mysql/#rowprocessingonly","title":"RowProcessingOnly","text":"<p>Only Row Processing is executed in a transaction. Preparation and Finalization without an explicit transaction (implicit commits).</p> <ul> <li>Advantage: DDL in 'Preparation and Finalization* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)</li> <li>Disadvantage: no rollback of Preparation/Finalization.</li> </ul>"},{"location":"documentation/destinations/mysql/#no-transaction","title":"No Transaction","text":"<p>No explicit transactions.</p> <ul> <li>Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.</li> <li>Disadvantage: no rollback </li> </ul>"},{"location":"documentation/destinations/mysql/#merge-data","title":"Merge Data","text":"<p>The following example depicts the update of the existing data records in a database by running an extraction to merge data.  In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP. </p> <p>With a merge command, the updated value is written to the destination database table.  The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.</p> <p></p> <p>Tip</p> <p>Alternatively to merging, the data can be also updated by means of full load.  The full load method is less efficient.</p>"},{"location":"documentation/destinations/mysql/#prerequisites","title":"Prerequisites","text":"<p>You need a table with existing SAP data, in which new data can be merged. Ideally, the table with existing data is created in the initial load with the corresponding Preparation option and filled with data with the Row Processing option Insert.</p> <p>After the table is created, open SAP and change a field value in the SAP table that is used for the data merge. </p> <p>Warning</p> <p>Faulty merge.  Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the General Settings of the extraction type to execute the merge command.</p>"},{"location":"documentation/destinations/mysql/#merge-command","title":"Merge Command","text":"<p>The merge process is performed using a staging table and takes place in three steps:</p> <ol> <li>A temporary table is created.</li> <li>The data is inserted in the temporary table.</li> <li>The temporary table is merged with the target table and then the temporary table is deleted.</li> </ol> <p>Follow the steps below to set up the merge process in Xtract Universal:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions. </li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</li> <li>Make sure to assign MySQL destination to the extraction. </li> <li>Apply the following destination settings: </li> <li>Click [OK] and run the extraction.</li> </ol> <p>More information about the updated fields can be found in the SQL statement. It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update. Fields that do not appear in the SQL statement are not affected by changes. </p>"},{"location":"documentation/destinations/mysql/#custom-sql-statements","title":"Custom SQL Statements","text":"<p>The MySQL destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the MySQL destination:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination], the window \"Destination Settings\" opens.</li> <li>Make sure to assign the MySQL destination to the extraction.</li> <li> <p>Select the option Custom SQL from the drop-down list   in one of the following sections:</p> <ul> <li>Preparation</li> <li>Row Processing</li> <li>Finalization</li> </ul> <p></p> </li> <li> <p>Click [Edit SQL] . The window \"Edit SQL\" opens.</p> </li> <li>Enter your custom SQL statement and click [OK] to confirm your input.</li> </ol>"},{"location":"documentation/destinations/mysql/#use-templates","title":"Use Templates","text":"<p>Existing SQL commands can be used as templates. You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:</p> <ul> <li>Preparation, e.g., in Drop &amp; Create or Create if Not Exists</li> <li>Row Processing, e.g., in Insert or Merge</li> <li>Finalization</li> </ul> <p>Follow the steps below to generate a Custom SQL command from a template:</p> <ol> <li>In one of the staging steps, select the Custom SQL option from the drop-down list  .</li> <li>Click [Edit SQL] . The dialogue \"Edit SQL\" opens. </li> <li>Navigate to the drop-down menu and select an existing command  . </li> <li>Click [Generate Statement]. A new statement is generated. </li> <li>Click [Copy] to copy the statement to the clipboard.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>Check out the Microsoft SQL Server example for details on predefined expressions.</p> <p>Note</p> <p>The custom SQL code is used for SQL Server destinations.  A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.</p>"},{"location":"documentation/destinations/mysql/#use-script-expressions","title":"Use Script Expressions","text":"<p>You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported: </p> Input Description <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.TableName }#</pre> Name of the database table extracted data is written to. <pre>#{Extraction.RowsCount }#</pre> Count of the extracted rows. <pre>#{Extraction.RunState}#</pre> Status of the extraction (Running, FinishedNoErrors, FinishedErrors). <pre>#{(int)Extraction.RunState}#</pre> Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors). <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <p>For more information, see Script Expressions.</p> Example: Verify the existence of a table in a database using 'ExistsTable'<pre><code>#{\n   iif\n   (\n      ExistsTable(\"MAKT\"),\n      \"TRUNCATE TABLE \\\"MAKT\\\";\",\n      \"\n         CREATE TABLE \\\"MAKT\\\"(\n            \\\"MATNR\\\" VARCHAR(18),\n            \\\"SPRAS\\\" VARCHAR(2),\n            \\\"MAKTX\\\" VARCHAR(40));\n      \"\n   )\n}#\n</code></pre>"},{"location":"documentation/destinations/mysql/#create-a-status-overview","title":"Create a Status Overview","text":"<p>The table \"ExtractionStatistics\" provides an overview and status of the executed Xtract Universal extractions. To create the \"ExtractionStatistics\" table, create an SQL table according to the following example:</p> Create ExtractionStatistics<pre><code>CREATE TABLE [dbo].[ExtractionStatistics](\n    [TableName] [nchar](50) NULL,\n    [RowsCount] [int] NULL,\n    [Timestamp] [nchar](50) NULL,\n    [RunState] [nchar](50) NULL\n) ON [PRIMARY]\nGO\n</code></pre> <p>The ExtractionStatistics table is filled in the Finalization process step, using the following SQL statement:</p> Fill ExtractionStatistics<pre><code>INSERT INTO [ExtractionStatistics]\n(\n     [TableName], \n     [RowsCount], \n     [Timestamp],\n     [RunState]\n)\nVALUES\n(\n     '#{Extraction.TableName}#', \n     '#{Extraction.RowsCount}#', \n     '#{Extraction.Timestamp}#',\n     '#{Extraction.RunState}#'\n);\n</code></pre>"},{"location":"documentation/destinations/oracle/","title":"Oracle","text":"<p>This page shows how to set up and use the Oracle destination.  The Oracle destination loads data to an Oracle database.</p>"},{"location":"documentation/destinations/oracle/#requirements","title":"Requirements","text":"<p>As of Xtract Universal version 4.2.34.0 the Oracle data provider is included in to the setup of Xtract Universal.  There are no additional installations needed to use Oracle database destination.</p>"},{"location":"documentation/destinations/oracle/#create-a-new-oracle-destination","title":"Create a new Oracle Destination","text":"<p>Follow the steps below to add a new Oracle destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Oracle from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/oracle/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination. The Oracle destination supports 3 types of connections:</p> Connection Type Description Default Connect to an Oracle database (on-premises). Wallet Connect to an Oracle Database using a connection from a wallet. Use this option when mTLS (Mutual TLS) authentication is required. Connect Descriptor Connect to an Oracle database (on-premises and cloud) via TLS. <p>The input fields vary depending on the selected authentication method.</p>"},{"location":"documentation/destinations/oracle/#default","title":"Default","text":""},{"location":"documentation/destinations/oracle/#host","title":"Host","text":"<p>Enter the name of the Oracle server.</p>"},{"location":"documentation/destinations/oracle/#port","title":"Port","text":"<p>Enter the Oracle server connection port (Default: 1521).</p>"},{"location":"documentation/destinations/oracle/#sid-service-name","title":"SID / Service name","text":"<p>Enter the unique name (SID) or the alias (service name) of the Oracle database.</p>"},{"location":"documentation/destinations/oracle/#username","title":"Username","text":"<p>Enter the user name.</p>"},{"location":"documentation/destinations/oracle/#password","title":"Password","text":"<p>Enter the password.</p>"},{"location":"documentation/destinations/oracle/#test-connection","title":"Test Connection","text":"<p>Check the database connection. </p>"},{"location":"documentation/destinations/oracle/#wallet","title":"Wallet","text":""},{"location":"documentation/destinations/oracle/#tns-name","title":"TNS Name","text":"<p>Enter the TNS name of the connection as it is stored in the <code>tnsnames.ora</code> file in your wallet. For more information, see Oracle Documentation: Download Client Credentials (Wallets).</p>"},{"location":"documentation/destinations/oracle/#username_1","title":"Username","text":"<p>Enter the user name.</p>"},{"location":"documentation/destinations/oracle/#password_1","title":"Password","text":"<p>Enter the password.</p>"},{"location":"documentation/destinations/oracle/#wallet-location","title":"Wallet location","text":"<p>Enter the path to your wallet, e.g., <code>C:\\Oracle\\Wallet</code>.  Note that the wallet location must be accessible for the user that runs the Xtract Universal service.</p>"},{"location":"documentation/destinations/oracle/#test-connection_1","title":"Test Connection","text":"<p>Check the database connection. </p>"},{"location":"documentation/destinations/oracle/#connect-descriptor","title":"Connect Descriptor","text":""},{"location":"documentation/destinations/oracle/#connect-descriptor_1","title":"Connect Descriptor","text":"<p>Enter a connect descriptor (connection string), see Oracle Documentation: View TNS Names and Connection Strings for an Autonomous Database Instance. A Connect Descriptor uses the following format:</p> <pre><code>(DESCRIPTION =\n(ADDRESS = (PROTOCOL = TCP)\n(HOST = [oracle host name])(PORT = [port number]))\n(CONNECT_DATA =\n(SERVER = DEDICATED)\n(SERVICE_NAME = [oracle service name])))\n</code></pre>"},{"location":"documentation/destinations/oracle/#username_2","title":"Username","text":"<p>Enter the user name.</p>"},{"location":"documentation/destinations/oracle/#password_2","title":"Password","text":"<p>Enter the password.</p>"},{"location":"documentation/destinations/oracle/#test-connection_2","title":"Test Connection","text":"<p>Check the database connection. </p>"},{"location":"documentation/destinations/oracle/#assign-the-oracle-destination-to-an-extraction","title":"Assign the Oracle Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/oracle/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/oracle/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/oracle/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/oracle/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/oracle/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/oracle/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/oracle/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/oracle/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/oracle/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/oracle/#preparation","title":"Preparation","text":"<p>Defines the action on the target database before the data is inserted into the target table.</p> Option Description Drop &amp; Create Remove table if available and create new table (default). Truncate Or Create Empty table if available, otherwise create. Create If Not Exists Create table if not available. Prepare Merge Prepares the merge process and creates e.g. a temporary staging table, see Merge Data. None No action. Custom SQL Here you can define your own script, see Custom SQL Statements. <p>To only create the table in the first step and not insert any data, you have two options:</p> <ol> <li>Copy the SQL statement and execute it directly on the target data database.</li> <li>Select the None option for Row Processing and execute the extraction.</li> </ol> <p>Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.</p>"},{"location":"documentation/destinations/oracle/#row-processing","title":"Row Processing","text":"<p>Defines how the data is inserted into the target table.</p> Option Description Insert Insert records (default). Fill merge staging table Insert records into the staging table. None No action. Custom SQL Define your own script, see Custom SQL Statements. Merge (deprecated) This option is obsolete, see Merge Data. Use the Fill merge staging table option."},{"location":"documentation/destinations/oracle/#finalization","title":"Finalization","text":"<p>Defines the action on the target database after the data has been successfully inserted into the target table.</p> Option Description Finalize Merge Closes the merge process and deletes the temporary staging table, for example. None No action (default). Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/oracle/#debugging","title":"Debugging","text":"<p>Warning</p> <p>Performance decrease! The performance decreases when bulk insert is disabled. Disable the bulk insert only when necessary, e.g., upon request of the support team.</p> <p>By activating the checkbox Disable bulk operations, the default bulk insert is deactivated when writing to a database.</p> <p>This option enables detailed error analysis, if certain data rows cannot be persisted on the database. Possible causes for the incorrect behavior are incorrect values with regard to the stored data type.</p> <p>Debugging needs to be deactivated again, after the successful error analysis, otherwise the performance of the database write processes remains low. </p> <p>Note</p> <p>Bulk operations are not supported when using Custom SQL statements, e.g., in Row Processing.  Bulk operations lead to performance decrease.  To increase performance when using Custom SQL statements, it is recommended to perform the custom processing in the Finalization step.</p>"},{"location":"documentation/destinations/oracle/#transaction-style","title":"Transaction style","text":"<p>Note</p> <p>The available options for Transaction Style vary depending on the destination.</p>"},{"location":"documentation/destinations/oracle/#one-transaction","title":"One Transaction","text":"<p>Preparation, Row Processing and Finalization are all performed in a single transaction.</p> <ul> <li>Advantage: clean rollback of all changes.</li> <li>Disadvantage: possibly extensive locking during the entire extraction period. </li> </ul> <p>Recommendation</p> <p>Only use One Transaction in combination with DML commands, e.g., \"truncate table\" and \"insert.  Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command.  Example: If a table is created in the preparation step, the opened \"OneTransaction\" is committed and a rollback in the next steps is not performed correctly.</p>"},{"location":"documentation/destinations/oracle/#three-transactions","title":"Three Transactions","text":"<p>Preparation, Row Processing and Finalization are each executed in a separate transaction.</p> <ul> <li>Advantage: clean rollback of the individual sections, possibly shorter locking phases than with One Transaction (e.g. with DDL in Preparation, the entire DB is only locked during preparation and not for the entire extraction duration). </li> <li>Disadvantage: no rollback of previous step possible (error in Row Processing only rolls back changes from Row Processing, but not Preparation). </li> </ul>"},{"location":"documentation/destinations/oracle/#rowprocessingonly","title":"RowProcessingOnly","text":"<p>Only Row Processing is executed in a transaction. Preparation and Finalization without an explicit transaction (implicit commits).</p> <ul> <li>Advantage: DDL in 'Preparation and Finalization* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)</li> <li>Disadvantage: no rollback of Preparation/Finalization.</li> </ul>"},{"location":"documentation/destinations/oracle/#no-transaction","title":"No Transaction","text":"<p>No explicit transactions.</p> <ul> <li>Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.</li> <li>Disadvantage: no rollback </li> </ul>"},{"location":"documentation/destinations/oracle/#merge-data","title":"Merge Data","text":"<p>The following example depicts the update of the existing data records in a database by running an extraction to merge data.  In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP. </p> <p>With a merge command, the updated value is written to the destination database table.  The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.</p> <p></p> <p>Tip</p> <p>Alternatively to merging, the data can be also updated by means of full load.  The full load method is less efficient.</p>"},{"location":"documentation/destinations/oracle/#prerequisites","title":"Prerequisites","text":"<p>You need a table with existing SAP data, in which new data can be merged. Ideally, the table with existing data is created in the initial load with the corresponding Preparation option and filled with data with the Row Processing option Insert.</p> <p>After the table is created, open SAP and change a field value in the SAP table that is used for the data merge. </p> <p>Warning</p> <p>Faulty merge.  Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the General Settings of the extraction type to execute the merge command.</p>"},{"location":"documentation/destinations/oracle/#merge-command","title":"Merge Command","text":"<p>The merge process is performed using a staging table and takes place in three steps:</p> <ol> <li>A temporary table is created.</li> <li>The data is inserted in the temporary table.</li> <li>The temporary table is merged with the target table and then the temporary table is deleted.</li> </ol> <p>Follow the steps below to set up the merge process in Xtract Universal:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions. </li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</li> <li>Make sure to assign Oracle destination to the extraction. </li> <li>Apply the following destination settings: </li> <li>Click [OK] and run the extraction.</li> </ol> <p>More information about the updated fields can be found in the SQL statement. It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update. Fields that do not appear in the SQL statement are not affected by changes. </p>"},{"location":"documentation/destinations/oracle/#custom-sql-statements","title":"Custom SQL Statements","text":"<p>The Oracle destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the Oracle destination:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination], the window \"Destination Settings\" opens.</li> <li>Make sure to assign the Oracle destination to the extraction.</li> <li> <p>Select the option Custom SQL from the drop-down list   in one of the following sections:</p> <ul> <li>Preparation</li> <li>Row Processing</li> <li>Finalization</li> </ul> <p></p> </li> <li> <p>Click [Edit SQL] . The window \"Edit SQL\" opens.</p> </li> <li>Enter your custom SQL statement and click [OK] to confirm your input.</li> </ol>"},{"location":"documentation/destinations/oracle/#use-templates","title":"Use Templates","text":"<p>Existing SQL commands can be used as templates. You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:</p> <ul> <li>Preparation, e.g., in Drop &amp; Create or Create if Not Exists</li> <li>Row Processing, e.g., in Insert or Merge</li> <li>Finalization</li> </ul> <p>Follow the steps below to generate a Custom SQL command from a template:</p> <ol> <li>In one of the staging steps, select the Custom SQL option from the drop-down list  .</li> <li>Click [Edit SQL] . The dialogue \"Edit SQL\" opens. </li> <li>Navigate to the drop-down menu and select an existing command  . </li> <li>Click [Generate Statement]. A new statement is generated. </li> <li>Click [Copy] to copy the statement to the clipboard.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>Check out the Microsoft SQL Server example for details on predefined expressions.</p> <p>Note</p> <p>The custom SQL code is used for SQL Server destinations.  A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.</p>"},{"location":"documentation/destinations/oracle/#use-script-expressions","title":"Use Script Expressions","text":"<p>You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported: </p> Input Description <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.TableName }#</pre> Name of the database table extracted data is written to. <pre>#{Extraction.RowsCount }#</pre> Count of the extracted rows. <pre>#{Extraction.RunState}#</pre> Status of the extraction (Running, FinishedNoErrors, FinishedErrors). <pre>#{(int)Extraction.RunState}#</pre> Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors). <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <p>For more information, see Script Expressions.</p> Example: Verify the existence of a table in a database using 'ExistsTable'<pre><code>#{\n   iif\n   (\n      ExistsTable(\"MAKT\"),\n      \"TRUNCATE TABLE \\\"MAKT\\\";\",\n      \"\n         CREATE TABLE \\\"MAKT\\\"(\n            \\\"MATNR\\\" VARCHAR(18),\n            \\\"SPRAS\\\" VARCHAR(2),\n            \\\"MAKTX\\\" VARCHAR(40));\n      \"\n   )\n}#\n</code></pre>"},{"location":"documentation/destinations/oracle/#create-a-status-overview","title":"Create a Status Overview","text":"<p>The table \"ExtractionStatistics\" provides an overview and status of the executed Xtract Universal extractions. To create the \"ExtractionStatistics\" table, create an SQL table according to the following example:</p> Create ExtractionStatistics<pre><code>CREATE TABLE [dbo].[ExtractionStatistics](\n    [TableName] [nchar](50) NULL,\n    [RowsCount] [int] NULL,\n    [Timestamp] [nchar](50) NULL,\n    [RunState] [nchar](50) NULL\n) ON [PRIMARY]\nGO\n</code></pre> <p>The ExtractionStatistics table is filled in the Finalization process step, using the following SQL statement:</p> Fill ExtractionStatistics<pre><code>INSERT INTO [ExtractionStatistics]\n(\n     [TableName], \n     [RowsCount], \n     [Timestamp],\n     [RunState]\n)\nVALUES\n(\n     '#{Extraction.TableName}#', \n     '#{Extraction.RowsCount}#', \n     '#{Extraction.Timestamp}#',\n     '#{Extraction.RunState}#'\n);\n</code></pre>"},{"location":"documentation/destinations/parquet/","title":"Flat File Parquet","text":"<p>This page shows how to set up and use the Flat File Parquet destination.  The Flat File Parquet destination loads data to a Parquet Database destination. </p>"},{"location":"documentation/destinations/parquet/#create-a-new-flat-file-parquet-destination","title":"Create a new Flat File Parquet Destination","text":"<p>Follow the steps below to add a new Flat File Parquet destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Flat File Parquet from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/parquet/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/parquet/#output-directory","title":"Output directory","text":"<p>Enter the folder path to save the destination flat files in.  If the entered folder does not exist, a new folder is created.</p> <p>Note</p> <p>To write flat files to a network drive, you need to:</p> <ul> <li>Enter the File output path in UNC format e.g., <code>\\\\Server2\\Share\\Folder1</code>.</li> <li>Run the Xtract Universal service by a user with write permission to the directory. </li> </ul>"},{"location":"documentation/destinations/parquet/#compatibility-mode","title":"Compatibility mode","text":"<p>The following compatibility modes are available:</p> <ul> <li>Pure </li> <li>Spark (does not support the data types used in pure mode, so other data types need to be used)</li> <li>BigQuery (formats columns names to be compatible with Google BigQuery, see BigQuery Documentation: Column Names)</li> </ul> SAP Pure / BigQuery Spark INT1 UINT_8 INT16 TIMS TIME_MILLIS UTF8"},{"location":"documentation/destinations/parquet/#assign-the-flat-file-parquet-destination-to-an-extraction","title":"Assign the Flat File Parquet Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/parquet/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/parquet/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/parquet/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/parquet/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/parquet/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/parquet/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/parquet/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/parquet/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/parquet/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/parquet/#existing-files","title":"Existing files","text":"<p>If a flat file by the same name already exists in the target directory, the following actions can be taken:</p> Option Description Replace file The export process overwrites existing files. Abort extraction The process is aborted, if the file already exists."},{"location":"documentation/destinations/postgreSQL/","title":"PostgreSQL","text":"<p>This page shows how to set up and use the PostgreSQL destination.  The PostgreSQL destination loads data to an EXASolution database.</p>"},{"location":"documentation/destinations/postgreSQL/#requirements","title":"Requirements","text":"<p>Data is pushed into the PostgreSQL DB system through the Npgsql data provider version 8.1.  The data provider is provided with the setup of Xtract Universal. </p> <p>Xtract Universal supports all PostgreSQL versions compatible with Npgsql. For more information, see PostgreSQL: Versioning.</p>"},{"location":"documentation/destinations/postgreSQL/#tls-encryption-with-postgresql","title":"TLS Encryption with PostgreSQL","text":"<p>Requirements for using TLS encryption with PostgreSQL:</p> <ul> <li>Xtract Universal and the Npgsql.dll driver must be up-to-date and must support TLS with new PostgreSQL versions. If necessary, install the newest version of Xtract Universal.</li> <li>the certificate for the authentication must be valid.</li> <li>the Subject Alternative Name of the certificate must be used as the PostgreSQL host, see Private endpoint.</li> <li>the certification authority (CA) that signed the certificate and the certificate itself must be trustworthy, see PostgreSQL Documentation: Secure TCP/IP Connections with SSL.</li> </ul>"},{"location":"documentation/destinations/postgreSQL/#create-a-new-postgresql-destination","title":"Create a new PostgreSQL Destination","text":"<p>Follow the steps below to add a new PostgreSQL destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type PostgreSQL from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/postgreSQL/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/postgreSQL/#server-name","title":"Server Name","text":"<p>Name or IP of the database server.</p>"},{"location":"documentation/destinations/postgreSQL/#tls-mode","title":"TLS Mode","text":"<p>The following TLS modes can be selected for the connection:</p> Mode Description Prefer Default Value that uses TLS encryption, if supported by the server. If this is not the case, unencrypted connections are used. Require Xtract Universal forces an encrypted TLS connection to the PostgreSQL server - unencrypted connections are not established. Disable Caution! An unencrypted, insecure connection is established to the PostgreSQL server. <p>Make sure that the Certification authority (CA) that signed the certificate or the certificate itself is trusted by the client. For more information, see Secure TCP/IP Connections with SSL.</p>"},{"location":"documentation/destinations/postgreSQL/#private-endpoint","title":"Private endpoint","text":"<p>This field is optional. You can enter an alternative hostname under which a connection is established.</p> <p>Example: The PostgreSQL database is hosted on a cloud and access to the database's cloud domain name is restricted by company policy.  In this case the database can be accessed through a private endpoint. Enter the private endpoint in this field.  The PostgreSQL cloud domain name must be entered in the field Server Name for certificate validation.</p>"},{"location":"documentation/destinations/postgreSQL/#port","title":"Port","text":"<p>Port of the database server. Port 5432 is selected by default.</p>"},{"location":"documentation/destinations/postgreSQL/#windows-authentication","title":"Windows Authentication","text":"<p>Uses the service account, under which the XU service is running, for authentication against the PostgreSQL server, see PostgreSQL Documentation: Client authentication.</p> <p>Note</p> <p>To successfully connect to the database using Windows authentication, make sure to run the XU service under a Windows AD user with access to the database.</p>"},{"location":"documentation/destinations/postgreSQL/#username","title":"Username","text":"<p>Enter the name of the database user.</p>"},{"location":"documentation/destinations/postgreSQL/#password","title":"Password","text":"<p>Enter the password of the database user.</p>"},{"location":"documentation/destinations/postgreSQL/#database","title":"Database","text":"<p>Enter the name of the database.</p>"},{"location":"documentation/destinations/postgreSQL/#test-connection","title":"Test Connection","text":"<p>Test the database connection.</p> <p>Warning</p> <p>The remote certificate is invalid according to the validation procedure  When using TLS encryption, this error message can have multiple causes e.g. invalid or untrustworthy certificates.  See TLS Encryption with PostgreSQL for information.</p>"},{"location":"documentation/destinations/postgreSQL/#assign-the-postgresql-destination-to-an-extraction","title":"Assign the PostgreSQL Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/postgreSQL/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/postgreSQL/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/postgreSQL/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/postgreSQL/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/postgreSQL/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/postgreSQL/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/postgreSQL/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/postgreSQL/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/postgreSQL/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/postgreSQL/#preparation","title":"Preparation","text":"<p>Defines the action on the target database before the data is inserted into the target table.</p> Option Description Drop &amp; Create Remove table if available and create new table (default). Truncate Or Create Empty table if available, otherwise create. Create If Not Exists Create table if not available. Prepare Merge Prepares the merge process and creates e.g. a temporary staging table, see Merge Data. None No action. Custom SQL Here you can define your own script, see Custom SQL Statements. <p>To only create the table in the first step and not insert any data, you have two options:</p> <ol> <li>Copy the SQL statement and execute it directly on the target data database.</li> <li>Select the None option for Row Processing and execute the extraction.</li> </ol> <p>Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.</p>"},{"location":"documentation/destinations/postgreSQL/#row-processing","title":"Row Processing","text":"<p>Defines how the data is inserted into the target table.</p> Option Description Insert Insert records (default). Fill merge staging table Insert records into the staging table. None No action. Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/postgreSQL/#finalization","title":"Finalization","text":"<p>Defines the action on the target database after the data has been successfully inserted into the target table.</p> Option Description Finalize Merge Closes the merge process and deletes the temporary staging table, for example. None No action (default). Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/postgreSQL/#debugging","title":"Debugging","text":"<p>Warning</p> <p>Performance decrease! The performance decreases when bulk insert is disabled. Disable the bulk insert only when necessary, e.g., upon request of the support team.</p> <p>By activating the checkbox Disable bulk operations, the default bulk insert is deactivated when writing to a database.</p> <p>This option enables detailed error analysis, if certain data rows cannot be persisted on the database. Possible causes for the incorrect behavior are incorrect values with regard to the stored data type.</p> <p>Debugging needs to be deactivated again, after the successful error analysis, otherwise the performance of the database write processes remains low. </p> <p>Note</p> <p>Bulk operations are not supported when using Custom SQL statements, e.g., in Row Processing.  Bulk operations lead to performance decrease.  To increase performance when using Custom SQL statements, it is recommended to perform the custom processing in the Finalization step.</p>"},{"location":"documentation/destinations/postgreSQL/#transaction-style","title":"Transaction style","text":"<p>Note</p> <p>The available options for Transaction Style vary depending on the destination.</p>"},{"location":"documentation/destinations/postgreSQL/#one-transaction","title":"One Transaction","text":"<p>Preparation, Row Processing and Finalization are all performed in a single transaction.</p> <ul> <li>Advantage: clean rollback of all changes.</li> <li>Disadvantage: possibly extensive locking during the entire extraction period. </li> </ul> <p>Recommendation</p> <p>Only use One Transaction in combination with DML commands, e.g., \"truncate table\" and \"insert.  Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command.  Example: If a table is created in the preparation step, the opened \"OneTransaction\" is committed and a rollback in the next steps is not performed correctly.</p>"},{"location":"documentation/destinations/postgreSQL/#three-transactions","title":"Three Transactions","text":"<p>Preparation, Row Processing and Finalization are each executed in a separate transaction.</p> <ul> <li>Advantage: clean rollback of the individual sections, possibly shorter locking phases than with One Transaction (e.g. with DDL in Preparation, the entire DB is only locked during preparation and not for the entire extraction duration). </li> <li>Disadvantage: no rollback of previous step possible (error in Row Processing only rolls back changes from Row Processing, but not Preparation). </li> </ul>"},{"location":"documentation/destinations/postgreSQL/#rowprocessingonly","title":"RowProcessingOnly","text":"<p>Only Row Processing is executed in a transaction. Preparation and Finalization without an explicit transaction (implicit commits).</p> <ul> <li>Advantage: DDL in 'Preparation and Finalization* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)</li> <li>Disadvantage: no rollback of Preparation/Finalization.</li> </ul>"},{"location":"documentation/destinations/postgreSQL/#no-transaction","title":"No Transaction","text":"<p>No explicit transactions.</p> <ul> <li>Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.</li> <li>Disadvantage: no rollback </li> </ul>"},{"location":"documentation/destinations/postgreSQL/#merge-data","title":"Merge Data","text":"<p>The following example depicts the update of the existing data records in a database by running an extraction to merge data.  In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP. </p> <p>With a merge command, the updated value is written to the destination database table.  The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.</p> <p></p> <p>Tip</p> <p>Alternatively to merging, the data can be also updated by means of full load.  The full load method is less efficient.</p>"},{"location":"documentation/destinations/postgreSQL/#prerequisites","title":"Prerequisites","text":"<p>You need a table with existing SAP data, in which new data can be merged. Ideally, the table with existing data is created in the initial load with the corresponding Preparation option and filled with data with the Row Processing option Insert.</p> <p>After the table is created, open SAP and change a field value in the SAP table that is used for the data merge. </p> <p>Warning</p> <p>Faulty merge.  Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the General Settings of the extraction type to execute the merge command.</p>"},{"location":"documentation/destinations/postgreSQL/#merge-command","title":"Merge Command","text":"<p>The merge process is performed using a staging table and takes place in three steps:</p> <ol> <li>A temporary table is created.</li> <li>The data is inserted in the temporary table.</li> <li>The temporary table is merged with the target table and then the temporary table is deleted.</li> </ol> <p>Follow the steps below to set up the merge process in Xtract Universal:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions. </li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</li> <li>Make sure to assign PostgreSQL destination to the extraction. </li> <li>Apply the following destination settings: </li> <li>Click [OK] and run the extraction.</li> </ol> <p>More information about the updated fields can be found in the SQL statement. It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update. Fields that do not appear in the SQL statement are not affected by changes. </p>"},{"location":"documentation/destinations/postgreSQL/#custom-sql-statements","title":"Custom SQL Statements","text":"<p>The PostgreSQL destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the PostgreSQL destination:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination], the window \"Destination Settings\" opens.</li> <li>Make sure to assign the PostgreSQL destination to the extraction.</li> <li> <p>Select the option Custom SQL from the drop-down list   in one of the following sections:</p> <ul> <li>Preparation</li> <li>Row Processing</li> <li>Finalization</li> </ul> <p></p> </li> <li> <p>Click [Edit SQL] . The window \"Edit SQL\" opens.</p> </li> <li>Enter your custom SQL statement and click [OK] to confirm your input.</li> </ol>"},{"location":"documentation/destinations/postgreSQL/#use-templates","title":"Use Templates","text":"<p>Existing SQL commands can be used as templates. You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:</p> <ul> <li>Preparation, e.g., in Drop &amp; Create or Create if Not Exists</li> <li>Row Processing, e.g., in Insert or Merge</li> <li>Finalization</li> </ul> <p>Follow the steps below to generate a Custom SQL command from a template:</p> <ol> <li>In one of the staging steps, select the Custom SQL option from the drop-down list  .</li> <li>Click [Edit SQL] . The dialogue \"Edit SQL\" opens. </li> <li>Navigate to the drop-down menu and select an existing command  . </li> <li>Click [Generate Statement]. A new statement is generated. </li> <li>Click [Copy] to copy the statement to the clipboard.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>Check out the Microsoft SQL Server example for details on predefined expressions.</p> <p>Note</p> <p>The custom SQL code is used for SQL Server destinations.  A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.</p>"},{"location":"documentation/destinations/postgreSQL/#use-script-expressions","title":"Use Script Expressions","text":"<p>You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported: </p> Input Description <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.TableName }#</pre> Name of the database table extracted data is written to. <pre>#{Extraction.RowsCount }#</pre> Count of the extracted rows. <pre>#{Extraction.RunState}#</pre> Status of the extraction (Running, FinishedNoErrors, FinishedErrors). <pre>#{(int)Extraction.RunState}#</pre> Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors). <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <p>For more information, see Script Expressions.</p> Example: Verify the existence of a table in a database using 'ExistsTable'<pre><code>#{\n   iif\n   (\n      ExistsTable(\"MAKT\"),\n      \"TRUNCATE TABLE \\\"MAKT\\\";\",\n      \"\n         CREATE TABLE \\\"MAKT\\\"(\n            \\\"MATNR\\\" VARCHAR(18),\n            \\\"SPRAS\\\" VARCHAR(2),\n            \\\"MAKTX\\\" VARCHAR(40));\n      \"\n   )\n}#\n</code></pre>"},{"location":"documentation/destinations/postgreSQL/#create-a-status-overview","title":"Create a Status Overview","text":"<p>The table \"ExtractionStatistics\" provides an overview and status of the executed Xtract Universal extractions. To create the \"ExtractionStatistics\" table, create an SQL table according to the following example:</p> Create ExtractionStatistics<pre><code>CREATE TABLE [dbo].[ExtractionStatistics](\n    [TableName] [nchar](50) NULL,\n    [RowsCount] [int] NULL,\n    [Timestamp] [nchar](50) NULL,\n    [RunState] [nchar](50) NULL\n) ON [PRIMARY]\nGO\n</code></pre> <p>The ExtractionStatistics table is filled in the Finalization process step, using the following SQL statement:</p> Fill ExtractionStatistics<pre><code>INSERT INTO [ExtractionStatistics]\n(\n     [TableName], \n     [RowsCount], \n     [Timestamp],\n     [RunState]\n)\nVALUES\n(\n     '#{Extraction.TableName}#', \n     '#{Extraction.RowsCount}#', \n     '#{Extraction.Timestamp}#',\n     '#{Extraction.RunState}#'\n);\n</code></pre>"},{"location":"documentation/destinations/qliksense-qlikview/","title":"QlikSense & QlikView","text":"<p>This page shows how to set up and use the QlikSense &amp; QlikView destination.  The QlikSense &amp; QlikView destination loads data to QlikSense or QlikView.</p>"},{"location":"documentation/destinations/qliksense-qlikview/#about","title":"About","text":"<p>The QlikSense &amp; QlikView destination generates a data load script, that needs to be inserted into the data load editor of your Qlik application.  Depending on whether you run a QlikSense of QlikView application, Xtract Univeral creates a different data load script.</p> <p>Running the Qlik application triggers the respective Xtract Universal extraction via the data load script.  Xtract Universal sends the extracted SAP data through an HTTP-CSV stream directly to your Qlik application.</p>"},{"location":"documentation/destinations/qliksense-qlikview/#create-a-new-qliksense-qlikview-destination","title":"Create a new QlikSense &amp; QlikView Destination","text":"<p>Follow the steps below to add a new QlikSense &amp; QlikView destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type QlikSense &amp; QlikView from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/qliksense-qlikview/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination. To use the QlikSense &amp; QlikView destination, no further settings are necessary.</p> <p></p>"},{"location":"documentation/destinations/qliksense-qlikview/#assign-the-qliksense-qlikview-destination-to-an-extraction","title":"Assign the QlikSense &amp; QlikView Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/qliksense-qlikview/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/qliksense-qlikview/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/qliksense-qlikview/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/qliksense-qlikview/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/qliksense-qlikview/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/qliksense-qlikview/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/qliksense-qlikview/#generate-a-data-load-script","title":"Generate a Data Load Script","text":"<p>Follow the steps below to generate a data load script in Xtract Universal. The data load script is required to run extractions in your Qlik application.</p> <ol> <li>In the main window of the Designer, select an extraction from the list of extractions.</li> <li>Make sure the extraction has the QlikSense &amp; QlikView destination assigned to it.</li> <li>Click [Run]. The window \"Run Extraction\" opens. </li> <li>Optional (when using QlikSense): In the Parameters section, mark the check boxes for the parameters that you want to add as SET variables in the QlikSense script.</li> <li>Click [Generate Qlik Script] to generate a data load script. The window \"Script\" opens.</li> <li>Select the QlikView Script or QlikSense Script tab.</li> <li>Click [Copy to Clipboard] to copy the script. </li> </ol> <p>When using QlikView, paste the copied script into the QlikView data load editor. For QlikSense, see Run Extractions from QlikSense.</p> <p>Note</p> <p>The \"SET methods\" cannot be edited in the \"Script\" window. Edit the SET methods in the Qlik data load editor. </p>"},{"location":"documentation/destinations/qliksense-qlikview/#run-extractions-from-qliksense","title":"Run Extractions from QlikSense","text":"<p>Before copying the Qlik script generated by Xtract Universal to QlikSense, perform the following steps in QlikSense:</p> <ol> <li> <p>Create a new data connection of type REST. </p> <p>Note</p> <p>In QlikSense the default value for the Timeout is 30 seconds.  Increase the timeout to a sufficiently high value if the time till the first data package arrives from SAP is higher than 30 seconds.  The maximum input value is 10.000 seconds.</p> </li> <li> <p>Enter the URL of the Xtract Universal Server and port into the URL text field. In the depicted example, the Xtract Universal server runs on <code>http://localhost:8065/</code>. </p> </li> <li>Enter Xtract_Universal into the name text field.</li> <li>Activate the security option Allow response headers. This option ensures that error messages from Xtract Universal are passed to QlikSense. </li> <li>Paste the QlikSense script from Xtract Universal into the QlikSense Data load editor. </li> </ol> <p>Warning</p> <p>Response headers are denied by the current connection. Please edit your connection in order to enable response headers loading. When this error message is displayed in the REST connection, activate the option \"Allow response headers\" in the Security settings of the connector.</p>"},{"location":"documentation/destinations/qliksense-qlikview/#about-the-qliksense-data-load-script","title":"About the QlikSense Data Load Script","text":"<p>Xtract Universal creates a QlikSense script with the following properties:</p> <ul> <li>The script uses QlikSense interpretation functions Num#, Text, Date and Time.  For fields, where an adequate data type can't be determined, no interpretation function is used.</li> <li>The field description and the SAP origin of the field are assigned as tags to all fields.</li> <li>All date fields with <code>$date</code> are explicitly tagged. This function assures that fields containing a date before January 1, 1980 are recognized as date fields in QlikSense.</li> <li>The usage of Xtract Universal Extraction Parameters is supported.  To make parameters available in the QlikSense script, activate the parameters in the \"Run Extraction\" window of Xtract Universal. </li> </ul> <p>Note</p> <p>Do not change the assigned value of variables xuOriginDateFormat and xuOriginTimeFormat. The chosen format enables Xtract Universal to send the data of date and time fields to QlikSense. Changing the format stops the QlikSense script from running.</p>"},{"location":"documentation/destinations/salesforce/","title":"Salesforce","text":"<p>This page shows how to set up and use the Salesforce destination.  The Salesforce destination loads data to Salesforce.</p>"},{"location":"documentation/destinations/salesforce/#requirements","title":"Requirements","text":"<p>Salesforce Edition This destination requires one of the following Salesforce editions:</p> <ul> <li>Enterprise</li> <li>Unlimited</li> <li>Performance Edition The Salesforce edition has to include the Integration via web service API feature.</li> </ul> <p>Permissions To run an extraction the API-Enabled permission is needed. If the object has to be created the user also needs following permissions:</p> <ul> <li>Modify All Data</li> <li>Customize Application</li> <li>Manage Profiles and Permission Sets</li> </ul>"},{"location":"documentation/destinations/salesforce/#create-a-new-salesforce-destination","title":"Create a new Salesforce Destination","text":"<p>Follow the steps below to add a new Salesforce destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Salesforce from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/salesforce/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/salesforce/#username","title":"Username","text":"<p>Enter your Salesforce username.</p>"},{"location":"documentation/destinations/salesforce/#password","title":"Password","text":"<p>Enter the corresponding password.</p>"},{"location":"documentation/destinations/salesforce/#security-token","title":"Security Token","text":"<p>Enter the Security Token that was generated by Salesforce and is used to access API functions. </p>"},{"location":"documentation/destinations/salesforce/#reset-security-token","title":"Reset Security Token","text":"<p>Opens a link to the website where you can reset your current Security Token. To reset your security token on Salesforce, at the top navigation bar go to your name &gt; Setup &gt; Personal Setup &gt; My Personal Information &gt; Reset My Security Token.</p>"},{"location":"documentation/destinations/salesforce/#test-connection","title":"Test Connection","text":"<p>Check the database connection. </p>"},{"location":"documentation/destinations/salesforce/#assign-the-salesforce-destination-to-an-extraction","title":"Assign the Salesforce Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/salesforce/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/salesforce/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/salesforce/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/salesforce/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/salesforce/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/salesforce/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/salesforce/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/salesforce/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/salesforce/#preparation","title":"Preparation","text":"Option Description None No action. Delete &amp; Create Deletes the object with the specified name and creates a new one. Create If Not Exists Creates a new object if no object with the specified name could be found."},{"location":"documentation/destinations/salesforce/#row-processing","title":"Row Processing","text":"Option Description None No action. Insert Inserts all records into the specified object. Merge Inserts all records into the specified object and updates already existing entries."},{"location":"documentation/destinations/salesforce/#concurrency-mode","title":"Concurrency Mode","text":""},{"location":"documentation/destinations/salesforce/#parallel","title":"Parallel","text":"<p>Process batches in parallel mode. This is the default value.</p>"},{"location":"documentation/destinations/salesforce/#serial","title":"Serial","text":"<p>Process batches in serial mode. Processing in parallel can cause database contention.  When this is severe, the job may fail.  If you experience this issue, submit the job with serial concurrency mode.  This guarantees that batches are processed one at a time.  Note that using this option may significantly increase the processing time for a job.</p>"},{"location":"documentation/destinations/sap-hana/","title":"SAP HANA","text":"<p>This page shows how to set up and use the SAP HANA destination.  The SAP HANA destination loads data to an SAP HANA database or to an SAP Data Warehouse Cloud.</p>"},{"location":"documentation/destinations/sap-hana/#requirements","title":"Requirements","text":"<p>To establish a connection to the HANA database or SAP Data Warehouse Cloud, the SAP HANA Data Provider for Microsoft ADO.NET version 2.17.22 is required The data provider is part of the SAP HANA Client setup.</p>"},{"location":"documentation/destinations/sap-hana/#create-a-new-sap-hana-destination","title":"Create a new SAP HANA Destination","text":"<p>Follow the steps below to add a new SAP HANA destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type SAP HANA from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/sap-hana/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination. The destination settings can be defined for the HANA database or SAP Data Warehouse Cloud (DWC) connection:</p> SAP HANA DatabaseSAP HANA Data Warehouse Cloud <p></p> <p></p>"},{"location":"documentation/destinations/sap-hana/#server-name","title":"Server Name","text":"<p>Enter the address of the server (including the port number). Note the different port numbers for HANA and DWC (see screenshots).</p>"},{"location":"documentation/destinations/sap-hana/#user-name","title":"User Name","text":"<p>Enter the SAP HANA/DWC user name. </p>"},{"location":"documentation/destinations/sap-hana/#password","title":"Password","text":"<p>Enter the user password.</p>"},{"location":"documentation/destinations/sap-hana/#database","title":"Database","text":"<p>Enter the name of the database.</p>"},{"location":"documentation/destinations/sap-hana/#schema","title":"Schema","text":"<p>Enter the name of the database schema.</p>"},{"location":"documentation/destinations/sap-hana/#use-encryption","title":"Use encryption","text":"<p>Activates connection encryption. This is required when connecting to SAP DWC.</p>"},{"location":"documentation/destinations/sap-hana/#test-connection","title":"Test Connection","text":"<p>Check the database connection.  </p>"},{"location":"documentation/destinations/sap-hana/#assign-the-sap-hana-destination-to-an-extraction","title":"Assign the SAP HANA Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/sap-hana/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/sap-hana/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/sap-hana/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/sap-hana/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/sap-hana/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/sap-hana/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/sap-hana/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/sap-hana/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/sap-hana/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/sap-hana/#preparation","title":"Preparation","text":"<p>Defines the action on the target database before the data is inserted into the target table.</p> Option Description Drop &amp; Create Remove table if available and create new table (default). Truncate Or Create Empty table if available, otherwise create. Create If Not Exists Create table if not available. Prepare Merge Prepares the merge process and creates e.g. a temporary staging table, see Merge Data. None No action. Custom SQL Here you can define your own script, see Custom SQL Statements. <p>To only create the table in the first step and not insert any data, you have two options:</p> <ol> <li>Copy the SQL statement and execute it directly on the target data database.</li> <li>Select the None option for Row Processing and execute the extraction.</li> </ol> <p>Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.</p>"},{"location":"documentation/destinations/sap-hana/#row-processing","title":"Row Processing","text":"<p>Defines how the data is inserted into the target table.</p> Option Description Insert Insert records (default). Fill merge staging table Insert records into the staging table. None No action. Custom SQL Define your own script, see Custom SQL Statements. Upsert (deprecated) This option is obsolete, see Merge Data. Use the Fill merge staging table option."},{"location":"documentation/destinations/sap-hana/#finalization","title":"Finalization","text":"<p>Defines the action on the target database after the data has been successfully inserted into the target table.</p> Option Description Finalize Merge Closes the merge process and deletes the temporary staging table, for example. None No action (default). Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/sap-hana/#debugging","title":"Debugging","text":"<p>Warning</p> <p>Performance decrease! The performance decreases when bulk insert is disabled. Disable the bulk insert only when necessary, e.g., upon request of the support team.</p> <p>By activating the checkbox Disable bulk operations, the default bulk insert is deactivated when writing to a database.</p> <p>This option enables detailed error analysis, if certain data rows cannot be persisted on the database. Possible causes for the incorrect behavior are incorrect values with regard to the stored data type.</p> <p>Debugging needs to be deactivated again, after the successful error analysis, otherwise the performance of the database write processes remains low. </p> <p>Note</p> <p>Bulk operations are not supported when using Custom SQL statements, e.g., in Row Processing.  Bulk operations lead to performance decrease.  To increase performance when using Custom SQL statements, it is recommended to perform the custom processing in the Finalization step.</p>"},{"location":"documentation/destinations/sap-hana/#transaction-style","title":"Transaction style","text":"<p>Note</p> <p>The available options for Transaction Style vary depending on the destination.</p>"},{"location":"documentation/destinations/sap-hana/#one-transaction","title":"One Transaction","text":"<p>Preparation, Row Processing and Finalization are all performed in a single transaction.</p> <ul> <li>Advantage: clean rollback of all changes.</li> <li>Disadvantage: possibly extensive locking during the entire extraction period. </li> </ul> <p>Recommendation</p> <p>Only use One Transaction in combination with DML commands, e.g., \"truncate table\" and \"insert.  Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command.  Example: If a table is created in the preparation step, the opened \"OneTransaction\" is committed and a rollback in the next steps is not performed correctly.</p>"},{"location":"documentation/destinations/sap-hana/#three-transactions","title":"Three Transactions","text":"<p>Preparation, Row Processing and Finalization are each executed in a separate transaction.</p> <ul> <li>Advantage: clean rollback of the individual sections, possibly shorter locking phases than with One Transaction (e.g. with DDL in Preparation, the entire DB is only locked during preparation and not for the entire extraction duration). </li> <li>Disadvantage: no rollback of previous step possible (error in Row Processing only rolls back changes from Row Processing, but not Preparation). </li> </ul>"},{"location":"documentation/destinations/sap-hana/#rowprocessingonly","title":"RowProcessingOnly","text":"<p>Only Row Processing is executed in a transaction. Preparation and Finalization without an explicit transaction (implicit commits).</p> <ul> <li>Advantage: DDL in 'Preparation and Finalization* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)</li> <li>Disadvantage: no rollback of Preparation/Finalization.</li> </ul>"},{"location":"documentation/destinations/sap-hana/#no-transaction","title":"No Transaction","text":"<p>No explicit transactions.</p> <ul> <li>Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.</li> <li>Disadvantage: no rollback </li> </ul>"},{"location":"documentation/destinations/sap-hana/#merge-data","title":"Merge Data","text":"<p>The following example depicts the update of the existing data records in a database by running an extraction to merge data.  In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP. </p> <p>With a merge command, the updated value is written to the destination database table.  The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.</p> <p></p> <p>Tip</p> <p>Alternatively to merging, the data can be also updated by means of full load.  The full load method is less efficient.</p>"},{"location":"documentation/destinations/sap-hana/#prerequisites","title":"Prerequisites","text":"<p>You need a table with existing SAP data, in which new data can be merged. Ideally, the table with existing data is created in the initial load with the corresponding Preparation option and filled with data with the Row Processing option Insert.</p> <p>After the table is created, open SAP and change a field value in the SAP table that is used for the data merge. </p> <p>Warning</p> <p>Faulty merge.  Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the General Settings of the extraction type to execute the merge command.</p>"},{"location":"documentation/destinations/sap-hana/#merge-command","title":"Merge Command","text":"<p>The merge process is performed using a staging table and takes place in three steps:</p> <ol> <li>A temporary table is created.</li> <li>The data is inserted in the temporary table.</li> <li>The temporary table is merged with the target table and then the temporary table is deleted.</li> </ol> <p>Follow the steps below to set up the merge process in Xtract Universal:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions. </li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</li> <li>Make sure to assign SAP HANA destination to the extraction. </li> <li>Apply the following destination settings: </li> <li>Click [OK] and run the extraction.</li> </ol> <p>More information about the updated fields can be found in the SQL statement. It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update. Fields that do not appear in the SQL statement are not affected by changes. </p>"},{"location":"documentation/destinations/sap-hana/#custom-sql-statements","title":"Custom SQL Statements","text":"<p>The SAP HANA destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the SAP HANA destination:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination], the window \"Destination Settings\" opens.</li> <li>Make sure to assign the SAP HANA destination to the extraction.</li> <li> <p>Select the option Custom SQL from the drop-down list   in one of the following sections:</p> <ul> <li>Preparation</li> <li>Row Processing</li> <li>Finalization</li> </ul> <p></p> </li> <li> <p>Click [Edit SQL] . The window \"Edit SQL\" opens.</p> </li> <li>Enter your custom SQL statement and click [OK] to confirm your input.</li> </ol>"},{"location":"documentation/destinations/sap-hana/#use-templates","title":"Use Templates","text":"<p>Existing SQL commands can be used as templates. You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:</p> <ul> <li>Preparation, e.g., in Drop &amp; Create or Create if Not Exists</li> <li>Row Processing, e.g., in Insert or Merge</li> <li>Finalization</li> </ul> <p>Follow the steps below to generate a Custom SQL command from a template:</p> <ol> <li>In one of the staging steps, select the Custom SQL option from the drop-down list  .</li> <li>Click [Edit SQL] . The dialogue \"Edit SQL\" opens. </li> <li>Navigate to the drop-down menu and select an existing command  . </li> <li>Click [Generate Statement]. A new statement is generated. </li> <li>Click [Copy] to copy the statement to the clipboard.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>Check out the Microsoft SQL Server example for details on predefined expressions.</p> <p>Note</p> <p>The custom SQL code is used for SQL Server destinations.  A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.</p>"},{"location":"documentation/destinations/sap-hana/#use-script-expressions","title":"Use Script Expressions","text":"<p>You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported: </p> Input Description <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.TableName }#</pre> Name of the database table extracted data is written to. <pre>#{Extraction.RowsCount }#</pre> Count of the extracted rows. <pre>#{Extraction.RunState}#</pre> Status of the extraction (Running, FinishedNoErrors, FinishedErrors). <pre>#{(int)Extraction.RunState}#</pre> Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors). <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <p>For more information, see Script Expressions.</p> Example: Verify the existence of a table in a database using 'ExistsTable'<pre><code>#{\n   iif\n   (\n      ExistsTable(\"MAKT\"),\n      \"TRUNCATE TABLE \\\"MAKT\\\";\",\n      \"\n         CREATE TABLE \\\"MAKT\\\"(\n            \\\"MATNR\\\" VARCHAR(18),\n            \\\"SPRAS\\\" VARCHAR(2),\n            \\\"MAKTX\\\" VARCHAR(40));\n      \"\n   )\n}#\n</code></pre>"},{"location":"documentation/destinations/sap-hana/#create-a-status-overview","title":"Create a Status Overview","text":"<p>The table \"ExtractionStatistics\" provides an overview and status of the executed Xtract Universal extractions. To create the \"ExtractionStatistics\" table, create an SQL table according to the following example:</p> Create ExtractionStatistics<pre><code>CREATE TABLE [dbo].[ExtractionStatistics](\n    [TableName] [nchar](50) NULL,\n    [RowsCount] [int] NULL,\n    [Timestamp] [nchar](50) NULL,\n    [RunState] [nchar](50) NULL\n) ON [PRIMARY]\nGO\n</code></pre> <p>The ExtractionStatistics table is filled in the Finalization process step, using the following SQL statement:</p> Fill ExtractionStatistics<pre><code>INSERT INTO [ExtractionStatistics]\n(\n     [TableName], \n     [RowsCount], \n     [Timestamp],\n     [RunState]\n)\nVALUES\n(\n     '#{Extraction.TableName}#', \n     '#{Extraction.RowsCount}#', \n     '#{Extraction.Timestamp}#',\n     '#{Extraction.RunState}#'\n);\n</code></pre>"},{"location":"documentation/destinations/sap-hana/#related-links","title":"Related Links","text":"<ul> <li>SAP Help: SAP HANA Data Provider</li> <li>SAP HANA Client Setup</li> </ul>"},{"location":"documentation/destinations/server-report-services/","title":"Power BI Report Server (SQL Server Reporting Services)","text":"<p>This page shows how to set up and use the Power BI Report Server (SQL Server Reporting Services) destination.  The Power BI Report Server (SQL Server Reporting Services) destination loads data in Power BI Report Server (SSRS).</p>"},{"location":"documentation/destinations/server-report-services/#requirements","title":"Requirements","text":"<p>The Power BI Report Server destination requires the following components:</p> <ul> <li>Visual Studio 2017 or higher</li> <li>the Microsoft Reporting Services Projects plugin (version 2.6.11 or higher) for Visual Studio.</li> <li>Power BI Report Server (January 2020 or later)</li> <li>Xtract Universal (version 4.29 or higher)</li> <li>To use Transport Layer Security, it is necessary to modify the Registry of the machine that runs the SSRS server according to the Microsoft Documentation: Configure Strong cryptography.</li> </ul> <p>Note</p> <p>The Power BI Report Builder is not supported.</p>"},{"location":"documentation/destinations/server-report-services/#installation","title":"Installation","text":"<p>To use the Power BI Report Server destination, install the Microsoft Reporting Services Projects plugin in Visual Studio.  After the installation is complete, close Visual Studio.</p> <p>Microsoft Power BI Report Server (SQL Server Reporting Services) supports a wide variety of data sources out of the box. To add Xtract Universal to the list of data sources, install the Xtract Universal Report Server Plugin for Visual Studio and the Report Server. The plugin must be installed on both environments to consume the data extracted by Xtract Universal, see graphic below:</p> <p></p> <p>To complete the installation close all Visual Studio windows.</p> <p>Note</p> <p>Make sure to install the latest version of the Microsoft Reporting Services Projects plugin and Xtract Universal.  </p>"},{"location":"documentation/destinations/server-report-services/#installation-of-the-xtract-universal-report-server-plugin","title":"Installation of the Xtract Universal Report Server Plugin","text":"<p>The Xtract Universal Report Server Plugin can be installed as part of the Xtract Universal Setup. To install the Xtract Universal Report Server Plugin on multiple environments without installing the Xtract Universal Designer, follow the steps below:</p> <ol> <li>Make sure the Microsoft Reporting Services Projects plugin for Visual Studio is installed and active. </li> <li>Close Visual Studio.</li> <li>Install Xtract Universal on the environment on which the license runs.</li> <li>Download the XtractUniversalReportServerPluginSetup.exe.</li> <li>Run the XtractUniversalReportServerPluginSetup.exe on any environment that uses Visual Studio to design reports or where the Report Server runs. This installs the Xtract Universal Report Server Plugin on all compatible versions of Visual Studio and/or Report Server found on the environment. </li> <li>After the installation on the Report Server is complete, restart the Report Server for the changes to take effect.  You can restart the server in the Report Server Configuration Manager by clicking [stop] and then [start].</li> </ol> <p>Note</p> <p>If the Reporting Services Projects plugin for Visual Studio is updated, the Xtract Universal Report Server Plugin is not available anymore.  The Report Server Plugin must be reinstalled.</p> <p>After installation of Xtract Universal Report Server Plugin the following entries and extensions are available in the Visual Studio installation directory:</p> <ul> <li><code>C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\SSRS\\XUDataExtension2020.05.dll</code></li> <li><code>C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\SSRS\\Theobald.Common.dll</code></li> <li><code>C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\SSRS\\Theobald.Distillery.Common.dll</code></li> <li><code>C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\SSRS\\Theobald.Net.dll</code></li> <li><code>C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Enterprise\\Common7\\IDE\\CommonExtensions\\Microsoft\\SSRS\\RSReportDesigner.config</code></li> </ul>"},{"location":"documentation/destinations/server-report-services/#create-a-new-power-bi-report-server-sql-server-reporting-services-destination","title":"Create a new Power BI Report Server (SQL Server Reporting Services) Destination","text":"<p>Follow the steps below to add a new Power BI Report Server (SQL Server Reporting Services) destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Power BI Report Server (SQL Server Reporting Services) from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/server-report-services/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination. To use the Power BI Report Server (SQL Server Reporting Services) destination, no further settings are necessary.</p> <p></p>"},{"location":"documentation/destinations/server-report-services/#assign-the-power-bi-report-server-sql-server-reporting-services-destination-to-an-extraction","title":"Assign the Power BI Report Server (SQL Server Reporting Services) Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/server-report-services/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/server-report-services/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/server-report-services/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/server-report-services/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/server-report-services/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/server-report-services/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/server-report-services/#ssrs-in-visual-studio","title":"SSRS in Visual Studio","text":"<p>The following section contains step-by-step instructions on how to access SAP data in the SSRS environment via Xtract Universal.</p>"},{"location":"documentation/destinations/server-report-services/#prerequisites-in-xtract-universal","title":"Prerequisites in Xtract Universal","text":"<p>To make SAP data available in the SSRS environment, create an extraction in the Xtract Universal Designer. Make sure the Power BI Report Server (SQL Server Reporting Services) destination is assigned to the extraction.</p>"},{"location":"documentation/destinations/server-report-services/#add-an-extraction-as-a-data-source-in-visual-studio","title":"Add an Extraction as a Data Source in Visual Studio","text":"<p>Follow the steps below to create an Xtract Universal data source in Visual Studio: </p> <ol> <li>Create a new \"Report Server Project\" in Visual Studio. </li> <li>Right-click the Shared Data Sources folder in the Solution Explorer and select Add New Data Source. The window \"Shared Data Source Properties\" opens.</li> <li>In the tab General, assign a name for the new data source  . </li> <li>Select the type Xtract Universal from the drop-down list  . If Xtract Universal is not available or displayed in all caps, make sure that the latest Xtract Universal version and Microsoft Reporting Services Projects plugin are both installed.</li> <li>Enter a connection string  to the Xtract Universal Web Server in the format <code>Url=http://[host]:[port]/</code> e.g, <code>Url=http://localhost:8065/</code>. Note that the connection string is case sensitive.</li> <li>Optional (only required if access restriction to the Xtract Universal web server is configured or if the SAP source requires SAP credentials):  Switch to the Credentials tab  and enter your username and password for Xtract Universal.</li> <li>Confirm your input with [OK].</li> </ol> <p>If Xtract Universal is not available in the drop-down list   even though the Microsoft Reporting Services Projects plugin (version 2.6.11 or higher) and the latest version of Xtract Universal are both installed, send the log file located in <code>C:\\Program Files\\XtractUniversal\\ssrs\\log.txt</code> to Theobald Support.</p> <p>Tip</p> <p>The connection string is part of the URL string displayed in the \"Run Extraction\" window of the Designer.  Copy the URL string up to '?' and paste it as the connection string into Visual Studio.</p> <p>Note</p> <p>When https Restricted... or https Unrestricted is activated in the XU web server settings, make sure to modify the Registry of the machine that runs the SSRS server according to the Microsoft Documentation: Configure Strong cryptography.</p>"},{"location":"documentation/destinations/server-report-services/#create-a-report-using-an-xtract-universal-data-source","title":"Create a Report using an Xtract Universal Data Source","text":"<p>The following steps guide you through the creation of a Report in Visual Studio using an Xtract Universal data source: </p> <ol> <li>Add a Power BI Report Server extraction as a data source in Visual Studio.</li> <li>Right-click the Reports folder in the Solution Explorer and select Add New Report. The \"Report Wizard\" opens.</li> <li>In the wizard, select a data source and click [Next].</li> <li>Click [Query Builder...]. The window \"Query Designer\" opens.</li> <li> <p>Select an Xtract Universal extraction from the drop-down menu . </p> <p>Note</p> <p>If no extractions are available, make sure the connection string  in the data source is correct and an extraction with an SQL Server Reporting Server destination exists in Xtract Universal.</p> <p>If you don't see the editor shown below, click Edit as Text. </p> </li> <li> <p>Optional: Change runtime parameters of the extraction . Parameter changes are visible in the Query string . Invalid input is marked with a red circle that displays an error message when hovering over it.</p> </li> <li>Click [Preview]  to run the extraction in preview mode.</li> <li>Confirm your input with [OK]. The Query Builder closes. </li> <li>Make sure the Query string from the Query Builder is displayed in the Report Wizard before clicking [Next]. </li> <li>Complete the Report Wizard according to your preferences.</li> </ol> <p>After creating the report, you can access the Query Builder by right-clicking the Dataset in Report Data and selecting Query....</p> <p>Tip</p> <p>Passing the (Windows) user that runs a report on the report server or in Visual Studio is supported. For this, you need to set up the web server authentication in Xtract Universal.  Single Sign On in SAP is also supported, see Single Sign On.</p>"},{"location":"documentation/destinations/server-report-services/#parameterization","title":"Parameterization","text":"<p>Xtract Universal Designer uses runtime parameters for parameterization. Runtime parameters are accessible in the Query Builder. They can have one of the following Behaviors: </p> Behaviour Description Default Uses the value specified in Xtract Universal Designer. Constant Enter a constant value to be used during runtime. Parameterized Enter the name of a dynamic query parameter to use as a runtime parameter. The value of that parameter can either come from an input field, from the user or from a computed parameter using a formula. <p>Note</p> <p>Every runtime parameter with Parameterized Behaviour allows only one query parameter name as its input.  If you want to use multiple inputs for a runtime parameter, you can use VS Report Designer tools to combine multiple parameters into a single Computed Query Parameter, see Use Computed Query Parameters for SSRS with Xtract Universal. </p> Set dynamic Runtime ParametersMake Parameters optional <p>Use VS query parameters as input for Xtract runtime parameters.</p> <ol> <li>To create a new query parameter right-click the data set in the Report Data section and select Dataset Properties. The window \"Dataset Properties\" opens.</li> <li>Switch to the tab Parameters and press [Add]. </li> <li>Enter a Parameter Name and a Parameter Value or use the [f(x)] button to use formulas and/or combine multiple input values.</li> <li>Switch to the Query tab and press [Query Designer...]. The window \"Query Designer\" opens. </li> <li>Select Parameterized as the Behaviour of the runtime parameter you want to dynamize.</li> <li>Enter the name of the new query parameter under Value.</li> <li>Confirm your input with [OK].</li> </ol> <p>If a query parameter is NULL, that parameter is not passed at runtime and thus will be ignored.</p> <p>Note</p> <p>Depending on the extraction type, some runtime parameters are mandatory, e.g., most custom parameters.</p> <ol> <li>Right-click the input field of the parameter you want to be optional and select Parameter Properties. The window \"Report Parameter Properties\" opens. </li> <li>In the General tab, activate the checkbox Allow null value.</li> <li>Confirm your input with [OK]. A checkbox NULL appears next to the input field.</li> <li>If the checkbox NULL is activated, the parameter will be ignored at runtime.</li> </ol> <p>Tip</p> <p>You can also use a computed query parameter to create a value of NULL. Create a formula that returns Nothing as the value.</p>"},{"location":"documentation/destinations/server-report-services/#related-links","title":"Related Links","text":"<ul> <li>Microsoft Documentation: Report Design Tips</li> <li>Microsoft Documentation: Reporting Services Tutorials (SSRS)</li> <li>Microsoft Documentation: Add a Query Parameter to Create a Report Parameter</li> <li>Knowledge Base Article: Use Computed Query Parameters for SSRS with Xtract Universal</li> <li>Integration via Azure Data Factory</li> </ul>"},{"location":"documentation/destinations/sharepoint/","title":"SharePoint","text":"<p>This page shows how to set up and use the SharePoint destination.  The SharePoint destination loads data into a custom list on a SharePoint server.</p>"},{"location":"documentation/destinations/sharepoint/#requirements","title":"Requirements","text":"<p>To extract data into a SharePoint Custom list, you need either your own SharePoint server or access to a SharePoint Online system as part of Office365.</p> <p>If your SharePoint server isn't configured for remote access already, go to Central Administration -&gt; Application Management -&gt; Configure alternate access mappings and add an appropriate mapping for the zone \"Internet\".</p>"},{"location":"documentation/destinations/sharepoint/#create-a-new-sharepoint-destination","title":"Create a new SharePoint Destination","text":"<p>Follow the steps below to add a new SharePoint destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type SharePoint from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/sharepoint/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/sharepoint/#sharepoint-on-premises-or-sharepoint-online","title":"SharePoint on-premises or SharePoint Online","text":"<p>Select the type of SharePoint environment you are going to export to.</p>"},{"location":"documentation/destinations/sharepoint/#site-url","title":"Site URL","text":"<p>Enter the URL of your SharePoint server here (optionally including sub-directories if you want to export into a specific site on the server). Make sure you only enter the base path, omit page information a browser might show you in the address line like <code>_layouts/15/start.aspx#/</code> or similar.</p>"},{"location":"documentation/destinations/sharepoint/#user","title":"User","text":"<p>Enter your SharePoint user name.</p>"},{"location":"documentation/destinations/sharepoint/#password","title":"Password","text":"<p>Enter the password for your SharePoint user account.</p>"},{"location":"documentation/destinations/sharepoint/#test-connection","title":"Test connection","text":"<p>Check the database connection. </p>"},{"location":"documentation/destinations/sharepoint/#assign-the-sharepoint-destination-to-an-extraction","title":"Assign the SharePoint Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/sharepoint/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/sharepoint/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/sharepoint/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/sharepoint/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/sharepoint/#mode","title":"Mode","text":"Option Description Drop, Create &amp; Insert Creates a new list on the SharePoint system, deleting any previous list with the same name. Create If Not Exists &amp; Merge Merges data with an existing list or creates a new list, if none with the specified name is found. Truncate &amp; Insert Creates a new list on the SharePoint system, deleting any previous list with the same name. Merge only* Merges rows, without deleting rows or the list itself."},{"location":"documentation/destinations/sharepoint/#max-threads","title":"Max. threads","text":"<p>Sets the number of threads for communication with the SharePoint server.  It is recommended to use the default value 2.</p> <p>Note</p> <p>Increasing the thread number may increase the upload speed, depending on the server and network setup.  Setting the value to 1 slows down the speed significantly, which may help with a heavy load of the SharePoint server or when connection problems occur.</p>"},{"location":"documentation/destinations/snowflake/","title":"Snowflake","text":"<p>This page shows how to set up and use the Snowflake destination.  The Snowflake destination loads data into a Snowflake environment.</p>"},{"location":"documentation/destinations/snowflake/#requirements","title":"Requirements","text":"<p>The connection to the Snowflake target environment is made via the ODBC driver for Windows 64-bit architectures. No additional installations are required to use the Snowflake destination.</p> <ul> <li>Download and install the SnowflakeDSIIDriver.</li> <li>To connect through a proxy server, configure the following environment variables: <code>http_proxy</code>, <code>https_proxy</code>, <code>no_proxy</code>. For more information, see Snowflake: ODBC Configuration and Connection Parameters.</li> <li>The ODBC default port (443) for HTTPS is enabled and allows outbound traffic from the network to Snowflake's services.</li> <li>The Snowflake account used to upload data to Snowflake needs the corresponding access and role privileges, see Snowflake Documentation: Overview of Access Control - Roles. The following privileges are required:<ul> <li>PUT command</li> <li>COPY command</li> <li>TABLE</li> </ul> </li> </ul> <p>The Snowflake destination only supports Snowflake managed data stages. To write SAP data to external stages in Snowflake, refer to the Azure Storage, AWS S3 or Google Cloud destination. </p>"},{"location":"documentation/destinations/snowflake/#create-a-new-snowflake-destination","title":"Create a new Snowflake Destination","text":"<p>Follow the steps below to add a new Snowflake destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Snowflake from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/snowflake/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p>"},{"location":"documentation/destinations/snowflake/#general","title":"General","text":""},{"location":"documentation/destinations/snowflake/#output-directory","title":"Output directory","text":"<p>Enter an existing local directory in which the extracted data is written as a csv file. Process during an extraction:</p> <ul> <li>A csv file is created in the output directory.</li> <li>Once the file has reached a certain size, it is zipped (gzip), see File Splitting.</li> <li>The zipped file is uploaded via PUT command to the Snowflake staging area.</li> <li>The data is then copied via COPY command to the target table in Snowflake.</li> </ul> <p>This process (gzip + PUT command) repeats itself until the extraction is finished. While running an extraction, the csv files in the local directory and the staging area are deleted. </p>"},{"location":"documentation/destinations/snowflake/#account-name","title":"Account Name","text":"<p>Enter the Snowflake authentication account.  The account name can be derived from the connection URL.</p> <ul> <li>URL for organization account identifier:  <code>https://[organization]-[account_name].snowflakecomputing.com/console#/</code></li> <li>URL for region account identifier (legacy):  <code>https://[account_name].[region].[cloud].snowflakecomputing.com/console#/</code></li> </ul>"},{"location":"documentation/destinations/snowflake/#database","title":"Database","text":"<p>Enter the name of the Snowflake database.</p>"},{"location":"documentation/destinations/snowflake/#schema","title":"Schema","text":"<p>Enter the schema of the Snowflake database.</p>"},{"location":"documentation/destinations/snowflake/#role","title":"Role","text":"<p>Enter a Snowflake user role. If no user role is entered, the default user role is used for the connection.</p>"},{"location":"documentation/destinations/snowflake/#account-identifier","title":"Account Identifier","text":""},{"location":"documentation/destinations/snowflake/#organization-preferred","title":"Organization (preferred)","text":"<p>Enter the name of your organization. Authentication via the account name in your organization is the preferred way of identifying youself against Snowflake, see Snowflake Documentation: Account Name in Your Organization</p>"},{"location":"documentation/destinations/snowflake/#region-legacy","title":"Region (legacy)","text":"<p>Select the region of the Snowflake environment from the drop-down-menu. In the example above, the region AWS - EU (Frankfurt) is selected.  The selected region must match the information in the assigned account. </p> <p>Note</p> <p>Regions with the suffix (legacy) are deprecated. Select the (legacy) option if you connect to Snowflake using an old Cloud Region ID. For more information on the current Cloud Region IDs, see Snowflake Documentation: Supported Cloud Regions.</p>"},{"location":"documentation/destinations/snowflake/#authentication","title":"Authentication","text":""},{"location":"documentation/destinations/snowflake/#username","title":"Username","text":"<p>Enter the Snowflake authentication user name. </p>"},{"location":"documentation/destinations/snowflake/#basic","title":"Basic","text":"<p>If this option is active, basic authentication is used for authentication. Enter the Snowflake authentication password of the user in the field Password.</p>"},{"location":"documentation/destinations/snowflake/#key-pair","title":"Key Pair","text":"<p>If this option is active, key pairs are used for authentication, see Snowflake Documentation: Key Pair Authentication &amp; Key Pair Rotation. Select the path to your private key in the field Private Key Path. Both encrypted and non-encrypted private keys are supported.  If you use encrypted private key for authentication, enter the password that is used by snowflake to decrypt it in the field Key password.</p>"},{"location":"documentation/destinations/snowflake/#stages","title":"Stages","text":"<p>Click [Test Connection] to fetch all stages and warehouses from Snowflake.  The stages and warehouses then become available for selection.</p> <p></p>"},{"location":"documentation/destinations/snowflake/#stage-name","title":"Stage name","text":"<p>Select a Snowflake Stage. Note that only Snowflake managed data stages are displayed. </p>"},{"location":"documentation/destinations/snowflake/#warehouse","title":"Warehouse","text":"<p>Select a Snowflake Data Warehouse.</p>"},{"location":"documentation/destinations/snowflake/#assign-the-snowflake-destination-to-an-extraction","title":"Assign the Snowflake Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/snowflake/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/snowflake/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/snowflake/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/snowflake/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/destinations/snowflake/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/snowflake/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/snowflake/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/snowflake/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/snowflake/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/snowflake/#file-splitting","title":"File Splitting","text":"<p>Writes extraction data of a single extraction to multiple files.  Each filename is appended by _part[nnn]. </p>"},{"location":"documentation/destinations/snowflake/#max-file-size","title":"Max. file size","text":"<p>The value set in Max. file size determines the maximum size of each file. </p> <p>Note</p> <p>The option Max. file size does not apply to gzip files.  The size of a gzipped file cannot be determined in advance.</p>"},{"location":"documentation/destinations/snowflake/#preparation","title":"Preparation","text":"<p>Defines the action on the target database before the data is inserted into the target table.</p> Option Description None No action. Drop &amp; Create Remove table if available and create new table (default). Truncate Or Create Empty table if available, otherwise create. Truncate Empty table if available. Create If Not Exists Create table if not available. Custom SQL Here you can define your own script, see Custom SQL Statements. <p>To only create the table in the first step and not insert any data, you have two options:</p> <ol> <li>Copy the SQL statement and execute it directly on the target data database.</li> <li>Select the None option for Row Processing and execute the extraction.</li> </ol> <p>Once the table is created, it is up to you to change the table definition, e.g., by creating corresponding key fields and indexes or additional fields.</p>"},{"location":"documentation/destinations/snowflake/#row-processing","title":"Row Processing","text":"<p>Defines how the data is inserted into the target table.</p> Option Description None No action. Copy file to table Insert records (default). Merge File to table Insert records into the staging table. Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/snowflake/#finalization","title":"Finalization","text":"<p>Defines the action on the target database after the data has been successfully inserted into the target table.</p> Option Description None No action (default). Custom SQL Define your own script, see Custom SQL Statements."},{"location":"documentation/destinations/snowflake/#transaction-style","title":"Transaction style","text":"<p>Note</p> <p>The available options for Transaction Style vary depending on the destination.</p>"},{"location":"documentation/destinations/snowflake/#one-transaction","title":"One Transaction","text":"<p>Preparation, Row Processing and Finalization are all performed in a single transaction.</p> <ul> <li>Advantage: clean rollback of all changes.</li> <li>Disadvantage: possibly extensive locking during the entire extraction period. </li> </ul> <p>Recommendation</p> <p>Only use One Transaction in combination with DML commands, e.g., \"truncate table\" and \"insert.  Using DDL commands commits the active transaction, causing rollback issues for the steps after the DDL command.  Example: If a table is created in the preparation step, the opened \"OneTransaction\" is committed and a rollback in the next steps is not performed correctly. For more information, see Snowflake Documentation: DDL Statements.</p>"},{"location":"documentation/destinations/snowflake/#rowprocessingonly","title":"RowProcessingOnly","text":"<p>Only Row Processing is executed in a transaction. Preparation and Finalization without an explicit transaction (implicit commits).</p> <ul> <li>Advantage: DDL in 'Preparation and Finalization* for DBMS that do not allow DDL in explicit transactions (e.g. AzureDWH)</li> <li>Disadvantage: no rollback of Preparation/Finalization.</li> </ul>"},{"location":"documentation/destinations/snowflake/#no-transaction","title":"No Transaction","text":"<p>No explicit transactions.</p> <ul> <li>Advantage: no transaction management required by DBMS (locking, DB transaction log, etc.). This means no locking and possible performance advantages.</li> <li>Disadvantage: no rollback </li> </ul>"},{"location":"documentation/destinations/snowflake/#empty-values","title":"Empty Values","text":"<p>Warning</p> <p>NULL result in a non-nullable column. By default empty strings are converted to NULL values when uploading data to Snowflake. To deactivate the conversion, disable Replace empty values with SQL NULL.</p> <p>This option controls the Snowflake file format parameter EMPTY_FIELD_AS_NULL. When Replace empty values with SQL NULL is active, empty strings are converted to NULL values when uploading data to Snowflake.</p>"},{"location":"documentation/destinations/snowflake/#append-extraction-timestamp","title":"Append Extraction Timestamp","text":"<p>When Add timestamp column to result set is active, an additional column ExtractionTimestamp is added to the output of the extraction. The timestamp uses the format  <code>yyyy-MM-dd'T'HH:mm:ss.SSS'Z'</code> (UTC).</p>"},{"location":"documentation/destinations/snowflake/#merge-data","title":"Merge Data","text":"<p>The following example depicts the update of the existing data records in a database by running an extraction to merge data.  In this case, merging means changing a value of a field or inserting a new data row or updating an existing record in SAP. </p> <p>With a merge command, the updated value is written to the destination database table.  The merge command ensures delta processing: new records are inserted into the database and / or existing records are updated.</p> <p></p> <p>Tip</p> <p>Alternatively to merging, the data can be also updated by means of full load.  The full load method is less efficient.</p>"},{"location":"documentation/destinations/snowflake/#prerequisites","title":"Prerequisites","text":"<p>You need a table with existing SAP data, in which new data can be merged. Ideally, the table with existing data is created in the initial load with the corresponding Preparation option and filled with data with the Row Processing option Insert.</p> <p>After the table is created, open SAP and change a field value in the SAP table that is used for the data merge. </p> <p>Warning</p> <p>Faulty merge.  Merge commands require a primary key. If no primary key is set, the merge command runs into an error. Create an appropriate primary key in the General Settings of the extraction type to execute the merge command.</p>"},{"location":"documentation/destinations/snowflake/#merge-command","title":"Merge Command","text":"<p>The merge process is performed using a staging table and takes place in three steps:</p> <ol> <li>A temporary table is created.</li> <li>The data is inserted in the temporary table.</li> <li>The temporary table is merged with the target table and then the temporary table is deleted.</li> </ol> <p>Follow the steps below to set up the merge process in Xtract Universal:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions. </li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</li> <li>Make sure to assign Snowflake destination to the extraction. </li> <li>Apply the following destination settings: </li> <li>Click [OK] and run the extraction.</li> </ol> <p>More information about the updated fields can be found in the SQL statement. It is possible to edit the SQL statement if necessary, e.g., to exclude certain columns from the update. Fields that do not appear in the SQL statement are not affected by changes. </p>"},{"location":"documentation/destinations/snowflake/#custom-sql-statements","title":"Custom SQL Statements","text":"<p>The Snowflake destination supports the use of custom SQl statements in the staging steps of the database. Follow the steps below to create custom SQL statements in the Snowflake destination:</p> <ol> <li>In the main window of the Xtract Universal Designer, select an extraction from the list of extractions..</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination], the window \"Destination Settings\" opens.</li> <li>Make sure to assign the Snowflake destination to the extraction.</li> <li> <p>Select the option Custom SQL from the drop-down list   in one of the following sections:</p> <ul> <li>Preparation</li> <li>Row Processing</li> <li>Finalization</li> </ul> <p></p> </li> <li> <p>Click [Edit SQL] . The window \"Edit SQL\" opens.</p> </li> <li>Enter your custom SQL statement and click [OK] to confirm your input.</li> </ol>"},{"location":"documentation/destinations/snowflake/#use-templates","title":"Use Templates","text":"<p>Existing SQL commands can be used as templates. You can write your user-defined SQL expressions and adapt the loading of the data to your needs. You can additionally execute stored procedures that exist in the database. To do so, use the SQL templates provided in the following staging steps:</p> <ul> <li>Preparation, e.g., in Drop &amp; Create or Create if Not Exists</li> <li>Row Processing, e.g., in Insert or Merge</li> <li>Finalization</li> </ul> <p>Follow the steps below to generate a Custom SQL command from a template:</p> <ol> <li>In one of the staging steps, select the Custom SQL option from the drop-down list  .</li> <li>Click [Edit SQL] . The dialogue \"Edit SQL\" opens. </li> <li>Navigate to the drop-down menu and select an existing command  . </li> <li>Click [Generate Statement]. A new statement is generated. </li> <li>Click [Copy] to copy the statement to the clipboard.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>Check out the Microsoft SQL Server example for details on predefined expressions.</p> <p>Note</p> <p>The custom SQL code is used for SQL Server destinations.  A syntactic adaptation of the code is necessary to use the custom SQL code for other database destinations.</p>"},{"location":"documentation/destinations/snowflake/#use-script-expressions","title":"Use Script Expressions","text":"<p>You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported: </p> Input Description <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.TableName }#</pre> Name of the database table extracted data is written to. <pre>#{Extraction.RowsCount }#</pre> Count of the extracted rows. <pre>#{Extraction.RunState}#</pre> Status of the extraction (Running, FinishedNoErrors, FinishedErrors). <pre>#{(int)Extraction.RunState}#</pre> Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors). <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <p>For more information, see Script Expressions.</p> Example: Verify the existence of a table in a database using 'ExistsTable'<pre><code>#{\n   iif\n   (\n      ExistsTable(\"MAKT\"),\n      \"TRUNCATE TABLE \\\"MAKT\\\";\",\n      \"\n         CREATE TABLE \\\"MAKT\\\"(\n            \\\"MATNR\\\" VARCHAR(18),\n            \\\"SPRAS\\\" VARCHAR(2),\n            \\\"MAKTX\\\" VARCHAR(40));\n      \"\n   )\n}#\n</code></pre>"},{"location":"documentation/destinations/snowflake/#create-a-status-overview","title":"Create a Status Overview","text":"<p>The table \"ExtractionStatistics\" provides an overview and status of the executed Xtract Universal extractions. To create the \"ExtractionStatistics\" table, create an SQL table according to the following example:</p> Create ExtractionStatistics<pre><code>CREATE TABLE [dbo].[ExtractionStatistics](\n    [TableName] [nchar](50) NULL,\n    [RowsCount] [int] NULL,\n    [Timestamp] [nchar](50) NULL,\n    [RunState] [nchar](50) NULL\n) ON [PRIMARY]\nGO\n</code></pre> <p>The ExtractionStatistics table is filled in the Finalization process step, using the following SQL statement:</p> Fill ExtractionStatistics<pre><code>INSERT INTO [ExtractionStatistics]\n(\n     [TableName], \n     [RowsCount], \n     [Timestamp],\n     [RunState]\n)\nVALUES\n(\n     '#{Extraction.TableName}#', \n     '#{Extraction.RowsCount}#', \n     '#{Extraction.Timestamp}#',\n     '#{Extraction.RunState}#'\n);\n</code></pre>"},{"location":"documentation/destinations/snowflake/#custom-sql-example","title":"Custom SQL Example","text":"<p>In the depicted example, the DataSource 0FI_AP_4 is extended by one column that is filled with the user-defined runtime parameter RUNTIMEPARAMETER. The filling of the new column is implemented dynamically in the Finalization section of the destination settings.</p> <ol> <li>In Snowflake, deselect the Error on Column Count Mismatch option in the XTRACT_UNIVERSAL File Format. </li> <li>Open the extraction and click Edit runtime parameter  to create the runtime parameter RUNTIMEPARAMETER . </li> <li>Choose a field in the sections Fields and click Edit  to add  a selection criterion that uses the runtime parameter . </li> <li>Close the extraction and open the Destination Settings of the extraction. </li> <li>Make sure to assign the Snowflake destination to the extraction.</li> <li>In the staging step Preparation, select the option Custom SQL from the drop-down list and click [Edit SQL]. The window \"Edit SQL\" opens.</li> <li>In the drop-down menu, select the option Drop &amp; Create and click [Generate Statement]  to use the template for Drop &amp; Create. </li> <li>Add the following line in the generated statement:  <pre><code>\"RUNTIMEPARAMETER\" VARCHAR(4),\n</code></pre></li> <li>Click [OK] to confirm your input. </li> <li>In the staging step Row Processing, select the option Copy file to table.  At this point, no data from the SAP source system but <code>NULL</code> values are written to the newly created column RUNTIMEPARAMETER.</li> <li>In the staging step Finalization, the <code>NULL</code> values can be filled by a custom SQL statement.  Select the option Custom SQL from the drop-down list and click [Edit SQL].  The window \"Edit SQL\" opens.</li> <li>Paste the following SQl statement into the editor:  <pre><code>UPDATE \"0FI_AP_4\"\nSET RUNTIMEPARAMETER= '@RUNTIMEPARAMETER'\nWHERE RUNTIMEPARAMETER IS NULL;\n</code></pre>     The <code>NULL</code> values are filled with the runtime parameter RUNTIMEPARAMETER and written into the SQL target table using the T-SQL command <code>UPDATE</code>.</li> <li>Click [OK] to confirm your input and close the destination settings.</li> <li>Open the \"Run Extraction\" window of the Designer  and enter a value for the runtime parameter , before running the extraction . </li> </ol> <p>Check the existence of the new column RUNTIMEPARAMETER in the Snowflake Console of the table 0FI_AP_4.</p> <p></p>"},{"location":"documentation/destinations/snowflake/#related-links","title":"Related Links","text":"<ul> <li>Requirements: .NET-Framework</li> <li>Requirements: 64-bit Architecture</li> <li>Snowflake Documentation: Installing and Configuring the ODBC Driver for Windows</li> <li>Snowflake Documentation: Snowflake Identifiers</li> <li>Knowledge Base Article: Integrate SAP Data into a Snowflake Data Warehouse</li> <li>Knowledge Base Article: SAP Integration with Matillion Data Loader</li> <li>Extraction Parameters</li> </ul>"},{"location":"documentation/destinations/tableau/","title":"Tableau","text":"<p>This page shows how to set up and use the Tableau destination.  The Tableau destination loads data into Tableau Analytics Platform. </p> <p>The Tableau destination enables you to save data extracted from SAP as Hyper files. It is also possible to upload the Hyper file to Tableau Server or Tableau Online.</p>"},{"location":"documentation/destinations/tableau/#requirements","title":"Requirements","text":"<ul> <li>Tableau Export Library</li> <li>Visual C++ 2015 Runtime</li> </ul> <p> Download the Tableau Export Library and Visual C++ 2015 Runtime</p> <p>If no Visual C++ 2015 Runtime is installed on your machine, run the vc_redist.x64.exe to install the Visual C++ 2015 Runtime.  Copy the <code>tableau</code> folder into your Xtract Universal directory so that the following folder structure is created: <code>C:\\Program Files\\XtractUniversal\\tableau\\hyper</code>.</p>"},{"location":"documentation/destinations/tableau/#create-a-new-tableau-destination","title":"Create a new Tableau Destination","text":"<p>Follow the steps below to add a new Tableau destination to Xtract Universal:</p> <ol> <li>In the main window of the Designer, navigate to Server &gt; Manage Destinations. The window \u201cManage Destinations\u201d opens. </li> <li>Click [Add] to create a new destination. The window \"Destination Details\" opens. </li> <li>Enter a Name for the destination.</li> <li>Select the destination type Tableau from the drop-down menu.  A list of connection details opens.</li> <li>Fill out the destination details to connect to the destination.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>The destination can now be assigned to extractions.</p>"},{"location":"documentation/destinations/tableau/#destination-details","title":"Destination Details","text":"<p>The destination details define the connection to the destination.</p> <p></p>"},{"location":"documentation/destinations/tableau/#output-directory","title":"Output directory","text":"<p>Enter the path to a folder on the Xtract Universal Server where the generated Tableau files are stored.</p> <p>Note</p> <p>Make sure that the directory exists.</p>"},{"location":"documentation/destinations/tableau/#tableau-server","title":"Tableau server","text":""},{"location":"documentation/destinations/tableau/#upload-to-tableau-server","title":"Upload to Tableau Server","text":"<p>Option to upload the extracted file (as a data source) to Tableau Server or Tableau Cloud.</p>"},{"location":"documentation/destinations/tableau/#delete-local-file-after-upload","title":"Delete local file after upload","text":"<p>Option to remove the local file after a successful upload.</p>"},{"location":"documentation/destinations/tableau/#host","title":"Host","text":"<p>Enter the IP address or domain name of the remote server, starting with <code>http://</code> or <code>https://</code>.</p>"},{"location":"documentation/destinations/tableau/#pat-name","title":"PAT name","text":"<p>Enter the name of your Personal Access Token (PAT). To extract data to Tableau Server the site role \"Server Administrator\" is required.</p>"},{"location":"documentation/destinations/tableau/#pat-secret","title":"PAT secret","text":"<p>Enter a valid token secret that corresponds to your token name.</p>"},{"location":"documentation/destinations/tableau/#site-for-tableau-cloud","title":"Site (for Tableau Cloud)","text":"<p>Enter the ID of the site for publishing your data source.  This field is mandatory and cannot be left blank.  Get the correct site URL from Tableau Cloud.</p>"},{"location":"documentation/destinations/tableau/#test-connection","title":"[Test Connection]","text":"<p>Check the database connection. </p>"},{"location":"documentation/destinations/tableau/#tableau-server-settings","title":"Tableau server settings","text":""},{"location":"documentation/destinations/tableau/#site","title":"Site","text":"<p>A drop-down list of the sites stored on the connected on-prem server. Selecting a site is mandatory for uploading files. </p>"},{"location":"documentation/destinations/tableau/#project","title":"Project","text":"<p>Select the project in which the extracted data is published. </p>"},{"location":"documentation/destinations/tableau/#assign-the-tableau-destination-to-an-extraction","title":"Assign the Tableau Destination to an Extraction","text":"<p>Extractions write data to their assigned destination. Follow the steps below to assign a destination to an extraction:</p> <ol> <li>In the main window of the Designer, select an extraction.</li> <li>Click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \u201cDestination Settings\u201d opens. </li> <li>In the \u201cDestination Settings\u201d window, select a destination from the drop down menu. </li> <li>Optional: edit the destination settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction, the extracted SAP data is now written to the destination.</p>"},{"location":"documentation/destinations/tableau/#destination-settings","title":"Destination Settings","text":"<p>The destination settings only affect the extraction type that the destination is assigned to. To open the destination settings, select an extraction in the main window of the Designer and click [ touch icon touch icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Destination]. The window \"Destination Settings\" opens.</p> <p></p>"},{"location":"documentation/destinations/tableau/#file-name","title":"File Name","text":"<p>Determines the name of the target table. The following options are available:</p> Option Description Same as name of SAP object Copy the name of the SAP object. Same as name of extraction Adopt the name of the extraction. Fully qualified extraction name Adopt the name of the extraction, including the path of the extraction. This option avoids conflicts, when the extraction names are not unique. Use ta Fully qualified extraction name when the same extraction name is used in multiple extraction groups. Custom Define a name of your choice."},{"location":"documentation/destinations/tableau/#append-timestamp","title":"Append timestamp","text":"<p>Add the timestamp in the UTC format (_YYYY_MM_DD_hh_mm_ss_fff) to the file name of the extraction</p>"},{"location":"documentation/destinations/tableau/#column-name-style","title":"Column Name Style","text":"<p>Defines the style of the column name. Following options are available: </p> <p></p> Option Description Code The SAP technical column name is used as column name in the destination e.g., MAKTX. PrefixedCode The SAP technical column name is prefixed by SAP object name and the tilde character e.g., MAKT~MAKTX CodeAndText The SAP technical column name and the SAP description separated by an underscore are used as column name in the destination e.g., MAKTX_Material Description (Short Text). TextAndCode The SAP description and the SAP technical column name description separated by an underscore are used as column name in the destination e.g., Material Description (Short Text)_MAKTX. Text The SAP description is used as column name in the destination e.g., Material Description (Short Text)."},{"location":"documentation/destinations/tableau/#convert-dates","title":"Convert dates","text":"<p>Converts the character-type SAP date (YYYYMMDD, e.g., 19900101) to a special date format (YYYY-MM-DD, e.g., 1990-01-01).  Target data uses a real date data-type and not the string data-type to store dates.</p>"},{"location":"documentation/destinations/tableau/#year-0","title":"Year 0","text":"<p>Converts the SAP date 00000000 to the entered value.</p>"},{"location":"documentation/destinations/tableau/#year-9999","title":"Year 9999","text":"<p>Converts the SAP date 9999XXXX to the entered value.</p>"},{"location":"documentation/destinations/tableau/#invalid-values","title":"Invalid values","text":"<p>If an SAP date cannot be converted to a valid date format, the invalid date is converted to the entered value.  NULL is supported as a value. When converting the SAP date the two special cases 00000000 and 9999XXXX are checked first.</p>"},{"location":"documentation/destinations/tableau/#existing-files","title":"Existing files","text":"<p>If a flat file by the same name already exists in the target directory, the following actions can be taken:</p> Option Description Replace file The export process overwrites existing files. Append results The export process appends new data to an already existing file, see Column Mapping. Abort extraction The process is aborted, if the file already exists."},{"location":"documentation/destinations/tableau/#column-mapping","title":"Column Mapping","text":"<p>Activate Column Mapping when appending data to an existing file or entity that has different column names or a different number of columns. This can be the case when extracting data from two or more extractions into the same destination file, where the column names of the extraction and the destination file differ.</p> <p>Note</p> <p>The column names in the extraction and destination must be unique.  If duplicated column names are found, an error message is displayed. The column names must be corrected, before column mapping can be used.</p> <ol> <li>When working with flat files, ensure that:<ol> <li>the XU server and the Designer both have access to the destination file.</li> <li>the output directory and the file name of the extraction match the destination file. </li> <li>the Column Name Style of the extraction and destination file match.</li> </ol> </li> <li>Select the option Append results in the subsection Existing Files.</li> <li> <p>Click [Map] to assign columns. The window \"Column Mapping\" opens.</p> <ul> <li>Destination Columns displays the names of the columns that are available in the destination file or entity.</li> <li>Not Mapped defines whether or not columns are mapped to the destination columns.</li> <li>Source Columns defines which SAP column is mapped to a destination column.</li> </ul> <p></p> </li> <li> <ol> <li>If the column names of the extraction and the names of the destination columns match, click [Auto map by name].</li> <li>If the column names do not match, assign columns manually by selecting the respective SAP column from the dropdown menu under Source Columns.</li> <li>If a column does not have a counterpart or is not supposed to be appended, activate the checkbox under Not Mapped.</li> </ol> </li> <li>Click [OK] to confirm your input.</li> </ol> <p>When running the extraction the extracted data is added to the destination file or entity as specified in the column mapping.</p> <p>Tip</p> <p>In case an error message pops up, click [Show more] to see a description of what caused the error.</p> <p>Note</p> <p>Columns that are not mapped are filled with <code>NULL</code> values.</p>"},{"location":"documentation/destinations/tableau/#related-links","title":"Related Links","text":"<ul> <li>Webinar: Visualize your SAP data in Tableau</li> <li>Knowledge Base Article: Link a BEx query with a Hierarchy in Tableau</li> </ul>"},{"location":"documentation/execute-and-automate/","title":"Execute and Automate","text":"<p>This page shows how to run extractions automatically and manually.  Examples on how to call extractions:</p> <ul> <li>Commandline</li> <li>Webservice </li> <li>Scheduler</li> <li>ETL-Tools</li> </ul>"},{"location":"documentation/execute-and-automate/#about-the-execution-of-extractions","title":"About the Execution of Extractions","text":"<p>Extractions are triggered by an HTTP request and executed on the Xtract Universal server.</p> <p>The configuration of source, destination and extraction type defines how the data transfer is performed.  This configuration can contain dynamic elements, like extraction parameters and script expressions. Depending on the destination, the execution of an extraction can be triggered either interactively or unattended.</p>"},{"location":"documentation/execute-and-automate/#interactive-extractions","title":"Interactive Extractions","text":"<p>Extractions are typically triggered interactively when a user requires new or updated data from SAP, and no additional data storage system (like a data warehouse) is present. In these scenarios, execution of an extraction is typically triggered by one of Xtract Universal's plugins  (like Alteryx, Power BI Connector or Power BI Report Server)  or directly by the target environment (QlikSense &amp; QlikView).</p>"},{"location":"documentation/execute-and-automate/#unattended-extractions","title":"Unattended Extractions","text":"<p>When an additional data storage system (database, cloud storage, flat files) is present, extractions are typically triggered as part of an ELT-process (Extract, Load, Transform), which is run unattended at regular intervals by a scheduler or other orchestration software. In these scenarios, execution of an extractions is typically triggered by running the XU command line tool from the orchestration software. </p> <p>For advanced scenarios or environments that do not support commandline programs, the HTTP Webservices can be used for triggering and monitoring executions.</p> <p>Note</p> <p>Xtract Universal does not have its own scheduler. You can use third party schedulers. </p>"},{"location":"documentation/execute-and-automate/#run-parallel-extractions","title":"Run parallel Extractions","text":"<p>The amount of possible parallel extractions depends on the hardware resources of the Windows server. </p> <p>Every triggered extraction is executed in a separate process of the operating system. Reliability and throughput of the network connection, available RAM and disk throughput (for logging and caching) are all crucial factors for the parallelization. Other factors are the performance of the SAP source system the destination.</p> <p>Note</p> <p>Xtract Universal scales corresponding to the available hardware resources of the runtime environment.</p>"},{"location":"documentation/execute-and-automate/#automate-the-creation-of-extractions","title":"Automate the Creation of Extractions","text":"<p>As of Xtract Universal Version 4.26.1, the command line tool xu-config.exe is available in the installation directory of Xtract Universal, e.g. <code>C:\\Program Files\\XtractUniversal\\xu-config.exe</code>. The tool creates extractions, sources and destinations outside of the Xtract Universal Designer.</p> <p>For more information, see Knowledge Base Article: Create Extractions via Commandline.</p>"},{"location":"documentation/execute-and-automate/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base Article: Call Extractions via Script</li> <li>Knowledge Base Article: Execute &amp; Schedule all Extractions using an SSIS Package</li> </ul>"},{"location":"documentation/execute-and-automate/call-via-commandline/","title":"Call via Commandline","text":"<p>Extractions can be run via the following command line tools:</p> Command Line Tool Operating System Directory <code>xu.exe</code> Windows The command line tool is available in the default directory <code>C:\\Program Files\\XtractUniversal\\xu.exe</code>. <code>xu.elf</code> Unix-, Linux environment Download-Link for the Linux version of the commandline tool. <p>Note</p> <p>Both command line tools do not differ in functionality and can be copied and run on any computer.  Make sure that the host of the Xtract Universal server is accessible in the network. </p>"},{"location":"documentation/execute-and-automate/call-via-commandline/#call-extractions","title":"Call Extractions","text":"<p>The command line tool connects to an Xtract Universal server (service) and starts an extraction with the following runtime parameters:</p> Runtime Parameters Description Syntax -h A short documentation of the command line tool <code>C:\\Program Files\\XtractUniversal&gt;xu.exe -h</code> -n Name of the extraction <code>C:\\Program Files\\XtractUniversal\\xu.exe -n &lt;name&gt;</code> -s The name or IP address of the machine on which the Xtract Universal service runs. The default is localhost. You can find the current value in the \"Run\" window of the Designer. <code>xu.exe -n &lt;name&gt; -s &lt;host&gt;</code> -p The port on which the Xtract Universal service runs. The default is 8065.  You can find the current value in the \"Run\" window of the Designer. <code>xu.exe -n &lt;name&gt; -s &lt;host&gt; -p &lt;port&gt;</code> -o Parameters to be set when running the extraction.  Multiple parameters can be set. <code>xu.exe -n &lt;name&gt; -s &lt;host&gt; -p &lt;port&gt; -o \"param1=&lt;wert1&gt;\" -o \"param2=&lt;wert2&gt;\"</code> -e Use Transport Layer Security (TLS) (1.2 or higher). <code>xu.exe  -n &lt;name&gt; -s &lt;host&gt; -p &lt;port&gt; -e</code> -a Cancel all running instances of the extraction. <code>xu.exe -a</code> -c Clear the result cache and options of the extraction.  This only works with Pull Destinations. <code>xu.exe -c</code>"},{"location":"documentation/execute-and-automate/call-via-commandline/#examples","title":"Examples","text":"<p>To run an extraction on the Xtract Universal server, call the command line tool as follows: <pre><code>    xu.exe -n MaterialText\n    xu.exe -n MaterialText -s 10.0.0.42 -p 80 -o \"rows=1000\"\n    xu.exe -n MaterialText -s xusrv.corp.local -p 443 -o \"rows=1000\" -o \"SPRAS=D\" -e\n    xu.exe -n MaterialText -a\n    xu.exe \"http://localhost:8065/start/MaterialText/&amp;rows=1000\"\n</code></pre></p>"},{"location":"documentation/execute-and-automate/call-via-commandline/#return-codes","title":"Return Codes","text":"<p>When an operation is completed successfully, the program returns 0. In case of an error, the program returns one of the following status codes:</p> HTTP Status Codes Description 404 Extraction does not exist 1001 An undefined error occurred 1002 File could not be found 1013 Invalid input data 1014 Invalid number of arguments 1015 Name of the parameter is unknown 1016 Invalid argument 1040 Timeout error: Waiting period for HTTP-answer of the command line tool is exceeded 1053 The URL of the extraction is incorrect 1087 Invalid parameter"},{"location":"documentation/execute-and-automate/call-via-commandline/#standard-output-and-standard-error-output","title":"Standard Output and Standard Error Output","text":"<p>The output depends on the destination type of the extraction. If the extraction call was successful, the output is 0.</p>"},{"location":"documentation/execute-and-automate/call-via-commandline/#standard-output-of-pull-destinations","title":"Standard Output of Pull Destinations","text":"<p>When using Pull Destinations like HTTP-CSV, HTTP-JSON, etc., extracted data is written into the standard output (sdtout).</p>"},{"location":"documentation/execute-and-automate/call-via-commandline/#standard-output-of-push-destinations","title":"Standard Output of Push Destinations","text":"<p>When using Push Destinations an Extraction Log in CSV format is written into the standard output (stdout).</p>"},{"location":"documentation/execute-and-automate/call-via-commandline/#standard-error-output","title":"Standard Error Output","text":"<p>Logs and error notifications are written into the standard error output (stderr).</p>"},{"location":"documentation/execute-and-automate/call-via-commandline/#basic-authentication-via-commandline","title":"Basic Authentication via Commandline","text":"<p>The command line tool supports extractions with basic authentication.</p> <p>When scheduling extractions by executing the command line tool, you can pass credentials for basic authentication as arguments. While the name of the Xtract Universal Custom User can be passed directly, the Custom User Password needs to be stored in a Base 64 encoded file that is accessible by the command line tool. The file format can be chosen freely, e.g. .txt, .json, .xml.</p> <ol> <li>Create a password file with the following command:  <pre><code>xu.exe -f &lt;path to the location and name of the file&gt;\n</code></pre> Example: <code>xu.exe -f \"C:\\temp\\[name of the password file]\"</code>. The Windows user needs access rights to the file location. You don't have to create this documents in advance. </li> <li>Enter a valid password as requested by the cmd screen. It must consist of at least 8 characters.</li> <li>Pass a user name and the path to the password file as arguments in the command line tool.  Example:  <pre><code>xu.exe -s todd.theobald.local -p 8165 -e -n MSEG -u Alice -b \"C:\\temp\\password_custom_user\"\n</code></pre></li> <li>Optional: When scheduling an extraction, make sure that the user of the task has access rights to the password file. </li> </ol> <p>For more information use the command <code>xu.exe -h</code>.</p>"},{"location":"documentation/execute-and-automate/call-via-commandline/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base Article: Call via Script</li> <li>Knowledge Base Article: Create Extractions via Commandline</li> <li>Knowledge Base Article: Execute &amp; Schedule all Extractions using an SSIS Package</li> </ul>"},{"location":"documentation/execute-and-automate/call-via-etl/","title":"Call via ETL-Tool","text":"<p>This page offers a list of resources about using Xtract Universal with ETL tools. All ETL tools offer the following options for running extractions:</p> <ul> <li>Call via command line </li> <li>Call via webservices</li> </ul>"},{"location":"documentation/execute-and-automate/call-via-etl/#integration-via-plug-in","title":"Integration via Plug-In","text":"<ul> <li>Xtract Universal Alteryx Plugin</li> <li>KNIME Integration via SAP Reader (Theobald Software)</li> <li>Dynamic Runtime Parameter within KNIME Workflow</li> <li>Xtract Universal Power BI Custom Connector</li> <li>Power BI Configure scheduled refresh</li> </ul>"},{"location":"documentation/execute-and-automate/call-via-etl/#integration-via-webservice","title":"Integration via Webservice","text":"<ul> <li>GCS Integration with Google Scheduler</li> <li>Schedule AWS Lambda Functions</li> <li>Generate Qlik data load script</li> <li>Azure Data Factory (ADF) Integration using Webservices</li> <li>SAP Integration with Matillion Data Loader and Xtract Universal</li> </ul>"},{"location":"documentation/execute-and-automate/call-via-etl/#integration-via-command-line","title":"Integration via Command Line","text":"<ul> <li>Azure Data Factory (ADF) Integration using Commandline</li> </ul>"},{"location":"documentation/execute-and-automate/call-via-etl/#integration-via-azure-data-factory","title":"Integration via Azure Data Factory","text":"<ul> <li>Azure Data Factory (ADF) Integration using Webservices</li> <li>Azure Data Factory (ADF) Integration using Commandline</li> <li>Call Dynamic Extractions with Variables in Azure Data Factory</li> <li>Run an ADF pipeline when an SAP extraction file is successfully uploaded to Azure storage</li> </ul>"},{"location":"documentation/execute-and-automate/call-via-scheduler/","title":"Call via Scheduler","text":"<p>This page shows how to use the command line tool xu.exe to schedule extractions with third party scheduling tools. Windows Task Scheduler and the SQL Server Agent are used as examples for setting up extraction schedules.</p>"},{"location":"documentation/execute-and-automate/call-via-scheduler/#call-via-windows-task-scheduler","title":"Call via Windows Task Scheduler","text":"<ol> <li>Open the Windows Task Scheduler by typing Taskschd.msc into the command line. </li> <li>Create a new task by clicking Actions &gt; Create Task ... . </li> <li>Enter a name for the task   and an optional description. </li> <li>In the tab Triggers click [New...] to add a time option.</li> <li>Set a start date and time and confirm the entry with [OK] . </li> <li>State the start program in Program / script in the tab Actions. Add the parameters of the extraction in Add arguments (optional). Example: <pre><code>\"C:\\Program Files\\XtractUniversal\\xu.exe\" -s todd.theobald.local -p 8065 -n SAPPlants\n</code></pre> </li> <li>Click [OK]  to confirm the input.</li> <li>Check the summary and finish the setup. </li> </ol> <p>The extraction is now scheduled and can be run by right-clicking the task and selecting the Run option. </p> <p>Tip</p> <p>Multiple extractions can be assigned to a single task. Edit the task and switch to the Actions tab. Create a new action as described above.</p>"},{"location":"documentation/execute-and-automate/call-via-scheduler/#call-via-sql-server-agent","title":"Call via SQL Server Agent","text":"<p>Note</p> <p>You must have all necessary authorization for creating and executing jobs with the SQL Server Agent.   </p> <ol> <li>Open the SQL-Server-Management-Studio (SSMS) to connect to an SQL-Server.</li> <li>Create a new job via SQL Server Agent &gt; New &gt; Job....  </li> <li>Navigate to Select a page &gt; Steps and click [New]. </li> <li>Enter a Step name, Type, Run as  and Command , e.g., xu, Operating system (CmdExec), SQL-Server-Agent Service Account. Example: <pre><code>\"C:\\Program Files\\XtractUniversal\\xu.exe\" -s localhost -p 8065 -n customers\n</code></pre> </li> <li>Click Select a page &gt; Advanced to set further options. Examples:<ul> <li>On success action, On failure action</li> <li>Retry attemps, Retry interval</li> <li>Output file</li> </ul> </li> <li>Click [OK] to confirm your input.</li> <li>Find the new job in SQL Server Agent &gt; Jobs. </li> <li>Right-click the job and select Start Job at Step... to execute the job. </li> <li>The successful execution of the SQL Server Agent job is displayed in SSMS. </li> </ol>"},{"location":"documentation/execute-and-automate/call-via-scheduler/#related-links","title":"Related Links","text":"<ul> <li>Microsoft Documentation: SQL Server Agent</li> <li>Microsoft Documentation: Start Task Scheduler</li> </ul>"},{"location":"documentation/execute-and-automate/run-an-extraction/","title":"Run Extractions in Xtract Universal","text":"<p>Xtract Universal Designer offers a test run option for extractions.  You can define runtime parameters and other options to run an extraction directly from the Xtract Universal Designer.</p>","boost":2},{"location":"documentation/execute-and-automate/run-an-extraction/#run-extractions-in-the-designer","title":"Run Extractions in the Designer","text":"<ol> <li>In the main window of the Designer, select an extraction and click [Run] . The window \"Run Extraction\" opens. </li> <li>If needed, define extraction parameters:<ul> <li>Select the checkbox of the parameter you want to override  . The parameter is automatically added to the extraction URL and the xu-exe string.  </li> <li>Enter a value for the parameter.</li> </ul> </li> <li>Click [Run] to execute the extraction. </li> </ol> <p>The status in the subsection General Info indicates if the extraction finished successfully and if the extracted data was written to the destination.</p>","boost":2},{"location":"documentation/execute-and-automate/run-an-extraction/#run-extraction-window","title":"Run Extraction Window","text":"<p>The \"Run extraction\" window consists the following subsections:</p> <ul> <li>General Info </li> <li>Runtime parameters </li> <li>URL and commandline </li> <li>Logs and Output </li> <li>Buttons </li> </ul> <p></p> <p>Tip</p> <p>You can open the \"Run extraction\" window by right-clicking an extraction or use the main menu bar to open the \"Run Extraction\" window.</p>","boost":2},{"location":"documentation/execute-and-automate/run-an-extraction/#general-info","title":"General Info","text":"Info Object Description Extraction name Name of the extraction Source Information about the source settings chosen for that extraction (Name, Host, Client, User Name, Instance No., Language) Destination Name of the destination (Name, Type, Pull destination info) Execution start Start date and time stamp of extraction run Time elapsed Elapsed time of the extraction run Rows extracted Number of extracted rows Status Extraction status Duration Extraction duration","boost":2},{"location":"documentation/execute-and-automate/run-an-extraction/#runtime-parameters","title":"Runtime Parameters","text":"<p>The three tabs \"Extraction\", \"Source\" and \"Custom\" contain extraction parameter to dynamize extractions. When editing extraction parameters, the URL and commandline of the extraction also change.</p> <p>For more information, see Extraction Parameters.</p> <p></p>","boost":2},{"location":"documentation/execute-and-automate/run-an-extraction/#url-and-commandline","title":"URL and Commandline","text":"<p>The strings displayed in URL and xu.exe are generated and update automatically when changing extraction parameters. Use the strings to run the extraction outside of the Xtract Universal Designer.</p>","boost":2},{"location":"documentation/execute-and-automate/run-an-extraction/#url","title":"URL","text":"<p>The extraction URL can be used in different integration scenarios and use cases. Examples: </p> <ul> <li>Use the extraction URL when it is not possible to use the command line tool xu.exe, e.g. in cloud based environments.</li> <li>Use the extraction URL when the extraction is set to a Pull Destination.</li> <li>Use the extraction URL to run the extraction in a web browser, e.g., for testing purposes. </li> </ul> <p>Copy the URL with Ctrl+C or use the [] button on the right.</p> <p>Tip</p> <p>You can run an extraction in the browser without opening the \"Run Extraction\" window by right-clicking an extraction and selecting the option Run in browser.</p>","boost":2},{"location":"documentation/execute-and-automate/run-an-extraction/#xuexe","title":"xu.exe","text":"<p>This command allows running an extraction with the command line tool xu.exe, that is installed with Xtract Universal. The tool is located in the installation directory of Xtract Universal, e.g., <code>C:\\Program Files\\XtractUniversal\\xu.exe</code>.</p> <p>Copy the expression with Ctrl+C or use the [] button on the right. </p> <p>It is recommended to use the command line tool with Push Destinations. It can be called from a Windows script or any scheduler, that can invoke Windows commandline calls.  In the most simple case, the Windows task scheduler can be used for calling and scheduling extractions using xu.exe. </p> <p>By default the following parameters are generated for any extraction:</p> <ul> <li>\"-s\" (Server for extraction)</li> <li>\"-p\" (Listening port of the Xtract Universal Server)</li> <li>\"-n\" (Name of the extraction)</li> </ul>","boost":2},{"location":"documentation/execute-and-automate/run-an-extraction/#logs-and-output","title":"Logs and Output","text":"<p>When an extraction is executed, information about the extraction is displayed in the following section.</p> <p></p>","boost":2},{"location":"documentation/execute-and-automate/run-an-extraction/#log","title":"Log","text":"<p>In the Log tab the extraction log is displayed in real time. Activate the checkbox Auto scroll to the end to automatically scroll down to the last protocol.  </p>","boost":2},{"location":"documentation/execute-and-automate/run-an-extraction/#xuexe_1","title":"xu.exe","text":"<p>In the xu.exe tab the log of the command line tool is displayed. </p>","boost":2},{"location":"documentation/execute-and-automate/run-an-extraction/#output","title":"Output","text":"<p>In the Output tab the results of the extraction are displayed. This option is only available for the following destinations: </p> <ul> <li>HTTP-CSV </li> <li>HTTP-JSON </li> <li>Alteryx</li> <li>Microsoft SQL Server</li> <li>Power BI Report Server (SQL Server Reporting Services) </li> <li>QlikSense&amp;QlikView</li> </ul> <p>Display rows from line The extraction results are displayed to a maximum amount of 500 rows. The number of rows to be displayed can be changed using the Display rows from line boxes.</p> <p>[Display] To filter the results, enter filter values above the columns of the results and click [Display].</p> <p>[Clear search] To undo any data filtering, click [Clear search], followed by [Display].</p>","boost":2},{"location":"documentation/execute-and-automate/run-an-extraction/#buttons","title":"Buttons","text":"<p>[Run] Runs the extraction.</p> <p>[Abort] Aborts the extraction.</p> <p>[Close] Closes the \"Run Extraction\" window.</p> <p>Note</p> <p>If the option Request SAP credentials from caller when running extractions is active in the extraction's source settings, you are prompted to enter your SAP credentials when running an extraction. For this option, extractions must be called via HTTPS - unrestricted.</p>","boost":2},{"location":"documentation/hierarchy/","title":"BW Hierarchy","text":"<p>This page shows how to use the BW Hierarchy extraction type. The BW Hierarchy extraction type can be used to extract Hierarchies and InfoObjects from SAP BW systems.</p> <p>Note</p> <p>To extract hierarchies from a BW system, use the BW Hierarchy extraction type. To extract hierarchies from an ERP system like ECC or S4, use the ODP extraction type.</p>"},{"location":"documentation/hierarchy/#prerequisites","title":"Prerequisites","text":"<ul> <li>A connection to an SAP system is available, see SAP Connection.</li> <li>The SAP user has sufficient user rights, see SAP Authority Objects.</li> </ul> <p>Warning</p> <p>Missing Authorization. To use the BW Hierarchy extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust SAP Authority Objects accordingly.</p>"},{"location":"documentation/hierarchy/#create-a-bw-hierarchy-extraction","title":"Create a BW Hierarchy Extraction","text":"<ol> <li>In the main window of the Designer, click [New] to create a new extraction. The window \"Create Extraction\" opens. </li> <li>Select an SAP Connection from the drop-down menu in Source and enter a unique name for your extraction.</li> <li>Select the extraction type BW Hierarchy and click [OK]. The main window of the extraction type opens automatically.</li> </ol> <p>The majority of the functions of the extraction type can be accessed in the main window.</p>"},{"location":"documentation/hierarchy/#look-up-a-hierarchy","title":"Look Up a Hierarchy","text":"<ol> <li>In the main window of the component, click [ ]. The window \u201cHierarchy Lookup\u201d opens. </li> <li>Enter the name of a Hierarchy in the field Hierarchy Name or the name of an InfoObject in the field InfoObject . Use wildcards (*) if needed.  </li> <li>Click [ ]. Search results are displayed.</li> <li>Select a Hierarchy   and click [Select].</li> </ol> <p>The application returns to the main window of the extraction type.</p>"},{"location":"documentation/hierarchy/#define-the-bw-hierarchy-extraction-type","title":"Define the BW Hierarchy Extraction Type","text":"<p>The BW Hierarchy extraction type offers the following options for Hierarchy extractions:</p> <ol> <li> <p>Define the output format of the Hierarchy in the extraction settings . The BW Hierarchy extraction type supports the following output formats:</p> <ul> <li>ParentChild Format</li> <li>Natural Format</li> <li>ParentChildWithNodeNames Format</li> </ul> <p> </p> </li> <li> <p>Click [Load live preview]  to display a live preview of the first 100 records. </p> </li> <li>Check the General Settings before running the extraction.. </li> <li>Optional: The default value for Date To is 99991231. To change the value, override the dateTo extraction parameter when calling the extraction. </li> <li>Click [OK] to save the extraction type.</li> </ol> <p>You can now run the extraction, see Execute and Automate Extractions.</p>"},{"location":"documentation/hierarchy/#related-links","title":"Related Links","text":"<ul> <li>Extraction Settings</li> <li>SAP Help: Uploading Hierarchies from Flat Files</li> </ul>"},{"location":"documentation/hierarchy/general-settings/","title":"General Settings","text":"<p>This page contains an overview of the settings in the window \"General Settings\". To open the general settings, click General Settings in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/hierarchy/general-settings/#misc-tab","title":"Misc. Tab","text":"<p>The Misc. tab covers cache settings, column encryption and keywords of an extraction type.</p> <p></p>"},{"location":"documentation/hierarchy/general-settings/#cache-results","title":"Cache results","text":"<p>The Cache results option is only available in pull destinations, e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times.  To decrease the SAP server load, you can select the Cache results option, this way the pull destination pulls the data from cache and not from the SAP.</p> <p>This increases the performance and limits the impact on the SAP system.  If this behavior is not desired (for example, because the data must be always 100% up to date), the cache option must be explicitly turned off.</p>"},{"location":"documentation/hierarchy/general-settings/#keywords","title":"Keywords","text":"<p>One or more keywords (tags) can be assigned to an extraction.  Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the \"Search Extractions\" window. To open the \"Search Extractions\" window, click [  Search] in the main window of the Designer. </p> <p>Tip</p> <p>To add keywords to multiple extractions at once, select the extractions in the main window of the Designer. Right-click + Add/Remove keywords opens the window \"Add/Remove Keywords To/From Multiple Extractions\".</p>"},{"location":"documentation/hierarchy/general-settings/#primary-key-tab","title":"Primary Key Tab","text":"<p>Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.  </p> <p></p> <p>The depicted example shows the SAP object MAKT with its primary key inherited from SAP in the general settings of the Designer.  In this example the primary key consists of MANDT, MATNR and SPRAS. The primary key is also taken over in the destination. </p> <p>Note</p> <p>A defined primary key field in a table is a prerequisite for merging data. </p>"},{"location":"documentation/hierarchy/general-settings/#generate-surrogate-key-column","title":"Generate Surrogate Key Column","text":"<p>If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs.  The surrogate keys are hash values of type signed 8 byte integer, e.g., <code>#-3008591679982390000</code>. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.</p>"},{"location":"documentation/hierarchy/general-settings/#security-tab","title":"Security Tab","text":"<p>Restrict user access to the extraction. For more information, see Restrict Designer Access.</p> <p></p>"},{"location":"documentation/hierarchy/general-settings/#columns-order-tab","title":"Columns Order Tab","text":"<p>The \"Columns Order\" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns.  Index 0 defines the first column in the result set, index 1 the second columns, etc.</p> <p></p> Option Description Enabled When this option is active, the defined column order is applied when running the extraction. Swap Swaps the index of 2 columns. All other columns keep their indexes. Insert Inserts the selected column into the selected index. All other indexes are recalculated. Reset default Restores the original column order."},{"location":"documentation/hierarchy/output-format/","title":"Output Formats","text":"<p>This page shows the which output formats are supported by the BW Hierarchy extraction type. The output format can be selected in the extraction settings of the BW Hierarchy extraction type.</p>"},{"location":"documentation/hierarchy/output-format/#parentchild-format","title":"ParentChild Format","text":"<p>The default output of the BW Hierarchy extraction type contains the following columns:</p> Column Description NodeID Unique node key. ParentNodeID Key for parent node. FirstChildNodeID Key for first child node. NextNodeID Key for next node in the same hierarchical level. InfoObjectName Name of InfoObject behind the corresponding node. NodeName The node\u2019s (technical) name. NodeText The description text of the node. This column is only created when Fetch description texts is active. DateFrom Date from which the node is valid. DateTo Date the node is valid to. Link If the value in Link is greater than 0, the node is a link node. The ID of the original node that was linked from is displayed. Row Number of the row. The row number can be used as an ID or sort criterion when processing the extracted data. <p>Example:</p> Hierarchy PM_COUNTRY in ParentChild Output FormatHierarchy PM_COUNTRY Format in SAP <p></p> <p></p>"},{"location":"documentation/hierarchy/output-format/#natural-format","title":"Natural Format","text":"Column Description LevelN Technical name of the node of the nth level. The number of levels is set in Level Count. The level count starts at level 0. LevelTextN The description text of the nth level's node. This column is only created when Description texts for levels is active. InfoObjectName Name of InfoObject behind the node of the highest level. NodeName Technical name of the node of the highest level. NodeText The description text of the node of the highest level. This column is only created when Fetch description texts is active. DateFrom Date from which the node is valid DateTo Date the node is valid to. Link If the value in Link is greater than 0, the node is a link node. The ID of the original node that was linked from is displayed. Row Number of the row. The row number can be used as an ID or sort criterion when processing the extracted data. <p>Example:</p> Hierarchy PM_COUNTRY in Natural Output FormatHierarchy PM_COUNTRY Format in SAP <p></p> <p></p>"},{"location":"documentation/hierarchy/output-format/#parentchildwithnodenames-format","title":"ParentChildWithNodeNames Format","text":"Column Description NodeID Unique node key. NodeName The node\u2019s (technical) name. NodeText The description text of the node. This column is only created when Fetch description texts is active. ParentNodeID Key for parent node. ParentNodeName Name of the parent node. InfoObjectName Name of InfoObject behind the corresponding node. DateFrom Date from which the node is valid. DateTo Date the node is valid to. Link If the value in Link is greater than 0, the node is a link node. The ID of the original node that was linked from is displayed. Row Number of the row. The row number can be used as an ID or sort criterion when processing the extracted data. <p>Example:</p> Hierarchy PM_COUNTRY in ParentChildWithNodeNames Output FormatHierarchy PM_COUNTRY Format in SAP <p></p> <p></p>"},{"location":"documentation/hierarchy/settings/","title":"Extraction Settings","text":"<p>This page contains an overview of the extraction settings in the BW Hierarchy extraction type. To open the extraction settings, click Extraction Settings in the main window of the extraction type. </p> <p></p>"},{"location":"documentation/hierarchy/settings/#extraction-settings","title":"Extraction Settings","text":""},{"location":"documentation/hierarchy/settings/#representation","title":"Representation","text":"<ul> <li>ParentChild: The Hierarchy is represented in the SAP parent-child format, see Output Formats: ParentChild. Example: </li> <li>Natural: The SAP parent-child Hierarchy is transformed into a regular hierarchy, see Output Formats: Natural. Example: </li> <li>ParentChildWithNodeNames: The Hierarchy is represented in a reduced SAP parent-child format that only includes single nodes and their parent, see Output Formats: ParentChildWithNodeNames. Example: </li> </ul>"},{"location":"documentation/hierarchy/settings/#remove-leading-zeros","title":"Remove Leading Zeros","text":"<p>If this option is active, all leading zeros in the column NodeName of the leaves are removed. NodeName can then be used in a JOIN-condition with the corresponding Dimension-Key of a BW Cube extraction. The conversion works for compound InfoObjects, too.  Example: 0CO_AREA (1000) and 0COSTCENTER (0000003100) become 1000/3100.</p>"},{"location":"documentation/hierarchy/settings/#fetch-description-texts","title":"Fetch description texts","text":"<p>Sets the node text in the column NodeText.  All texts of InfoObjects that have language dependent texts will be retrieved in the language that the SAP source connection uses.  If no texts exist, the result does not contain any texts for entries of that InfoObject.</p>"},{"location":"documentation/hierarchy/settings/#natural-settings","title":"Natural Settings","text":"<p>Note</p> <p>The subsection Natural Settings is only active, when the Representation is set to Natural.</p>"},{"location":"documentation/hierarchy/settings/#level-count","title":"Level Count","text":"<p>Defines the maximum number of levels. The following example shows a Hierarchy with four levels.  </p>"},{"location":"documentation/hierarchy/settings/#fill-empty-levels","title":"Fill empty levels","text":"<p>Copies the bottom element of the Hierarchy until the last level. The following example depicts the previously shown Hierarchy with the activated Repeat Leaves option. </p>"},{"location":"documentation/hierarchy/settings/#description-texts-for-levels","title":"Description texts for levels","text":"<p>Sets the output field LevelTextN for each field LevelN containing the text based on the system language settings. </p>"},{"location":"documentation/hierarchy/settings/#leaves-only","title":"Leaves only","text":"<p>Returns only the leaves as data records. </p>"},{"location":"documentation/hierarchy/settings/#debug","title":"Debug","text":""},{"location":"documentation/hierarchy/settings/#enable-debug-logging","title":"Enable Debug Logging","text":"<p>Adds more detailed logs for the BW Hierarchy extraction type to the extraction logs.  Activate Enable Debug Logging only when necessary, e.g., upon request of the support team.</p>"},{"location":"documentation/hierarchy/settings/#related-links","title":"Related Links","text":"<ul> <li>SAP Help: About SAP BW Hierarchies</li> </ul>"},{"location":"documentation/odp/","title":"ODP","text":"<p>This page shows how to use the ODP extraction type. The ODP extraction type can be used to extract data via the SAP Operational Data Provisioning (ODP) framework.</p>"},{"location":"documentation/odp/#about-odp","title":"About ODP","text":"<p>Operational data provisioning (ODP) is a framework in SAP ABAP applications for transferring data between systems. ODP provides a technical infrastructure for data extraction and replication from different SAP (ABAP) Systems, e.g.:</p> <ul> <li>ECC</li> <li>S/4 HANA</li> <li>BW </li> <li>BW/4 HANA</li> </ul> <p>The ODP extraction type acts as a subscriber (consumer) and subscribes to a data provider.  ODP supports mechanisms to load data incrementally from data providers. For SAP BW/4HANA, ODP is the central infrastructure for data extraction and replication from SAP (ABAP) applications to an SAP BW/4HANA Data Warehouse. </p> <p>The ODP extraction type provides data transfer from the following providers (also called Provider Context): </p> Provider Context SAP Source Objects ABAP Core Data Services [ABAP_CDS] - CDS Views SAP NetWeaver Business Warehouse [BW] BW/4HANA: - DSO / aDSO - CompositeProvider - InfoObjects  - Query as InfoProvider BW systems: - CompositeProvider - InfoCubes - Semantically partitioned objects - HybridProviders - MultiProviders - InfoSets SAP HANA Information Views [HANA] - Analysis Views - Calculation Views - Associated Attribute Views DataSources/Extractors [SAPI] - DataSources and Extractors SAP LT Queue Alias [SLT~your_queue_alias] - SAP Tables - Cluster tables - Pool tables <p>Depending on the connected SAP source system there are differences in available provider contexts. For more information on SAP ODP, see SAP Wiki: Operational Data Provisioning (ODP) and Delta Queue (ODQ).</p>"},{"location":"documentation/odp/#prerequisites","title":"Prerequisites","text":"<ul> <li>A connection to an SAP system is available, see SAP Connection.</li> <li>The SAP user has sufficient user rights, see SAP Authority Objects.</li> <li>Implement the following SAP notes to use ODP:<ul> <li>1931427 - ODP Data Replication API 2.0</li> <li>2232584 - Release of SAP extractors for ODP replication (ODP SAPI)</li> <li>1560241 - Release of DataSources for ODP data replication API</li> <li>2196500 - ODP Package size cannot be reduced below 50 MB</li> <li>2191995 - ODQ Package Size cannot be reduced below 50 MByte</li> </ul> </li> <li>DataSources have to be activated in SAP, see SAP Help: Activating DataSources in the SAP OLTP System.</li> <li>Before creating ODP extractions, test the ODP source in SAP using the ABAP report RODPS_REPL_TEST to rule out and troubleshoot ODP problems in the ODP source. For more information, see SAP Wiki: Replication test with RODPS_REPL_TEST.</li> </ul> <p>Note</p> <p>The ODP API 1.0 has limitations compared to ODP API 2.0, e.g., ODP API 1.0 does not support the extraction of Hierarchy DataSources.  For more information, see SAP Wiki: Limitation of ODP API 1.0. </p> <p>Warning</p> <p>Missing Authorization. To use the ODP extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust SAP Authority Objects accordingly.</p>"},{"location":"documentation/odp/#create-an-odp-extraction","title":"Create an ODP Extraction","text":"<ol> <li>In the main window of the Designer, click [New] to create a new extraction. The window \"Create Extraction\" opens. </li> <li>Select an SAP Connection from the drop-down menu in Source and enter a unique name for your extraction.</li> <li>Select the extraction type ODP and click [OK]. The main window of the extraction type opens automatically.</li> </ol> <p>The majority of the functions of the extraction type can be accessed in the main window.</p>"},{"location":"documentation/odp/#look-up-data-objects","title":"Look up Data Objects","text":"<ol> <li>In the main window of the extraction type, click [ ]. The window \u201cOperational Data Provider Lookup\u201d opens. </li> <li>In the field Name, enter the name of an extractor  . Use wildcards (*), if needed. </li> <li>Select a Context . Depending on the connected SAP source system there are differences in available Provider Contexts.</li> <li>Click [ ]. Search results are displayed.</li> <li>Select an extractor   and click [OK] to confirm.</li> </ol> <p>The application now returns to the main window of the extraction type.</p> <p>Note</p> <p>To find DataSources, they have to be activated in SAP.</p>"},{"location":"documentation/odp/#define-the-odp-extraction-type","title":"Define the ODP Extraction Type","text":"<p>The ODP extraction type offers the following options for data extractions:</p> <ol> <li> <p>In the section Fields, select the items you want to extract. </p> <p>Note</p> <p>TS_SEQUENCE_NUMBER is a technical primary key that can be added to the output. When working with identical data sets, the data set with the highest sequence number is the most current data set.</p> </li> <li> <p>Optional: edit a selection you want to change or dynamize.  For more information, see Edit Selections.</p> <p>Note</p> <p>If your data source is a Hierarchy, see Hierarchy Segments for filter options.</p> </li> <li> <p>Select an Update Mode, e.g., to initialize delta extractions.</p> </li> <li>Click [Load live preview] to display a live preview of the first 100 records.</li> <li>Check the Extraction Settings and the General Settings before running the extraction.</li> <li>Click [OK] to save the extraction type.</li> </ol> <p>You can now run the extraction, see Execute and Automate Extractions.</p>"},{"location":"documentation/odp/#related-links","title":"Related Links","text":"<ul> <li>Youtube Tutorial: SAP ODP incremental load to SQL server</li> </ul>"},{"location":"documentation/odp/edit-runtime-parameters/","title":"Runtime Parameters","text":"<p>Runtime parameters are are placeholders for values that are passed at runtime, see Extraction Parameters - Custom. They can be created in context of Selections.</p>"},{"location":"documentation/odp/edit-runtime-parameters/#create-runtime-parameters","title":"Create Runtime Parameters","text":"<p>There are two types of runtime parameters:</p> Scalar parametersList parameters <p>Scalar runtime parameters represent a single value.  Follow the steps below to create a scalar runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add Scalar] to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime. </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.</p> Type Description Text Can be used for any type of SAP selection field. Number Can be used for numeric SAP selection fields. Flag Can only be used for SAP selection fields THAT require an \u2018X\u2019 (true) or a blank \u2018\u2018 (false) as input value. </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p> <p>List runtime parameters represent multiple values.  Follow the steps below to create a list runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add List] to define list parameters that contain multiple values separated by commas e.g., 1,10 or \u201c1\u201d, \u201c10\u201d. The placeholders need to be populated with actual values at runtime.  </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p>"},{"location":"documentation/odp/edit-runtime-parameters/#assign-runtime-parameters","title":"Assign Runtime Parameters","text":"<p>Follow the steps below to assign the runtime parameters to selections.</p> <ol> <li>In the main window of the extraction type, click the [Edit] button next to the selection you want to parameterize.  The window \"Edit Selections\" opens.</li> <li>Add a filter to the selection, see Selections and Filters. </li> <li>Click the icon button next to the input field to switch between static values ( ) and runtime parameters ( ). If no icon button is available, create a runtime parameter. </li> <li>Select a runtime parameter from the dropdown-list.</li> <li>Click [OK] to confirm the input.</li> </ol> <p>Pass values during runtime, see Extraction Parameters - Custom.</p>"},{"location":"documentation/odp/general-settings/","title":"General Settings","text":"<p>This page contains an overview of the settings in the window \"General Settings\". To open the general settings, click General Settings in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/odp/general-settings/#misc-tab","title":"Misc. Tab","text":"<p>The Misc. tab covers cache settings, column encryption and keywords of an extraction type.</p> <p></p>"},{"location":"documentation/odp/general-settings/#cache-results","title":"Cache results","text":"<p>The Cache results option is only available in pull destinations, e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times.  To decrease the SAP server load, you can select the Cache results option, this way the pull destination pulls the data from cache and not from the SAP.</p> <p>This increases the performance and limits the impact on the SAP system.  If this behavior is not desired (for example, because the data must be always 100% up to date), the cache option must be explicitly turned off.</p>"},{"location":"documentation/odp/general-settings/#keywords","title":"Keywords","text":"<p>One or more keywords (tags) can be assigned to an extraction.  Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the \"Search Extractions\" window. To open the \"Search Extractions\" window, click [  Search] in the main window of the Designer. </p> <p>Tip</p> <p>To add keywords to multiple extractions at once, select the extractions in the main window of the Designer. Right-click + Add/Remove keywords opens the window \"Add/Remove Keywords To/From Multiple Extractions\".</p>"},{"location":"documentation/odp/general-settings/#primary-key-tab","title":"Primary Key Tab","text":"<p>Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.  </p> <p></p> <p>The depicted example shows the SAP object MAKT with its primary key inherited from SAP in the general settings of the Designer.  In this example the primary key consists of MANDT, MATNR and SPRAS. The primary key is also taken over in the destination. </p> <p>Note</p> <p>A defined primary key field in a table is a prerequisite for merging data. </p>"},{"location":"documentation/odp/general-settings/#generate-surrogate-key-column","title":"Generate Surrogate Key Column","text":"<p>If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs.  The surrogate keys are hash values of type signed 8 byte integer, e.g., <code>#-3008591679982390000</code>. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.</p>"},{"location":"documentation/odp/general-settings/#security-tab","title":"Security Tab","text":"<p>Restrict user access to the extraction. For more information, see Restrict Designer Access.</p> <p></p>"},{"location":"documentation/odp/general-settings/#columns-order-tab","title":"Columns Order Tab","text":"<p>The \"Columns Order\" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns.  Index 0 defines the first column in the result set, index 1 the second columns, etc.</p> <p></p> Option Description Enabled When this option is active, the defined column order is applied when running the extraction. Swap Swaps the index of 2 columns. All other columns keep their indexes. Insert Inserts the selected column into the selected index. All other indexes are recalculated. Reset default Restores the original column order."},{"location":"documentation/odp/provider-context/","title":"Provider Context","text":"<p>This page shows how to use the Provider Contexts of the ODP extraction type. The ODP extraction type supports the following Provider Contexts:</p> Provider Context SAP Source Objects ABAP Core Data Services [ABAP_CDS] - CDS Views SAP NetWeaver Business Warehouse [BW] BW/4HANA: - DSO / aDSO - CompositeProvider - InfoObjects  - Query as InfoProvider BW systems: - CompositeProvider - InfoCubes - Semantically partitioned objects - HybridProviders - MultiProviders - InfoSets SAP HANA Information Views [HANA] - Analysis Views - Calculation Views - Associated Attribute Views DataSources/Extractors [SAPI] - DataSources and Extractors SAP LT Queue Alias [SLT~your_queue_alias] - SAP Tables - Cluster tables - Pool tables"},{"location":"documentation/odp/provider-context/#abap-cds-views","title":"ABAP CDS Views","text":"<p>According to the Core Data Service (CDS) concept, data models based on CDS serve as central definitions that can be used in many different domains, such as transactional and analytical applications.  </p> <p>CDS is defined using an SQL-based data definition language (DDL).  DLL leverages the standard SQL with several additional concepts, such as associations, which define the relationships between CDS views, and annotations that direct the domain-specific use of CDS artifacts.</p>"},{"location":"documentation/odp/provider-context/#available-cds","title":"Available CDS","text":"<p>There are two types of CDS available:</p> <ul> <li>HANA CDS (defined in XS engine) </li> <li>ABAP CDS</li> </ul> <p>For each ABAP CDS entity defined in the DDL source code, an SQL view is generated in the ABAP Dictionary.  The same way as the views created with Dictionary tools (transaction SE11), ABAP CDS entities can be accessed in ABAP using Open SQL statements. </p> <p>CDS is important for SAP application development. For example, S/4HANA uses CDS to provide both core transactional functionality as well as analytical content for reporting.  In BW/4 HANA, ABAP based CDS views can be used for data extraction.  </p> <p>Two possible use cases for CDS:</p> <ul> <li>Direct access to a CDS view from BW/4 HANA  </li> <li>Delta extraction from a CDS view in S/4 HANA to BW</li> </ul> <p>CDS Views support Full and Delta Extraction. </p>"},{"location":"documentation/odp/provider-context/#replace-bw-extractors-with-cds-views-in-s4hana","title":"Replace BW Extractors with CDS Views in S/4HANA","text":"<p>In majority of cases traditional BW extractors can be used to extract data from an S/4HANA systems.  Due to simplifications in the S/4HANA system, several extractors are now deprecated or can no longer be used in a usual manner.  To use the deprecated or altered extractors, SAP may deliver extraction relevant (via the analytical annotations) CDS Views.</p>"},{"location":"documentation/odp/provider-context/#use-abap-cds-views","title":"Use ABAP CDS Views","text":"<p>Note</p> <p>To find an ABAP CDS view it must have the following Annotation: <code>@Analytics.dataExtraction.Enabled: true</code>.  If the source SAP system is not on a HANA DB, an additional Annotation is needed: <code>@Analytics.dataCategory: #CUBE/#FACT/#DIMENSION</code>.</p> <p>Things that need to be considered when using BW InfoProviders:</p> <ol> <li>When looking up ABAP CDS Views in the ODP extraction type, make sure to select the correct context.  </li> <li>When the delta load is available for the source object in the SAP source system, the Delta Update option is available in the ODP extraction type. </li> <li>Click [Load live preview] to preview the data without running an extraction.</li> </ol>"},{"location":"documentation/odp/provider-context/#bw-infoproviders","title":"BW InfoProviders","text":"<p>The Operational Data Provisioning (ODP) framework allows you to extract data from the InfoProviders in your source BW and BW/4 HANA systems.  Available InfoProvider types are dependent on your source BW and BW/4 HANA system.</p>"},{"location":"documentation/odp/provider-context/#available-infoproviders","title":"Available InfoProviders","text":"<ul> <li> <p>Available InfoProviders in SAP BW\u22154HANA</p> <ul> <li>CompositeProvider - object type HCPR (full extraction)</li> <li>DataStore objects (with delta extraction)<ul> <li>InfoObjects</li> <li>Master data</li> <li>Texts</li> <li>Hierarchies</li> </ul> </li> <li>Query as InfoProvider</li> </ul> </li> <li> <p>Additional InfoProviders in BW Systems</p> <ul> <li>CompositeProvider - object type HCPR (full extraction)</li> <li>InfoCubes (with delta extraction)</li> <li>Semantically partitioned objects</li> <li>HybridProviders</li> <li>MultiProviders</li> <li>InfoSets</li> </ul> </li> </ul>"},{"location":"documentation/odp/provider-context/#use-bw-infoproviders","title":"Use BW InfoProviders","text":"<p>Things that need to be considered when using BW InfoProviders:</p> <ol> <li>When looking up BW InfoProviders in the ODP extraction type, make sure to select the correct context.  </li> <li>When the delta load is available for the source object in the SAP source system, the Delta Update option is available in the ODP extraction type. </li> <li>Click [Load live preview] to preview the data without running an extraction.</li> </ol>"},{"location":"documentation/odp/provider-context/#extractors","title":"Extractors","text":"<p>The ODP extraction type can be used to extract data from Business Content DataSource (Extractors).  The majority of DataSources, including generic (custom) DataSources, can be released for Operational Data Provisioning. </p> <p>The ODP extraction type does not change the implementation of application extractors. All features and capabilities remain unchanged. </p> <p>An extractor (in ERP or S/4 HANA) is an encapsulated business object, representing multiple source tables already in the source system and containing business logic.</p>"},{"location":"documentation/odp/provider-context/#available-extractors","title":"Available Extractors","text":"<ul> <li>Transactional data</li> <li>Master data</li> <li>Text data</li> <li>Hierarchy data</li> </ul> <p>There are standard delta extraction methods available for master data and transaction data. </p>"},{"location":"documentation/odp/provider-context/#use-extractors","title":"Use Extractors","text":"<p>Things that need to be considered when using Extractors:</p> <ul> <li>DataSources have to be activated in SAP, see SAP Help: Set Up and Activate DataSources.</li> <li>When looking up DataSources or Extractors, make sure to select the correct context.  </li> <li>If delta load is available for the source object in the SAP source system, the Delta Update option is available in the ODP extraction type. </li> <li>If the DataSource is a Hierarchy, there are additional settings, see Hierarchies.</li> </ul>"},{"location":"documentation/odp/provider-context/#hierarchies","title":"Hierarchies","text":"<p>If the selected source object is of type Hierarchy, the window \"Select Hierarchy\" opens.</p> <ol> <li>Select a Hierarchy from the list of Hierarchies in the \"Select Hierarchy\" window.  </li> <li>Confirm your selection with [OK].  The name of the selected Hierarchy is displayed under Selected Hierarchy .</li> <li>Select which segments to extract  .  </li> </ol>"},{"location":"documentation/odp/provider-context/#segments-to-extract","title":"Segment(s) to extract","text":"<p>Hierarchies are divided into segments by the API. Choose which segments of the Hierarchy to extract  . The selected data is displayed in the Fields section.</p> Option Description Merges All segments contain the field Node ID.  Using the Node ID Merges automatically combines all 3 segments. Elements The segment Elements contains information about the elements of the Hierarchy e.g., name, parent, child, etc. Texts The segment Texts contains the description texts of the Elements.  The language of the descriptions depends on the language settings of the SAP connection. Intervals The segment Intervals contains additional information if an element is an interval. In ODP no TO and FROM columns are displayed, see SAP Note 3090500. <p>Note</p> <p>Hierarchies can be passed as runtime parameters at runtime.</p>"},{"location":"documentation/odp/provider-context/#hana-views","title":"HANA Views","text":"<p>Operational Data Provisioning (ODP) can be used to connect SAP HANA database of an SAP ABAP source system. The connection is provided via RFC. </p>"},{"location":"documentation/odp/provider-context/#available-hana-views","title":"Available HANA Views:","text":"<ul> <li>Analysis Views</li> <li>Calculation Views </li> <li>Associated Attribute Views </li> </ul>"},{"location":"documentation/odp/provider-context/#use-hana-views","title":"Use HANA Views","text":"<p>Things that need to be considered when using HANA Views:</p> <ol> <li>When looking up HANA Views in the ODP extraction type, make sure to select the correct context.  </li> <li>When the delta load is available for the source object in the SAP source system, the Delta Update option is available in the ODP extraction type. </li> <li>Click [Load live preview] to preview the data without running an extraction.</li> </ol>"},{"location":"documentation/odp/provider-context/#slt-server","title":"SLT Server","text":"<p>The Operational Data Provisioning (ODP) framework allows you to extract tables and simple views from SAP HANA systems via an SAP Landscape Transformation Replication Server (SLT). The SLT server is a trigger-based CDC solution that can replicate SAP tables and views and make them available as delta extracts.</p>"},{"location":"documentation/odp/provider-context/#available-tables","title":"Available Tables","text":"<ul> <li>Regular SAP tables</li> <li>Cluster tables</li> <li>Pool tables</li> </ul>"},{"location":"documentation/odp/provider-context/#requirements","title":"Requirements","text":"<p>The SLT server context requires an SAP Landscape Transformation Replication Server (SLT) that is set up for ODP, see SAP Help: Transferring Data from SLT Using Operational Data Provisioning.</p> <p>The following requirements apply to the SLT server:</p> <ul> <li>Minimum release version of the SLT server:<ul> <li>Add-On DMIS 2011 SP05 </li> <li>SAP NetWeaver 7.3 SPS10, 7.31 SPS09 or 7.4 SPS04 (ODP infrastructure)</li> </ul> </li> <li>Add-On DMIS_2011 SP03/SP04 or higher or 2010 SP08/SP09 is installed in the SAP source system.</li> <li>The following SAP Notes are published in the SAP source system: <ul> <li>SAP Note 1863476 </li> <li>SAP Note 1817467 when using Add-On DMIS 2011 SP05</li> </ul> </li> </ul>"},{"location":"documentation/odp/provider-context/#use-slt-server","title":"Use SLT Server","text":"<p>Consider the following when using an SLT server:</p> <ol> <li>When looking up data via an SLT server, make sure to select the correct context.  </li> <li>A live preview of the data is not available in the SLT server context. </li> </ol> <p>For more information on SLT servers, see SAP Help: Transferring Data from SLT Using Operational Data Provisioning or download the SLT-Performance-Guide (Nov 2022).</p>"},{"location":"documentation/odp/selections/","title":"Selections","text":"<p>This page shows how to filter the data that is extracted by the ODP extraction type. Selections limit the result set of the ODP extraction type to extract only records that match the selection.</p>"},{"location":"documentation/odp/selections/#edit-selections","title":"Edit Selections","text":"<p>Follow the steps below to edit selection fields and filter data:</p> <ol> <li>In the subsection Fields, click Edit next to the field you want to edit. The window \u201cEdit selection\u201d opens. </li> <li>Add one or more of the following filter types:<ul> <li>Click [Single] to compare the data to a single specified value.</li> <li>Click [Range] to check if the data is (not) within a specified range of values.</li> <li>Click [List] to check if the data is (not) part of a specified list of values. </li> </ul> </li> <li>In the column Sign , select Include to add the filtered data to the output or select Exclude to remove the filtered data from the output. </li> <li> <p>In the column Option , select an operator. The operator filters data according to the table below.</p> Operator Description (not) like pattern True if data values do (not) contain to the content of operand 1. Not not all ODP contexts and data sources support this option. (not) equal to True if data is (not) equal to the content of operand 1. at least True if data is greater than or equal to the content of operand 1. more than True if data is greater than the content of operand 1. at most True if data is less than or equal to the content of operand 1. less than True if data is less than the content of operand 1. (not) between True if data values do (not) lie between the values of operand 1 and operand 2. element of True if data values are part of operand 1. This option is only available for type List. </li> <li> <p>In the column Value, enter values directly into the input fields Low and High or assign existing  runtime parameters  to the selection fields  .</p> <p>Note</p> <p>When runtime parameters are available, you can use the icon button inside the input field to switch between static values ( ) and runtime parameters ( ).</p> </li> <li> <p>Click [OK] to confirm your input. </p> </li> <li>Click [Load live preview] in the main window of the extraction type to check the result of your selection.  If runtime parameters are defined, you are prompted to populate the parameters with actual values.</li> </ol> <p>The number of defined filters is displayed in square brackets next to the Edit option.</p>"},{"location":"documentation/odp/selections/#data-format","title":"Data Format","text":"<p>Use the following internal SAP representation for input:</p> <ul> <li>Date: The date 01.01.1999 has the internal representation 19990101 (YYYYMMDD).</li> <li>Year period: The year period 001.1999 has the internal representation 1999001 (YYYYPPP).</li> <li>Numbers: Numbers must contain the leading zeros, e.g., customer number 1000 has the internal representation 0000001000.</li> </ul> <p>Warning</p> <p>Values accept only the internal SAP representation. Input that does not use the internal SAP representation results in error messages.  Use the internal SAP representation.  Example:  <pre><code>ERPConnect.ABAPProgramException: RfcInvoke failed(RFC_ABAP_MESSAGE): Enter date in the format \\_.\\_.\\_\n</code></pre></p>"},{"location":"documentation/odp/settings/","title":"Extraction Settings","text":"<p>This page contains an overview of the extraction settings in the ODP extraction type. To open the extraction settings, click Extraction Settings in the main window of the extraction type. </p> <p></p>"},{"location":"documentation/odp/settings/#extraction-settings","title":"Extraction Settings","text":""},{"location":"documentation/odp/settings/#package-size","title":"Package Size","text":"<p>The extracted data is be split into packages of the defined size. The default value is 50000 lines. A package size between 20000 and 50000 is advisable for large amounts of data. 0 means no packaging.  Not using packaging can lead to an RFC timeout for large data extracts.</p> <p>Warning</p> <p>RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.</p> <p>Make sure that your SAP version includes the necessary notes, see Prerequisites. </p>"},{"location":"documentation/odp/settings/#adjust-currency-decimals","title":"Adjust Currency Decimals","text":"<p>The default number of decimal places for a currency in the SAP database is 2 decimals. Currencies that do not have decimals are also stored in this format, e.g. JPY, VND, KRW, etc.</p> <p>Example:</p> Currency Actual Amount Amount stored in SAP database JPY 100 1.00 KRW 10000 100.00 <p>When extracting currencies with no decimals, the amount stored in SAP is returned e.g., 100 JPY are extracted as 1.00. To correct the decimal placement of the extracted data, activate Adjust Currency Decimals. If Adjust Currency Decimals is active, currencies without decimals are multiplied by a factor that balances out the decimals.</p> <p>Adjust Currency Decimals also requires the extraction of the corresponding CURRENCY field that can be used as a reference for the multiplication factor. Use the [Load live preview] function to find the correct currency field/s. </p> <ul> <li>If the currency field is part of the table, add it to the output.</li> <li>If the currency field is in another table, join the tables. </li> <li>If the reference is not part of a table, Adjust Currency Decimals cannot be used.</li> </ul> <p>Note</p> <p>The multiplication factor used in Adjust Currency Decimals is determined by the SAP currency table TCURX.  To access the table, the following SAP Authority objects must be set in SAP: S_TABU_NAM    ACTVT=03; TABLE=TCURX.</p>"},{"location":"documentation/odp/subscriptions/","title":"Subscriptions","text":"<p>This page contains a description of the \"Show active subscriptions\" menu in the ODP extraction type. To open the menu, click Show active subscriptions in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/odp/subscriptions/#show-active-subscriptions","title":"Show Active Subscriptions","text":"<p>The ODP extraction type acts as a subscriber (consumer) to data providers to extract data from the data provider. The window \"Delta Subscriptions for product\" displays details about subscribers.</p> <p></p> Column Name Description Name Technical name of all subscriptions of a specific Theobald Software Xtract product (e.g., Xtract Universal). Process Technical name of a subscription R. (number of requests) Number of executed delta requests Last request Timestamp of the last delta request <p>To delete a subscription, click [ ] on the right side of the window.</p> <p>Tip</p> <p>The information displayed in the \"Delta Subscriptions for product\" window can also be viewed in SAP transaction ODQMON.  </p>"},{"location":"documentation/odp/update-mode/","title":"Update Mode","text":"<p>The ODP extraction type can be used for delta extractions. This means that only recently added or changed data is extracted, instead of a full load. The data that is extracted is defined by the Update Mode setting in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/odp/update-mode/#update-modes","title":"Update Modes","text":""},{"location":"documentation/odp/update-mode/#full-update","title":"Full update","text":"<p>Extracts all data (full mode).</p>"},{"location":"documentation/odp/update-mode/#delta-update","title":"Delta update","text":"<p>Note</p> <p>The Delta update option is ready for input only if the ODP provider in the SAP source system supports delta updates.</p> <p>Runs a delta initialization, if no delta initialization is available for the selected subscriber.  Runs a delta update, if there is a delta initialization for the selected subscriber. A delta update only extracts data that was added or changed on the SAP system since the last delta request.</p> <ul> <li>Extract data  Allows extracting data when running a delta initialization.  Leaving this checkbox unchecked runs a delta initialization without extracting data. </li> <li>Auto-sync subscription Allows deletion of the existing subscription and creates a new subscription, if required. Each extraction has an internal ID, which is part of the subscriber.  If you change the filter of an extraction after the delta initialization, the Auto-sync subscription option automatically deletes the existing subscription and creates a new one.  A subscription is deleted, if the error message \"Illegal change in selection parameters\" returns from the SAP system. To delete subscriptions manually, see Subscriptions.</li> </ul>"},{"location":"documentation/odp/update-mode/#delta-recovery","title":"Delta recovery","text":"<p>Re-runs the last delta update.</p>"},{"location":"documentation/odp/update-mode/#direct-read-without-odq","title":"Direct read (without ODQ)","text":"<p>Directly reads all available data, bypassing the ODQ (Operational Delta Queue).  Direct read is the only update mode that supports data aggregation functions (Maximum, Minimum and Sum).</p>"},{"location":"documentation/odp/update-mode/#tutorial","title":"Tutorial","text":"<p>The following YouTube tutorial shows how to use the ODP extraction type with Delta Update to load data increments to an SQL database destination.</p>"},{"location":"documentation/ohs/","title":"Open Hub Services (OHS)","text":"<p>This page shows how to use the OHS extraction type. The OHS extraction type can be used to extract data from Open Hub Service (OHS) destinations.</p>"},{"location":"documentation/ohs/#prerequisites","title":"Prerequisites","text":"<ul> <li>A connection to an SAP system is available, see SAP Connection.</li> <li>The SAP user has sufficient user rights, see SAP Authority Objects.</li> <li>Configure your SAP BW system to make data sources accessible, see Customization for OHS in BW.</li> </ul> <p>Warning</p> <p>Missing Authorization. To use the OHS extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust SAP Authority Objects accordingly.</p>"},{"location":"documentation/ohs/#create-an-ohs-extraction","title":"Create an OHS Extraction","text":"<ol> <li>In the main window of the Designer, click [New] to create a new extraction. The window \"Create Extraction\" opens. </li> <li>Select an SAP Connection from the drop-down menu in Source and enter a unique name for your extraction.</li> <li>Select the extraction type OHS and click [OK]. The main window of the extraction type opens automatically.</li> </ol> <p>The majority of the functions of the extraction type can be accessed in the main window.</p>"},{"location":"documentation/ohs/#look-up-an-ohs-destination","title":"Look up an OHS Destination","text":"<ol> <li> <p>In the main window of the extraction type, select an Extraction type in the main window of the component.</p> <ul> <li>Select Table if you use BW4Hana2.0 and make sure that the open hub destination type in SAP is set to database table.</li> <li>Select Third party tool (legacy) if you use BW4Hana2.0 and make sure that the open hub destination type in SAP is set to third party tool.</li> </ul> <p></p> </li> <li> <p>Click [ Lookup]. The window \u201cOHS Lookup\u201d opens.</p> </li> <li>In the field OHS Destination, enter the name of an OHS destination  . Use wildcards (*) if needed. </li> <li>Click [ ]. Search results are displayed  .</li> <li>Select an OHS destination and click [OK] to confirm.</li> </ol> <p>The application now returns to the main window of the extraction type.</p>"},{"location":"documentation/ohs/#define-the-ohs-extraction-type","title":"Define the OHS  Extraction Type","text":"<p>The OHS extraction type offers the following options for OHS extractions:</p> <ol> <li> <p>If Process Chain is empty, enter an SAP process chain assigned to your OHS destination, see SAP Help: Display/Maintenance of Process Chain Attributes.</p> <p>Note</p> <p>If Process Chain is left empty, the extraction fails.</p> <p></p> </li> <li> <p>Optional: Use the Timeout setting to set a maximum time period to wait for a notification from the BW system.  If the time limit is reached, the extraction fails.</p> </li> <li>Check the Extraction Settings and the General Settings before running the extraction.</li> <li>Click [OK] to save the extraction type.</li> </ol> <p>You can now run the extraction, see Execute and Automate Extractions.</p>"},{"location":"documentation/ohs/general-settings/","title":"General Settings","text":"<p>This page contains an overview of the settings in the window \"General Settings\". To open the general settings, click General Settings in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/ohs/general-settings/#misc-tab","title":"Misc. Tab","text":"<p>The Misc. tab covers cache settings, column encryption and keywords of an extraction type.</p> <p></p>"},{"location":"documentation/ohs/general-settings/#cache-results","title":"Cache results","text":"<p>The Cache results option is only available in pull destinations, e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times.  To decrease the SAP server load, you can select the Cache results option, this way the pull destination pulls the data from cache and not from the SAP.</p> <p>This increases the performance and limits the impact on the SAP system.  If this behavior is not desired (for example, because the data must be always 100% up to date), the cache option must be explicitly turned off.</p>"},{"location":"documentation/ohs/general-settings/#keywords","title":"Keywords","text":"<p>One or more keywords (tags) can be assigned to an extraction.  Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the \"Search Extractions\" window. To open the \"Search Extractions\" window, click [  Search] in the main window of the Designer. </p> <p>Tip</p> <p>To add keywords to multiple extractions at once, select the extractions in the main window of the Designer. Right-click + Add/Remove keywords opens the window \"Add/Remove Keywords To/From Multiple Extractions\".</p>"},{"location":"documentation/ohs/general-settings/#primary-key-tab","title":"Primary Key Tab","text":"<p>Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.  </p> <p></p> <p>The depicted example shows the SAP object MAKT with its primary key inherited from SAP in the general settings of the Designer.  In this example the primary key consists of MANDT, MATNR and SPRAS. The primary key is also taken over in the destination. </p> <p>Note</p> <p>A defined primary key field in a table is a prerequisite for merging data. </p>"},{"location":"documentation/ohs/general-settings/#generate-surrogate-key-column","title":"Generate Surrogate Key Column","text":"<p>If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs.  The surrogate keys are hash values of type signed 8 byte integer, e.g., <code>#-3008591679982390000</code>. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.</p>"},{"location":"documentation/ohs/general-settings/#security-tab","title":"Security Tab","text":"<p>Restrict user access to the extraction. For more information, see Restrict Designer Access.</p> <p></p>"},{"location":"documentation/ohs/general-settings/#columns-order-tab","title":"Columns Order Tab","text":"<p>The \"Columns Order\" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns.  Index 0 defines the first column in the result set, index 1 the second columns, etc.</p> <p></p> Option Description Enabled When this option is active, the defined column order is applied when running the extraction. Swap Swaps the index of 2 columns. All other columns keep their indexes. Insert Inserts the selected column into the selected index. All other indexes are recalculated. Reset default Restores the original column order."},{"location":"documentation/ohs/settings/","title":"Extraction Settings","text":"<p>This page contains an overview of the extraction settings in the OHS extraction type. To open the extraction settings, click Extraction Settings in the main window of the extraction type. </p> <p></p> <p>The OHS settings consist of two tabs: </p> <ul> <li>Table</li> <li>Third party tool (legacy)</li> </ul> <p>The settings correspond to the selected extraction type. Set either Table or Third party tool (legacy) settings.</p> <p>Warning</p> <p>Could not load list of available function modules because permission for table ENLFDIR is missing This warning appears if a technical SAP user does not have authorization rights to access the SAP table ENLFDIR. Confirm the warning as the user can still adjust the extraction settings.</p>"},{"location":"documentation/ohs/settings/#table-settings","title":"Table Settings","text":""},{"location":"documentation/ohs/settings/#extraction-settings","title":"Extraction Settings","text":""},{"location":"documentation/ohs/settings/#package-size","title":"Package Size","text":"<p>The extracted data is be split into packages of the defined size. The default value is 50000 lines. A package size between 20000 and 50000 is advisable for large amounts of data. 0 means no packaging.  Not using packaging can lead to an RFC timeout for large data extracts.</p> <p>Warning</p> <p>RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.</p>"},{"location":"documentation/ohs/settings/#row-limit","title":"Row Limit","text":"<p>Specifies the maximum number of extracted records. 0 extracts all data. You can use this option to perform tests with a small amount of data by entering a row limit of e.g., 1000.</p>"},{"location":"documentation/ohs/settings/#function-module","title":"Function Module","text":"<p>Specifies the name of the function module used for data extraction.  This field is filled automatically depending on what function modules are installed on your SAP system. Custom function modules are supported.</p> <p>The following function modules can be used to extract tables:</p> <ul> <li>RFC_READ_TABLE (TAB512)</li> <li>/BODS/RFC_READ_TABLE (TAB2048)</li> <li>/SAPDS/RFC_READ_TABLE (TAB2048)</li> <li>/BODS/RFC_READ_TABLE2</li> <li>/SAPDS/RFC_READ_TABLE2</li> <li>Z_THEO_READ_TABLE</li> <li>/THEO/READ_TABLE (recommended)</li> </ul> <p>Warning</p> <p>Duplicates in the target environment. The SAP standard modules for table extraction do not have pointers for table fields. In larger tables this may cause low performance and duplicates in the target environment. Use the function module THEO/READ_TABLE from Theobald Software to ensure smooth extractions.</p> <p>Note the necessary SAP Authority Objects:</p> <pre><code>S_TABU_NAM ACTVT=03; TABLE=ENLFDIR\n</code></pre>"},{"location":"documentation/ohs/settings/#extract-data-in-background-job","title":"Extract Data in Background Job","text":"<p>If Background job timeout (seconds) checkbox is activated, the table extraction is executed as a background job in SAP.  This setting is optional and is supported in combination with function module THEO/READ_TABLE or Z_THEO_READ_TABLE version 2.0.  Activate the setting Background job timeout (seconds) for long-running extractions with a large amounts of data that may run into a timeout error (\u201cTime limit exceeded\u201d), when using the foreground mode.</p> <p>Tip</p> <p>The extraction jobs can be found in the SAP JobLog (SM37) under the JobName theo_read_table.</p> <p>Warning</p> <p>Shared Memory ran out of memory!  If this error message pops up when running an extraction in the background, adjust the size of the Shared Memory. SAP recommends a Shared Memory size of 800MB~1.5GB for a production/test system or 2GB~4GB for S/4 systems, see SAP Support: How to solve SYSTEM_NO_SHM_MEMORY runtime error.</p>"},{"location":"documentation/ohs/settings/#advanced-settings","title":"Advanced Settings","text":""},{"location":"documentation/ohs/settings/#background-job-timeout-seconds","title":"Background Job Timeout (seconds)","text":"<p>Sets a timeout period for extractions that run in background mode. The default value is 180 seconds. The maximum timeout value is 3600 seconds. </p> <p>This option can be used if the data transfer to a destination takes a lot of time, e.g., when bulk-inserts are deactivated for database destinations. </p> <p>Note</p> <p>The background job timeout setting only takes effect if the extractions run in background mode using /THEO/READ_TABLE.</p>"},{"location":"documentation/ohs/settings/#adjust-currency-decimals","title":"Adjust Currency Decimals","text":"<p>The default number of decimal places for a currency in the SAP database is 2 decimals. Currencies that do not have decimals are also stored in this format, e.g. JPY, VND, KRW, etc.</p> <p>Example:</p> Currency Actual Amount Amount stored in SAP database JPY 100 1.00 KRW 10000 100.00 <p>When extracting currencies with no decimals, the amount stored in SAP is returned e.g., 100 JPY are extracted as 1.00. To correct the decimal placement of the extracted data, activate Adjust Currency Decimals. If Adjust Currency Decimals is active, currencies without decimals are multiplied by a factor that balances out the decimals.</p> <p>Adjust Currency Decimals also requires the extraction of the corresponding CURRENCY field that can be used as a reference for the multiplication factor. Use the [Load live preview] function to find the correct currency field/s. </p> <ul> <li>If the currency field is part of the table, add it to the output.</li> <li>If the currency field is in another table, join the tables. </li> <li>If the reference is not part of a table, Adjust Currency Decimals cannot be used.</li> </ul> <p>Note</p> <p>The multiplication factor used in Adjust Currency Decimals is determined by the SAP currency table TCURX.  To access the table, the following SAP Authority objects must be set in SAP: S_TABU_NAM    ACTVT=03; TABLE=TCURX.</p>"},{"location":"documentation/ohs/settings/#third-party-tool-settings","title":"Third Party Tool Settings","text":""},{"location":"documentation/ohs/settings/#gateway-host","title":"Gateway host","text":"<p>Enter the data of your gateway host. It is often the same host as the SAP application server.</p>"},{"location":"documentation/ohs/settings/#gateway-service","title":"Gateway service","text":"<p>Enter the data of your gateway service (sapgwXX, where XX is the system number).</p>"},{"location":"documentation/ohs/settings/#program-id","title":"Program ID","text":"<p>Enter the program ID of the SAP RFC destination.</p>"},{"location":"documentation/parameters/","title":"Parameters","text":"<p>Xtract Universal supports multiple options to dynamize extractions:</p> <ul> <li>Extraction parameters</li> <li>Script expressions</li> <li>SQL parameters </li> </ul> <p>While script expressions compute values, extraction parameters and by extension SQL parameters require users to pass actual values when running an extraction. Extraction parameters affect the extraction settings, destination settings, the SAP connection settings, and the user-defined (custom) runtime parameters of an extraction.  This includes the runtime parameters that are used in SQl commands (SQL parameters).</p>"},{"location":"documentation/parameters/#about-custom-runtime-parameters","title":"About Custom Runtime Parameters","text":"<p>User-defined runtime parameters can be used to filter SAP data before writing the data to the destination. They are part of the extraction parameters and can be used in SQL statements, see SQL parameters. There are two types of custom runtime parameters:</p> <ul> <li>scalar parameters that represent a single value.</li> <li>list parameters that represent multiple values separated by a comma, e.g., 1,10 or \u201c1\u201d, \u201c10\u201d.</li> </ul> <p>Most extractions types offer an Edit runtime parameters menu that allows users to create custom runtime parameters.</p> <p></p> <p>Once runtime parameters are available, a switch is added to all input fields that support runtime parameters. The switch allows users to switch between a static input value ( ) and an existing runtime parameter ( ), e.g., Parameter0.</p> <p></p> <p>The following table shows what extractions types and settings support custom runtime parameters:</p> Extraction Type Settings that Support Custom Runtime Parameters  BAPI Import parameters, Table parameters  BWCube BEx variables, dimension filters  DeltaQ Selections  ODP Selections  Query Selections  Report Selections  Table WHERE clause, HAVING clause  Table CDC WHERE clause"},{"location":"documentation/parameters/extraction-parameters/","title":"Extraction Parameters","text":"<p>The Xtract Universal Designer can run extractions by passing parameters that define how data is extracted from the source.</p> <p>The following categories of extraction parameters are available:</p> <ul> <li>Extraction parameters affect the extraction and destination settings.</li> <li>Source parameters affect the SAP connection settings.</li> <li>Custom parameters correspond to the user-defined runtime parameters or SQL parameters of an extraction.</li> </ul> <p>The parameters can be accessed in the \"Run Extraction\" window. To open the \"Run Extraction\" window, select an extraction from the list of extractions and click [Run]. For more information, see Run Extractions.</p> <p></p>","boost":2},{"location":"documentation/parameters/extraction-parameters/#extraction","title":"Extraction","text":"<p>The number of available parameters depends on the extraction type and destination. Example: decimalSeparator is a parameter specific to CSV destinations.</p> Parameter Description Information clearBuffer Clears/Keeps the result buffer Default value is false preview Enables/Disables the preview mode Default value is false source Selects the SAP source system from which data is extracted (e.g., SAP_DEV and SAP_PROD). Applies only if more than one SAP system is used. - destination Selects the destination to which extraction is written (e.g., db_1 and db_2). Applies only if more than one destination is used. - rows Sets the maximum number of rows to be extracted Available for Table where Sets a WHERE clause Available for Table packageSize Sets the package size Available for Table updateMode Sets the update mode to use for the run Available for ODP subscriptionSuffix Suffix to use multiple inits on a single SAP system Available for ODP extractDataOnDeltaInit Extracts data if request is delta init Available for ODP hierarchyName The name of the hierarchy to extract Available for ODP, DeltaQ &amp; Hierarchy representation The representation / output format of the hierarchy to extract: \"ParentChild\", \"Natural\" or \"ParentChildWithNodeNames\" Available for Hierarchy dateTo The valid-to-date of the hierarchy to extract in the format YYYYMMDD Available for Hierarchy variant Name of a variant Available for Report &amp; SAP Query batchJobName Name of the Batch Job Available for Report gatwewayHost Gateway Host Available for DeltaQ gatewayService Gateway Service Available for DeltaQ programID Program ID Available for DeltaQ logicalDestination Logical Destination Available for DeltaQ requestID Request ID (for Repair Request only) Available for DeltaQ updateType F (Full), C (Delta Init), S (Init no data), D (Delta Update), R (Repeat) Available for DeltaQ decimalSeparator Sets a symbol between integer and fractional part Available for CSV destinations columnSeparator Sets a symbol which indicates the start of a new column Available for CSV destinations","boost":2},{"location":"documentation/parameters/extraction-parameters/#example","title":"Example","text":"<ol> <li>Select the checkbox of the parameter you want to override.</li> <li>Enter the value and confirm by pressing enter. <ul> <li>Extraction URL before changing the parameter: <code>http://sherri.theobald.local:8065/start/KNA1/</code></li> <li>Extraction URL after editing the parameter source (name of the SAP source system): <code>http://sherri.theobald.local:8065/start/KNA1/?source=SAP_PROD</code> </li> </ul> </li> </ol>","boost":2},{"location":"documentation/parameters/extraction-parameters/#source","title":"Source","text":"<p>The connection settings to an SAP source can be changed dynamically via the URL and the command-line tool xu.exe.  In the Source tab you can override the values that are defined in SAP source details.</p> <p></p> Parameter name Parameter description lang Changes the logon language of the SAP source system logonTicket Changes the ticket issuer of the SAP logon ticket <p>Note</p> <p>The parameter logonTicket can only be set if SAP logon ticket is selected as the authentication method in SAP source details.</p>","boost":2},{"location":"documentation/parameters/extraction-parameters/#example_1","title":"Example","text":"<ol> <li>Select the checkbox of the parameter you want to override.</li> <li>Enter the value and confirm by pressing enter. <ul> <li>Extraction URL before changing the parameter: <code>http://sherri.theobald.local:8065/start/KNA1/</code></li> <li>Extraction URL after editing the parameter lang (language setting for the SAP source system): <code>http://sherri.theobald.local:8065/start/KNA1/&amp;lang=DE</code> </li> </ul> </li> </ol>","boost":2},{"location":"documentation/parameters/extraction-parameters/#custom","title":"Custom","text":"<p>The tab Custom is only active when user-defined runtime parameters or SQL parameters are available.  Check the checkbox and enter a new value to set the user defined parameter.</p> <p></p>","boost":2},{"location":"documentation/parameters/extraction-parameters/#example_2","title":"Example","text":"<ol> <li>Select the checkbox of the parameter you want to override.</li> <li>Enter the value and confirm by pressing enter. <ul> <li>Extraction URL before changing the parameter: <code>http://sherri.theobald.local:8065/start/KNA1/</code></li> <li>Extraction URL after editing the parameter myParameter (name of a runtime parameter): <code>http://sherri.theobald.local:8065/start/KNA1/&amp;myParameter=EN</code> </li> </ul> </li> </ol>","boost":2},{"location":"documentation/parameters/extraction-parameters/#related-links","title":"Related Links:","text":"<ul> <li>Web API</li> <li>Script Expressions</li> </ul>","boost":2},{"location":"documentation/parameters/script-expressions/","title":"Script Expressions","text":"<p>Script expressions offer a way of adding dynamic parameters to Xtract Universal.  Script expressions are resolved at extraction runtime.  The output of a script expression is a string.  This string can be used as input for further .NET string operations.</p> <p>Script expressions can be used in the following scenarios:</p> <ul> <li>as dynamic folder paths in cloud storage destinations.</li> <li>as dynamic file names in database destinations, cloud storage destinations and flat-file destinations.</li> <li>as custom SQL commands in database destinations.</li> <li>as selection parameters for Table or DeltaQ extractions.</li> </ul>"},{"location":"documentation/parameters/script-expressions/#syntax-of-script-expressions","title":"Syntax of Script Expressions","text":"<p>Script expressions use the C# syntax.  They must begin and end with a hash symbol (#). The formula starts and ends with curly brackets ({}), e.g., <code>#{ Extraction.TableName }#</code></p> <p>Note</p> <p>Expressions that are specific to Xtract Universal are case sensitive. Make sure to use the exact syntax as documented below.</p>"},{"location":"documentation/parameters/script-expressions/#if-statements","title":"IF-Statements","text":"<p>An IF-statement (ternary operator) is supported and uses the following syntax: </p> <pre><code>iif([bool condition], [string trueResult], [string falseResult])\n</code></pre> <p>Examples: </p> Input Output Description <code>#{ iif(DateTime.Now.Month==7, \"July\",\"Unknown\")}#</code> July  In month 7 the output is \"July\", all else is \"Unknown\". <code>#{Extraction.ExtractionName}##{ iif(string.IsNullOrEmpty(Extraction.Context), string.Empty, \"/\" + Extraction.Context)}#</code> *Extraction.Context* returns a result only with ODP extractions. With all other extraction types the result is empty. <ul> <li>If the extraction name is 'SAP_1' and the extraction type is 'Table', the resulting file path would be <code>SAP_1/[filename]</code>. </li> <li>If the extraction name is 'SAP_2' and the extraction type is 'ODP' and a SAP DataSource (extraction context: SAPI) is being extracted, the resulting file path would be <code>SAP_2/SAPI/[filename]</code>. </li> </ul>"},{"location":"documentation/parameters/script-expressions/#script-expressions-based-on-net","title":"Script Expressions based on .NET","text":"<p>Xtract Universal script expressions support the following .NET objects, properties and methods from the .NET System Namespace of Xtract Universal's current .NET framework:</p> <p>Object, Boolean, Char, String, SByte, Byte, Int16, UInt16, Int32, UInt32, Int64, UInt64, Single, Double, Decimal, DateTime, TimeSpan, Guid, Math, Convert.</p> <p>Note</p> <p>The most common usage scenario is using the methods and properties of the .NET DateTime and String classes.  For further information of supported .NET classes and their properties and methods including DateTime and String see the Microsoft online documentation.</p>"},{"location":"documentation/parameters/script-expressions/#supported-keywords","title":"Supported Keywords","text":"<p>The following key words are supported: </p> <p>true, false, null.</p>"},{"location":"documentation/parameters/script-expressions/#list-of-available-script-expressions","title":"List of available Script Expressions","text":"File NamesFolder PathsCustom SQL StatementsSelections in Table &amp; DeltaQ"},{"location":"documentation/parameters/script-expressions/#use-script-expressions-as-dynamic-file-names","title":"Use Script Expressions as Dynamic File Names","text":"<p>Script expressions can be used to generate a dynamic file name.  This allows generating file names that are composed of an extraction's properties, e.g. extraction name, SAP source object. This scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].LowerValue}#</pre> Lower value of the range selection. <pre>#{Extraction.Fields[\"FISCPER\"].RangeSelections[0].UpperValue}#</pre> Upper value of the range selection. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFiels]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction."},{"location":"documentation/parameters/script-expressions/#use-script-expressions-as-dynamic-folder-paths","title":"Use Script Expressions as Dynamic Folder Paths","text":"<p>Script expressions can be used to generate a dynamic folder path. This allows generating folder paths that are composed of an extraction's properties, e.g., extraction name, SAP source object. The described scenario supports script expressions based on .NET and the following XU-specific custom script expressions:</p> Input Description <pre>#{Source.Name}#</pre> Name of the extraction's SAP source. <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.Type}#</pre> Extraction type (Table, ODP, BAPI, etc.). <pre>#{Extraction.SapObjectName}#</pre> Name of the SAP object the extraction is extracting data from. <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction. <pre>#{Extraction.SapObjectName.TrimStart(\"/\".ToCharArray())}#</pre> Removes the first slash '/' of an SAP object.  Example: /BIO/TMATERIAL to BIO/TMATERIAL - prevents creating an empty folder in a file path. <pre>#{Extraction.SapObjectName.Replace('/', '_')}#</pre> Replaces all slashes '/' of an SAP object.  Example: /BIO/TMATERIAL to _BIO_TMATERIAL - prevents splitting the SAP object name by folders in a file path. <pre>#{Extraction.Context}#</pre> Only for ODP extractions: returns the context of the ODP object (SAPI, ABAP_CDS, etc). <pre>#{Extraction.Fields[\"[NameSelectionFields]\"].Selections[0].Value}#</pre> Only for ODP extractions: returns the input value of a defined selection / filter. <pre>#{Odp.UpdateMode}#</pre> Only for ODP extractions: returns the update mode (Delta, Full, Repeat) of the extraction. <pre>#{TableExtraction.WhereClause}#</pre> Only for Table extractions: returns the WHERE clause of the extraction. <pre>#{Extraction.Fields[\"[0D_NW_CODE]\"].Selections[0].Value}#</pre> Only for BWCube extractions (MDX mode): returns the input value of a defined selection. <pre>#{Extraction.Fields[\"[0D_NW_CHANN]\"].RangeSelections[0].LowerValue}#</pre> Only for BWCube extractions (MDX mode): returns the lower input value of a defined selection range. <pre>#{Extraction.Fields[\"[0D_NW_CHANN]\"].RangeSelections[0].UpperValue}#</pre> Only for BWCube extractions (MDX mode): returns the upper input value of a defined selection range. <pre>#{Extraction.Fields[\"0D_NW_CODE\"].Selections[0].Value}#</pre> Only for BWCube extractions (BICS mode): returns the input value of a defined selection. <pre>#{Extraction.Fields[\"0D_NW_CHANN\"].RangeSelections[0].LowerValue}#</pre> Only for BWCube extractions (BICS mode): returns the lower input value of a defined selection range. <pre>#{Extraction.Fields[\"0D_NW_CHANN\"].RangeSelections[0].UpperValue}#</pre> Only for BWCube extractions (BICS mode): returns the upper input value of a defined selection range."},{"location":"documentation/parameters/script-expressions/#use-script-expressions-in-custom-sql-statements","title":"Use Script Expressions in Custom SQL Statements","text":"<p>You can use script expressions for Custom SQL commands. The following Xtract Universal specific custom script expressions are supported: </p> Input Description <pre>#{Extraction.ExtractionName}#</pre> Name of the extraction. If the extraction is part of an extraction group, the name of the extraction group is included in the extraction name, e.g, <code>group,extraction</code>, <code>Tables,KNA1</code>. <pre>#{Extraction.TableName }#</pre> Name of the database table extracted data is written to. <pre>#{Extraction.RowsCount }#</pre> Count of the extracted rows. <pre>#{Extraction.RunState}#</pre> Status of the extraction (Running, FinishedNoErrors, FinishedErrors). <pre>#{(int)Extraction.RunState}#</pre> Status of the extraction as number (2 = Running, 3 = FinishedNoErrors, 4 = FinishedErrors). <pre>#{Extraction.Timestamp}#</pre> Timestamp of the extraction."},{"location":"documentation/parameters/script-expressions/#use-script-expressions-as-selection-parameters-in-table-and-deltaq","title":"Use Script Expressions as Selection Parameters in Table and DeltaQ","text":"<p>Script expressions are usually used to determine a dynamic date based on the current date.  When using script expressions in a WHERE Clause, the value must be entered in single quotation marks.</p> Input Description <pre>#{ DateTime.Now.ToString(\"yyyyMMdd\") }#</pre> Current date in SAP format (yyyyMMdd) <pre>#{ String.Concat(DateTime.Now.Year.ToString(), \"0101\") }#</pre> Current year concatenated with \"0101\" (yyyy0101) <pre>#{ String.Concat(DateTime.Now.ToString(\"yyyy\"), \"0101\") }#</pre> Current year concatenated with \"0101\" (yyyy0101) <pre>#{ String.Concat(DateTime.Now.ToString(\"yyyyMMdd\").Substring(0,4), \"0101\") }#</pre> Current year concatenated with \"0101\" (yyyy0101)"},{"location":"documentation/parameters/sql-parameters/","title":"SQL Parameters","text":"<p>In Xtract Universal you can define custom runtime parameters that can be set dynamically when calling extractions. When using an SQL destination, these parameters are available for SQL commands. </p> <p>A typical use case is the dynamization of WHERE clauses in the Table extraction type. The following table extraction has a custom parameter WNAME in the WHERE-Clause:</p> <p></p>"},{"location":"documentation/parameters/sql-parameters/#custom-sql-statement","title":"Custom SQL Statement","text":"<p>In the window Destination Settings you can use a custom SQL statement for the three database process steps and / or edit the SQL statement according to your requirements.  </p> <ol> <li>In the main window select an extraction with a custom parameter in the WHERE-Clause  .</li> <li>Click [Destination] . The window \"Destination Settings\" opens.</li> <li>Select the option Custom SQL from the drop-down-menus   in the following sections:<ul> <li>Preparation </li> <li>Finalization</li> </ul> </li> <li>Click [Edit SQL]. The window \"Edit SQL\" opens.</li> <li>Define an SQL statement and click [OK] to confirm your input .</li> </ol>"},{"location":"documentation/parameters/sql-parameters/#custom-sql-example-for-custom-parameters","title":"Custom SQL Example for Custom Parameters","text":"<p>In the following example the SAP table KNA1 is expanded by adding the column Custom_Parameter of the type NATIONAL CHARACTER VARYING(10). The column is filled dynamically by runtime parameters.</p> <p>In the section Row Processing the column values from SAP are written into the previously created SQL target table. This SQL statement is therefore used as the default Insert statement. When rows are processed, only <code>NULL</code> values are written into the Custom_Parameter column.</p> <p>In the section Finalization these <code>NULL</code> values are replaced using the SQL statements of the runtime parameter WNAME and the T-SQL command <code>UPDATE</code>.</p> <p>Note</p> <p>The data types that can be used in SQL statements depend on your SQL database version.</p> <ol> <li>In the window \"Destination Settings\", select the option Custom SQL in the section Preparation. Click Edit SQL. </li> <li>Select Drop &amp; Create from the drop-down-menu and click [Generate Statement] . </li> <li>Add the following line to the generated statement: <pre><code>[Custom_Parameter] NATIONAL CHARACTER VARYING(10)\n</code></pre></li> <li>Click [OK] to confirm your input. </li> <li>In the window \"Destination Settings\", select the option Custom SQL in the section Finalization. Click Edit SQL. </li> <li>Select Insert from the drop-down-menu and add the following SQL statement : <pre><code>UPDATE [dbo].[KNA1] \nSET [Custom_Parameter] = @WNAME \nWHERE [Custom_Parameter] IS NULL; \n</code></pre></li> <li>Click [OK] to confirm your input . </li> </ol>"},{"location":"documentation/parameters/sql-parameters/#set-the-custom-parameter-wname","title":"Set the Custom Parameter WNAME","text":"<ol> <li>Select the checkbox next to the parameter name to overwrite the parameter WNAME. </li> <li>Enter the new value US and confirm your input by pressing enter.</li> <li>Click [Run]  to run the extraction.</li> </ol>"},{"location":"documentation/parameters/sql-parameters/#result-in-ssms","title":"Result in SSMS","text":"<p>Check the result of the column Custom_Parameter in the SQL Server View of the KNA1 table.</p> <p></p>"},{"location":"documentation/parameters/sql-parameters/#related-links","title":"Related Links","text":"<ul> <li>Post-Processing Column Name Style</li> </ul>"},{"location":"documentation/query/","title":"SAP Query","text":"<p>This page shows how to use the Query extraction type. The Query extraction type can be used to extract data via SAP queries.</p>"},{"location":"documentation/query/#about-sap-queries","title":"About SAP Queries","text":"<p>SAP Queries are used to access data sets in SAP, see SAP Help - Working with Queries for more information. The SAP queries that can be used with Query extraction type are created by the SAP transactions SQ02 and SQ01. To use a BW Query as a data source, see BW InfoCubes and BExQueries.</p>"},{"location":"documentation/query/#prerequisites","title":"Prerequisites","text":"<ul> <li>A connection to an SAP system is available, see SAP Connection.</li> <li>The SAP user has sufficient user rights, see SAP Authority Objects.</li> </ul> <p>Warning</p> <p>Missing Authorization. To use the Report extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust SAP Authority Objects: SAP Query accordingly.</p>"},{"location":"documentation/query/#create-a-query-extraction","title":"Create a Query Extraction","text":"<ol> <li>In the main window of the Designer, click [New] to create a new extraction. The window \"Create Extraction\" opens. </li> <li>Select an SAP Connection from the drop-down menu in Source and enter a unique name for your extraction.</li> <li>Select the extraction type Query and click [OK]. The main window of the extraction type opens automatically.</li> </ol> <p>The majority of the functions of the extraction type can be accessed in the main window.</p>"},{"location":"documentation/query/#look-up-an-sap-query","title":"Look up an SAP Query","text":"<ol> <li>In the main window of the extraction type, click [ ]. The window \"Query Lookup\" opens. </li> <li>Enter the name of an SAP query in the field Query Name or the name of a user group in the field User group .  Use wildcards (*) if needed. </li> <li>Select the query work area that contains the query object  . For more information, see SAP Help: Query Areas.</li> <li>Click [ ]. Search results are displayed.</li> <li>Select a query   and click [OK].</li> </ol> <p>The application returns to the main window of the extraction type.</p>"},{"location":"documentation/query/#define-the-query-extraction-type","title":"Define the Query Extraction Type","text":"<p>The Query extraction type offers the following options for query extractions:</p> <ol> <li>If the SAP query has variants, select a variant from the drop-down-list Variant. For more information, see Choose a Variant. </li> <li>In the section Selection Screen, edit a selection criterion you want to change or dynamize  . For more information, see Edit Selections. </li> <li>Check the Extraction Settings and the General Settings before running the extraction.</li> <li>Click [OK] to save the extraction type.</li> </ol> <p>You can now run the extraction, see Execute and Automate Extractions.</p>"},{"location":"documentation/query/edit-runtime-parameters/","title":"Runtime Parameters","text":"<p>Runtime parameters are are placeholders for values that are passed at runtime, see Extraction Parameters - Custom. They can be created in context of Selections.</p>"},{"location":"documentation/query/edit-runtime-parameters/#create-runtime-parameters","title":"Create Runtime Parameters","text":"<p>There are two types of runtime parameters:</p> Scalar parametersList parameters <p>Scalar runtime parameters represent a single value.  Follow the steps below to create a scalar runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add Scalar] to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime. </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.</p> Type Description Text Can be used for any type of SAP selection field. Number Can be used for numeric SAP selection fields. Flag Can only be used for SAP selection fields THAT require an \u2018X\u2019 (true) or a blank \u2018\u2018 (false) as input value. </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p> <p>List runtime parameters represent multiple values.  Follow the steps below to create a list runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add List] to define list parameters that contain multiple values separated by commas e.g., 1,10 or \u201c1\u201d, \u201c10\u201d. The placeholders need to be populated with actual values at runtime.  </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p>"},{"location":"documentation/query/edit-runtime-parameters/#assign-runtime-parameters","title":"Assign Runtime Parameters","text":"<p>Follow the steps below to assign the runtime parameters to selections.</p> <ol> <li>In the main window of the extraction type, click the [Edit] button next to the selection you want to parameterize.  The window \"Edit Selections\" opens.</li> <li>Add a filter to the selection, see Edit Selections. </li> <li>Click the icon button next to the input field to switch between static values ( ) and runtime parameters ( ). If no icon button is available, create a runtime parameter. </li> <li>Select a runtime parameter from the dropdown-list.</li> <li>Click [OK] to confirm the input.</li> </ol> <p>Pass values during runtime, see Extraction Parameters - Custom.</p>"},{"location":"documentation/query/general-settings/","title":"General Settings","text":"<p>This page contains an overview of the settings in the window \"General Settings\". To open the general settings, click General Settings in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/query/general-settings/#misc-tab","title":"Misc. Tab","text":"<p>The Misc. tab covers cache settings, column encryption and keywords of an extraction type.</p> <p></p>"},{"location":"documentation/query/general-settings/#cache-results","title":"Cache results","text":"<p>The Cache results option is only available in pull destinations, e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times.  To decrease the SAP server load, you can select the Cache results option, this way the pull destination pulls the data from cache and not from the SAP.</p> <p>This increases the performance and limits the impact on the SAP system.  If this behavior is not desired (for example, because the data must be always 100% up to date), the cache option must be explicitly turned off.</p>"},{"location":"documentation/query/general-settings/#keywords","title":"Keywords","text":"<p>One or more keywords (tags) can be assigned to an extraction.  Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the \"Search Extractions\" window. To open the \"Search Extractions\" window, click [  Search] in the main window of the Designer. </p> <p>Tip</p> <p>To add keywords to multiple extractions at once, select the extractions in the main window of the Designer. Right-click + Add/Remove keywords opens the window \"Add/Remove Keywords To/From Multiple Extractions\".</p>"},{"location":"documentation/query/general-settings/#primary-key-tab","title":"Primary Key Tab","text":"<p>Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.  </p> <p></p> <p>The depicted example shows the SAP object MAKT with its primary key inherited from SAP in the general settings of the Designer.  In this example the primary key consists of MANDT, MATNR and SPRAS. The primary key is also taken over in the destination. </p> <p>Note</p> <p>A defined primary key field in a table is a prerequisite for merging data. </p>"},{"location":"documentation/query/general-settings/#generate-surrogate-key-column","title":"Generate Surrogate Key Column","text":"<p>If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs.  The surrogate keys are hash values of type signed 8 byte integer, e.g., <code>#-3008591679982390000</code>. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.</p>"},{"location":"documentation/query/general-settings/#security-tab","title":"Security Tab","text":"<p>Restrict user access to the extraction. For more information, see Restrict Designer Access.</p> <p></p>"},{"location":"documentation/query/general-settings/#columns-order-tab","title":"Columns Order Tab","text":"<p>The \"Columns Order\" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns.  Index 0 defines the first column in the result set, index 1 the second columns, etc.</p> <p></p> Option Description Enabled When this option is active, the defined column order is applied when running the extraction. Swap Swaps the index of 2 columns. All other columns keep their indexes. Insert Inserts the selected column into the selected index. All other indexes are recalculated. Reset default Restores the original column order."},{"location":"documentation/query/settings/","title":"Extraction Settings","text":"<p>This page contains an overview of the extraction settings in the Query extraction type. To open the extraction settings, click Extraction Settings in the main window of the extraction type. </p> <p></p>"},{"location":"documentation/query/settings/#extraction-settings","title":"Extraction Settings","text":""},{"location":"documentation/query/settings/#max-rows","title":"Max Rows","text":"<p>Specifies the maximum number of extracted records. 0 extracts all data. You can use this option to perform tests with a small amount of data by entering a row limit of e.g., 1000.</p>"},{"location":"documentation/query/settings/#treat-no-data-selected-as-error","title":"Treat 'No Data Selected' as Error","text":"<p>If this option is active, an error message is displayed when there is no data to be extracted.</p>"},{"location":"documentation/query/variants-and-selections/","title":"Variants and Selections","text":"<p>Most queries allow entering selections before query execution. Selections limit the result set of the query to extract only records that match the selection. A selection variant can be created in SAP, see SAP Help: Query Variants.  The purpose of a variant is to minimize the necessity to enter selections when running a query.</p> <p>Note</p> <p>Manual selections and variants can be combined. Manual selections overwrite any selections in the variant.</p>"},{"location":"documentation/query/variants-and-selections/#choose-a-variant","title":"Choose a Variant","text":"<p>Choose a variant from the drop-down-list Variant.  When creating a new variant in SAP after creating the extraction, click [ ] to load the new variant.</p> <p></p> <p>The selections of the variant are not displayed in the Selection Screen section of the window.  To see the definition of a variant, open the variant in SAP.</p> <p>Tip</p> <p>You can define the variant at runtime by using a corresponding parameter in the extraction URL, see Extraction Parameters.</p>"},{"location":"documentation/query/variants-and-selections/#edit-selections","title":"Edit Selections","text":"<p>The Selection Screen in the main window of the component corresponds to the input screen in SAP.</p> <p>Note</p> <p>Some selection fields only have a technical name and no description.  To understand which field corresponds to a field in SAP, open the input screen in SAP.  Click on a selection field and press the function key F1 to display the technical name of the selection field. </p> <p>Follow the steps below to edit selection fields and filter data:</p> <ol> <li>In the subsection Selection Screen, click [Edit] next to the field you want to edit. The window \u201cEdit selection\u201d opens. </li> <li>Add one or more of the following filter types:<ul> <li>Click [Single] to compare the data to a single specified value.</li> <li>Click [Range] to check if the data is (not) within a specified range of values.</li> <li>Click [List] to check if the data is (not) part of a specified list of values. </li> </ul> </li> <li>In the column Sign , select Include to add the filtered data to the output or select Exclude to remove the filtered data from the output. </li> <li> <p>In the column Option , select an operator. The operator filters data according to the table below.</p> Operator Description (not) like pattern True if data values do (not) contain to the content of operand 1. (not) equal to True if data is (not) equal to the content of operand 1. at least True if data is greater than or equal to the content of operand 1. more than True if data is greater than the content of operand 1. at most True if data is less than or equal to the content of operand 1. less than True if data is less than the content of operand 1. (not) between True if data values do (not) lie between the values of operand 1 and operand 2. element of True if data values are part of operand 1. This option is only available for type List. </li> <li> <p>In the column Value, enter values directly into the input fields Low and High or assign existing  runtime parameters  to the selection fields  .</p> <p>Note</p> <p>When runtime parameters are available, you can use the icon button inside the input field to switch between static values ( ) and runtime parameters ( ).</p> </li> <li> <p>Click [OK] to confirm your input. </p> </li> </ol> <p>Note that edited selection fields overwrite the selection fields in the variant. </p> <p>Tip</p> <p>If you use multiple selection parameters, it is more efficient to create a variant in SAP.</p>"},{"location":"documentation/query/variants-and-selections/#data-format","title":"Data Format","text":"<p>Use the following internal SAP representation for input:</p> <ul> <li>Date: The date 01.01.1999 has the internal representation 19990101 (YYYYMMDD).</li> <li>Year period: The year period 001.1999 has the internal representation 1999001 (YYYYPPP).</li> <li>Numbers: Numbers must contain the leading zeros, e.g., customer number 1000 has the internal representation 0000001000.</li> </ul> <p>Warning</p> <p>Values accept only the internal SAP representation. Input that does not use the internal SAP representation results in error messages.  Use the internal SAP representation.  Example:  <pre><code>ERPConnect.ABAPProgramException: RfcInvoke failed(RFC_ABAP_MESSAGE): Enter date in the format \\_.\\_.\\_\n</code></pre></p>"},{"location":"documentation/report/","title":"Report","text":"<p>This page shows how to use the Report extraction type. The Report extraction type can be used to extract data from most standard and custom ABAP reports and SAP transactions.  A report extraction is possible if the report returns a table-like structure in SAP. </p>"},{"location":"documentation/report/#about-reports","title":"About Reports","text":"<p>ABAP report programs extract and present huge amounts of data for SAP business applications. SAP offers predefined standard reports that cover the basic needs of customers. They can be executed via transaction codes. </p> <p>Tip</p> <p>Use transaction code SAP1 to get a list of all reports for all modules.</p>"},{"location":"documentation/report/#custom-reports","title":"Custom Reports","text":"<p>The extraction of custom reports (Z reports) is possible if the report returns a table-like structure in SAP. Issues specific to Z reports are not included in the scope of support provided by Theobald Software.</p>"},{"location":"documentation/report/#prerequisites","title":"Prerequisites","text":"<ul> <li>A connection to an SAP system is available, see SAP Connection.</li> <li>The SAP user has sufficient user rights, see SAP Authority Objects.</li> <li>The custom function module <code>Z_XTRACT_IS_REMOTE_REPORT</code> is installed in your SAP system, see Function Module for Reports. As of <code>Z_XTRACT_IS_REMOTE_REPORT</code> version 1.2 access to reports must be explicitly granted, see Knowledge BAse Article: Authorize Access to Specific Reports.</li> <li>The report must return a table-like structure in SAP.</li> </ul> <p>Warning</p> <p>Missing Authorization. To use the Report extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust SAP Authority Objects: Report accordingly.</p>"},{"location":"documentation/report/#general-workflow","title":"General Workflow","text":"<p>The following graphic shows the general workflow of using the Report extraction type:</p> <p></p>"},{"location":"documentation/report/#create-a-report-extraction","title":"Create a Report Extraction","text":"<ol> <li>In the main window of the Designer, click [New] to create a new extraction. The window \"Create Extraction\" opens. </li> <li>Select an SAP Connection from the drop-down menu in Source and enter a unique name for your extraction.</li> <li>Select the extraction type Report and click [OK]. The main window of the extraction type opens automatically.</li> </ol> <p>The majority of the functions of the extraction type can be accessed in the main window.</p>"},{"location":"documentation/report/#look-up-a-report-or-transaction","title":"Look up a Report or Transaction","text":"<ol> <li>In the main window of the extraction type, click [ ]. The window \u201cReport Lookup\u201d opens. </li> <li> <p>In the field Report Name, enter the name of a report to extract  . Use wildcards (), if needed. Alternatively, select TCODE* to look up SAP Transaction codes.  </p> <p>Tip</p> <p>In some cases reports cannot be determined based on the TCODE.  You can check the report name of a TCODE using the SAP GUI menu System &gt; Status....</p> </li> <li> <p>Click [ ] . Search results are displayed.</p> </li> <li>Select a report and click [OK] to confirm.</li> </ol> <p>The application now returns to the main window of the extraction type.</p>"},{"location":"documentation/report/#define-the-report-extraction-type","title":"Define the Report  Extraction Type","text":"<p>The Report extraction type offers the following options for report extractions:</p> <ol> <li>If the report has variants, select a variant from the drop-down-list Variant. For more information, see Choose a Variant. </li> <li>In the section Selection Screen, edit a selection criterion you want to change or dynamize  . For more information, see Edit Selections. </li> <li>Optional: If your report has varying column widths, activate Dynamic column widths and offsets. The column widths and offsets are then adjusted dynamically at report runtime. </li> <li>Click [Automatically detect columns] to execute the report based on the selected variant or selections and detect columns automatically. </li> <li>Click [Load live preview] to display a live preview of the first 100 records.</li> <li>Check if the automatically detected columns are accurate.  When automatic column detection is not possible, the column names, widths and offsets must be set manually, see Define Columns manually.</li> <li>Check the Extraction Settings and the General Settings before running the extraction.</li> <li>Click [OK] to save the extraction type.</li> </ol> <p>You can now run the extraction, see Execute and Automate Extractions.</p>"},{"location":"documentation/report/#example","title":"Example","text":"<p>The depicted example shows how to set up a simple report extraction:</p> <ul> <li>Look up report RLT10010</li> <li>Select variant VAR01</li> <li>Load a live preview</li> <li>Remove the header (skip the first 7 rows)</li> <li>Automatically detect columns</li> <li>Save the extraction type</li> </ul> <p></p>"},{"location":"documentation/report/#related-links","title":"Related Links","text":"<ul> <li>SAP Wiki: Types of ABAP Reports</li> <li>Knowledge Base Article: Authorize Access to Specific Reports</li> </ul>"},{"location":"documentation/report/edit-runtime-parameters/","title":"Runtime Parameters","text":"<p>Runtime parameters are are placeholders for values that are passed at runtime, see Extraction Parameters - Custom. They can be created in context of Selections.</p>"},{"location":"documentation/report/edit-runtime-parameters/#create-runtime-parameters","title":"Create Runtime Parameters","text":"<p>There are two types of runtime parameters:</p> Scalar parametersList parameters <p>Scalar runtime parameters represent a single value.  Follow the steps below to create a scalar runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add Scalar] to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime. </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.</p> Type Description Text Can be used for any type of SAP selection field. Number Can be used for numeric SAP selection fields. Flag Can only be used for SAP selection fields THAT require an \u2018X\u2019 (true) or a blank \u2018\u2018 (false) as input value. </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p> <p>List runtime parameters represent multiple values.  Follow the steps below to create a list runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add List] to define list parameters that contain multiple values separated by commas e.g., 1,10 or \u201c1\u201d, \u201c10\u201d. The placeholders need to be populated with actual values at runtime.  </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p>"},{"location":"documentation/report/edit-runtime-parameters/#assign-runtime-parameters","title":"Assign Runtime Parameters","text":"<p>Follow the steps below to assign the runtime parameters to selections.</p> <ol> <li>In the main window of the extraction type, click the [Edit] button next to the selection you want to parameterize.  The window \"Edit Selections\" opens.</li> <li>Add a filter to the selection, see Edit Selections. </li> <li>Click the icon button next to the input field to switch between static values ( ) and runtime parameters ( ). If no icon button is available, create a runtime parameter. </li> <li>Select a runtime parameter from the dropdown-list.</li> <li>Click [OK] to confirm the input.</li> </ol> <p>Pass values during runtime, see Extraction Parameters - Custom.</p>"},{"location":"documentation/report/general-settings/","title":"General Settings","text":"<p>This page contains an overview of the settings in the window \"General Settings\". To open the general settings, click General Settings in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/report/general-settings/#misc-tab","title":"Misc. Tab","text":"<p>The Misc. tab covers cache settings, column encryption and keywords of an extraction type.</p> <p></p>"},{"location":"documentation/report/general-settings/#cache-results","title":"Cache results","text":"<p>The Cache results option is only available in pull destinations, e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times.  To decrease the SAP server load, you can select the Cache results option, this way the pull destination pulls the data from cache and not from the SAP.</p> <p>This increases the performance and limits the impact on the SAP system.  If this behavior is not desired (for example, because the data must be always 100% up to date), the cache option must be explicitly turned off.</p>"},{"location":"documentation/report/general-settings/#keywords","title":"Keywords","text":"<p>One or more keywords (tags) can be assigned to an extraction.  Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the \"Search Extractions\" window. To open the \"Search Extractions\" window, click [  Search] in the main window of the Designer. </p> <p>Tip</p> <p>To add keywords to multiple extractions at once, select the extractions in the main window of the Designer. Right-click + Add/Remove keywords opens the window \"Add/Remove Keywords To/From Multiple Extractions\".</p>"},{"location":"documentation/report/general-settings/#primary-key-tab","title":"Primary Key Tab","text":"<p>Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.  </p> <p></p> <p>The depicted example shows the SAP object MAKT with its primary key inherited from SAP in the general settings of the Designer.  In this example the primary key consists of MANDT, MATNR and SPRAS. The primary key is also taken over in the destination. </p> <p>Note</p> <p>A defined primary key field in a table is a prerequisite for merging data. </p>"},{"location":"documentation/report/general-settings/#generate-surrogate-key-column","title":"Generate Surrogate Key Column","text":"<p>If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs.  The surrogate keys are hash values of type signed 8 byte integer, e.g., <code>#-3008591679982390000</code>. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.</p>"},{"location":"documentation/report/general-settings/#security-tab","title":"Security Tab","text":"<p>Restrict user access to the extraction. For more information, see Restrict Designer Access.</p> <p></p>"},{"location":"documentation/report/general-settings/#columns-order-tab","title":"Columns Order Tab","text":"<p>The \"Columns Order\" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns.  Index 0 defines the first column in the result set, index 1 the second columns, etc.</p> <p></p> Option Description Enabled When this option is active, the defined column order is applied when running the extraction. Swap Swaps the index of 2 columns. All other columns keep their indexes. Insert Inserts the selected column into the selected index. All other indexes are recalculated. Reset default Restores the original column order."},{"location":"documentation/report/report-columns-define/","title":"Define Columns and Rows","text":"<p>This page shows how to define the columns of a report. A report column is defined by its name, offset and length.  Per default, all columns are of data type string.  To identify a columns, the report needs to be executed. Columns can then be identified based on the output.</p> <p>Tip</p> <p>At this stage, use a selection or variant that returns only a few records. This can be adapted later on.</p> <ul> <li>Some classical ABAP reports are developed to use the pipe symbol '|' as a delimiter for the output columns.  In this case the Report extraction type can automatically identify the columns.  Automatic column detection also works for most ALV reports. Example: </li> <li>For reports where the output is not separated by the pipe symbol, the columns must be identified manually. Example: </li> </ul>"},{"location":"documentation/report/report-columns-define/#define-columns-automatically","title":"Define Columns Automatically","text":""},{"location":"documentation/report/report-columns-define/#automatically-detect-columns","title":"Automatically detect columns","text":"<p>Click [Automatically detect columns] to execute the report based on the selected variant or selections.  If they can be identified automatically, the column name, width and offset are displayed in the Columns section in the main window of the extraction type.</p>"},{"location":"documentation/report/report-columns-define/#dynamic-column-width-and-offset","title":"Dynamic column width and offset","text":"<p>If this option is active, the column width and offset is adjusted dynamically at report runtime.  This can be required for reports that have varying column widths depending on the report's selection criteria.</p>"},{"location":"documentation/report/report-columns-define/#define-columns-manually","title":"Define Columns Manually","text":"<p>When automatic column detection is not possible, the column names, widths and offsets must be set manually.</p> <ol> <li>Make sure no columns are defined yet.  If columns are defines, click [ ] in the Columns section in the main window of the extraction type to delete the columns. </li> <li>Click [Load live preview]. The report is executed based on the selected report variant or selections.  The output of the report is displayed in the Load Preview section.</li> <li>To define the beginning (offset) of a column, press and hold down the left mouse button in the Load Preview section.  </li> <li>To define the length of a column, move the mouse pointer to the right while still holding down the left mouse button.</li> <li>Let go of the cursor. The report column is highlighted with a green background and an entry is added to the Columns section.  </li> <li>To change the column name, offset and width, click in the respective fields in the Columns section and enter a new value.</li> <li>Repeat steps 3 to 6 until all columns are defined.</li> </ol> <p>Note</p> <p>Once a column is set and highlighted with a green background, its width and offset cannot be changed via the graphics editor.  Change it using the Columns section.</p>"},{"location":"documentation/report/report-columns-define/#row-settings","title":"Row Settings","text":""},{"location":"documentation/report/report-columns-define/#skip-rows-from-top","title":"Skip rows from top","text":"<p>Enter the number of rows you want to skip at the beginning of the report. Some reports display meta information in the header section of the report, before the actual report body. This setting allows skipping the meta information. </p>"},{"location":"documentation/report/report-columns-define/#skip-rows-from-bottom","title":"Skip rows from bottom","text":"<p>Similar to skip rows from top. Enter the number of rows you want to skip in the footer section of the report.</p>"},{"location":"documentation/report/report-columns-define/#report-rows-per-data-row","title":"Report rows per data row","text":"<p>Use this setting for ABAP reports, that return two or more \"physical\" rows to display a single \"semantic\" data row. Enter the number of physical rows that represent a single data row.  Example: Report RIEQUI20 </p> <p></p>"},{"location":"documentation/report/report-columns-define/#report-width","title":"Report width","text":"<p>Use this setting in combination with Report rows per data row. Report width defines the length of each physical row.  The maximum width of extracted reports is limited to 1024 characters per row.</p>"},{"location":"documentation/report/settings/","title":"Extraction Settings","text":"<p>This page contains an overview of the extraction settings in the Report extraction type. To open the extraction settings, click Extraction Settings in the main window of the extraction type. </p> <p></p>"},{"location":"documentation/report/settings/#batch-processing","title":"Batch Processing","text":""},{"location":"documentation/report/settings/#use-background-mode","title":"Use Background Mode","text":"<p>If you choose this option, the ABAP report is executed as a batch job in SAP.  A spool is generated in SAP (transaction SP01) which is later fetched by the Report extraction type.  Use this option for long running reports in SAP that would run into an RFC timeout when called in dialog mode.  Some reports that throw an error message when running in dialog mode can be extracted when run in background mode.</p>"},{"location":"documentation/report/settings/#background-job-timeout","title":"Background Job Timeout","text":"<p>Enter a time period (in seconds).  The Report extraction type polls the status of the batch job in SAP for the specified time period.  If the SAP batch job is not finished by the specified time period, the extraction aborts.</p>"},{"location":"documentation/report/settings/#background-job-name","title":"Background Job Name","text":"<p>The name of the background job under which the report is run in SAP.</p>"},{"location":"documentation/report/settings/#spool-destination","title":"Spool Destination","text":"<p>Enter the name of the spool destination (printer).</p>"},{"location":"documentation/report/settings/#automatic-detection","title":"Automatic Detection","text":""},{"location":"documentation/report/settings/#header-pattern","title":"Header pattern","text":"<p>Enter a search pattern (e.g., Created on) to detect the table header.  The Report extraction type scans the report output for this pattern and uses the complete line this pattern occurs in as the report header.</p> <p>This setting is usually not required if the report's columns can be detected automatically and Dynamic column widths and offsets is active in the main window of the extraction type.</p>"},{"location":"documentation/report/settings/#row-skip-pattern","title":"Row skip pattern","text":"<p>Enter a search pattern. All report rows that contain the pattern are removed from the result set.  Rows are removed after the report data was extracted from SAP. Regular expressions is supported. Multiple row skip patterns can be entered separated by the pipe symbol '|'.</p> <p>Example: <code>2020|2021|-|Sum</code> removes all rows containing the pattern '2020', '2021', '-' and 'Sum'. </p> <p>The skip row setting can be used for skipping header rows that are repeated in the output body of reports. For more information, see Knowledge Base Article: Skip Rows in Reports.</p> <p>Note</p> <p>This setting is usually not required if the report columns can be detected automatically and Dynamic column widths and offsets is checked in the Report window.</p>"},{"location":"documentation/report/settings/#function-module","title":"Function Module","text":"<p>The Report component requires installation of the custom function module <code>Z_XTRACT_IS_REMOTE_REPORT</code> in your SAP system, see Install Report Custom Function Module.  If you manually created the function module in your SAP system and gave it a different name, enter that name in this field.  The default is <code>Z_XTRACT_IS_REMOTE_REPORT</code>.</p>"},{"location":"documentation/report/settings/#related-links","title":"Related Links","text":"<ul> <li>Regular Expressions in the Microsoft Online Help</li> <li>Knowledge Base Article: Skip Rows in Reports</li> <li>Install Report Custom Function Module</li> </ul>"},{"location":"documentation/report/variants-and-selections/","title":"Variants and Selections","text":"<p>Most reports allow entering selections before report execution.  Selections limit the result set of the report to extract only records that match the selection. </p> <p>In SAP a selection variant can be created in the input screen of an ABAP report.  The purpose of a variant is to save selection settings on your input screen and to minimize the necessity to enter selections when running a report.</p> <p>Note</p> <p>Manual selections and variants can be combined. Manual selections overwrite any selections in the variant.</p>"},{"location":"documentation/report/variants-and-selections/#choose-a-variant","title":"Choose a Variant","text":"<p>Choose a variant from the drop-down-list Variant.  When creating a new variant in SAP after creating the extraction, click [ ] to load the new variant.</p> <p></p> <p>The selections of the variant are not displayed in the Selection Screen section of the window.  To see the definition of a variant, open the variant in SAP.</p> <p>Tip</p> <p>You can define the variant at runtime by using a corresponding parameter in the extraction URL, see Extraction Parameters.</p>"},{"location":"documentation/report/variants-and-selections/#edit-selections","title":"Edit Selections","text":"<p>The Selection Screen in the main window of the component corresponds to the input screen in SAP.</p> <p>Note</p> <p>Some selection fields only have a technical name and no description.  To understand which field corresponds to a field in SAP, open the input screen in SAP.  Click on a selection field and press the function key F1 to display the technical name of the selection field. </p> <p>Follow the steps below to edit selection fields and filter data:</p> <ol> <li>In the subsection Selection Screen, click [Edit] next to the field you want to edit. The window \u201cEdit selection\u201d opens. </li> <li>Add one or more of the following filter types:<ul> <li>Click [Single] to compare the data to a single specified value.</li> <li>Click [Range] to check if the data is (not) within a specified range of values.</li> <li>Click [List] to check if the data is (not) part of a specified list of values. </li> </ul> </li> <li>In the column Sign , select Include to add the filtered data to the output or select Exclude to remove the filtered data from the output. </li> <li> <p>In the column Option , select an operator. The operator filters data according to the table below.</p> Operator Description (not) like pattern True if data values do (not) contain to the content of operand 1. (not) equal to True if data is (not) equal to the content of operand 1. at least True if data is greater than or equal to the content of operand 1. more than True if data is greater than the content of operand 1. at most True if data is less than or equal to the content of operand 1. less than True if data is less than the content of operand 1. (not) between True if data values do (not) lie between the values of operand 1 and operand 2. element of True if data values are part of operand 1. This option is only available for type List. </li> <li> <p>In the column Value, enter values directly into the input fields Low and High or assign existing  runtime parameters  to the selection fields  .</p> <p>Note</p> <p>When runtime parameters are available, you can use the icon button inside the input field to switch between static values ( ) and runtime parameters ( ).</p> </li> <li> <p>Click [OK] to confirm your input. </p> </li> <li>Click [Load live preview] in the main window of the extraction type to check the result of your selection.  If runtime parameters are defined, you are prompted to populate the parameters with actual values.</li> </ol> <p>Note that edited selection fields overwrite the selection fields in the variant. </p> <p>Tip</p> <p>If you use multiple selection parameters, it is more efficient to create a variant in SAP.</p>"},{"location":"documentation/report/variants-and-selections/#data-format","title":"Data Format","text":"<p>Use the following internal SAP representation for input:</p> <ul> <li>Date: The date 01.01.1999 has the internal representation 19990101 (YYYYMMDD).</li> <li>Year period: The year period 001.1999 has the internal representation 1999001 (YYYYPPP).</li> <li>Numbers: Numbers must contain the leading zeros, e.g., customer number 1000 has the internal representation 0000001000.</li> </ul> <p>Warning</p> <p>Values accept only the internal SAP representation. Input that does not use the internal SAP representation results in error messages.  Use the internal SAP representation.  Example:  <pre><code>ERPConnect.ABAPProgramException: RfcInvoke failed(RFC_ABAP_MESSAGE): Enter date in the format \\_.\\_.\\_\n</code></pre></p>"},{"location":"documentation/report/variants-and-selections/#related-links","title":"Related Links","text":"<ul> <li>SAP Help: Report variants in SAP</li> </ul>"},{"location":"documentation/sap-connection/","title":"SAP Connection","text":"<p>This page shows how to connect to SAP. An SAP connection is required to use any Xtract Universal extraction type.</p> <p>Warning</p> <p>Missing Authorization. To establish a connection to SAP, the access to general authority objects must be available. Adjust the SAP Authority Objects accordingly.</p> <p>Supported Connection Methods </p> <ul> <li>Connection to a single application server</li> <li>Connection to a message server (Load Balancing) </li> <li>Connect to a single application server or public or private cloud instance via RFC over WebSocket.</li> </ul> <p>Supported Authentication Methods</p> <ul> <li>Plain login using SAP username and password (system or dialogue user)</li> <li>Secure Network Communication (SNC) using username and password via basic authentication</li> <li>SSO with Logon-Ticket using username and password via basic authentication</li> </ul>","boost":2},{"location":"documentation/sap-connection/#connect-to-sap","title":"Connect to SAP","text":"<p>Follow the steps below to create a source that connects to SAP:</p> <ol> <li>In the main window of the Designer, navigate to the menu bar and select Server &gt; Manage Sources. The window \"Manage Sources\" opens.   </li> <li>Click [Add] to add a new SAP connection or click [ ] to edit an existing connection. The window \"Change Source\" opens.  </li> <li>Enter a name for the SAP connection in the field Name.</li> <li> <p>In the General tab, select a connection method and enter the system details of your SAP system.  </p> <p>Tip</p> <p>Input values for the SAP connection can be found in the Properties of the SAP Logon Pad or they can be requested from the SAP Basis team.</p> </li> <li> <p>In the Authentication tab, select one of the following authentication methods:</p> <ul> <li>Plain uses the SAP username and password.</li> <li>Secure Network communication (SNC) uses an encrypted connection between Xtract Universal and SAP with an SAP username and password.</li> <li>SAP Logon Ticket uses SAP Logon-Tickets in place of user credentials. This connection is not encrypted.</li> </ul> </li> <li>In the RFC Options tab, select an RFC library for the SAP connection. The default is the NetWeaver RFC library.</li> <li>Optional: In the Access Control tab, you can restrict read and write access to the SAP source, see Access Management.</li> <li>Click [Test designer connection] to validate the connection between the Xtract Universal Designer and the SAP system. </li> <li>Click [Test server connection] to validate the connection between the Xtract Universal Server and the SAP system. </li> <li>Click [OK] to save the SAP source.</li> </ol> <p>For more information on the input options, see Connection Settings.</p>","boost":2},{"location":"documentation/sap-connection/#assign-an-sap-source-to-extractions","title":"Assign an SAP Source to Extractions","text":"<p>An SAP source is assigned when creating an extraction.  Follow the steps below to change the SAP source of an existing extraction:</p> <ol> <li>Select an extraction from the list of extractions in the main window of the Designer.</li> <li>Click [ graph_from icon graph_from icon from the IconExperience.com O-Collection. Copyright by INCORS GmbH (www.incors.com). Source]. The window \u201cChange Source\u201d opens. </li> <li>Select an SAP source from the dropdown list. </li> <li>Click [OK] to confirm your input.</li> </ol>","boost":2},{"location":"documentation/sap-connection/#single-sign-on-sso","title":"Single-Sign-On (SSO)","text":"<p>BI client tools such as Power BI, Power Pivot, Alteryx, etc. can start extractions in Xtract Universal.  Xtract Universal loads the extracted data directly into the tools.  In this use case, it is often required that the extraction is executed with the SAP credentials of the (Windows AD) user, whose BI client triggered the extraction.  This means that the SAP authorizations of the user apply. This is especially important when extracting BW/BEx queries.</p> <p>The Windows credentials of the user are forwarded to SAP using Xtract Universal.  On the way to SAP or on the SAP side, the Windows user and its SAP credentials are mapped.</p>","boost":2},{"location":"documentation/sap-connection/#supported-sso-scenarios","title":"Supported SSO Scenarios","text":"<p>Xtract Universal supports the following procedures for Single Sign-On (SSO):</p> <ul> <li>Secure Network Communication (SNC) with Client Certificates</li> <li>Secure Network Communication (SNC) with PSE and External ID</li> <li>Secure Network Communication (SNC) with SAP\u2019s Kerberos Wrapper Library (deprecated)</li> <li>SAP Logon Ticket</li> </ul> <p>The authentication method can be selected in the SAP source connection settings.</p>","boost":2},{"location":"documentation/sap-connection/#connect-via-router","title":"Connect via Router","text":"<p>If you access the SAP source system (Application server or Message server) via an SAP router, set the router string before the host name.  For more information on SAP routers, see SAP Help: SAP-Router.</p> <p>Example: If the application server is \"hamlet\" and the router string is <code>/H/lear.theobald-software.com/H/</code>, set the host property to <code>/H/lear.theobald-software.com/H/hamlet</code>.</p>","boost":2},{"location":"documentation/sap-connection/#related-links","title":"Related Links","text":"<ul> <li>Connection Settings</li> </ul>","boost":2},{"location":"documentation/sap-connection/settings/","title":"Connection Settings","text":"<p>This page contains an overview of the SAP connection settings in the window \"Change Source\". To open the server settings, navigate to [Server] &gt; [Manager Sources] in the main window of the Designer and click [ ].</p> <p></p>","boost":2},{"location":"documentation/sap-connection/settings/#general","title":"General","text":"<p>Some input fields for the SAP connection vary depending on the selected connection method. </p> Single Application ServerLoad BalancingWebSocket <p></p> Input Field Description Host Host name or IP address of the application server (Property Host). Instance No. A two-digit number between 00 and 99 (Property SystemNumber). Client A three-digit number of the SAP client between 000 and 999, e.g., 800. Language The logon language for the SAP system, e.g., EN for English or DE for German. <p></p> Input Field Description Message Server Name or IP address of the message server (Property MessageServer). System ID Three-digit System ID (Property SID e.g.,  MSS). Logon group Property LogonGroup, usually PUBLIC. Client A three-digit number of the SAP client between 000 and 999, e.g., 800. Language The logon language for the SAP system, e.g., EN for English or DE for German. <p></p> Input Field Description Host Name or IP address of the SAP (cloud) system. Port Port of the SAP (cloud) system, e.g., 443. Library Path to the SAP Cryptographic Library (download available in the SAP Service Marketplace). Client PSE Path to the client .pse file, see Knowledge Base Article: Create a Client PSE to connect to SAP Cloud Systems Client A three-digit number of the SAP client between 000 and 999, e.g., 800. Language The logon language for the SAP system, e.g., EN for English or DE for German.","boost":2},{"location":"documentation/sap-connection/settings/#authentication","title":"Authentication","text":"<p>Some input fields for the SAP connection vary depending on the selected authentication method. </p> PlainSecure Network communication (SNC)SAP Logon Ticket <p></p> Input Field Description User SAP username. Password Password of the SAP user. User name is alias Activate this option when connecting to an SAP cloud system using the WebSocket connection method. When this option is active, the name entered in the field User is used as the internet user alias. Request SAP credentials from caller when running extractions If this option is active, SAP credentials entered in the fields User and Password are not applied. Instead, SAP credentials need to be provided via basic authentication when running an extraction. Caching the result of extractions is inactive. <p></p> Input Field Description User SAP username. Password Password of the SAP user. User name is alias Activate this option when connecting to an SAP cloud system using the WebSocket connection method. When this option is active, the name entered in the field User is used as the internet user alias. SNC library Path to the SNC library, e.g., <code>C:\\Program Files\\SAP\\FrontEnd\\SecureLogin\\sapcrypto.dll</code> SNC Partner Name The SAP Partner Name configured for the SAP application server, e.g., <code>p:SAPserviceERP/Alice@THEOBALD.LOCAL</code>. Use static SAP credentials / Windows service account This option activates SNC without SSO. If available, the SAP credentials in the fields User and Password are used for authentication. The Windows Active Directory user used to open the connection is the service account under which the Xtract Universal Windows service runs. Request SAP credentials from caller This option activates SNC with user and password. If this option is active, SAP credentials entered in the fields User and Password are not applied. Instead, SAP credentials need to be provided via basic authentication when running an extraction. SSO - Log in as caller via External ID This option activates SSO with External ID. SSO with External ID uses a Personal Security Environment (PSE) to create a trust relationship between the SAP application server and the service account that runs Xtract Universal. This allows Xtract Universal to impersonate any SAP user. For more information, see Knowledge Base Article: SSO with External ID. SSO - Impersonate caller via Kerberos This option activates Kerberos SSO. The Windows Active Directory user is used for authentication. For this scenario \u201cHTTPS - Restricted to AD users with Designer read access\u201d must be selected and configured in the Server Settings. For more information, see Knowledge Base Article: SSO with Kerberos SNC. SSO - Enroll certificate on behalf of caller This option activates Certificate SSO. The Certificate SSO authentication uses Certificate Enrollment (Enroll-On-Behalf-Of) via Active Directory Certificate Services for the Windows Active Directory user who calls the extraction. For this scenario \u201cHTTPS - Restricted to AD users with Designer read access\u201d must be configured in the Server Settings. For more information, see Knowledge Base Article: SSO with Client Certificates. <p></p> Input Field Description Ticket issuer URL URL of an Application Server Java (AS Java) that is configured to issue logon tickets. For more information, see SAP Help: Configuring the AS Java to Issue Logon Tickets. Impersonate caller when running extractions (Kerberos SSO) Activate this option to open the connection in the Windows Active Directory user context of the caller. Otherwise the connection is opened in the context of the service account under which the Xtract Universal Windows service runs. For more information, see Knowledge Base Article: SSO with Logon-Ticket.","boost":2},{"location":"documentation/sap-connection/settings/#rfc-options","title":"RFC Options","text":"","boost":2},{"location":"documentation/sap-connection/settings/#rfc-libraries","title":"RFC Libraries","text":"<p>Select an RFC library. The following RFC libraries are supported:</p> <ul> <li>NetWeaver RFC library (sapnwrfc.dll)</li> <li>Classic RFC library (librfc32.dll)</li> </ul> <p>The RFC API (Remote Function Call) allows to establish an RFC connection to an SAP system from an external system that communicates as Client or Server with the SAP system.  For more information on SAP libraries, see SAP Help: RFC Libraries. </p> <p>SAP does not support librfc32.dll anymore. </p> <p>Note</p> <p>For some older SAP releases, e.g., R/3 4.6C, it is necessary to enter the user name in upper case when using the NetWeaver RFC library.</p> <p>Note</p> <p>When using the NetWeaver RFC library with the DeltaQ extraction type or the OHS extraction type, the RFC destination in SAP transaction SM59 must be set to Unicode.  We recommend using the not supported librfc32.dll for some extractions types, e.g., DeltaQ as it runs more stable than the NetWeaver RFC library.</p>","boost":2},{"location":"documentation/sap-connection/settings/#trace-directory","title":"Trace Directory","text":"<p>You can log debug information and save it locally.  Enter a path to a local directory in the field Trace directory to save the debug information. For more information, see Troubleshooting: Trace Directory.</p> <p>Clear the Trace Directory field when it is not needed.</p> <p>Warning</p> <p>Increase of used hard drive memory.  A big amount of information is collected when debug logging is activated. This can decrease the capacity of your hard drives dramatically. Activate the debug logging only when necessary e.g., upon request of the support team.</p>","boost":2},{"location":"documentation/sap-connection/settings/#use-sapgui","title":"Use SAPGUI","text":"<p>There are SAP Reports and BAPIs that require an installed SAP GUI even when they are called remotely.  Activate this option only if necessary.</p> <p>Warning</p> <p>'sapgui' start failed. Sometimes SAP opens a pop-up window that requires input when running extractions. To deactivate pop-up windows, open the SAP GUI Logon pad and navigate to Options... &gt; Security Settings.  Click the [Open Security Configuration] button and select Allow as the Default Action. Apply the changes and close the SAP GUI Logon pad.</p>","boost":2},{"location":"documentation/sap-connection/settings/#access-control","title":"Access Control","text":"<p>Access control can be performed at the source level. This access control overrides the settings at the server level. For more information, see Access Management.</p>","boost":2},{"location":"documentation/sap-connection/snc-authentication/","title":"SNC Authentication","text":"<p>This page shows how to encrypt communication between Xtract Universal and the SAP system via Secure Network Communication (SNC).</p>","boost":2},{"location":"documentation/sap-connection/snc-authentication/#prerequisites","title":"Prerequisites","text":"<ul> <li>SNC must be configured in your SAP system. For more information about SNC configuration in SAP, see SAP Help: Configuring the Application Server. </li> <li>Check the SAP profile parameter snc/gssapi_lib in SAP (transaction RZ10) to determine, which library is used for encryption in your SAP system.  Your SAP Basis has to import and configure the same library on the application server and on the machine that runs Xtract Universal, e.g., <code>sapcrypto.dll</code>.</li> <li>The Xtract Universal server must be set up to use the HTTPS protocol, see Server Settings.</li> </ul> <p>For information on how to set up SNC via X.509 certificate, refer to the Knowledge Base Article: Enable Secure Network Communication (SNC) via X.509 certificate.</p>","boost":2},{"location":"documentation/sap-connection/snc-authentication/#configure-snc-in-the-sap-source","title":"Configure SNC in the SAP Source","text":"<p>Follow the steps below to set up an SAP connection that uses SNC:</p> <ol> <li>Create or open an SAP source. For more information, see Connect to SAP.</li> <li> <p>In the General tab, enter the system details of your SAP system.  </p> <p>Tip</p> <p>Input values for the SAP connection can be found in the Properties of the SAP Logon Pad or they can be requested from the SAP Basis team.</p> </li> <li> <p>In the Authentication tab, select the authentication method Secure Network Communication (SNC). </p> </li> <li>Enter the SAP username and password of an SAP system or dialogue user in the fields User and Password.</li> <li>Enter the path to the SNC library in the field SNC library, e.g., <code>C:\\Program Files\\SAP\\FrontEnd\\SecureLogin\\sapcrypto.dll</code>.</li> <li>Enter the SAP Partner Name configured for the SAP application server in the field SNC Partner Name, e.g., <code>p:SAPserviceERP/Alice@THEOBALD.LOCAL</code>.</li> <li> <p>In the subsection When running extractions, select one of the following SNC implementations:</p> SNC Implementation Description Use static SAP credentials / Windows service account This option activates SNC without SSO. If available, the SAP credentials in the fields User and Password are used for authentication. The Windows Active Directory user used to open the connection is the service account under which the Xtract Universal Windows service runs. Request SAP credentials from caller This option activates SNC with user and password. If this option is active, SAP credentials entered in the fields User and Password are not applied. Instead, SAP credentials need to be provided via basic authentication when running an extraction. SSO - Log in as caller via External ID This option activates SSO with External ID. SSO with External ID uses a Personal Security Environment (PSE) to create a trust relationship between the SAP application server and the service account that runs Xtract Universal. This allows Xtract Universal to impersonate any SAP user. For more information, see Knowledge Base Article: SSO with External ID. SSO - Impersonate caller via Kerberos This option activates Kerberos SSO. The Windows Active Directory user is used for authentication. For this scenario \u201cHTTPS - Restricted to AD users with Designer read access\u201d must be selected and configured in the Server Settings. For more information, see Knowledge Base Article: SSO with Kerberos SNC. SSO - Enroll certificate on behalf of caller This option activates Certificate SSO. The Certificate SSO authentication uses Certificate Enrollment (Enroll-On-Behalf-Of) via Active Directory Certificate Services for the Windows Active Directory user who calls the extraction. For this scenario \u201cHTTPS - Restricted to AD users with Designer read access\u201d must be configured in the Server Settings. For more information, see Knowledge Base Article: SSO with Client Certificates. </li> <li> <p>Click [Test designer connection] to validate the connection between the Xtract Universal Designer and the SAP system. </p> </li> <li>Click [Test server connection] to validate the connection between the Xtract Universal Server and the SAP system. </li> <li>Click [OK] to save the SAP source.</li> </ol>","boost":2},{"location":"documentation/sap-connection/snc-authentication/#download-kerberos-dlls","title":"Download Kerberos DLLs","text":"<p>It is possible to use Kerberos libraries for encryption between the client and the SAP server. For more information, see SAP Note 2115486.  Different DLLs for 32-bit (<code>gsskrb5.dll</code>) and 64-bit (<code>gx64krb5.dll</code>) platforms are provided with SAP Note 2115486.</p>","boost":2},{"location":"documentation/sap-connection/snc-authentication/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base Article: SSO with External ID</li> <li>Knowledge Base Article: SSO with Kerberos SNC</li> <li>Knowledge Base Article: SSO with Client Certificates</li> </ul>","boost":2},{"location":"documentation/sap-connection/sso-with-logon-ticket/","title":"SSO with Logon-Ticket","text":"<p>This page shows how to use SSO via Logon-Tickets between Xtract Universal and SAP. Note that this connection is not encrypted.</p>","boost":2},{"location":"documentation/sap-connection/sso-with-logon-ticket/#prerequisites","title":"Prerequisites","text":"<ul> <li>SSO with Logon-Tickets must be configured on the SAP system and and the application server, see Knowledge Base Article: SSO with Logon-Ticket.</li> <li>The Xtract Universal service must run under an XU Service Account.</li> <li>The Xtract Universal server must be set up to use the HTTPS protocol, see Server Settings.</li> </ul>","boost":2},{"location":"documentation/sap-connection/sso-with-logon-ticket/#configure-sso-with-logon-ticket-in-the-sap-source","title":"Configure SSO with Logon-Ticket in the SAP Source","text":"<p>Follow the steps below to set up an SAP connection that uses SSO with Logon-Ticket:</p> <ol> <li>Create or open an SAP source. For more information, see Connect to SAP.</li> <li> <p>In the General tab, enter the system details of your SAP system.  </p> <p>Tip</p> <p>Input values for the SAP connection can be found in the Properties of the SAP Logon Pad or they can be requested from the SAP Basis team.</p> </li> <li> <p>In the Authentication tab, select the authentication method SAP Logon Ticket. </p> </li> <li>Enter the URL of an Application Server Java (AS Java) that is configured to issue logon tickets in the field Ticket issuer Url. For more information, see SAP Help: Configuring the AS Java to Issue Logon Tickets.</li> <li>To open the connection in the Windows Active Directory user context of the caller, activate the option Impersonate caller when running extractions (Kerberos SSO).  Otherwise the connection is opened in the context of the service account under which the Xtract Universal Windows service runs.</li> <li>Click [Test designer connection] to validate the connection between the Xtract Universal Designer and the SAP system. </li> <li>Click [Test server connection] to validate the connection between the Xtract Universal Server and the SAP system. </li> <li>Click [OK] to save the SAP source.</li> </ol>","boost":2},{"location":"documentation/sap-connection/sso-with-logon-ticket/#related-links","title":"Related Links","text":"<ul> <li>Single Sign-On (SSO)</li> <li>Knowledge Base Article: SSO with Logon-Ticket</li> </ul>","boost":2},{"location":"documentation/server/","title":"Server","text":"<p>Xtract Universal is created for distributed use.  The Xtract Universal Service can be installed on a central server instance, e.g., a company-wide application server. As a result, several users can connect their Xtract Universal Designer to the server instance to create and modify extractions. The users must have access to the <code>C:\\Program Files\\XtractUniversal\\config</code> directory on the server instance.</p> <p>Tip</p> <p>Use the access restrictions in Xtract Universal to allow only users with administrator rights to perform fundamental changes on the central repository. </p>"},{"location":"documentation/server/#install-the-xtract-universal-server","title":"Install the Xtract Universal Server","text":"<p>When executing XtractUniversalSetup.exe, the Xtract Universal server is installed and started as a standard Windows service. Make sure to mark the option Server &gt; Install Service during the Installation.</p> <p></p> <p>The Xtract Universal Service can be installed and removed using the <code>C:\\Program Files\\XtractUniversal\\XtractService.exe</code> application. XtractService.exe is used via the Windows command line and supports the following commands:</p> <ul> <li><code>/i</code> - install Windows service</li> <li><code>/u</code> - uninstall Windows service</li> </ul> <p>Example: <pre><code>C:\\Program Files\\XtractUniversal&gt;XtractService.exe /i\nC:\\Program Files\\XtractUniversal&gt;XtractService.exe /u\n</code></pre></p> <p>Tip</p> <p>There is a standalone version of the Xtract Universal Designer that can be used to connect to a central Xtract Universal server instance without any software installation, see Installation of the Standalone Designer.</p>"},{"location":"documentation/server/#windows-service","title":"Windows Service","text":"<p>After installation, the Windows service can be started, stopped and configured via the Windows Services administration or the Task Manager. For information on how to run the service under a dedicated Windows service account, see Change Service Account.</p> <p></p>"},{"location":"documentation/server/#ports","title":"Ports","text":"<p>The Xtract Universal Server runs as a Windows service with the main process being the XtractService.exe that is located in the installation directory of Xtract Universal, e.g., <code>C:\\Program Files\\XtractUniversal\\XtractService.exe</code>.  The XtractService.exe starts two listener processes that listen on the following ports by default:</p> Listener Process Default Port Comment Theobald.Xu.Web.Listener.exe 8065 (HTTP) and 8165 (HTTPS) Port can be changed. The web server accepts extraction calls via HTTP(S). Theobald.Xu.Rpc.Listener.exe 8064 Port can be changed. The configuration server communicates with the Xtract Universal Designer via a dedicated port. <p>If manual adjustments of the port number are necessary, ask your network team for the correct ports.</p> <p>Note</p> <p>In the case of distributed use, create corresponding Inbound Port Rules for the protocol type TCP of the listener processes mentioned above.</p> <p>Warning</p> <p>Changing the default ports After manually adjusting the default ports, the Xtract Universal service does not start. The stored ports are already assigned and must be adapted in the following config files.  Administrator access rights are required for altering the config files. <code>C:\\Program Files\\XtractUniversal\\config\\server\\config\\general.json</code> <code>C:\\Program Files\\XtractUniversal\\config\\server\\web\\general.json</code></p>"},{"location":"documentation/server/server-settings/","title":"Server Settings","text":"<p>This page contains an overview of the server settings of Xtract Universal. To open the server settings, navigate to [Server] &gt; [Settings] in the main window of the Designer.</p> <p></p> <p>Note</p> <p>The settings are stored in the following directory: <code>C:\\Program Files\\XtractUniversal\\config\\server</code> (by default).</p>"},{"location":"documentation/server/server-settings/#configuration-server","title":"Configuration Server","text":"<p>The configuration server communicates with the Xtract Universal Designer.</p> <p></p>"},{"location":"documentation/server/server-settings/#port","title":"Port","text":"<p>Defines the port number for communication between Server and Designer.  The default is 8064. If you set a different port, add the new port number to the host name on the logon screen ([host name]:[port]).</p>"},{"location":"documentation/server/server-settings/#max-age-of-log-files-days","title":"Max. age of log files (days)","text":"<p>Defines the maximum age of the config server log files in days.  After this period the log files are deleted.</p>"},{"location":"documentation/server/server-settings/#authentication-methods","title":"Authentication methods","text":"<p>Defines the authentication methods that are available when connecting a Designer to the Xtract Universal Server. For more information, see Authentication Between Designer and Server.</p>"},{"location":"documentation/server/server-settings/#select-x509-certificate","title":"Select X.509 certificate","text":"<p>Select an X.509 certificate that is used for for transport encryption and authentication, when custom users connect a Designer to the Server. For more information, see Install an X.509 certificate.</p>"},{"location":"documentation/server/server-settings/#access-management","title":"Access Management","text":"<p>Defines which users and user groups have access to the Designer. For more information, see Access Management.</p>"},{"location":"documentation/server/server-settings/#web-server","title":"Web Server","text":"<p>The web server accepts extraction calls via HTTP(S).</p> <p> </p>"},{"location":"documentation/server/server-settings/#protocol","title":"Protocol","text":"Protocol Description HTTP - Unrestricted Runs extractions as an HTTP-URL. HTTPS - Unrestricted Runs extractions as an HTTPS-URL. This enables secure data transfer via HTTPS. HTTPS - Restricted to AD users with Designer read access This setting enables an additional access control for running an extraction. Extractions can only be executed by Windows AD users with at least a read permission (Read) in the Configuration Server tab. For more information, see Restrict Access to Windows AD Users (Kerberos Authentication). HTTPS - Restricted to custom users with Designer read access This setting enables an additional access control for running an extraction. Extractions can only be executed by custom users with at least a read permission (Read) in the Configuration Server tab. For more information, see Restrict Access to Custom Users (Basic Authentication). <p>Note</p> <p>To receive data via HTTPS, the installation of a TLS certificate is required on the server that runs the Xtract Universal service. </p>"},{"location":"documentation/server/server-settings/#http-port","title":"HTTP Port","text":"<p>Defines the port number, on which the Server receives HTTP requests of an extraction. </p>"},{"location":"documentation/server/server-settings/#https-port","title":"HTTPS Port","text":"<p>Defines the port number, on which the Server receives HTTPS requests of an extraction.</p>"},{"location":"documentation/server/server-settings/#select-x509-certificate_1","title":"Select X.509 certificate","text":"<p>Select an X.509 certificate that is used for for transport encryption and authentication, when running extractions. For more information, see Install an X.509 certificate.</p>"},{"location":"documentation/server/server-settings/#keep-log-files-days","title":"Keep log files (days)","text":"<p>Defines the maximum age of the web server log files in days. After this period the log files are deleted.</p>"},{"location":"documentation/server/server-settings/#enable-setup-distribution-for-clients","title":"Enable setup distribution for clients","text":"<p>Defines whether the setup of the product version that runs on the server needs to be downloaded.  When an older Designer version is connected to a newer Server version, you are be prompted to download and update the Designer with the product version. </p>"},{"location":"documentation/server/server-settings/#result-cache","title":"Result cache","text":"<p>Note</p> <p>The Cache results option is only available in pull destinations (e.g., PBI, Qlik etc.).</p> Option Description Target directory Sets the directory for the buffer files. The default directory is the result-cache folder in the installation directory, e.g., <code>C:\\Program Files\\XtractUniversal\\result-cache</code>. Max. cached runs Defines the maximum count of results of different extractions in the buffer. Max. age (minutes) Defines the maximum age in minutes of an extraction in the buffer."},{"location":"documentation/server/server-tasks/","title":"Server Tasks","text":"<p>This page contains an overview of the Xtract Universal server tasks and the underlying server architecture to execute the tasks. The server performs two main tasks:</p> <ul> <li>Run extractions stored in the Config directory. </li> <li>Make extractions stored in the Config directory available to the Designer.</li> </ul>"},{"location":"documentation/server/server-tasks/#run-extractions-on-the-server","title":"Run Extractions on the Server","text":"<p>The execution of an extraction is triggered by an HTTP request.   The HTTP request can be triggered from the target environment for pull destinations or from the xu command line tool (xu.exe /xu.elf), see Execute and Automate Extractions. </p> <p>Tip</p> <p>The process can be traced in the Extraction Log.</p> <ol> <li>The Theobald.xu.Web.Worker.exe checks the authentication and authorization of the request.</li> <li>The target environment is prepared for writing the extracted data (e.g. establish database connection, create file).</li> <li>The license is checked.</li> <li>A connection to the SAP source system is established.</li> <li>The data of the defined extraction type is requested.</li> <li>Each extracted data package is written to the target environment.</li> <li>After all packages are received, the Theobald.xu.Web.Worker.exe terminates the connection to the SAP source system and informs the target environment that the extraction is complete.</li> </ol> <p>Tip</p> <p>The Theobald.xu.Web.Worker.exe logs its actions in log files.  The log files are located in the logs subdirectory of the program directory:<code>C:\\Program Files\\XtractUniversal\\logs\\servers\\web\\worker</code> (default)  The logs can also be displayed in the Designer under [Server]&gt;[Logs (Web Worker)].</p>"},{"location":"documentation/server/server-tasks/#access-the-settings-using-the-designer","title":"Access the Settings using the Designer","text":"<p>The steps below show what happens when a Designer connects to the Server and changes settings.</p> <ol> <li>The Theobald.xu.Rpc.Worker.exe checks the authentication and authorization of the request.</li> <li>The Designer requests a certain setting, e.g., list of all extractions. </li> <li>The Theobald.xu.Rpc.Worker.exe reads the requested settings from the config directory and sends these settings to the Designer.</li> <li>The user changes settings in the Designer, e.g., destination settings.</li> <li>The Designer sends the changed settings back to the Theobald.xu.Rpc.Worker.exe.  Theobald.xu.Rpc.Worker.exe saves the changed settings in the config directory.</li> </ol> <p>Tip</p> <p>The Theobald.xu.Rpc.Worker.exe logs its actions in log files. The log files are located in the logs subdirectory of the program directory: <code>C:\\Program Files\\XtractUniversal\\logs\\server\\rpc\\worker</code> (default).</p>"},{"location":"documentation/server/server-tasks/#server-architecture","title":"Server Architecture","text":"<p>The server runs as a Windows Service and the main process of the service is the XtractService.exe.  The XtractService.exe starts two listener processes:</p> <ul> <li>Theobald.xu.Web.Listener.exe</li> <li>Theobald.xu.Rpc.Listener.exe</li> </ul> <p>Both listener processes listen on the Ports defined in the Server Settings.</p> <p>Tip</p> <p>The XtractService.exe logs its actions in ServiceLog.txt  The log file is located in the logs subdirectory of the program directory: <code>C:\\Program Files\\XtractUniversal\\logs</code> (default).</p>"},{"location":"documentation/server/server-tasks/#theobaldxurpclistenerexe","title":"Theobald.xu.Rpc.Listener.exe","text":"<p>The Theobald.xu.Rpc.Listener.exe waits for new connection requests from the Designer.  For each TCP connection the Theobald.xu.Rpc.Listener.exe starts a new instance of Theobald.xu.Rpc.Worker.exe, which processes all Designer requests coming in over the particular TCP connection, see Access the Settings using the Designer.</p> <p>Tip</p> <p>The Theobald.xu.Rpc.Listener.exe logs its actions in log files.  The log files are located in the logs subdirectory of the program directory: <code>C:\\Program Files\\XtractUniversal\\logs\\server\\rpc\\listener</code> (default).</p>"},{"location":"documentation/server/server-tasks/#theobaldxuweblistenerexe","title":"Theobald.xu.Web.Listener.exe","text":"<p>The Theobald.xu.Web.Listener.exe waits for HTTP requests.  For each TCP connection the Theobald.xu.Web.Listener.exe starts a new instance of Theobald.xu.Web.Worker.exe, which processes all HTTP requests coming in over the particular TCP connection, see Run Extraction on the Server.</p> <p>The following HTTP requests are possible:</p> <ul> <li>running an extraction</li> <li>emptying the result cache of an extraction</li> <li>canceling all runs of an extraction</li> <li>Web API requests</li> </ul> <p>Tip</p> <p>The Theobald.xu.Web.Listener.exe logs its actions in log files.  The log files are located in the logs subdirectory of the program directory: <code>C:\\Program Files\\XtractUniversal\\logs\\server\\web</code> (default).</p>"},{"location":"documentation/server/service-account/","title":"Change Service Account","text":"<p>This page shows how to run the Xtract Universal service under a dedicated Windows domain user account instead of the Local System account. After the installation, the Xtract Universal service runs under the Local System account by default.</p> <p>The following scenarios require the service to run under a dedicated Windows domain user account:</p> <ul> <li>Enabling Kerberos authentication for the Xtract Universal web server</li> <li>Enabling Windows authentication for an Xtract Universal destination that allows Windows credentials for log on, e.g., SQL Server destination, PostgreSQL destination.</li> <li>Enabling SSO with Kerberos SNC</li> <li>Enabling SSO with SAP Logon Tickets</li> </ul> <p>Note</p> <p>As of Xtract Universal version 5.0 SAP passwords are encrypted with a key that is derived from the Windows account that runs the Xtract Universal Service. The passwords can only be accessed from the same service account, when restoring a backup or moving the files to a different machine.  If the service account changes, passwords need to be re-entered manually.</p>"},{"location":"documentation/server/service-account/#basic-settings","title":"Basic Settings","text":"<ol> <li> <p>Create a Windows AD service account and assign an SPN (Service Principle Name) to the service account in the following format: <code>HTTP/[FQDN of XU Server]</code>.</p> <p>Tip</p> <p>Use the <code>setspn</code> command to check the SPNs of a user account.</p> <p></p> </li> <li> <p>Grant access rights to the installation folder of Xtract Universal and all sub folders to the service account as shown in the following screenshot: </p> </li> <li>If applicable, make sure the service account has Read access to the private key of the X.509 certificate used by Xtract Universal. </li> <li>Let the Xtract Universal service run under the service account. Make sure to use the correct domain, e.g., .company.local instead of .company.com. </li> <li>In the Xtract Universal Designer startup window \"Connect to Xtract Universal Server\", set Authentication to Windows credentials or Custom Credentials (Kerberos authentication). </li> <li>Enter the User Principal Name (UPN) of the service account in the Target Principal field. For more information, see Knowledge Base Article: Target Principal Field.</li> </ol>"},{"location":"documentation/server/service-account/#settings-for-sso-with-kerberos-snc","title":"Settings for SSO with Kerberos SNC","text":"<p>When using SSO with Kerberos SNC additional steps are necessary:</p> <ol> <li>Set constrained delegation for the Windows domain account under which the Xtract Universal Service runs. </li> <li>Enter the SPN of the service account under which the SAP ABAP application server is running (SAP Service Account), e.g., <code>SAPServiceERP/do_not_care</code>. For more information about the partner name notation in SAP, see the SAP Help: Preparing the Primary Application Server Instance.</li> </ol>"},{"location":"documentation/server/service-account/#related-links","title":"Related Links","text":"<ul> <li>Microsoft Documentation: About Service Logon Accounts</li> <li>Microsoft Documentation: Service Principal Names</li> <li>Knowledge Base Article: Target Principal Field</li> </ul>"},{"location":"documentation/setup/","title":"Setup","text":"<p>This section covers installation and maintenance topics for IT admins. This includes system requirements, backups and license information.</p>"},{"location":"documentation/setup/#popular-topics","title":"Popular Topics","text":"<ul> <li>   System Requirements</li> <li>  Backup &amp; Update</li> <li>  Supported SAP Releases &amp; Databases</li> <li>   License Installation</li> <li>  Configuration Files</li> <li>  Migration to a Different Machine</li> </ul>"},{"location":"documentation/setup/#related-topics","title":"Related Topics","text":"<p>Setup in SAP</p><p>SAP system requirements, e.g., user rights and customization.</p> <p>Server</p><p>Change server ports and configure data security.</p> <p>Access Management</p><p>Create and manage users with access restrictions.</p>"},{"location":"documentation/setup/download-and-evaluation/","title":"Download and Evaluation","text":""},{"location":"documentation/setup/download-and-evaluation/#download-xtract-universal","title":"Download Xtract Universal","text":"<p>You can download a 30 day trial version from the Theobald Software website. The trial version is only time limited and works otherwise without any restrictions.</p>"},{"location":"documentation/setup/download-and-evaluation/#evaluate-xtract-universal","title":"Evaluate Xtract Universal","text":"<p>You are guaranteed to get unrestricted support by the Theobald Software support team during the evaluation phase. In case of questions or doubts, feel free to contact Theobald Software at any time: </p> <ul> <li>  Support Portal</li> <li>  Contact Forms</li> </ul>"},{"location":"documentation/setup/download-and-evaluation/#technical-support","title":"Technical Support","text":"<p>Theobald Software offers support in  English and  German.</p> <ol> <li>Open a ticket in our Support Portal.</li> <li>Provide as much information as possible for the Theobald Software support team to understand and analyze the issue.</li> <li>If there is an error message, copy and paste the error message into the ticket.</li> <li>Copy and paste the software logs in the ticket, see Required Support Information for Xtract Universal.</li> </ol> <p>Disclaimer</p> <p>SAP versions that are no longer supported by the manufacturer are excluded from the Theobald Software support. Issues specific to custom BAPIs (Z function modules) or custom reports (Z reports) are also excluded from the scope of support.</p>"},{"location":"documentation/setup/installation/","title":"Installation","text":"<p>This page shows how and where to install Xtract Universal. </p>"},{"location":"documentation/setup/installation/#prerequisites","title":"Prerequisites","text":"<p>Administrator permissions are required to install Xtract Universal.</p>"},{"location":"documentation/setup/installation/#setup","title":"Setup","text":"<p><code>XtractUniversalSetup.exe</code> is an industry standard setup.  Execute the <code>XtractUniversalSetup.exe</code> file and follow the instructions of the setup program.</p> <p>When starting the installation program, optional components can be selected during the setup. </p> <p></p> Component Sub Component Description Main Product Files - All required files to use Board Connector. Designer - Installs the Designer application, uncheck this option if you want to use Board Connector without a graphical interface. Server Installs the Board Connector Server Install Service Installs the server component as a windows service, see Server. Convert config files Converts extractions, sources, destinations, etc. from previous version format to new format. Crucial when installing major releases and upgrading from e.g., version 3.x to 4.x. XtractUniversal Report Server Plugin - Plugin required by the Power BI Report Server destination. Start Menu Shortcuts - Component that adds shortcuts to the start menu. <p>Note</p> <p>Make sure to mark the option \u201cServer &gt; Install Service\u201d during the Installation, as installing a server without the service is only used for development purposes.</p> <p>For information on how to install a license, see Licensing.</p>"},{"location":"documentation/setup/installation/#installation-directory-files","title":"Installation Directory Files","text":"<p>The list below shows several most important files that are placed into the default directory <code>C:\\Program Files\\XtractUniversal</code> after installation:</p> Filename Description ABAP directory Directory with SAP function modules. Read the readme.txt within the directory for more information. See also SAP Customization. Alteryx directory Directory with a plugin setup for the Alteryx destination. logs directory Directory with server and extraction etc. logs, see Logs. config directory Directory containing all SAP connections, extractions, destinations and other settings. See also Backup and Migration. powerbi directory Directory containing files related to Power BI Connector destination. result-cache directory Directory with extraction cache files, only applicable for Pull Destinations. xu.exe Command line tool used for executing extractions, see Schedule an Extraction. xu-config.exe Command line tool used for creating extractions, see Create Extractions without the Xtract Universal Designer GUI. XtractCleanup.exe Application that deletes all cached results and log files, depending on the Web Server settings of Xtract Universal. XtractDesigner.exe Xtract Universal Designer application to create, test and monitor extractions. ConfigConverter.exe Application that converts extractions, sources, destinations, etc. from previous version format to new format. Crucial when installing major releases and upgrading from e.g., version 3.x to 4.x. uninstall.exe Tool for uninstalling and removing Xtract Universal with all its components from your machine. XtractUniversalSetup.exe Setup of the currently installed version, see Migration to a Different Machine. XtractUniversal Report Server Plugin Plugin required by the Power BI Report Server destination. Eula_XtractUniversal.rtf Document containing the license agreement for the use of the software Xtract Universal. XtractUniversalLicense.json License file with information about the server, the component and runtime. <p>Note</p> <p>The Xtract Universal Server can be started as a console program for test purposes. For more details on starting Xtract Universal Server as a console program, see Knowledge Base Article: Target Principal Field.</p> <p>The installation of Xtract Universal can be initiated unattended without the GUI in a non-interactive way via the Windows Command Prompt. Execute the <code>XtractUniversalSetup.exe</code> via command line and use the switch <code>--unattended</code>. </p> <pre><code>start /wait XtractUniversalSetup.exe --unattended\n</code></pre> <p><code>XtractUniversalSetup.exe</code> is Windows applications, meaning the Windows Command Prompt does not wait until the installation is complete.  To wait until the installation is complete, use the start command with the <code>/wait</code> switch. </p> <p>Note</p> <p>All switches are case sensitive.    </p>"},{"location":"documentation/setup/installation/#version-number","title":"Version Number","text":"<p>When installing Xtract Universal, the version of the product is displayed in the installation menu. To check the version of the installed product in Windows, open the Windows Settings and navigate to Apps &gt; Installed apps.  </p>"},{"location":"documentation/setup/installation/#installation-of-the-standalone-designer","title":"Installation of the Standalone Designer","text":"<p>My Theobald Software - Portal for Customers and Partners offers a download option for a standalone version of the Xtract Universal Designer. The standalone Designer can be used to connect to a central Xtract Universal repository without any software installation.</p>"},{"location":"documentation/setup/installation/#prerequisites_1","title":"Prerequisites","text":"<p>The Xtract Universal Server has to be installed on a central server instance e.g., a company-wide application server. As a result, access to a common Xtract Universal repository, e.g., <code>C:\\Program Files\\XtractUniversal\\config</code> is possible and can be used by several Xtract Universal users.</p>"},{"location":"documentation/setup/installation/#how-to-use-the-standalone-designer","title":"How to Use the Standalone-Designer","text":"<p>After the standalone Designer is downloaded, unpack the files from the <code>.zip</code> folder to any directory. The folder contains two executable files:</p> <ul> <li><code>XtractDesigner.exe</code> starts the Designer</li> <li><code>xu.exe</code> is the command line tool used for executing extractions, see Call via Commandline</li> </ul> <p>Use the <code>XtractDesigner.exe</code> file to start the Designer.  Before you connect to a central Xtract Universal Server, make sure you have access rights to the server and repository, see Access Management.</p> <p>Note</p> <p>When updating the software, you have to manually download the latest version of the standalone Designer from My Theobald Software - Portal for Customers and Partners and replace the old files.</p> <p>Note</p> <p>The standalone Designer might be classified as 'dangerous' by antivirus software. Make sure the Designer is not blocked by antivirus software.</p>"},{"location":"documentation/setup/installation/#related-links","title":"Related Links","text":"<ul> <li>Start the Designer</li> <li>Server Settings</li> <li>Access Management</li> </ul>"},{"location":"documentation/setup/license/","title":"Licensing","text":""},{"location":"documentation/setup/license/#about-the-licensing-concept-of-xtract-universal","title":"About the Licensing Concept of Xtract Universal","text":"<p>Xtract Universal is licensed per Windows server. The license is bound to your company and a specific server name. If you run the Xtract Universal Designer and the Xtract Universal Server on different machines, it is only necessary to replace the license on the server.</p> <p>A trial license is automatically installed with the installation of Xtract Universal. A regular license is provided in the customer portal - My Theobald Software after purchasing the product. </p> <p></p> <p>The license defines the following properties:</p> <ul> <li>The name of the server that runs the Xtract Universal Server.</li> <li>The destinations to which you can extract data to.</li> <li>The number of extractions you can define.</li> </ul> <p>These properties are checked when the XtractUniversal Server runs an extraction.</p> <p>Note</p> <p>The number of defined extractions and other license information is displayed in the status bar at the bottom of the Designer.</p> <p>Recommendation</p> <p>According to our experience, medium-sized businesses use less than 100 extractions.</p>"},{"location":"documentation/setup/license/#install-the-xtract-universal-license","title":"Install the Xtract Universal License","text":"<p>Place the \"XtractUniversalLicense.json\" file that is provided in the Customer Portal - My Theobald Software into the installation directory on the server, e.g., <code>C:\\Program Files\\XtractUniversal</code>.</p> <p></p> <p>Tip</p> <p>To inspect your current license data, open the Xtract Universal Designer and navigate to [Help] &gt; [Info] in the main menu bar. Alternatively, press [F12].</p>"},{"location":"documentation/setup/license/#move-a-license-to-a-new-server","title":"Move a License to a new Server","text":"<p>When moving your setup to a new server, a new license file must be issued for that server. Contact our sales team at sales@theobald-software.com to let us know the name of the new server.</p> <p>For more information on the migration process, see Migration to a Different Machine.</p>"},{"location":"documentation/setup/license/#maintenance","title":"Maintenance","text":"<p>Contact the sales department for information about available maintenance options. In case of technical difficulties, contact our support.</p>"},{"location":"documentation/setup/license/#related-links","title":"Related Links","text":"<ul> <li>My Theobald Software - Portal for Customers and Partners</li> </ul>"},{"location":"documentation/setup/migration/","title":"Migration","text":"<p>This page shows how to migrate the Xtract Universal configuration from one machine to another.</p>"},{"location":"documentation/setup/migration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create a backup of the <code>config</code> folder that is located in the installation directory of the current machine, e.g., <code>C:\\Program Files\\XtractUniversal\\config</code>.</li> <li>Make sure that the same version of Xtract Universal is installed on both machines.</li> </ul> <p>Tip</p> <p>To install a matching version of Xtract Universal on the new machine, use the <code>XtractUniversalSetup.exe</code> from the installation directory of the current machine, e.g., in <code>C:\\Program Files\\XtractUniversal</code>.</p>"},{"location":"documentation/setup/migration/#migration-to-a-different-machine","title":"Migration to a Different Machine","text":"<p>The Xtract Universal configuration can be migrated entirely (full migration) or partially.</p> <ul> <li>Conduct a full migration when replacing a machine completely. For a full migration, restore the entire <code>config</code> folder from your backup on the new machine (full migration).</li> <li>Conduct a partial migration when moving parts of your setup to a different machine, e.g., if you transport defined extractions types from a test environment to a production environment. For a partial migration, restore the parts of the backup you want to migrate by copying only the relevant configuration files, e.g., the <code>sources</code> or <code>extractions</code> folder, on the new machine.</li> </ul> <p>Restart the Xtract Universal Service if you copy the <code>server</code> folder to the new machine.</p> <p>Note</p> <p>When migrating to a new server, a new license file must be issued for that server. Contact our sales team at sales@theobald-software.com for more information.</p>"},{"location":"documentation/setup/migration/#configuration-files","title":"Configuration Files","text":"<p>All configuration files of defined extractions types, sources and the server are stored in the sub-directory <code>config</code> of the Xtract Universal installation directory, e.g., <code>C:\\Program Files\\XtractUniversal\\config</code>. Use the <code>config</code> directory to set up a version control system or to create manual backups.</p> Filename Description <pre>extractions</pre> Directory containing the defined extractions types. Each sub-directory contains an extraction of the same name. <pre>servers</pre> Directory containing the server settings. <pre>sources</pre> Directory containing SAP connections. Each sub-directory contains a source of the same name. <pre>version.txt</pre> Text file containing the product version of the current configuration, see Installation. <pre>destinations</pre> Directory containing the destinations. Each .json file contains a destination of the same name."},{"location":"documentation/setup/migration/#related-links","title":"Related Links","text":"<ul> <li>Installation </li> <li>Changelog</li> </ul>"},{"location":"documentation/setup/requirements/","title":"Requirements","text":"<p>This page contains information about system requirements and compatible SAP systems for Xtract Universal.</p>"},{"location":"documentation/setup/requirements/#supported-sap-systems-and-releases","title":"Supported SAP Systems and Releases","text":"<p>The following SAP Systems are supported:</p> <ul> <li>All SAP ABAP based systems that provide RFC connectivity are supported (all communication with SAP is performed via the RFC protocol). </li> <li>SAP ABAP Systems on any database are supported (including HANA). The database used by the SAP system is irrelevant, because the integration occurs at SAP application server level.</li> <li>SAP Systems running on Big Endian and Little Endian hardware are supported.</li> <li>SAP industry solutions like IS-U, IS-R, etc. are supported.</li> <li>SAP Releases 4.6C and newer are supported. </li> <li>All operating systems are supported.</li> </ul> <p>Examples</p> <ul> <li>SAP S/4HANA</li> <li>mySAP</li> <li>SAP Application Server ABAP</li> <li>Message Server</li> <li>Router</li> <li>Standalone Gateway</li> <li>SAP Business Suite 7 (CRM, SRM, SCM etc.)</li> <li>SAP Business All in One, CAR, APO, PI</li> <li>SAP BW 3.1 and later</li> <li>SAP BW/BI 7.x</li> <li>SAP ERP / ECC 5.0 / ECC 6.0 (including all EhPs)</li> <li>SAP R/3 Version 4.6C and later</li> <li>SAP BW/4HANA </li> <li>...and more.</li> </ul> <p>Disclaimer</p> <p>While SAP versions that are no longer supported by SAP still work with Theobald Software products, they are excluded from the Theobald Software support service.</p>"},{"location":"documentation/setup/requirements/#not-supported-sap-systems","title":"Not Supported SAP Systems","text":"<p>The following SAP Systems are not supported:</p> <ul> <li>SAP systems that don\u2019t run on ABAP systems </li> <li>SAP systems that don't provide RFC connectivity. </li> </ul> <p>Examples</p> <ul> <li>Business By Design</li> <li>Business One</li> <li>Business Objects</li> <li>Sybase</li> <li>Ariba</li> <li>Success Factors</li> <li>Concur</li> </ul>"},{"location":"documentation/setup/requirements/#hana-database","title":"HANA Database","text":"<p>You can use Operational Data Provisioning (ODP) to connect the SAP HANA database of an SAP ABAP source system.  Communication is done via RFC.  With the ODP context for SAP HANA (HANA) the following HANA View types are available for extracting:</p> <ul> <li>Analysis Views</li> <li>Calculation Views</li> <li>Associated Attribute Views</li> </ul> <p>Direct access to a HANA database without an SAP ABAP source system running on the corresponding HANA database is not supported.</p>"},{"location":"documentation/setup/requirements/#ports","title":"Ports","text":"<p>The following ports between the Windows server that runs Xtract Universal and the SAP server, must be open:</p> SAP NetWeaver Component Port (NN = System number of the SAP system) SAP Application Server 33\\ SAP Message Server 36\\ Secure Network Communication (SNC) 48\\&lt;NN&gt; SAP-Router 3299 <p>For more information, see SAP Help: TCP/IP Ports of All SAP Products.</p>"},{"location":"documentation/setup/requirements/#installation-and-configuration-on-sap","title":"Installation and Configuration on SAP","text":"Extraction Type SAP Release Requirements on the SAP System Table Rel. &gt; 4.6C Installation of a custom function module /THEO/READ_TABLE is recommended. BAPI Rel. &gt; 4.6C No requirements. Only remote-enabled functions are supported. Query Rel. &gt; 4.6C No requirements. Report Rel. &gt; 4.6C Installation of a custom function module Z_XTRACT_IS_REMOTE_REPORT is required. BWCube Rel. &gt; BW 3.1 No requirements. BEx Queries require external access (\"Allow External Access to this Query\" option must be active). BW Hierarchy Rel. &gt; BW 3.1 No requirements. ODP SAP_BASIS &gt;= 730, BW &gt;= 7,3X No requirements. DeltaQ Rel. &gt; 4.6C Customization required, see Customization for DeltaQ. OHS Rel. &gt; BW 3.5 Customization required, see Customization for OHS in BW. Table CDC SAP ECC 5.0 or above Installation of a custom function modules is necessary, see Table CDC Requirements. <p>For Information about the installation of the custom function modules and the SAP customization, check the section SAP Customization.</p>"},{"location":"documentation/setup/requirements/#sap-licenses","title":"SAP Licenses","text":"<p>Additional SAP licenses might be required for extracting data from SAP. Contact SAP to verify these requirements.</p>"},{"location":"documentation/setup/requirements/#operating-systems","title":"Operating Systems","text":"<ul> <li>Windows 11</li> <li>Windows Server 2022 (until 2031-10-14)</li> <li>Windows 10  (until 2025-10-14)</li> <li>Windows Server 2022</li> <li>Windows Server 2019 (until 2029-01-09)</li> <li>Windows Server 2016 (until 2027-01-12)</li> </ul>"},{"location":"documentation/setup/requirements/#other-applications-and-frameworks","title":"Other Applications and Frameworks","text":"<ul> <li>.NET Framework 4.7.2 or higher, see Download .NET Framework from the Microsoft website.</li> <li>To use TLS encryption via HTTPS, .NET Framework 4.8 or higher is required.</li> </ul>"},{"location":"documentation/setup/requirements/#hardware","title":"Hardware","text":"<p>The following requirements apply to the hardware:</p> Hardware Minimum Requirement Processor Cores 2 Cores, 1 additional core is required for each additional parallel extraction Processor Speed 1.4 GHz, recommended: 2.0 GHz or faster Main Memory 8 GB, recommended: 12 GB for up to two parallel extractions  4 GB dedicated memory in average for each additional parallel extraction. Check the exact storage requirements of an extraction in your scenario. Disk Space min. 150 MB total for Installation (when using specific destinations additional disc space may be required) 64-Bit Environment 64-Bit operating systems only Display Resolution 1920x1080x1.25 with 1.25 display scaling"},{"location":"documentation/setup/requirements/#destinations","title":"Destinations","text":"<p>Depending on the destination, an appropriate driver or library can be required.  For more information, refer to the requirements section of each destination.</p> <p>Additional disk space may be required when using specific destinations (e.g. Alteryx, Power BI, Tableau, Qlik, KNIME).</p>"},{"location":"documentation/setup/update/","title":"Backup & Update","text":"<p>This page shows how to backup and update an already installed Xtract Universal version.</p>"},{"location":"documentation/setup/update/#preparations","title":"Preparations","text":"<p>Recommendations:</p> <ul> <li>Use a working test environment with a valid license and maintenance. The test environment is a copy of the current production environment. </li> <li>Update the software regularly, see recommended update interval.</li> <li>Create a backup of the current installation.</li> </ul> <p>Before updating, check the Changelog for breaking changes: </p> <ul> <li>Make any necessary preparations and changes as described in the Release Notes.</li> <li>Note the current product version so that you can switch back to the same version in the event of an error.</li> </ul> <p>Tip</p> <p>Subscribe to our technical newsletter that informs you about new software releases.  </p>"},{"location":"documentation/setup/update/#create-a-backup","title":"Create a Backup","text":"<p>On the machine that runs the Xtract Universal service, copy the following files and folders and store them in a secure location:</p> <ul> <li>copy the complete <code>config</code> folder in the Xtract Universal installation directory, e.g., <code>C:\\Program Files\\XtractUniversal\\config</code>. </li> <li>optionally, copy the complete <code>logs</code> directory to create a backup of the log files. </li> <li>copy the <code>XtractUniversalLicense.json</code> file to backup your license.</li> </ul> <p>Warning</p> <p>Warning! Data security. Both <code>config</code> and <code>logs</code> directories may contain sensitive information.  Make sure to secure any sensitive data.</p> <p>It is recommended to create backups at regular intervals or use a versioning tool to switch back to the previous version, if necessary.</p> <p>Recommendation</p> <p>A version control system helps manage the data transfer from test environment to production environment by keeping track of all modifications. Git or Azure DevOps are version control systems that can be used to ensure the following:</p> <ul> <li>The deployment of new extractions, sources and destinations can not cause fundamental damage to the data load of the productive landscape.</li> <li>User rights and other features allow for changes to be checked and corrected in advance.</li> <li>Quick and easy rollbacks of changes.</li> </ul> <p>For more information on how to set up version control, refer to the knowledge base article Deploy Extractions Using Git Version Control.</p>"},{"location":"documentation/setup/update/#restore-a-backup","title":"Restore a Backup","text":"<ol> <li>On the machine that runs the Xtract Universal service, remove the <code>config</code> folder from the installation directory of Xtract Universal, e.g., <code>C:\\Program Files\\XtractUniversal</code>.</li> <li>Copy the backup of the <code>config</code> folder into the installation directory.</li> <li>To restore the license, replace the <code>XtractUniversalLicense.json</code> file in the installation directory with the backup.</li> </ol> <p>Warning</p> <p>Warning! Data loss. Restoring the backup of the <code>config</code> folder replaces the existing configuration.  Make sure to remove the existing files before restoring the backups.</p>"},{"location":"documentation/setup/update/#update-a-test-environment","title":"Update a Test Environment","text":"<p>A newer version can be installed over the old version. It is not necessary to uninstall the previous version.  New versions are available in the Theobald Software Customer Portal.</p> <p>During the installation, the setup program checks whether other processes are running in the background that are related to the software being installed.  If this is the case, a message with various options is displayed: </p> Option Description Check Again Close the affected programs and click [Check Again] to continue the installation. Kill Process Kill the affected process and continue with the installation. Ignore Ignore the message, but the installation may be canceled due to the locking of certain files. Quit Install Cancel the installation and resume it at another time."},{"location":"documentation/setup/update/#upgrade-major-releases-configconverter","title":"Upgrade Major Releases - ConfigConverter","text":"<p>When upgrading from one major product version to another, it can be necessary to convert components of your extractions. For this purpose, Xtract Universal provides the conversion tool ConfigConverter. The ConfigConverter ensures that all defined extractions types, source systems, server and user settings from the previous version are available in the new version. </p> <p>There are two ways to use the ConfigConverter:</p> Upgrade During InstallationManual Upgrade <ol> <li>Execute the Xtract Universal setup.</li> <li>During the setup, activate the option Convert config files.  The option Convert config files starts the ConfigConverter application. </li> <li>Confirm the conversion process in the command line pop-up window. </li> </ol> <p>If the ConfigConverter is not executed automatically during installation, the converter can also be started manually. Run the ConfigConverter.exe in the Xtract Universal installation directory, e.g., <code>C:\\Program Files\\XtractUniversal</code>.  </p> <p>Note</p> <p>Any issues that may occur during the conversion process are displayed in the command line window. Copy and send the command line context to the support team, if required.</p>"},{"location":"documentation/setup/update/#test-the-update","title":"Test the Update","text":"<p>Install the software update on the test environment and test the new version carefully. </p> <ol> <li>Test all your existing extractions.  </li> <li>After successful testing, install the current version on the production environment.    </li> <li>In the case of an error, create a ticket in the Support Portal.   Register if you do not have a customer account yet. Use your backup until a solution is provided.</li> </ol>"},{"location":"documentation/setup/update/#update-a-production-environment","title":"Update a Production Environment","text":"<p>After successfully testing the update on the test environment, install the update on the production environment.  Make sure to create a backup beforehand and conduct all the necessary preparations and changes. </p> <p>Warning</p> <p>Warning! Critical errors. Support cannot be provided. The versions of the test environment and of the production environment must be identical. Different versions can cause critical errors. No support can be provided, if the versions are not identical.  Make sure to keep the versions of the test environment and production environment identical - upgrade or downgrade, if needed.</p>"},{"location":"documentation/setup/update/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base: Deploy Extractions Using Git Version Control </li> <li>Customer Portal</li> <li>Changelog</li> <li>Support Portal</li> </ul>"},{"location":"documentation/setup-in-sap/","title":"SAP Customization","text":"<p>This section covers user rights and customization topics for the SAP Basis. For information on supported SAP systems and other IT topics, see Setup.</p>"},{"location":"documentation/setup-in-sap/#custom-function-modules","title":"Custom Function Modules","text":"<p>The following extractions types require the installation of a custom function module:</p> Extraction Type Custom Function Module Installation Report Z_XTRACT_IS_REMOTE_REPORT Required Table /THEO/READ_TABLE Recommended Table CDC /THEO/READ_TABLE and THEO_CDC Required <p>Theobald Software distributes custom function modules as part of Xtract Universal.  The custom function modules can be installed in SAP using transport requests.  The transport requests are available in the installation directory of Xtract Universal, e.g., <code>C:\\Program Files\\XtractUniversal\\ABAP\\</code>.</p> <p>For information on how to use transport request, see Import an SAP Transport Request. </p> <p>Note</p> <p>All objects that come with any of the transport requests, can be deleted by importing the <code>Z_THEO_READ_TABLE-deletion_request.zip</code> transport request.</p>"},{"location":"documentation/setup-in-sap/#customizations-in-sap","title":"Customizations in SAP","text":"<p>The following extractions types require a customization in the SAP system:</p> <ul> <li>DeltaQ, see Customization for DeltaQ.</li> <li>OHS, see Customization for OHS in BW.</li> </ul>"},{"location":"documentation/setup-in-sap/#popular-topics","title":"Popular Topics","text":"<ul> <li>  SAP User Rights</li> <li>  Download SAP Roles</li> <li>  Function Module for Table Extractions</li> <li>  Function Module for TableCDC Extractions</li> <li>  Function Module for Report Extractions</li> <li>  Customization for DeltaQ</li> <li>  Customization for OHS</li> </ul>"},{"location":"documentation/setup-in-sap/custom-function-module-for-reports/","title":"Function Module for Reports","text":"<p>The extraction of reports requires the installation of a custom function module in your SAP system. If you cannot install the function module, turn to your SAP Basis team for help.</p> <p>Note</p> <p>As of version 1.2 of the custom function module <code>Z_XTRACT_IS_REMOTE_REPORT</code> access to reports must be explicitly granted, see Authority Objects for Z_XTRACT_IS_REMOTE_REPORT.</p>"},{"location":"documentation/setup-in-sap/custom-function-module-for-reports/#installation-of-z_xtract_is_remote_report","title":"Installation of Z_XTRACT_IS_REMOTE_REPORT","text":"<p>Install the function module using the transport request Z_XTRACT_IS_REMOTE_REPORT-transport.zip.   The transport request is located in the following installation directory:  <code>C:\\Program Files\\[XtractProduct]\\ABAP\\Report\\Z_XTRACT_IS_REMOTE_REPORT-transport.zip</code>.</p> <p>The transport request needs to be imported into SAP by your SAP Basis team.</p>"},{"location":"documentation/setup-in-sap/custom-function-module-for-reports/#authority-objects-for-z_xtract_is_remote_report","title":"Authority Objects for Z_XTRACT_IS_REMOTE_REPORT","text":"<p>As of Z_XTRACT_IS_REMOTE_REPORT version 1.2 access to reports must be explicitly granted. There are 2 ways to verify that the SAP user is allowed to extract a report:</p> <ul> <li>Use authentication groups, see Authorizing Access to Reports via Authorization Groups.</li> <li>Use the custom authorization object Z_TS_PROG. </li> </ul>"},{"location":"documentation/setup-in-sap/custom-function-module-for-reports/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base: Import an SAP Transport Request</li> <li>Knowledge Base: Authorizing Access to Specific Reports</li> <li>SAP Help: Create Function Group</li> </ul>"},{"location":"documentation/setup-in-sap/custom-function-module-for-table-extraction/","title":"Function Module for Tables","text":"<p>SAP customization for the Table extraction type is optional.  The installation of the custom function module <code>/THEO/READ_TABLE</code> is recommended to improve performance and to bypass restrictions of the SAP standard function module RFC_READ_TABLE.</p>","tags":["Z_THEO_READ_TABLE"]},{"location":"documentation/setup-in-sap/custom-function-module-for-table-extraction/#rfc_read_table-restrictions","title":"RFC_READ_TABLE Restrictions","text":"<p>Especially with older SAP releases you may encounter a few restrictions when using the SAP standard function module (RFC_READ_TABLE) for table extraction:</p> <ul> <li>The overall width of all columns to be extracted must not exceed 512 bytes.</li> <li>It is not possible to extract data from tables that contain one or more columns of the data type f (FLTP, floating point), DEC (decimal, e.g. for percentage) or x (RAW, LRAW).</li> <li>Poor extraction performance with larger tables.  Can cause also duplicates.</li> <li>Depending on the SAP version there may be other restrictions. </li> </ul> <p>When facing restrictions, install the Theobald Software custom function module /THEO/READ_TABLE on your SAP system. </p> <p>Warning</p> <p>Converting issues Error while converting value '*.0' of row 1530, column 3. The SAP standard module RFC_READ_TABLE for table extraction can only extract the ABAP data type DEC to a limited extent. This leads to the mentioned example error during extraction. Use the function module /THEO/READ_TABLE.</p>","tags":["Z_THEO_READ_TABLE"]},{"location":"documentation/setup-in-sap/custom-function-module-for-table-extraction/#installation-of-theoread_table","title":"Installation of /THEO/READ_TABLE","text":"<p>An SAP transport request for the installation of the function module is provided in the installation directory:  <code>C:\\Program Files\\XtractUniversal\\ABAP\\</code>. Transport requests are imported into SAP by your SAP Basis team.</p> <p>Note</p> <p>Take a look at the README.pdf in the installation directory (e.g.,<code>C:\\Program Files\\XtractUniversal\\ABAP\\README.pdf</code>) before installing any custom function modules.</p> <p>It is recommended to install the latest custom function module THEO/READ_TABLE:</p> Transport Request Compatible SAP Systems <code>THEO_READ_TABLE_740SP05.zip</code> ABAP version 7.40 SP05 and higher <code>THEO_READ_TABLE_710.zip</code> ABAP version 7.10 to 7.40 SP04 <code>THEO_READ_TABLE_640.zip</code> ABAP versions from 6.40 until 7.03 <code>THEO_READ_TABLE_46C.zip</code> ABAP versions from 4.6C <p>When importing the transport requests on older SAP releases a syntax error may occur. Contact Theobald Support and send the dedicated error message text.</p> <p>Warning</p> <p>Generating Short Dumps. Testing the function modules on an SAP system is not possible. Function modules /THEO/READ_TABLE and Z_THEO_READ_TABLE can only be called by Theobald Software products due to the callback function of the module. Avoid calling function modules /THEO/READ_TABLE and Z_THEO_READ_TABLE directly from an SAP system.</p>","tags":["Z_THEO_READ_TABLE"]},{"location":"documentation/setup-in-sap/custom-function-module-for-table-extraction/#supported-features","title":"Supported Features","text":"Supported Features by THEO_READ_TABLE _740SP05 _710 _640 46C WHERE Clause HAVING Clause INNER JOIN LEFT OUTER JOIN Conversion exits Aggregate functions SQL expressions (subqueries) Background jobs","tags":["Z_THEO_READ_TABLE"]},{"location":"documentation/setup-in-sap/custom-function-module-for-table-extraction/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base: Import an SAP Transport Request</li> <li>Table Extraction Type</li> </ul>","tags":["Z_THEO_READ_TABLE"]},{"location":"documentation/setup-in-sap/custom-function-module-for-tablecdc/","title":"Function Module for Table CDC","text":"<p>The Table CDC extraction type requires the installation of the custom function modules /THEO/CDC and /THEO/READ_TABLE in your SAP system. If you cannot install the function modules, turn to your SAP Basis team for help.</p>"},{"location":"documentation/setup-in-sap/custom-function-module-for-tablecdc/#installation-of-theo_cdc_ecc-or-theo_cdc_s4","title":"Installation of THEO_CDC_ECC or THEO_CDC_S4","text":"<p>All transport requests for the custom function groups are provided in the installation directory of Xtract Universal, e.g., <code>C:\\Program Files\\XtractUniversal</code>. Make sure to install the correct transport request for your SAP system:</p> SAP System Function Group Directory SAP ECC Systems THEO_CDC_ECC <pre>C:\\Program Files\\XtractUniversal\\ABAP\\TableCDC\\THEO_CDC_ECC.zip</pre> SAP S/4 Systems with SAP_BASIS Version &lt; 7.55 THEO_CDC_S4 <pre>C:\\Program Files\\XtractUniversal\\ABAP\\TableCDC\\THEO_CDC_S4.zip</pre> SAP S/4 Systems with SAP_BASIS Version \u2265 7.55 THEO_CDC_S4_755 <pre>C:\\Program Files\\XtractUniversal\\ABAP\\TableCDC\\THEO_CDC_S4_755.zip</pre> <p>The transport request needs to be imported into SAP by your SAP Basis team. The function groups /THEO/CDC_ECC and /THEO/CDC_S4 both contain the following function modules:</p> Function Modules Description /THEO/CLEAR_LOGTAB Clear entries of log tab up to a given sequence number /THEO/COUNT_LOGTAB_ENTRIES Count log table entries /THEO/CREATE_LOG_TABLE Function module for creating log tables /THEO/CREATE_TRIGGERS Function module for creating DB triggers for CDC /THEO/DELETE_LOG_TABLE Function module for deleting log tables /THEO/DELETE_TRIGGERS Function module for deleting DB triggers for CDC /THEO/GET_DB Get database system identifier /THEO/GET_INFO Get package information /THEO/GET_TRIGGERS Function module to retrieve triggers <p>Note</p> <p>Take a look at the README.pdf in the installation directory (e.g.,<code>C:\\Program Files\\XtractUniversal\\ABAP\\README.pdf</code>) before installing any custom function modules.</p>"},{"location":"documentation/setup-in-sap/custom-function-module-for-tablecdc/#installation-of-theoread_table","title":"Installation of /THEO/READ_TABLE","text":"<p>Install the custom function module /THEO/READ_TABLE, see Function Module for Tables.</p> <p>Note</p> <p>Take a look at the README.pdf in the installation directory (e.g.,<code>C:\\Program Files\\XtractUniversal\\ABAP\\README.pdf</code>) before installing any custom function modules.</p>"},{"location":"documentation/setup-in-sap/custom-function-module-for-tablecdc/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base: Import an SAP Transport Request</li> <li>Knowledge Base: Delta Mechanism of Table CDC</li> <li>Table CDC - Prerequisites</li> </ul>"},{"location":"documentation/setup-in-sap/customization-for-deltaq/","title":"Customization for DeltaQ","text":"<p>Before using the DeltaQ extraction type, an RFC destination has to be created in the SAP system.  This page shows how to create and configure the RFC destination.</p> <p>Follow the steps on this page in the following order:</p> <ol> <li>Create an RFC Destination of Type R/3</li> <li>Execute the Function Module RSAP_BIW_CONNECT_40</li> <li>Delete the RFC Destination </li> <li>Create an RFC Destination of Type T</li> <li>Execute the Function Module RSAS_RBWBCRL_STORE</li> <li>Register the RFC Server</li> <li>qRFC Monitor (QOUT Scheduler)</li> </ol>"},{"location":"documentation/setup-in-sap/customization-for-deltaq/#create-an-rfc-destination-of-type-r3","title":"Create an RFC Destination of Type R/3","text":"<ol> <li>Go to SAP transaction SM59 to create an RFC destination of type R/3.</li> <li>Enter a name in the field Program ID, e.g., XTRACT01. The name of the Program ID is needed again for later configuration.</li> </ol>"},{"location":"documentation/setup-in-sap/customization-for-deltaq/#execute-the-function-module-rsap_biw_connect_40","title":"Execute the Function Module RSAP_BIW_CONNECT_40","text":"<p>Note</p> <p>Executing the Function Module RSAP_BIW_CONNECT_40 can be performed on modifiable that the SAP systems.</p> <p>The function module RSAP_BIW_CONNECT_40 creates a connection to a Business Information Warehouse. Go to SAP transaction SE37 and execute the function module RSAP_BIW_CONNECT_40 with the following import parameters: </p> Import Parameter Example Value Comment I_LANGU EN I_SLOGSYS T90CLNT090 Logical name of the source system. If you do not know this, look in table T000 for the respective client (LOGSYS field). I_LOGSYS XTRACT01 I_STEXT Xtract Destination I_BASIC_IDOC ZXTIDOC Unique name of the RFC destinations. I_TSPREFIX XT Unique name of the RFC destinations. I_SAPRL 700 Automatically set by the SAP system. I_RESTORE X"},{"location":"documentation/setup-in-sap/customization-for-deltaq/#delete-the-rfc-destination","title":"Delete the RFC Destination","text":"<p>Go to SAP transaction SM59 and delete the RFC destination of type R/3 via Detailed View &gt; Menu &gt; Delete.</p>"},{"location":"documentation/setup-in-sap/customization-for-deltaq/#create-an-rfc-destination-of-type-t","title":"Create an RFC Destination of Type T","text":"<ol> <li>Go to SAP transaction SM59 to create an RFC destination of type T=TCP/IP that has the same name as the deleted RFC destination of type R/3.</li> <li>Select the activation type Registered Server Program. </li> <li> <p>Set the following parameters:</p> Tab Field Example Value Comment Technical Settings RFC Destination XTRACT01 Technical Settings Connection Type TCP/IP Connection Technical Settings Description 1 Xtract Destination Technical Settings Activation Type Registered Server Program Technical Settings Program ID XTRACT01 Technical Settings Gateway Host sap-erp-as05.example.com Name or IP address of the SAP system. Technical Settings Gateway service sapgw00 In the form sapgwnn, where nn is the SAP instance number (between 00 and 99). Special Options Serializer Classic Serializer Select the \"Classic serializer\". </li> </ol>"},{"location":"documentation/setup-in-sap/customization-for-deltaq/#execute-the-function-module-rsas_rbwbcrl_store","title":"Execute the Function Module RSAS_RBWBCRL_STORE","text":"<p>Go to SAP transaction SE37 and execute the function module RSAS_RBWBCRL_STORE to activate the target system.</p> Import Parameter Example Value Comment I_RBWBCRL 700 The current SAP system release number I_RLOGSYS XTRACT01"},{"location":"documentation/setup-in-sap/customization-for-deltaq/#register-the-rfc-server","title":"Register the RFC Server","text":"<p>Note</p> <p>Registering an RFC Server in SAP Releases only applies to SAP kernel version 720 and higher.</p> <p>Follow the instructions in the Knowledge Base Article: Register an RFC Server in SAP with Kernel Release 720 and higher.</p>"},{"location":"documentation/setup-in-sap/customization-for-deltaq/#qrfc-monitor-qout-scheduler","title":"qRFC Monitor (QOUT Scheduler)","text":"<ol> <li>Go to SAP transaction SMQS. </li> <li>Select the created RFC destination, e.g., XTRACT01. </li> <li>Click Register without activation and set the parameter Max.Verb. to 10.  Increase this value in case of parallel execution of several DeltaQ extractions types on the same RFC destination. </li> </ol> <p>Note</p> <p>For DeltaQ customizing errors, refer to the DeltaQ Troubleshooting Guide.</p>"},{"location":"documentation/setup-in-sap/customization-for-ohs-in-bw/","title":"Customization for OHS","text":"<p>Before using the OHS extraction type, an RFC destination has to be created in the SAP system.  This page shows how to set up the RFC destination and SAP process chain.</p> <p>Depending on the SAP release, OHS can be used as follows:</p> SAP Release SAP Object Details BI &lt; 7.0 InfoSpoke InfoSpokes and Process Chains (BI &lt; 7.0) BI &gt;= 7.0 OHS-Destination OHS Destinations and Data Transfer Processes (BI &gt;= 7.0)"},{"location":"documentation/setup-in-sap/customization-for-ohs-in-bw/#create-an-rfc-destination","title":"Create an RFC destination","text":"<ol> <li>Go to SAP transaction SM59 to create an RFC destination of type TCP/IP .</li> <li>Select the activation type Registered Server Program .</li> <li>Enter a name in the field Program ID , e.g., XTRACT01. The name of the Program ID is needed again for later configuration. </li> </ol>"},{"location":"documentation/setup-in-sap/customization-for-ohs-in-bw/#infospokes-and-process-chains-bi-70","title":"InfoSpokes and Process Chains (BI &lt; 7.0)","text":"<ol> <li>Go to SAP transaction RSA1 to open the Administrator Workbench. </li> <li>Navigate to Tools &gt; Open Hub Service &gt; Infospoke to create an InfoSpoke. </li> <li>Store data source in the InfoSpoke, e.g. an ODS object or a cube. </li> <li>Define an InfoSpoke for data extraction into an external system in the Destination tab. </li> <li>Specify the RFC destination created in advance. </li> <li>Fill in the columns to be transferred and, if necessary, a selection. </li> <li>Save and activate the InfoSpoke. </li> <li>Generate a process chain with transaction RSA1 in the menu Edit -&gt; Process Chains. </li> <li>Activate Start by API in the variant for the process chain. </li> <li>Insert the created InfoSpoke into the process chain. </li> <li>Save and activate the process chain.</li> </ol>"},{"location":"documentation/setup-in-sap/customization-for-ohs-in-bw/#ohs-destinations-and-data-transfer-processes-bi-70","title":"OHS Destinations and Data Transfer Processes (BI &gt;= 7.0)","text":"<ol> <li>Go to SAP transaction RSA1 to open the Administrator Workbench.</li> <li>Navigate to Open Hub Destination in the left tree and right-click on an InfoArea. Select Create Open Hub Destination in the context menu.  </li> <li>In the edit mode of the destination, set the Destination Type to Third Party Tool and select your RFC destination. </li> <li>Save and activate the OHS destination. </li> <li>Click on the newly created OHS destination in the middle tree of the InfoAreas and select Create Data Transfer Process to create a new data transfer process (DTP). </li> <li>Save and activate the DTP. If necessary, change the extraction type from Delta to Full before activating)  The arrangement of Destination, Transformations and DTP in the OHS tree is done afterwards.</li> <li>Create a process chain that contains the new DTP. </li> <li>Make sure that the planning option Start Using Meta Chain or API is selected in the process chain start variant.  </li> <li>Activate the process chain. </li> </ol>"},{"location":"documentation/setup-in-sap/customization-for-ohs-in-bw/#related-links","title":"Related Links","text":"<ul> <li>SAP Note 2437637</li> <li>SAP Note 1983356</li> </ul>"},{"location":"documentation/setup-in-sap/customizing-for-deltaq/","title":"Customization for DeltaQ","text":"<p>Before using the DeltaQ extraction type, an RFC destination has to be created in the SAP system.  This page shows how to create and configure the RFC destination.</p> <p>Follow the steps on this page in the following order:</p> <ol> <li>Create an RFC Destination of Type R/3</li> <li>Execute the Function Module RSAP_BIW_CONNECT_40</li> <li>Delete the RFC Destination </li> <li>Create an RFC Destination of Type T</li> <li>Execute the Function Module RSAS_RBWBCRL_STORE</li> <li>Register the RFC Server</li> <li>qRFC Monitor (QOUT Scheduler)</li> </ol>"},{"location":"documentation/setup-in-sap/customizing-for-deltaq/#create-an-rfc-destination-of-type-r3","title":"Create an RFC Destination of Type R/3","text":"<ol> <li>Go to SAP transaction SM59 to create an RFC destination of type R/3.</li> <li>Enter a name in the field Program ID, e.g., XTRACT01. The name of the Program ID is needed again for later configuration.</li> </ol>"},{"location":"documentation/setup-in-sap/customizing-for-deltaq/#execute-the-function-module-rsap_biw_connect_40","title":"Execute the Function Module RSAP_BIW_CONNECT_40","text":"<p>Note</p> <p>Executing the Function Module RSAP_BIW_CONNECT_40 can be performed on modifiable that the SAP systems.</p> <p>The function module RSAP_BIW_CONNECT_40 creates a connection to a Business Information Warehouse. Go to SAP transaction SE37 and execute the function module RSAP_BIW_CONNECT_40 with the following import parameters: </p> Import Parameter Example Value Comment I_LANGU EN I_SLOGSYS T90CLNT090 Logical name of the source system. If you do not know this, look in table T000 for the respective client (LOGSYS field). I_LOGSYS XTRACT01 I_STEXT Xtract Destination I_BASIC_IDOC ZXTIDOC Unique name of the RFC destinations. I_TSPREFIX XT Unique name of the RFC destinations. I_SAPRL 700 Automatically set by the SAP system. I_RESTORE X"},{"location":"documentation/setup-in-sap/customizing-for-deltaq/#delete-the-rfc-destination","title":"Delete the RFC Destination","text":"<p>Go to SAP transaction SM59 and delete the RFC destination of type R/3 via Detailed View &gt; Menu &gt; Delete.</p>"},{"location":"documentation/setup-in-sap/customizing-for-deltaq/#create-an-rfc-destination-of-type-t","title":"Create an RFC Destination of Type T","text":"<ol> <li>Go to SAP transaction SM59 to create an RFC destination of type T=TCP/IP that has the same name as the deleted RFC destination of type R/3.</li> <li>Select the activation type Registered Server Program. </li> <li> <p>Set the following parameters:</p> Tab Field Example Value Comment Technical Settings RFC Destination XTRACT01 Technical Settings Connection Type TCP/IP Connection Technical Settings Description 1 Xtract Destination Technical Settings Activation Type Registered Server Program Technical Settings Program ID XTRACT01 Technical Settings Gateway Host sap-erp-as05.example.com Name or IP address of the SAP system. Technical Settings Gateway service sapgw00 In the form sapgwnn, where nn is the SAP instance number (between 00 and 99). Special Options Serializer Classic Serializer Select the \"Classic serializer\". </li> </ol>"},{"location":"documentation/setup-in-sap/customizing-for-deltaq/#execute-the-function-module-rsas_rbwbcrl_store","title":"Execute the Function Module RSAS_RBWBCRL_STORE","text":"<p>Go to SAP transaction SE37 and execute the function module RSAS_RBWBCRL_STORE to activate the target system.</p> Import Parameter Example Value Comment I_RBWBCRL 700 The current SAP system release number I_RLOGSYS XTRACT01"},{"location":"documentation/setup-in-sap/customizing-for-deltaq/#register-the-rfc-server","title":"Register the RFC Server","text":"<p>Note</p> <p>Registering an RFC Server in SAP Releases only applies to SAP kernel version 720 and higher.</p> <p>Follow the instructions in the Knowledge Base Article: Register an RFC Server in SAP with Kernel Release 720 and higher.</p>"},{"location":"documentation/setup-in-sap/customizing-for-deltaq/#qrfc-monitor-qout-scheduler","title":"qRFC Monitor (QOUT Scheduler)","text":"<ol> <li>Go to SAP transaction SMQS. </li> <li>Select the created RFC destination, e.g., XTRACT01. </li> <li>Click Register without activation and set the parameter Max.Verb. to 10.  Increase this value in case of parallel execution of several DeltaQ extractions types on the same RFC destination. </li> </ol> <p>Note</p> <p>For DeltaQ customizing errors, refer to the DeltaQ Troubleshooting Guide.</p>"},{"location":"documentation/setup-in-sap/sap-authority-objects/","title":"SAP Authorization Objects","text":"<p>To use Xtract Universal you need an SAP connection user with sufficient authorization in SAP.  Authorizations are assigned via authorization objects in SAP.  Redirect this page to your SAP Basis administrators to get the relevant authorization objects for your SAP connection user.  </p> <p>The authorizations in the section General authorization objects are required to establish an SAP connection with the SAP application server.  The required authorizations for each extraction type are listed in their respective section.  </p>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#sap-authorization-profiles","title":"SAP Authorization Profiles","text":"<p>Theobald Software collected and combined the necessary authorizations for all extractions types into SAP roles.  You can download the SAP profiles and upload them to your SAP system:</p> Extraction Type SAP Role File General Authorization Objects ZXTGENERAL.SAP BAPI ZXTBAPI.SAP BW Cube ZXTQUERY.SAP BW Hierarchy ZXTBWHIERARCHY.SAP ODP (Operational Data Provisioning) ZXTODP.SAP OHS (Open Hub Services) ZXTOHS.SAP Query ZXTQUERY.SAP Report ZXREPORT.SAP Table ZXTABLE.SAP Table CDC ZXTABLECDC.SAP DeltaQ  ZXTDELTAQ.SAP,DELTAQ_CUSTOMIZING_CHECK <p>Note</p> <p>If you still get an authorization error, ask SAP Basis to record an ST01-/ or SU53-authorization trace in SAP. This trace shows which authorizations objects are missing.</p>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#general-authorization-objects","title":"General Authorization Objects","text":"<p>The following authorization objects are required to establish a connection to SAP. </p> Necessary SAP authorizations<pre><code>S_RFC            RFC_TYPE=FUGR; RFC_NAME=SYST; ACTVT=16\nS_RFC            RFC_TYPE=FUGR; RFC_NAME=SRFC; ACTVT=16\nS_RFC            RFC_TYPE=FUGR; RFC_NAME=RFC1; ACTVT=16\n</code></pre> <p> Download SAP profile for general authorization</p>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#bapi","title":"BAPI","text":"Necessary SAP authorizations<pre><code>S_RFC            ACTVT=16; RFC_TYPE=FUGR; RFC_NAME=DDIF_FIELDINFO_GET, SDIFRUNTIME     \n</code></pre> <p> Download SAP profile for BAPI Extractions</p>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#bw-cube-bw-query","title":"BW Cube / BW Query","text":"<p>Authorizations for the underlying Queries, Cubes, InfoAreas and analysis need to be assigned via: </p> <ul> <li><code>S_RS_COMP</code></li> <li><code>S_RS_COMP1</code></li> <li><code>S_RS_AUTH</code></li> </ul> Authorization objectsBICS modeDate conversion (optional) Necessary SAP authorizations<pre><code>S_RFC            RFC_TYPE=FUGR; RFC_NAME=RSOB; ACTVT=16\nS_RFC            RFC_TYPE=FUGR; RFC_NAME=RRX1; ACTVT=16\nS_TABU_NAM       ACTVT=03; TABLE=RSRREPDIR\nS_TABU_NAM       ACTVT=03; TABLE=RSZGLOBV\n</code></pre> Necessary SAP authorizations<pre><code>S_RFC            RFC_TYPE=FUGR;RFC_NAME=SYST;ACTVT=16;type=RF;name=RFCPING;\nS_RFC            RFC_TYPE=FUGR; RFC_NAME=RSOBJS_RFC_INTERFACE; ACTVT=16; type=RF;name=RSOBJS_GET_NODES;\nS_RFC            RFC_TYPE=FUGR;RFC_NAME=RSAO_CORE;ACTVT=16;type=RF;name=RSAO_BICS_SESSION_INITIALIZE\nS_RFC            RFC_TYPE=FUGR;RFC_NAME=RSBOLAP_BICS_CONSUMER;ACTVT=16;type=RF;name=BICS_CONS_CREATE\nS_RFC            RFC_TYPE=FUGR;RFC_NAME=RSBOLAP_BICS_PROVIDER;ACTVT=16;type=RF;name=BICS_PROV_OPEN;\nS_RFC            RFC_TYPE=FUGR;RFC_NAME=RSBOLAP_BICS_PROVIDER_VAR;ACTVT=16;type=RF;name=BICS_PROV_VA\nS_ADMI_FCD       S_ADMI_FCD=PADM;\n</code></pre> Necessary SAP authorizations<pre><code>S_TABU_NAM       ACTVT=03; TABLE=DD03L\n</code></pre> <p>Alternatively, you can assign the SAP role template  <code>S_RS_RREPU</code>. </p> <p> Download SAP profile for BW Cube / BW Query</p>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#bw-hierarchy","title":"BW Hierarchy","text":"Necessary SAP authorizations<pre><code>S_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=RSNDI_SHIE; ACTVT=16\nS_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=BAPI_IOBJ_GETDETAIL; ACTVT=16\nS_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=DDIF_FIELDINFO_GET; ACTVT=16\nS_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=RFC1; ACTVT=16\nS_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=RFC_GET_FUNCTION_INTERFACE; ACTVT=16\nS_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=RFC_READ_TABLE; ACTVT=16\nS_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=RSBAPI_IOBJ; ACTVT=16 \nS_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=RSNDI_SHIE; ACTVT=16\nS_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=SDIFRUNTIME; ACTVT=16\nS_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=SDTX; ACTVT=16\nS_RFC          RFC_TYPE=FUNC, FUGR; RFC_NAME=SYST; ACTVT=16\nS_RS_ADMWB     RSADMWBOBJ=INFOOBJECT; ACTVT=03\nS_TABU_DIS     ACTVT=02, 03; DICBERCLS=BWC\nS_TABU_DIS     ACTVT=02, 03; DICBERCLS=BWG\nS_TABU_NAM     ACTVT=02, 03; TABLE=/BIC/*\nS_TABU_NAM     ACTVT=02, 03; TABLE=ENLFDIR\nS_TABU_NAM     ACTVT=02, 03; TABLE=RSHIEDIR\n</code></pre> <p> Download SAP profile for BW Hierarchy</p>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#odp","title":"ODP","text":"<p>For a complete and detailed list of authorization objects refer to SAP Note 2855052 - Authorizations required for ODP Data Replication API 2.0. </p> Necessary SAP authorizations<pre><code>S_TABU_NAM       ACTVT=03; TABLE=TCURX\n</code></pre> <p> Download SAP profile for ODP</p>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#ohs","title":"OHS","text":"<p>Alternatively, you can assign the SAP role template  <code>S_BI-WHM_RFC</code>. </p> Necessary SAP authorizations<pre><code>S_RFC      RFC_TYPE=FUGR; RFC_NAME=RSB3RD; ACTVT=16\nS_RFC      RFC_TYPE=FUGR; RFC_NAME=SDTX; ACTVT=16\nS_RFC      RFC_TYPE=FUGR; RFC_NAME=BAPT; ACTVT=16\nS_RFC      RFC_TYPE=FUGR; RFC_NAME=BATG; ACTVT=16\nS_RFC      RFC_TYPE=FUGR; RFC_NAME=RSPC_API; ACTVT=16\nS_TABU_DIS RC=0 ACTVT=03; DICBERCLS=&amp;NC&amp; \nS_RS_PC    RSPCCHAIN=*;RSPCAPPLNM=*; RSPCPART=DEFINITION; ACTVT=03\nS_RS_PC    RSPCCHAIN=*;RSPCAPPLNM=*; RSPCPART=RUNTIME; ACTVT=16\nS_CTS_ADMI CTS_ADMFCT=TABL\nS_RS_DTP   RSTLDTPSRC=CUBE; RSSTDTPSRC=*; RSONDTPSRC=0D_DECU; RSTLDTPTGT=DEST; RSSTDTPTGT=*; ACTVT=16\nS_BTCH_ADM BTCADMIN=Y\nS_BTCH_JOB JOBGROUP=*; JOBACTION=RELE\nS_RS_TR    RSTLOGOSRC=CUBE; RSSTTRSRC=*; RSOBJNMSRC=0D_DECU; RSTLOGOTGT=DEST; RSSTTRTGT=' '; RSOBJNMTG=*; ACTVT=16\nS_RS_AUTH  BIAUTH=0BI_ALL\nS_ADMI_FCD S_ADMI_FCD=ST0R\n</code></pre> <p> Download SAP profile for OHS</p>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#query","title":"Query","text":"Necessary SAP authorizations<pre><code>S_RFC            RFC_TYPE=FUGR; RFC_NAME=AQRC; ACTVT=16 \n</code></pre> <p> Download SAP profile for SAP Query</p>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#report","title":"Report","text":"Necessary SAP authorizations<pre><code>S_RFC            RFC_TYPE=FUGR; RFC_NAME=ZXTRACTABAP; ACTVT=16\nS_TABU_NAM       ACTVT=03; TABLE=TRDIR, TRDIRT, TSTC, VARID\nS_GUI            ACTVT=61 \nS_TABU_DIS       ACTVT=03; DICBERCLS=&amp;NC&amp; \nS_TABU_DIS       ACTVT=03; DICBERCLS=SS\nS_BTCH_ADM       BTCADMIN=Y\nS_BTCH_JOB       JOBGROUP=*; JOBACTION=RELE\n</code></pre> <p>Note</p> <p>The necessary transport request for function group ZXTRACTABAP is located in the following path: <code>C:\\Program Files\\[XtractProduct]\\ABAP\\Report\\Z_XTRACT_IS_REMOTE_REPORT-transport.zip</code> of the default installation.</p> <p> Download SAP profile for Report</p> <p>To execute a report with Xtract Universal, the SAP connection user needs explicit authorization to execute the report. Authorization can be granted using one of the following methods:</p> <ul> <li>Assign the authorization object Z_TS_PROG </li> <li>Assign an authorization group</li> </ul>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#table","title":"Table","text":"Necessary SAP authorizations<pre><code>S_RFC            ACTVT=16; RFC_TYPE=FUGR; RFC_NAME=SDTX, SDIFRUNTIME, /THEO/READ_TABLE                   \nS_TABU_DIS       ACTVT=03; DICBERCLS=XXXX\nS_TABU_NAM       ACTVT=03; TABLE=DD02V, DD17S, DD27S, ENLFDIR\nS_DSAUTH         ACTVT=16;    \n</code></pre> <p>XXXX (stands for a placeholder) is the authorization group for the table. To determine, which authorization group belongs to which table, check the table TDDAT - Maintenance Areas for Tables.  If the table is not listed, the authorization group is &amp;NC&amp;. For authorizing specific tables use authorization object S_TABU_NAM instead of S_TABU_DIS.</p> <p>Note</p> <p>The transport request for function group /THEO/READ_TABLE and Z_THEO_READ_TABLE is located in the following path: <code>C:\\Program Files\\[XtractProduct]\\ABAP\\Table</code> of the default installation. </p> <p>Additional options:</p> Run Z_THEO_READ_TABLE in the backgroundCount Rows buttonAdjust currency decimals setting Necessary SAP authorizations<pre><code>S_BTCH_ADM       BTCADMIN=Y\nS_BTCH_JOB       JOBGROUP=*; JOBACTION=RELE\n</code></pre> Necessary SAP authorizations<pre><code>S_RFC            RFC_TYPE=FUNC; RFC_NAME=EM_GET_NUMBER_OF_ENTRIES; ACTVT=16  \n</code></pre> Necessary SAP authorizations<pre><code>S_TABU_NAM       ACTVT=03; TABLE=TCURX\n</code></pre> <p> Download SAP profile for Table</p>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#table-cdc","title":"Table CDC","text":"Necessary SAP authorizations<pre><code>S_RFC            ACTVT=16; RFC_TYPE=FUGR, FUNC; RFC_NAME=SDTX, SDIFRUNTIME, /THEO/CDC_*, /THEO/READ_TABLE            \nS_TABU_DIS       ACTVT=02, 03; DICBERCLS=*\nS_TABU_CLI       CLIIDMAINT=X \nS_TABU_NAM       ACTVT=03; TABLE=DD02V, D17S, D27S, ENLFDIR\nS_DEVELOP        ACTVT=02; DEVCLASS=$TMP; OBJNAME=ZTSCDC_*; OBJTYPE=*; P_GROUP=*\n</code></pre> <p>XXXX (stands for a placeholder) is the authorization group for the source table. To determine, which authorization group belongs to which table, check the table TDDAT - Maintenance Areas for Tables. If the table is not listed, the authorization group is &amp;NC&amp;. For authorizing specific tables use authorization object S_TABU_NAM instead of S_TABU_DIS.</p> <p>Note</p> <p>The transport requests for the required function groups /THEO/READ_TABLE are located in <code>C:\\Program Files\\[XtractProduct]\\ABAP\\TableCDC</code> and <code>C:\\Program Files\\[XtractProduct]\\ABAP\\Table</code>. </p> <p> Download SAP profile for Table CDC</p>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#deltaq","title":"DeltaQ","text":"Customizing checkInitial DataSource activationRe-activating a DataSourceProcessing Necessary SAP authorizations<pre><code>S_RFC           RFC_TYPE=FUGR; RFC_NAME=SUSR; ACTVT=16  \nS_RFC           RFC_TYPE=FUNC; RFC_NAME=RFC_GET_SYSTEM_INFO; ACTVT= 16 \nS_ADMI_FCD      S_ADMI_FCD=NADM\nS_TABU_DIS      ACTVT = 02; DICBERCLS=SA\nS_TABU_DIS      ACTVT = 03; DICBERCLS=SA\nS_TABU_NAM      ACTVT = 02; TABLE=EDIPOA\nS_TABU_NAM      ACTVT = 03; TABLE=EDIPOA\n</code></pre> Necessary SAP authorizations<pre><code>S_RFC            RFC_TYPE=FUGR; RFC_NAME=SDIFRUNTIME; ACTVT=16 \nS_RFC            RFC_TYPE=FUGR; RFC_NAME=RSAG; ACTVT=16 \nS_TABU_DIS       ACTVT=03; DICBERCLS=SS                                                \nS_TABU_DIS       ACTVT=03; DICBERCLS=SC                                               \nS_IDOCDEFT       EDI_TCD=WE30; ACTVT=01; EDI_CIM=*; EDI_DOC=*                            \nS_IDOCDEFT       EDI_TCD=WE30; ACTVT=03; EDI_CIM=*; EDI_DOC=*   \n</code></pre> Necessary SAP authorizations<pre><code>S_RFC           RFC_TYPE=FUGR; RFC_NAME=SDIFRUNTIME; ACTVT=16\nS_TABU_DIS      ACTVT=03; DICBERCLS=SS                                 \nS_TABU_DIS      ACTVT=03; DICBERCLS=SC                                               \nS_IDOCDEFT      EDI_TCD=WE30; ACTVT=02; EDI_CIM=*; EDI_DOC=*\n</code></pre> Necessary SAP authorizations<pre><code>S_RFC            RFC_TYPE=FUGR; RFC_NAME=EDI1; ACTVT=16\nS_RFC            RFC_TYPE=FUGR; RFC_NAME=BATG; ACTVT=16\nS_RFC            RFC_TYPE=FUGR; RFC_NAME=EDIMEXT; ACTVT=16 \nS_RFC            RFC_TYPE=FUGR; RFC_NAME=ARFC; ACTVT=16 \nS_RFC            RFC_TYPE=FUGR; RFC_NAME=ERFC; ACTVT=16 \nS_RFC            RFC_TYPE=FUGR; RFC_NAME=EDIN; ACTVT=16 \nS_RFC            RFC_TYPE=FUGR; RFC_NAME=/BIC/*; ACTVT=16 \nS_RFC            RFC_TYPE=FUGR; RFC_NAME=/BI0/*; ACTVT=16\nS_RFC            RFC_TYPE=FUGR; RFC_NAME=RSAK; ACTVT=16\nS_RFC            RFC_TYPE=FUGR; RFC_NAME=EDIW; ACTVT=16\nS_RFC            RFC_TYPE=FUGR; RFC_NAME=SDTX; ACTVT=16\nS_RFC            RFC_TYPE=FUGR; RFC_NAME=EDIMEXT; ACTVT=16\nS_RFC            RFC_TYPE=FUGR; RFC_NAME=SYSU; ACTVT=16\nS_RFC            RFC_TYPE=FUGR; RFC_NAME=RSC1; ACTVT=16\nS_RFC            RFC_TYPE=FUNC; RFC_NAME=RSAP_REMOTE_HIERARCHY_CATALOG;  ACTVT=16\nS_RFC            RFC_TYPE=FUNC; RFC_NAME=RSA1_OLTPSOURCE_GET_ALL;  ACTVT=16\nS_RFC            RFC_TYPE=FUNC; RFC_NAME=RSA1_OLTPSOURCE_GET_SELECTIONS;  ACTVT=16\nS_RFC_ADM        ACTVT=03; RFCDEST=XTRACT*; RFCTYPE = T; ICF_VALUE=* \nB_ALE_RECV       EDIMES=RSRQST\nS_IDOCMONI       ACTVT=03; EDI_DIR=*; EDI_MES=*; EDI_PRN=*; EDI_PRT=*; EDI_TCD=WE*\nS_IDOCDEFT       EDI_TCD=WE30; ACTVT=02; EDI_CIM=*; EDI_DOC=*\nS_IDOCDEFT       EDI_TCD=WE30; ACTVT=03; EDI_CIM=*; EDI_DOC=*\nS_TABU_DIS       ACTVT=03; DICBERCLS=SS                                   \nS_TABU_DIS       ACTVT=03; DICBERCLS=SC\nS_TABU_DIS       ACTVT=03; DICBERCLS=&amp;NC&amp;  \nS_BTCH_ADM       BTCADMIN=Y          \nS_BTCH_JOB       JOBGROUP=*; JOBACTION=RELE\nS_SPO_DEV        SPODEVICE=*\nS_RO_OSOA        OLTPSOURCE=*; OSOAAPCO=*; OSOAPART=DATA; ACTVT=03;  | Only in SAP Releases  7.0 and higher\n</code></pre> <p> Download SAP profile for DeltaQ</p> <p> Download SAP profile for DeltaQ Customizing Check</p>","tags":["permissions"]},{"location":"documentation/setup-in-sap/sap-authority-objects/#related-links","title":"Related Links","text":"<ul> <li>SAP Help: Role Administration</li> <li>SAP ONE Support Launchpad</li> </ul>","tags":["permissions"]},{"location":"documentation/table/","title":"Table","text":"<p>This page shows how to use the Table extraction type. The Table extraction type can be used to extract data from SAP Tables and Views.</p>"},{"location":"documentation/table/#supported-sap-objects","title":"Supported SAP Objects","text":"<ul> <li>Transparent tables</li> <li>Views</li> <li>ABAP CDS Views</li> <li>HANA CDS Views</li> <li>Pool tables (joining not possible)</li> <li>Cluster tables (joining not possible)</li> </ul>"},{"location":"documentation/table/#prerequisites","title":"Prerequisites","text":"<ul> <li>A connection to an SAP system is available, see SAP Connection.</li> <li>The SAP user has sufficient user rights, see SAP Authority Objects.</li> </ul> <p>Warning</p> <p>Missing Authorization. To use the Table extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust SAP Authority Objects accordingly.</p>"},{"location":"documentation/table/#create-a-table-extraction","title":"Create a Table Extraction","text":"<ol> <li>In the main window of the Designer, click [New] to create a new extraction. The window \"Create Extraction\" opens. </li> <li>Select an SAP Connection from the drop-down menu in Source and enter a unique name for your extraction.</li> <li>Select the extraction type Table and click [OK]. The main window of the extraction type opens automatically.</li> </ol> <p>The majority of the functions of the extraction type can be accessed in the main window.</p>"},{"location":"documentation/table/#look-up-an-sap-table","title":"Look up an SAP Table","text":"<ol> <li>In the main window of the extraction type, click [Add] to add a table. The window \"Table Lookup\" opens. </li> <li>In the field Table Name, enter the name of the table to extract  . Use wildcards (*) if needed. </li> <li>Click [ ] . Search results are displayed.</li> <li>Select a table   and click [OK]. </li> </ol> <p>All relevant metadata information of the table is retrieved from SAP. The application returns to the main window of the extraction type.</p>"},{"location":"documentation/table/#define-the-table-extraction-type","title":"Define the Table Extraction Type","text":"<p>The Table extraction type offers the following options for table extractions:</p> <ol> <li>Select the columns you want to extract. By default all columns are selected. Deselect the columns you do not want to extract. </li> <li>Optional: Define a WHERE clause or a HAVING clause to filter table records. By default all data is extracted.</li> <li>Optional: Join two or more tables and extract the result of the join. For more information, see Table Joins.</li> <li>Click [Load live preview] to display a live preview of the first 100 records. </li> <li>Check the Extraction Settings and the General Settings before running the extraction.</li> <li>Click [OK] to save the extraction type.</li> </ol> <p>You can now run the extraction, see Execute and Automate Extractions.</p>"},{"location":"documentation/table/#rfc_read_table-restrictions","title":"RFC_READ_TABLE Restrictions","text":"<p>Especially with older SAP releases you may encounter a few restrictions when using the SAP standard function module (RFC_READ_TABLE) for table extraction:</p> <ul> <li>The overall width of all columns to be extracted must not exceed 512 bytes.</li> <li>It is not possible to extract data from tables that contain one or more columns of the data type f (FLTP, floating point), DEC (decimal, e.g. for percentage) or x (RAW, LRAW).</li> <li>Poor extraction performance with larger tables.  Can cause also duplicates.</li> <li>Depending on the SAP version there may be other restrictions. </li> </ul> <p>When facing restrictions, install the Theobald Software custom function module /THEO/READ_TABLE on your SAP system. </p> <p>Warning</p> <p>Converting issues Error while converting value '*.0' of row 1530, column 3. The SAP standard module RFC_READ_TABLE for table extraction can only extract the ABAP data type DEC to a limited extent. This leads to the mentioned example error during extraction. Use the function module /THEO/READ_TABLE.</p>"},{"location":"documentation/table/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base Article: Change Data Capture with CDHDR</li> <li>Knowledge Base Article: Delta Table Extraction</li> <li>Knowledge Base Article: Read data from Cluster Fields in Tables PCL1 and PCL2 (Payroll)</li> </ul>"},{"location":"documentation/table/edit-runtime-parameters/","title":"Runtime Parameters","text":"<p>Runtime parameters are are placeholders for values that are passed at runtime, see Extraction Parameters - Custom. They can be created in context of using the WHERE Clause.</p>"},{"location":"documentation/table/edit-runtime-parameters/#create-runtime-parameters","title":"Create Runtime Parameters","text":"<p>There are two types of runtime parameters:</p> Scalar parametersList parameters <p>Scalar runtime parameters represent a single value.  Follow the steps below to create a scalar runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add Scalar] to define scalar parameters to be used as placeholders for actual values. The placeholders need to be populated with actual values at runtime. </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Select one of the following data types from the drop-down menu. The data types may correlate to SAP data types.</p> Type Description Text Can be used for any type of SAP selection field. Number Can be used for numeric SAP selection fields. Flag Can only be used for SAP selection fields THAT require an \u2018X\u2019 (true) or a blank \u2018\u2018 (false) as input value. </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p> <p>List runtime parameters represent multiple values.  Follow the steps below to create a list runtime parameter:</p> <ol> <li>In the main window of the component click Edit runtime parameters.  The window \u201cEdit Runtime Parameters\u201d opens. </li> <li> <p>Click [Add List] to define list parameters that contain multiple values separated by commas e.g., 1,10 or \u201c1\u201d, \u201c10\u201d. The placeholders need to be populated with actual values at runtime.  </p> <p>Tip</p> <p>Parameter0..-n is the default naming for the added parameter. You can enter a name of your choice.</p> </li> <li> <p>Click [OK] to confirm.</p> </li> </ol> <p>The runtime parameters are now available in the extraction type.</p>"},{"location":"documentation/table/edit-runtime-parameters/#use-runtime-parameters-in-the-where-clause-editor","title":"Use Runtime Parameters in the WHERE Clause Editor","text":"<ol> <li>Navigate to the WHERE clause tab in the main window of the extraction and open the WHERE clause editor.</li> <li>Add a new WHERE clause criteria that uses the [Default with Parameter] template.</li> <li>Click the Parameter component. A drop-down list that displays all available parameters opens.  Select a parameter from the list. </li> <li>To test the WHERE clause, click [Load live preview]. Provide parameter values when prompted.</li> </ol>"},{"location":"documentation/table/edit-runtime-parameters/#use-runtime-parameters-in-the-where-clause-text-mode","title":"Use Runtime Parameters in the WHERE Clause Text Mode","text":"<p>Add an @ symbol before a value to mark it as a runtime parameter, e.g., enter <code>@myParameter</code> instead of a value. Example: <code>T001W~WERKS BETWEEN @PlantLow AND @PlantHigh</code>.</p> <p></p> <p>Pass values during runtime, see Extraction Parameters - Custom.</p>"},{"location":"documentation/table/general-settings/","title":"General Settings","text":"<p>This page contains an overview of the settings in the window \"General Settings\". To open the general settings, click General Settings in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/table/general-settings/#misc-tab","title":"Misc. Tab","text":"<p>The Misc. tab covers cache settings, column encryption and keywords of an extraction type.</p> <p></p>"},{"location":"documentation/table/general-settings/#cache-results","title":"Cache results","text":"<p>The Cache results option is only available in pull destinations, e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times.  To decrease the SAP server load, you can select the Cache results option, this way the pull destination pulls the data from cache and not from the SAP.</p> <p>This increases the performance and limits the impact on the SAP system.  If this behavior is not desired (for example, because the data must be always 100% up to date), the cache option must be explicitly turned off.</p>"},{"location":"documentation/table/general-settings/#keywords","title":"Keywords","text":"<p>One or more keywords (tags) can be assigned to an extraction.  Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the \"Search Extractions\" window. To open the \"Search Extractions\" window, click [  Search] in the main window of the Designer. </p> <p>Tip</p> <p>To add keywords to multiple extractions at once, select the extractions in the main window of the Designer. Right-click + Add/Remove keywords opens the window \"Add/Remove Keywords To/From Multiple Extractions\".</p>"},{"location":"documentation/table/general-settings/#primary-key-tab","title":"Primary Key Tab","text":"<p>Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.  </p> <p></p> <p>The depicted example shows the SAP object MAKT with its primary key inherited from SAP in the general settings of the Designer.  In this example the primary key consists of MANDT, MATNR and SPRAS. The primary key is also taken over in the destination. </p> <p>Note</p> <p>A defined primary key field in a table is a prerequisite for merging data. </p>"},{"location":"documentation/table/general-settings/#generate-surrogate-key-column","title":"Generate Surrogate Key Column","text":"<p>If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs.  The surrogate keys are hash values of type signed 8 byte integer, e.g., <code>#-3008591679982390000</code>. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.</p>"},{"location":"documentation/table/general-settings/#security-tab","title":"Security Tab","text":"<p>Restrict user access to the extraction. For more information, see Restrict Designer Access.</p> <p></p>"},{"location":"documentation/table/general-settings/#columns-order-tab","title":"Columns Order Tab","text":"<p>The \"Columns Order\" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns.  Index 0 defines the first column in the result set, index 1 the second columns, etc.</p> <p></p> Option Description Enabled When this option is active, the defined column order is applied when running the extraction. Swap Swaps the index of 2 columns. All other columns keep their indexes. Insert Inserts the selected column into the selected index. All other indexes are recalculated. Reset default Restores the original column order."},{"location":"documentation/table/having-clause/","title":"HAVING Clause","text":"<p>The HAVING clause can be used to filter groups of rows, see SAP ABAP Documentation: SELECT-HAVING.</p>"},{"location":"documentation/table/having-clause/#create-a-having-clause","title":"Create a HAVING Clause","text":"<p>The following example shows how many materials are assigned to a material type (MTART).  After applying the HAVING Clause, the preview shows only the material types with more than 10 materials assigned.</p> <ol> <li> <p>Select an aggregate function   for a field you want to use in the HAVING-clause. </p> <p>Note</p> <p>Aggregate functions are only supported for numeric field types, which is why the field BRGEW (Gross Weight) is used in the example.</p> <p></p> </li> <li> <p>Navigate to tab HAVING Clause .</p> </li> <li>Enter a HAVING Clause, e.g., <code>COUNT(BRGEW) &gt; 10</code>. </li> <li>Click [Load live preview] to display the results in the Preview section.</li> </ol> <p>Note</p> <p>When fields with the same name exist in different tables, the field names must be formatted as [Table]~[Column], e.g., MAKT~MATNR.  This can be the case with table joins.</p>"},{"location":"documentation/table/having-clause/#having-clause-syntax","title":"HAVING Clause Syntax","text":"<p>With regard to syntax and formulas, the same rules apply as for the WHERE Clause. </p>"},{"location":"documentation/table/main-window/","title":"Main Window","text":"<p>This section contains an overview and description of the Tables and Fields tab in the main window of the Table extraction type.  The tab Tables and Fields consists of the following subsections:</p>"},{"location":"documentation/table/main-window/#link-buttons","title":"Link Buttons","text":"Link Button Description Extraction Settings Opens the Extraction Settings menu. Extraction Settings affect only the respective extraction type. Edit runtime parameters Opens the runtime parameters menu. General Settings Opens the General Settings menu. General Settings are the same for all extractions types."},{"location":"documentation/table/main-window/#tables","title":"Tables","text":"<p>The subsection Tables lists all SAP Tables and Views that were added to the extraction type.</p> <ul> <li>Click [Add] to add an SAP Table or View to the extraction type.</li> <li>Click [Remove] to remove an SAP Table or View  from the extraction type. </li> </ul> <p>Tip</p> <p>Select an SAP Table to define the corresponding settings, e.g, output columns, WHERE clause, etc. You can switch between the SAP Tables.</p> <p></p>"},{"location":"documentation/table/main-window/#fields","title":"Fields","text":"<p>The subsection Fields displays all columns in the selected SAP Table or View.</p> <p></p> <p>The Table extraction type imports and highlights the dedicated indexes of an SAP Table, such as primary key and/or sorting options. </p> <p>Note</p> <p>Use the indicated fields for filtering increases performance when applying WHERE-clause.</p> Columns Description  /  Defines whether or not a table column is added to the output of the extraction type. By default, all table columns are extracted. Name Name of a column in the SAP Table. The column name can be filtered. Description Description of the column. The column description can be filtered. Aggregate Function Applies aggregate functions to numeric fields. Conv. Activates SAP conversion routines."},{"location":"documentation/table/main-window/#aggregate-functions","title":"Aggregate Functions","text":"<p>Aggregate functions are only available for numeric field data types, e.g., INT, FLOAT, DECIMAL.  The following aggregation functions are available: </p> <ul> <li>None: No aggregation </li> <li>MEAN: Average</li> <li>COUNT: Number </li> <li>MAX: Maximum</li> <li>MIN: Minimum </li> <li>SUM: Total</li> </ul>"},{"location":"documentation/table/main-window/#conversion-routines","title":"Conversion Routines","text":"<p>Conversion routines are stored in the Data Dictionary that is used for the respective fields.  Activating the conversion routines in SAP leads to significant performance decrease.  After the conversion, the value is displayed as it appears in the transaction SE16N in the SAP GUI. </p> <p>Examples: </p> <ul> <li>Language keys, e.g., <code>D</code> in the database becomes <code>DE</code> after conversion</li> <li>Project numbers, e.g., <code>T000012738GT</code> becomes <code>T/12738/GT</code> after conversion.</li> </ul> <p></p> State Description Safety no conversion routines selected - conversion routines enabled requires the function module Z_XTRACT_IS_TABLE_COMPRESSION no data type safety conversion routines enabled requires the function module /THEO/READ_TABLE assured data type safety"},{"location":"documentation/table/main-window/#preview","title":"Preview","text":"<p>The subsection Preview displays a real-time preview of the first 100 fields of the SAP table when clicking the button [Load live preview].</p> <p>Note</p> <p>The use non-indexed fields in the WHERE-clause can lead to timeouts during the preview of large tables.</p> <p></p>"},{"location":"documentation/table/main-window/#buttons","title":"Buttons","text":"<p>[Load live preview]  Allows a real-time preview of the extraction data without executing the extraction.  You can also preview the data with aggregation functions. </p> <p>[Count rows]  Returns the number of rows/data records of an extraction, considering the WHERE and HAVING clauses stored. </p> <p>[Refresh metadata] A new lookup is performed on the selected table(s). Existing mappings and field selections are retained, which is not the case when the table is added again.  It can be necessary to refresh the metadata when a table has been adjusted in SAP, when another source system is assigned to the extraction type, or when the source system was updated.  In such cases, data inconsistencies can occur that are resolved by this function.   </p>"},{"location":"documentation/table/settings/","title":"Extraction Settings","text":"<p>This page contains an overview of the extraction settings in the Table extraction type. To open the extraction settings, click Extraction Settings in the main window of the extraction type. </p> <p></p> <p>Warning</p> <p>Could not load list of available function modules because permission for table ENLFDIR is missing This warning appears if a technical SAP user does not have authorization rights to access the SAP table ENLFDIR. Confirm the warning as the user can still adjust the extraction settings.</p>"},{"location":"documentation/table/settings/#extraction-settings","title":"Extraction Settings","text":""},{"location":"documentation/table/settings/#package-size","title":"Package Size","text":"<p>The extracted data is be split into packages of the defined size. The default value is 50000 lines. A package size between 20000 and 50000 is advisable for large amounts of data. 0 means no packaging.  Not using packaging can lead to an RFC timeout for large data extracts.</p> <p>Warning</p> <p>RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.</p>"},{"location":"documentation/table/settings/#row-limit","title":"Row Limit","text":"<p>Specifies the maximum number of extracted records. 0 extracts all data. You can use this option to perform tests with a small amount of data by entering a row limit of e.g., 1000.</p>"},{"location":"documentation/table/settings/#function-module","title":"Function Module","text":"<p>Specifies the name of the function module used for data extraction.  This field is filled automatically depending on what function modules are installed on your SAP system. Custom function modules are supported.</p> <p>The following function modules can be used to extract tables:</p> <ul> <li>RFC_READ_TABLE (TAB512)</li> <li>/BODS/RFC_READ_TABLE (TAB2048)</li> <li>/SAPDS/RFC_READ_TABLE (TAB2048)</li> <li>/BODS/RFC_READ_TABLE2</li> <li>/SAPDS/RFC_READ_TABLE2</li> <li>Z_THEO_READ_TABLE</li> <li>/THEO/READ_TABLE (recommended)</li> </ul> <p>Warning</p> <p>Duplicates in the target environment. The SAP standard modules for table extraction do not have pointers for table fields. In larger tables this may cause low performance and duplicates in the target environment. Use the function module THEO/READ_TABLE from Theobald Software to ensure smooth extractions.</p> <p>Note the necessary SAP Authority Objects:</p> <pre><code>S_TABU_NAM ACTVT=03; TABLE=ENLFDIR\n</code></pre>"},{"location":"documentation/table/settings/#extract-data-in-background-job","title":"Extract Data in Background Job","text":"<p>If Background job timeout (seconds) checkbox is activated, the table extraction is executed as a background job in SAP.  This setting is optional and is supported in combination with function module THEO/READ_TABLE or Z_THEO_READ_TABLE version 2.0.  Activate the setting Background job timeout (seconds) for long-running extractions with a large amounts of data that may run into a timeout error (\u201cTime limit exceeded\u201d), when using the foreground mode.</p> <p>Tip</p> <p>The extraction jobs can be found in the SAP JobLog (SM37) under the JobName theo_read_table.</p> <p>Warning</p> <p>Shared Memory ran out of memory!  If this error message pops up when running an extraction in the background, adjust the size of the Shared Memory. SAP recommends a Shared Memory size of 800MB~1.5GB for a production/test system or 2GB~4GB for S/4 systems, see SAP Support: How to solve SYSTEM_NO_SHM_MEMORY runtime error.</p>"},{"location":"documentation/table/settings/#advanced-settings","title":"Advanced Settings","text":""},{"location":"documentation/table/settings/#background-job-timeout-seconds","title":"Background Job Timeout (seconds)","text":"<p>Sets a timeout period for extractions that run in background mode. The default value is 180 seconds. The maximum timeout value is 3600 seconds. </p> <p>This option can be used if the data transfer to a destination takes a lot of time, e.g., when bulk-inserts are deactivated for database destinations. </p> <p>Note</p> <p>The background job timeout setting only takes effect if the extractions run in background mode using /THEO/READ_TABLE.</p>"},{"location":"documentation/table/settings/#adjust-currency-decimals","title":"Adjust Currency Decimals","text":"<p>The default number of decimal places for a currency in the SAP database is 2 decimals. Currencies that do not have decimals are also stored in this format, e.g. JPY, VND, KRW, etc.</p> <p>Example:</p> Currency Actual Amount Amount stored in SAP database JPY 100 1.00 KRW 10000 100.00 <p>When extracting currencies with no decimals, the amount stored in SAP is returned e.g., 100 JPY are extracted as 1.00. To correct the decimal placement of the extracted data, activate Adjust Currency Decimals. If Adjust Currency Decimals is active, currencies without decimals are multiplied by a factor that balances out the decimals.</p> <p>Adjust Currency Decimals also requires the extraction of the corresponding CURRENCY field that can be used as a reference for the multiplication factor. Use the [Load live preview] function to find the correct currency field/s. </p> <ul> <li>If the currency field is part of the table, add it to the output.</li> <li>If the currency field is in another table, join the tables. </li> <li>If the reference is not part of a table, Adjust Currency Decimals cannot be used.</li> </ul> <p>Note</p> <p>The multiplication factor used in Adjust Currency Decimals is determined by the SAP currency table TCURX.  To access the table, the following SAP Authority objects must be set in SAP: S_TABU_NAM    ACTVT=03; TABLE=TCURX.</p>"},{"location":"documentation/table/table-join/","title":"Table Joins","text":"<p>The Join functionality allows joining two or more tables and extract the result of the join.  To perform the extraction the corresponding SQL command is generated dynamically and the join is executed on the SAP server. </p> <p>Possible scenarios can be joining tables for header and item data or tables for master data and texts. </p>"},{"location":"documentation/table/table-join/#supported-join-types","title":"Supported Join Types","text":"<p>The following join types are supported:</p> <ul> <li>Inner Join</li> <li>Left Outer Join, also referred to as Left Join.</li> </ul> <p>For more information on join types see SAP Help: Inner Join and Outer Join. </p> <p>Note</p> <p>Joining of cluster or pool tables is not supported. Cluster or pool tables need to be extracted individually and joined in the destination.</p>"},{"location":"documentation/table/table-join/#prerequisites","title":"Prerequisites","text":"<p>To use table join, the function module /THEO/READ_TABLE needs to be available in SAP. </p>"},{"location":"documentation/table/table-join/#join-tables","title":"Join Tables","text":"<p>The following example shows how to join the tables KNA1 and KNVV.</p> <ol> <li>In the tab Tables and Fields, click [Add] to add two tables (e.g., KNA1 and KNVV). </li> <li>Select both tables on the left and check the fields you want to extract  . </li> <li>Optional: Switch to the WHERE clause tab and specify a WHERE clause.</li> <li>Switch to the Joins tab  . A Join condition with default values is automatically available.  The join condition is based on the foreign key relationship of the joined tables.</li> <li>Click [ ] to edit the join condition. The window \"Join\" opens.  </li> <li> <p>Select a table column in the Left Table field and in the Right Table field to map the table contents.  In the depicted example a left outer join on tables KNA1 (left table) and KNVV (right table) on the field KUNNR is performed. It is possible to add multiple join conditions.</p> <ul> <li>Click [Add] to extend the join condition to more fields.</li> <li>Click [ ] to delete existing joins.</li> </ul> <p>Tip</p> <p>Different tables can have identical field / column names.  Defining a join condition based on the identical field names not always delivers the expected result, e.g., VBAK~VBELN &lt;&gt; LIPS~VBELN. Make sure the fields you use in a join condition contain the same content/data.</p> </li> <li> <p>Click [OK] to save the join.</p> </li> </ol> <p>You can join additional tables by adding tables more in the tab Tables and Fields.</p> <p>Warning</p> <p>RFC_ERROR_SYSTEM_FAILURE - Illegal access to the right table of a LEFT OUTER JOIN. Using a WHERE clause on the right table of a LEFT OUTER JOIN is only possible as of SAP Release 7.40, SP05. </p>"},{"location":"documentation/table/table-join/#auto-mapping","title":"Auto Mapping","text":"<p>The [Auto-map] button deletes existing join conditions and performs a new field mapping based on the foreign key relationship of the joined tables. </p> <p></p> <p>Recommendation</p> <p>To avoid poor extraction performance, do not join more than five tables.</p>"},{"location":"documentation/table/where-clause/","title":"WHERE Clause","text":"<p>A WHERE clause can be used to filter table records, see SAP ABAP Documentation: SELECT-WHERE. Enter WHERE clauses manually in Text mode or use the WHERE Clause Editor in Editor Mode.</p>"},{"location":"documentation/table/where-clause/#create-a-where-clause","title":"Create a WHERE Clause","text":"<ol> <li>Open a Table extraction type. </li> <li>Navigate to the tab WHERE Clause. </li> <li>Enter a WHERE clause manually or use the WHERE Clause Editor.</li> <li>Click [Load live preview] to display the results in the Preview section.</li> </ol>"},{"location":"documentation/table/where-clause/#where-clause-text-mode","title":"WHERE Clause Text Mode","text":"<p>The WHERE clause text mode allows you to directly enter a WHERE clauses. The text mode of the WHERE clause supports script expressions.</p> <p>Warning</p> <p>Extraction fails due to incorrect syntax.  The extractions fail, if incorrect syntax is used in the WHERE clause.  Make sure to use correct SAP OpenSQL syntax. Several important syntax rules are listed in this help section.</p> <p>Tip</p> <p>To check the syntax of the WHERE clause, click [Load live preview]. This way there is no need to run an extraction to see, if the syntax is correct.</p>"},{"location":"documentation/table/where-clause/#where-clause-syntax","title":"WHERE Clause Syntax","text":"<p>The WHERE Clause syntax generally uses the following structure:</p> <pre><code>[Table]~[Column][Space][Operator][Space][Filter-Value]\n</code></pre> <p>Examples: </p> <pre><code>KNA1~LAND1 = 'US'\n</code></pre> Rule Correct Wrong Enter a space before and after the equal sign YEAR = '1999' YEAR= '1999 ', YEAR ='1999' or YEAR='1999' Set floating point numbers in single quotation mark KMENG &gt; '10.3' KMENG &gt; 10.3 Values must use the internal SAP representation:  Date: YYYYMMDD   Year Period: YYYYPPP   Numbers with leading zeroes, e.g., customer numbers  19990101   1999001   0000001000  01.01.1999   001.1999   1000 <p>The following operations are supported in the WHERE clause:</p> Operator Description =, EQ True if the content of operand1 is equal to the content of operand2. &lt;&gt;, NE True if the content of operand1 is not equal to the content of operand2. &lt;, LT True if the content of operand1 is less than the content of operand2. &gt;, GT True if the content of operand1 is greater than the content of operand2. &lt;=, LE True if the content of operand1 is less than or equal to the content of operand2. &gt;=, GE True if the content of operand1 is greater than or equal to the content of operand2. (NOT) LIKE True if the value of operand1 matches (does not match) the pattern in operand2. (NOT) IN True if the content of operand1 is (not) part of the content of operand2. Operand2 must be of type LIST or SQL. <p>For more details on the OpenSQL syntax, see SAP Help: Select WHERE </p> <p>Tip</p> <p>To increase extracting performance, make sure to place the indexed fields as the first selection filter operation in the WHERE clause.</p> <p>Note</p> <p>When fields with the same name exist in different tables, the field names must be formatted as [table name]~[field name], e.g., MARC~MATNR.  This can be the case when extracting multiple tables.</p>"},{"location":"documentation/table/where-clause/#script-expressions","title":"Script Expressions","text":"<p>The [Text Mode] of the WHERE clause supports script expressions.  Script expressions are usually used to determine a dynamic date based on the current date.  When using script expressions in a WHERE Clause, the value must be entered in single quotation marks.</p> <p>Syntax:</p> <pre><code>[Table]~[Column][Space][Operator][Space]'#[Script-Expression]#'\n</code></pre> <p><code>BKPF~BUDAT &gt;= '#{DateTime.Now.AddYears(-5).ToString(\"yyyyMMdd\")}#'</code></p> <p>Examples:</p> Input Description <pre>#{ DateTime.Now.ToString(\"yyyyMMdd\") }#</pre> Current date in SAP format (yyyyMMdd) <pre>#{ String.Concat(DateTime.Now.Year.ToString(), \"0101\") }#</pre> Current year concatenated with \"0101\" (yyyy0101) <pre>#{ String.Concat(DateTime.Now.ToString(\"yyyy\"), \"0101\") }#</pre> Current year concatenated with \"0101\" (yyyy0101) <pre>#{ String.Concat(DateTime.Now.ToString(\"yyyyMMdd\").Substring(0,4), \"0101\") }#</pre> Current year concatenated with \"0101\" (yyyy0101) <p>For more information on script expression, see Script Expressions.</p>"},{"location":"documentation/table/where-clause/#subqueries","title":"Subqueries","text":"<p>Note</p> <p>The usage of subqueries is only possible as of SAP Release 7.40, SP05.</p> <p>A subquery is an SQL query nested inside a larger query.  Subqueries are nested queries that provide data to the enclosing query. Subqueries need be enclosed with parenthesis and can return individual values or a list of records. Get more details about subqueries on the SAP help site - Conditions.</p> <p>In the following example a subquery is used with the IN operator.  The following statement returns all the active customers (rows in the table KNA1) that have i.e. a sales document in the table VBAK for sales document header data.</p> <p></p>"},{"location":"documentation/table/where-clause/#where-clause-editor","title":"WHERE Clause Editor","text":"<p>The WHERE clause editor offers a toolkit for those who are not familiar with the syntax of the WHERE clause. Click [Editor mode] to open the editor.  </p> <p>There are 2 options for adding criteria to the WHERE clause:</p> <ul> <li>[Add Criteria] adds single criteria. <ul> <li>The default structure for a single criteria with static values is <code>[Table~Column][Operator][Filer-Value]</code> e.g., MARC~WERKS = 1000.</li> <li>The default structure for a single criteria with parameters is <code>[Column][Operator][Parameter]</code> e.g., MARC~WERKS = [p_WERKS].</li> </ul> </li> <li>[Add Criteria Group] adds a group of criteria.<ul> <li>The default structure for a criteria group is <code>([Table~Column1][Operator1][Filter-Value1][Boolean][Table~Column2][Operator2][Filter-Value2])</code> e.g., (MARC~PSTAT = 'L' OR MARC~PSTAT = 'LB').</li> </ul> </li> </ul> <p>Tip</p> <p>Combine multiple criteria and criteria groups to create complex filters e.g.,  MARC~WERKS = 1000 AND (MARC~PSTAT = 'L' OR MARC~PSTAT = 'LB') extracts only data where the column WERKS equals 1000 and the column PSTAT equals either 'L' or 'LB'.</p>"},{"location":"documentation/table/where-clause/#components-of-the-where-clause-editor","title":"Components of the WHERE Clause Editor","text":"<p>The following buttons and options are available in the WHERE Clause Editor:</p> Icon Component Function Delete row deletes a criteria. Move row up changes the sequence of the criteria. The selected criteria moves up. The sequence of criteria can also be changed with Drag and drop. Move row down changes the sequence of the criteria. The selected criteria moves down. The sequence of criteria can also be changed with Drag and drop. Column adds a column. Click on the component to select a column from the available tables. SQL adds an Open SQL statement, see SAP Help: Open SQL. Operator adds an operator e.g., =, &lt;, &gt;, etc. Value adds a static value of type String, Number, Flag or List. List offers a separate editor to create lists of type String, Number or Select. Select enables usage of SELECT statements. For more information, see Working with Lists in the WHERE-Clause Editor. Criteria adds a new criteria after the selected criteria. Group adds a new group of criteria the selected criteria. Parameter adds a previously defined runtime parameter, see Runtime Parameters. <p>Note</p> <p>When adding or editing a criteria only the relevant components are displayed e.g., Add Operator is only available if there is a column or SQL statement to use an operator on.</p>"},{"location":"documentation/table/where-clause/#edit-and-delete-components","title":"Edit and Delete Components","text":"<ul> <li>Click on a component to edit it. All areas that are marked green can be edited.</li> <li>To delete a component, click the (x) icon above the component.</li> </ul>"},{"location":"documentation/table/where-clause/#sap-system-fields","title":"SAP System Fields","text":"<p>You can use SAP system fields for date and time in a WHERE clause.  The usage of SAP system fields requires SAP NW 7.4 SP5 or higher and the custom function module /THEO/READ_TABLE.</p> <p>Example:</p> <ol> <li>Navigate to WHERE Clause Editor and select a column of the type Date here: BUDAT .  </li> <li>Delete the criterion \"Value\" and use the criterion \"SQL\"  .  </li> <li>Within the \"SQL\" criterion, use the supported system fields for date and time with a preceding \"@\" character, here: @sy-datum .  </li> <li>Click [Load live preview] to check the result. </li> </ol>"},{"location":"documentation/table/where-clause/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base Article: Delta Table Extraction</li> <li>Knowledge Base Article: Change Data Capture with CDHDR</li> <li>Knowledge Base Article: Working with Lists in the WHERE-Clause Editor</li> <li>Knowledge Base Article: LIKE Operand in WHERE Clauses</li> </ul>"},{"location":"documentation/table-cdc/","title":"Table CDC","text":"<p>This page shows how to use the Table CDC extraction type. The Table CDC extraction type can be used to extract delta content from SAP Tables. It creates a log table in SAP that records insert, update, and delete activities that apply to a selected SAP table.</p>"},{"location":"documentation/table-cdc/#about-table-cdc","title":"About Table CDC","text":"<p>Change Data Capture (CDC) is a method to keep track of data changes such as insert, update and delete in SAP tables. The Table CDC component creates a log table in SAP that records any changes made to a selected table.  The content of the log table is cleared after every successful run of the extraction. Data that was not extracted is not cleared from the log table.</p> <p>For a detailed overview of this process, refer to the Knowledge Base article Delta Mechanism of TableCDC.</p> <p>Note</p> <p>Clusters, pool tables and views are not supported by the Table CDC extraction type. </p>"},{"location":"documentation/table-cdc/#system-requirements","title":"System Requirements","text":"<ul> <li>The Table CDC extraction type is compatible with SAP ECC 5.0 and higher</li> <li>Supported databases:<ul> <li>HANA</li> <li>SQL Server</li> <li>Oracle</li> <li>IBM Db2</li> <li>IBM for i (Db4)</li> <li>IBM for z/os (Db6)</li> <li>MaxDB</li> </ul> </li> </ul>"},{"location":"documentation/table-cdc/#prerequisites","title":"Prerequisites","text":"<ul> <li>SAP custom function module /THEO/CDC_ECC or /THEO/CDC_S4 is installed in SAP.</li> <li>SAP custom function module /THEO/READ_TABLE or Z_THEO_READ_TABLE version 2.10 or higher is installed in SAP.</li> <li>A connection to an SAP system is available, see SAP Connection.</li> <li>The SAP user has sufficient user rights, see SAP Authority Objects.</li> </ul> <p>The SAP transport requests for the function modules are provided in the installation directory: <code>C:\\Program Files\\XtractUniversal\\ABAP\\</code>, see Custom function module for TableCDC.</p> <p>Warning</p> <p>Missing Authorization. To use the Table CDC extraction type, access to the designated authority objects (RFC) in SAP must be available. Adjust SAP Authority Objects accordingly.</p>"},{"location":"documentation/table-cdc/#create-a-table-cdc-extraction","title":"Create a Table CDC Extraction","text":"<ol> <li>In the main window of the Designer, click [New] to create a new extraction. The window \"Create Extraction\" opens. </li> <li>Select an SAP Connection from the drop-down menu in Source and enter a unique name for your extraction.</li> <li>Select the extraction type Table CDC and click [OK]. The main window of the extraction type opens automatically.</li> </ol> <p>The majority of the functions of the extraction type can be accessed in the main window.</p>"},{"location":"documentation/table-cdc/#look-up-a-table","title":"Look Up a Table","text":"<ol> <li>In the main window of the extraction type, click [ ]. The window \u201cTable Lookup\u201d opens. </li> <li>In the field Table Name, enter the name of the table to track and extract  . Use wildcards (*) if needed. </li> <li>Click [ ] . Search results are displayed.</li> <li>Select a table   and click [OK]. </li> </ol> <p>All relevant metadata information of the table is retrieved from SAP. The application returns to the main window of the extraction type.</p>"},{"location":"documentation/table-cdc/#define-the-table-cdc-extraction-type","title":"Define the Table CDC Extraction Type","text":"<p>The Table CDC extraction type offers the following options for tracking SAP tables:</p> <ol> <li>Select the table columns you want to track and extract. By default all columns are selected.  Deselect the columns you do not want to extract. </li> <li>Optional: If you do not want to extract the table when first running the extraction, deactivate [Extract table on first run].</li> <li> <p>Set a maximum number of rows that can be tracked. The default is 10000 rows. If the row limit is reached, the extraction fails with an exception. The maximum row limit of a log table is 500000.</p> <p>Note</p> <p>Once a log table is initialized, you cannot change the size limit anymore.</p> </li> <li> <p>Optional: Define a WHERE Clause to filter table records. By default all data is extracted.</p> </li> <li>Click [Load live preview] to display a live preview of the first 1000 records.<ul> <li>The column TS_TIMESTAMP contains a timestamp of when the data was last changed. </li> <li>The column TS_OPERATION indicates if a row was inserted (I), updated (U) or deleted (D).</li> </ul> </li> <li>Check the Extraction Settings and the General Settings before running the extraction.</li> <li>Click [OK] to confirm your settings.</li> </ol> <p>To initialize the tracking of the selected SAP table, run the extraction once. </p>"},{"location":"documentation/table-cdc/#run-the-extraction-for-the-first-time","title":"Run the Extraction for the First Time","text":"<p>Run the extraction for the first time to create a log table in SAP that records any changes made to a selected table or view.</p> <ol> <li>Select the extraction in the main window of the Designer. </li> <li>Click [ Destination] to assign the destination where you want to write data to  . </li> <li>Click [Run] . The window \"Run Extraction\" opens.</li> <li>Click [Run]  to run the extraction. </li> <li>If the extraction is successful, the status in the General Info section of the window changes to \"finished successfully\". If an error occurred, you can find information on the error in the Log section of the window.</li> </ol> <p>The log table in SAP is now available for the Table CDC component. The extracted SAP table is now available in your destination.</p> <p>Note</p> <p>When running the extraction regularly the content of the log table in SAP is extracted and written to the destination. The content of the log table in SAP is cleared after every successful run of the extraction. Data that was not extracted is not cleared.</p>"},{"location":"documentation/table-cdc/#delete-log-table-and-triggers","title":"Delete Log Table and Triggers","text":"<p>When a Table CDC extraction is no longer in use or if you need to change the structure of the source table, simply deleting the extraction is not enough. To delete the log table and all associated triggers from your SAP system, open the Table CDC extraction and click [Delete CDC resources]. </p> <p>To delete the SAP resources of multiple extractions or extractions that are already deleted, see Active CDC Watches.</p> <p>Warning</p> <p>Table change not possible  The source table cannot be changed, if any CDC-related resources connected to the source table in SAP exist.  Clear the CDC-related resources connected to the source table in SAP, see SAP Note 2284776. </p>"},{"location":"documentation/table-cdc/active-cdc-watches/","title":"Active CDC Watches","text":"<p>This page shows how to keep track of Table CDC log tables and triggers in your SAP system. The Active CDC Watches menu lists all active log tables and their DB triggers in the SAP source system. All listed Table CDC resources can be deleted from the SAP source system.</p> <p>To open the Active CDC Watches menu, click Show Active CDC Watches in the main window of the extraction type. </p>"},{"location":"documentation/table-cdc/active-cdc-watches/#active-cdc-watches","title":"Active CDC Watches","text":""},{"location":"documentation/table-cdc/active-cdc-watches/#source-table","title":"Source Table","text":"<p>Name of the source table that is tracked by the Table CDC component.</p>"},{"location":"documentation/table-cdc/active-cdc-watches/#log-table","title":"Log Table","text":"<p>Name of the log table that tracks changes in the source table.</p>"},{"location":"documentation/table-cdc/active-cdc-watches/#created-on","title":"Created on","text":"<p>Timestamp when the log table was created.</p>"},{"location":"documentation/table-cdc/active-cdc-watches/#created-by","title":"Created by","text":"<p>SAP username that was used to create the initial log table.</p>"},{"location":"documentation/table-cdc/active-cdc-watches/#rows","title":"Rows","text":"<p>Number of rows in the log table. </p>"},{"location":"documentation/table-cdc/active-cdc-watches/#show-details","title":"Show Details","text":"<p>Click [] to display more information about the DB triggers in the \"CDC watch details\" window.</p> <p></p>"},{"location":"documentation/table-cdc/active-cdc-watches/#delete-cdc-resources","title":"Delete CDC resources","text":"<p>Click [ ] to delete the log table and all associated triggers from your SAP system. It is not possible to delete the log table and the triggers of the current extraction, see Delete Log Table and DB Triggers.</p>"},{"location":"documentation/table-cdc/general-settings/","title":"General Settings","text":"<p>This page contains an overview of the settings in the window \"General Settings\". To open the general settings, click General Settings in the main window of the extraction type.</p> <p></p>"},{"location":"documentation/table-cdc/general-settings/#misc-tab","title":"Misc. Tab","text":"<p>The Misc. tab covers cache settings, column encryption and keywords of an extraction type.</p> <p></p>"},{"location":"documentation/table-cdc/general-settings/#cache-results","title":"Cache results","text":"<p>The Cache results option is only available in pull destinations, e.g., PBI, Qlik etc. Pull destinations often pull the data from SAP for several times.  To decrease the SAP server load, you can select the Cache results option, this way the pull destination pulls the data from cache and not from the SAP.</p> <p>This increases the performance and limits the impact on the SAP system.  If this behavior is not desired (for example, because the data must be always 100% up to date), the cache option must be explicitly turned off.</p>"},{"location":"documentation/table-cdc/general-settings/#keywords","title":"Keywords","text":"<p>One or more keywords (tags) can be assigned to an extraction.  Keywords can be entered directly in the keyword field. You can use these keywords to filter extractions in the \"Search Extractions\" window. To open the \"Search Extractions\" window, click [  Search] in the main window of the Designer. </p> <p>Tip</p> <p>To add keywords to multiple extractions at once, select the extractions in the main window of the Designer. Right-click + Add/Remove keywords opens the window \"Add/Remove Keywords To/From Multiple Extractions\".</p>"},{"location":"documentation/table-cdc/general-settings/#primary-key-tab","title":"Primary Key Tab","text":"<p>Table extractions inherit the primary keys from SAP. Other objects such as SAP Query, BW Cube etc. require manual setting of the primary keys.  </p> <p></p> <p>The depicted example shows the SAP object MAKT with its primary key inherited from SAP in the general settings of the Designer.  In this example the primary key consists of MANDT, MATNR and SPRAS. The primary key is also taken over in the destination. </p> <p>Note</p> <p>A defined primary key field in a table is a prerequisite for merging data. </p>"},{"location":"documentation/table-cdc/general-settings/#generate-surrogate-key-column","title":"Generate Surrogate Key Column","text":"<p>If this option is active, an additional column THEO_SURR_KEY is added to the extracted data. The THEO_SURR_KEY column contains surrogate keys that can be used as row IDs.  The surrogate keys are hash values of type signed 8 byte integer, e.g., <code>#-3008591679982390000</code>. They are generated from the selected primary key columns and the name of the SAP source that is assigned to the extraction.</p>"},{"location":"documentation/table-cdc/general-settings/#security-tab","title":"Security Tab","text":"<p>Restrict user access to the extraction. For more information, see Restrict Designer Access.</p> <p></p>"},{"location":"documentation/table-cdc/general-settings/#columns-order-tab","title":"Columns Order Tab","text":"<p>The \"Columns Order\" feature enables users to rearrange the order of result columns when running an extraction. To rearrange the result order, assign indexes to the available result columns.  Index 0 defines the first column in the result set, index 1 the second columns, etc.</p> <p></p> Option Description Enabled When this option is active, the defined column order is applied when running the extraction. Swap Swaps the index of 2 columns. All other columns keep their indexes. Insert Inserts the selected column into the selected index. All other indexes are recalculated. Reset default Restores the original column order."},{"location":"documentation/table-cdc/settings/","title":"Extraction Settings","text":"<p>This page contains an overview of the extraction settings in the Table CDC extraction type. To open the extraction settings, click Extraction Settings in the main window of the extraction type. </p> <p></p>"},{"location":"documentation/table-cdc/settings/#initial-load","title":"Initial Load","text":""},{"location":"documentation/table-cdc/settings/#package-size","title":"Package size","text":"<p>The extracted data is be split into packages of the defined size. The default value is 50000 lines. A package size between 20000 and 50000 is advisable for large amounts of data. 0 means no packaging.  Not using packaging can lead to an RFC timeout for large data extracts.</p> <p>Warning</p> <p>RFC_ERROR_SYSTEM_FAILURE - No more storage space available for extending an internal table To avoid a memory overflow on the SAP source system and to avoid huge overheads, choose a package size that suits your memory capacity.</p>"},{"location":"documentation/table-cdc/settings/#extract-data-in-background-job","title":"Extract data in background job","text":"<p>If Background job timeout (seconds) checkbox is activated, the table extraction is executed as a background job in SAP.  This setting is optional and is supported in combination with function module THEO/READ_TABLE or Z_THEO_READ_TABLE version 2.0.  Activate the setting Background job timeout (seconds) for long-running extractions with a large amounts of data that may run into a timeout error (\u201cTime limit exceeded\u201d), when using the foreground mode.</p> <p>Tip</p> <p>The extraction jobs can be found in the SAP JobLog (SM37) under the JobName theo_read_table.</p> <p>Warning</p> <p>Shared Memory ran out of memory!  If this error message pops up when running an extraction in the background, adjust the size of the Shared Memory. SAP recommends a Shared Memory size of 800MB~1.5GB for a production/test system or 2GB~4GB for S/4 systems, see SAP Support: How to solve SYSTEM_NO_SHM_MEMORY runtime error.</p>"},{"location":"documentation/table-cdc/settings/#background-job-timeout-seconds","title":"Background job timeout (seconds)","text":"<p>Sets a timeout period for extractions that run in background mode. The default value is 180 seconds. The maximum timeout value is 3600 seconds. </p> <p>This option can be used if the data transfer to a destination takes a lot of time, e.g., when bulk-inserts are deactivated for database destinations. </p> <p>Note</p> <p>The background job timeout setting only takes effect if the extractions run in background mode using /THEO/READ_TABLE.</p>"},{"location":"documentation/table-cdc/where-clause/","title":"WHERE Clause","text":"<p>A WHERE clause can be used to filter table records, see SAP ABAP Documentation: SELECT-WHERE. Enter WHERE clauses manually in Text mode or use the WHERE Clause Editor in Editor Mode.</p>"},{"location":"documentation/table-cdc/where-clause/#create-a-where-clause","title":"Create a WHERE Clause","text":"<ol> <li>Open a Table CDC extraction type. </li> <li>Navigate to the tab WHERE Clause. </li> <li>Enter a WHERE clause using the WHERE Clause Editor. </li> <li>Click [Load live preview] to display the results in the Preview section.</li> </ol>"},{"location":"documentation/table-cdc/where-clause/#where-clause-syntax","title":"WHERE Clause Syntax","text":"<p>The WHERE Clause syntax generally uses the following structure:</p> <pre><code>[Table~Column][Operator][Filter-Value]\n</code></pre> <p>Filter values in the WHERE clause must use the internal SAP representation:</p> Examples Correct Wrong Date: YYYYMMDD  19990101  01.01.1999 Year Period: YYYYPPP  1999001  001.1999 Numbers with leading zeroes, e.g., customer numbers  0000001000  1000 <p>The following operations are supported in the WHERE clause:</p> Operator Description =, EQ True if the content of operand1 is equal to the content of operand2. &lt;&gt;, NE True if the content of operand1 is not equal to the content of operand2. &lt;, LT True if the content of operand1 is less than the content of operand2. &gt;, GT True if the content of operand1 is greater than the content of operand2. &lt;=, LE True if the content of operand1 is less than or equal to the content of operand2. &gt;=, GE True if the content of operand1 is greater than or equal to the content of operand2. (NOT) LIKE True if the value of operand1 matches (does not match) the pattern in operand2. (NOT) IN True if the content of operand1 is (not) part of the content of operand2. Operand2 must be of type LIST or SQL. <p>For more details on the OpenSQL syntax, see SAP Help: Select WHERE </p> <p>Tip</p> <p>To increase extracting performance, make sure to place the indexed fields as the first selection filter operation in the WHERE clause.</p>"},{"location":"documentation/table-cdc/where-clause/#where-clause-editor","title":"WHERE Clause Editor","text":"<p>There are 2 options for adding criteria to the WHERE clause:</p> <ul> <li> <p>[Add Criteria] adds single criteria. </p> <ul> <li>The default structure for a single criteria with static values is <code>[Table~Column][Operator][Filer-Value]</code> e.g., MARC~WERKS = 1000.</li> </ul> </li> <li> <p>[Add Criteria Group] adds a group of criteria.</p> <ul> <li>The default structure for a criteria group is <code>([Table~Column1][Operator1][Filter-Value1][Boolean][Table~Column2][Operator2][Filter-Value2])</code> e.g., (MARC~PSTAT = 'L' OR MARC~PSTAT = 'LB').</li> </ul> </li> </ul> <p>Tip</p> <p>Combine multiple criteria and criteria groups to create complex filters e.g.,  MARC~WERKS = 1000 AND (MARC~PSTAT = 'L' OR MARC~PSTAT = 'LB') extracts only data where the column WERKS equals 1000 and the column PSTAT equals either 'L' or 'LB'.</p>"},{"location":"documentation/table-cdc/where-clause/#components-of-the-where-clause-editor","title":"Components of the WHERE Clause Editor","text":"<p>The following buttons and options are available in the WHERE Clause Editor:</p> Icon Component Function Delete row deletes a criteria. Move row up changes the sequence of the criteria. The selected criteria moves up. The sequence of criteria can also be changed with Drag and drop. Move row down changes the sequence of the criteria. The selected criteria moves down. The sequence of criteria can also be changed with Drag and drop. Column adds a column. Click on the component to select a column from the available tables. SQL adds an Open SQL statement, see SAP Help: Open SQL. Operator adds an operator e.g., =, &lt;, &gt;, etc. Value adds a static value of type String, Number, Flag or List. List offers a separate editor to create lists of type String, Number or Select. Select enables usage of SELECT statements. For more information, see Working with Lists in the WHERE-Clause Editor. Criteria adds a new criteria after the selected criteria. Group adds a new group of criteria the selected criteria. <p>Note</p> <p>When adding or editing a criteria only the relevant components are displayed e.g., Add Operator is only available if there is a column or SQL statement to use an operator on.</p>"},{"location":"documentation/table-cdc/where-clause/#edit-and-delete-components","title":"Edit and Delete Components","text":"<ul> <li>Click on a component to edit it. All areas that are marked green can be edited.</li> <li>To delete a component, click the (x) icon above the component.</li> </ul>"},{"location":"documentation/table-cdc/where-clause/#sap-system-fields","title":"SAP System Fields","text":"<p>You can use SAP system fields for date and time in a WHERE clause.  The usage of SAP system fields requires SAP NW 7.4 SP5 or higher and the custom function module /THEO/READ_TABLE.</p> <p>Example:</p> <ol> <li>Navigate to WHERE Clause Editor and select a column of the type Date here: BUDAT .  </li> <li>Delete the criterion \"Value\" and use the criterion \"SQL\"  .  </li> <li>Within the \"SQL\" criterion, use the supported system fields for date and time with a preceding \"@\" character, here: @sy-datum .  </li> <li>Click [Load live preview] to check the result. </li> </ol>"},{"location":"documentation/table-cdc/where-clause/#related-links","title":"Related Links","text":"<ul> <li>Knowledge Base Article: Working with Lists in the WHERE-Clause Editor</li> <li>Knowledge Base Article: LIKE Operand in WHERE Clauses</li> </ul>"},{"location":"knowledge-base/","title":"Knowledge Base","text":"<p> This section contains in-depth articles and sample use cases for Xtract Universal.</p>"},{"location":"knowledge-base/#xtract-universal","title":"Xtract Universal","text":"<ul> <li>Call Extractions via Script</li> <li>Create Extractions via Commandline</li> <li>Deploy Extractions Using Git Version Control</li> <li>Execute &amp; Schedule all Extractions using an SSIS Package</li> <li>Insert Extraction Events into Windows Logs</li> <li>Load Balancing</li> <li>SAP Access with Xtract Universal and Powershell</li> <li>Run Xtract Universal in a VM on AWS EC2</li> <li>Target Principal Field (TPN)</li> <li>Proxy Server Settings in Xtract Universal</li> <li>SharePoint Lists Notification using Intelligent Merge Procedure</li> </ul>"},{"location":"knowledge-base/#sap","title":"SAP","text":"<ul> <li>Check the Accessibility to an SAP System</li> <li>Authorize Access to Reports via Authorization Groups</li> <li>Create the Custom Authorization Object Z_TS_PROG</li> <li>Create a Client PSE to connect to SAP Cloud Systems</li> <li>Enable Secure Network Communication (SNC) via X.509 certificate</li> <li>Import an SAP Transport Request</li> <li>Register an RFC Server in SAP with Kernel Release 720 and higher</li> <li>Supported SAP S/4HANA Versions</li> </ul>"},{"location":"knowledge-base/#sso-scenarios","title":"SSO Scenarios","text":"<ul> <li>SSO with Client Certificates</li> <li>SSO with External ID</li> <li>SSO with Kerberos SNC</li> <li>SSO with Logon Ticket</li> </ul>"},{"location":"knowledge-base/#destinations","title":"Destinations","text":"<p>The following articles refer to specific destinations defined in Xtract Unviersal.</p>"},{"location":"knowledge-base/#microsoft-azure","title":"Microsoft Azure","text":"<ul> <li> Authentication via Azure Active Directory for Azure Storage</li> <li>Integration in Azure Data Factory using Commandline</li> <li>Integration in Azure Data Factory using Webservices</li> <li>Call Dynamic Extractions with Variables in ADF</li> <li>Run an ADF Pipeline when an SAP Extraction File is uploaded to Azure Storage</li> </ul>"},{"location":"knowledge-base/#microsoft-sql-server","title":"Microsoft SQL Server","text":"<ul> <li>Collation Settings for MSSQL Server Destination</li> <li>Post-Processing Column Name Style</li> </ul>"},{"location":"knowledge-base/#power-bi-report-server","title":"Power BI Report Server","text":"<ul> <li>Connect Xtract Universal to Power BI Service</li> <li>Use Computed Query Parameters for SSRS</li> </ul>"},{"location":"knowledge-base/#google-cloud","title":"Google Cloud","text":"<ul> <li>Set Up OAuth 2.0 for the Google Cloud Storage Destination</li> </ul>"},{"location":"knowledge-base/#knime","title":"KNIME","text":"<ul> <li>Dynamic Runtime Parameter in KNIME Workflows</li> </ul>"},{"location":"knowledge-base/#snowflake","title":"Snowflake","text":"<ul> <li>Integrate SAP Data into a Snowflake Data Warehouse</li> <li>SAP Integration with Matillion Data Loader</li> </ul>"},{"location":"knowledge-base/#tableau","title":"Tableau","text":"<ul> <li>Link a BEx query with a Hierarchy in Tableau</li> </ul>"},{"location":"knowledge-base/#amazon-redshift","title":"Amazon Redshift","text":"<ul> <li>Configure AnySQL Maestro to Manage Amazon Redshift</li> </ul>"},{"location":"knowledge-base/#extraction-types","title":"Extraction Types","text":"<p>The following articles refer to specific extractions types defined in Xtract Unviersal.</p>"},{"location":"knowledge-base/#bapi","title":"BAPI","text":"<ul> <li>Read Data from Cluster Fields in Tables PCL1 and PCL2 (Payroll)</li> </ul>"},{"location":"knowledge-base/#deltaq","title":"DeltaQ","text":"<ul> <li>Create Generic DataSources</li> <li>Register an RFC Server in SAP with Kernel Release 720 and higher</li> </ul>"},{"location":"knowledge-base/#report","title":"Report","text":"<ul> <li>Authorize Access to Reports via Authorization Groups</li> <li>Create the Custom Authorization Object Z_TS_PROG</li> <li>Skip Rows in Reports</li> </ul>"},{"location":"knowledge-base/#table","title":"Table","text":"<ul> <li>Change Data Capture with CDHDR</li> <li>Delta Table Extraction</li> <li>Read data from Cluster Fields in Tables PCL1 and PCL2 (Payroll)</li> <li>Working with Lists in the WHERE-Clause Editor</li> <li>Working with LIKE operand in WHERE-Clauses</li> </ul>"},{"location":"knowledge-base/#table-cdc","title":"Table CDC","text":"<ul> <li>Delta Mechanism of Table CDC</li> <li>Initial Table Load in SAP Versions &lt; 7.10</li> </ul>"},{"location":"knowledge-base/adf-integration-using-command-line/","title":"Integration in Azure Data Factory using Commandline","text":"<p>The following article describes a scenario that uses Azure Data Factory (ADF) to trigger and automate SAP data movements using Xtract Universal's command line tool. This article targets customers that utilize ADF as a platform for orchestrating data movement and transformation. </p> <p>Note</p> <p>The depicted scenario is no best practice or recommendation. The following is a suggestion of how an orchestration of Xtract Universal extractions from ADF can look like, see also Integration in Azure Data Factory using Webservices.</p>"},{"location":"knowledge-base/adf-integration-using-command-line/#prerequisites","title":"Prerequisites","text":"<ul> <li>Xtract Universal is installed on a cloud VM and is accessible remotely over http(s).</li> <li>The extraction uses a push-destination, e.g., Azure Blob Storage or Azure SQL Server. </li> <li>The extraction runs successfully when called from a remote machine via commandline, see Execute and Automate - Call via Commandline. This ensures that the XU server is reachable. </li> <li>Access to the Azure portal and Azure Data Factory.</li> <li>Knowledge on how to build ADF pipelines.</li> </ul>"},{"location":"knowledge-base/adf-integration-using-command-line/#configure-an-azure-batch-account","title":"Configure an Azure Batch Account","text":"<p>When creating a batch account in the Azure portal, make sure to consider the following:</p> <ul> <li>A Storage account needs to be associated with your Batch account.  This can a be new storage account dedicated to batch processing, or an existing storage account. Microsoft recommends a general-purpose v2 storage account in the same region as your Batch account (for better performance).</li> <li>The Pool allocation mode under Advanced can be the default Batch service (no need to select User subscription). </li> </ul> <p>For information on how to configure a batch account in the Azure portal, see Microsoft Documentation: Create a Batch account in the Azure portal.</p>"},{"location":"knowledge-base/adf-integration-using-command-line/#add-a-pool-to-the-azure-batch-account","title":"Add a Pool to the Azure Batch Account","text":"<p>The pool provides the computing resources (VM) to execute a task, in this case running the commandline tool xu.exe.  When creating a pool from a managed image in the Azure portal, make sure to consider the following:</p> <ul> <li>The commandline tool xu.exe is not a very resource-intensive application, but if Azure Batch is used for other processing, choose an appropriately sized resource for your needs. Note that there is an Azure cost associated with the selected Pool. The depicted example uses a Window Server 2019 Datacenter with small disk configuration. </li> <li>When creating the pool, set the Scale property Target dedicated nodes to at least 1.</li> </ul> <p>For information on how to create a pool, see Microsoft Documentation: Use a managed image to create a custom image pool.</p>"},{"location":"knowledge-base/adf-integration-using-command-line/#upload-xuexe-to-storage-account","title":"Upload xu.exe to Storage Account","text":"<p>Follow the steps below to make the command line tool xu.exe available in Azure:</p> <ol> <li>Create a container for the Xtract Universal commandline tool in the Azure storage account associated with the Azure Batch account. In the depicted example, the container is named \u2018xuexe\u2019. </li> <li>Upload the files xu.exe and xu.exe.config from the Xtract Universal server installation to the Azure storage account. The files are located in <code>C:\\Program Files\\XtractUniversal</code>.</li> </ol> <p>Note</p> <p>Do not confuse the xu.exe.config file with the xu.config file.</p>"},{"location":"knowledge-base/adf-integration-using-command-line/#create-a-linked-service-to-azure-batch-in-adf","title":"Create a Linked Service to Azure Batch in ADF","text":"<p>Follow the steps below to create a Batch Linked Service and a Storage Linked Service in Azure Data Factory:</p> <ol> <li>In ADF, navigate to Manage &gt; Connections &gt; Linked Services and click [New] . The menu \"New linked service\" opens.  </li> <li>In the tab Compute category, select Azure Batch  and click [Continue] .</li> <li>Specify the Batch Account, Access Key, Batch URL and Pool name of the batch account. The data is available in the key settings of the batch account. </li> <li>In Storage linked service name, select New to create a new linked service that references the storage account that contains the xu.exe file in the linked service.  </li> </ol>"},{"location":"knowledge-base/adf-integration-using-command-line/#create-an-adf-pipeline-with-custom-activity","title":"Create an ADF Pipeline with Custom Activity","text":"<p>Follow the steps below to create a pipeline that runs extractions:</p> <ol> <li>Create a new Pipeline in ADF.</li> <li>Drag the Custom Activity under Batch Service into your pipeline.   </li> <li>In the General tab, provide a name for the activity, e.g., \u2018KNA1\u2019  .</li> <li>In the Azure Batch tab, reference the Batch Linked Service from Create a Linked Service to Azure Batch in ADF. </li> <li>In the Settings tab, specify the xu.exe command that you want to execute  , e.g., <code>xu.exe [protocol]://[host or IP address]:[port]/?name=[name of the extraction]</code> to run an extraction.  </li> <li>Reference the Storage Linked Service from Create a Linked Service to Azure Batch in ADF in the Advanced Settings .</li> <li>Specify the container / folder  path where the xu.exe file is located in the Azure storage account .</li> <li>Click [Debug] to testrun the SAP data extraction.</li> </ol> <p>When the activity is finished, review the output of the activity in the Output tab. If the exitcode from xu.exe is 0, the data extraction was successful and the following folders / files are available in the Azure storage account:</p> <ul> <li>the storage account contains a folder adfjobs.</li> <li>for every pipeline execution, there is a subfolder with log information.</li> <li>the files stderr.txt and stdout.txt contain the output from xu.exe.</li> </ul> <p> </p> <p>The pipeline can be run in debug mode or can be triggered via a scheduler. </p>"},{"location":"knowledge-base/adf-integration-using-command-line/#related-links","title":"Related Links","text":"<ul> <li>Call Dynamic Extractions with Variables in ADF.</li> <li>Integration in Azure Data Factory using Webservices</li> </ul>"},{"location":"knowledge-base/adf-integration-using-webservices/","title":"Integration in Azure Data Factory using Webservices","text":"<p>The following article describes a scenario that uses Azure Data Factory (ADF) to trigger and automate SAP data movements using Xtract Universal's webservices.  This article targets customers that utilize ADF as a platform for orchestrating data movement and transformation. </p> <p>Note</p> <p>The depicted scenario is no best practice or recommendation. The following is a suggestion of how an orchestration of Xtract Universal extractions from ADF can look like, see also Integration in Azure Data Factory using Commandline.</p>"},{"location":"knowledge-base/adf-integration-using-webservices/#prerequisites","title":"Prerequisites","text":"<ul> <li>A self-hosted Integration runtime is set up on the server Xtract Universal runs on.  This ensures that Xtract Universal's web server is accessible from ADF over http(s).   </li> <li>The extraction uses a push-destination, e.g., Azure Blob Storage or Azure SQL Server. </li> <li>The extraction runs successfully when called from a web browser, see Web-API.</li> <li>Access to Azure Data Factory.</li> <li>Knowledge on how to build ADF pipelines.</li> </ul>"},{"location":"knowledge-base/adf-integration-using-webservices/#basic-principles","title":"Basic Principles","text":"<p>The depicted scenario builds upon the following basic principles:</p> <ul> <li>Xtract Universal offers a Web-API through which various actions can be performed via http(s) calls. The depicted scenario uses the web API to:</li> <li>Microsoft's self-hosted Integration runtime enables access to on-prem resources, such as Xtract Universal, from ADF.</li> <li>Microsoft's ADF offers a Web Activity that allows calling resources via http(s) and a self-hosted Integration runtime.</li> </ul> <p>The depicted scenario uses two ADF pipelines to run extractions from ADF:</p> <ul> <li>a Child Pipeline that extracts data from SAP.</li> <li>a Master pipeline that executes the child pipeline for different extractions.</li> </ul>"},{"location":"knowledge-base/adf-integration-using-webservices/#child-pipeline","title":"Child Pipeline","text":"<p>Follow the steps below to create a child pipeline that extracts data from SAP:</p> <ol> <li>Run an extraction using a web activity  , see Web-API - Run Extractions. </li> <li>Query the extraction status in regular intervals using a web activity  , see Web-API - Get Status of an Extraction.  </li> <li>Add a condition that checks the extraction status   and executes follow up activities in case the extractions fails. Example: When the extraction fails, use a web activity to query the extraction log, see Web-API - Get Extraction Logs, and write the logs to an Azure Blob Storage account.  A follow up event can then be triggered by the Storage event, e.g., sending a notification email. </li> </ol> <p>The pipeline functions as a standalone solution. It can be run in debug mode or can be triggered via a scheduler. </p>"},{"location":"knowledge-base/adf-integration-using-webservices/#master-pipeline","title":"Master Pipeline","text":"<p>Follow the steps below to create a master pipeline that executes the child pipeline multiple times, each time for a different extraction. This allows automatic iteration through all extractions defined in Xtract Universal. </p> <ol> <li>Query a list of extractions   using a web activity, see Web-API - Get Extraction Details. </li> <li>Loop over the list of extractions  . </li> <li>In the loop, pass the name of the current extraction to the Child pipeline and execute the Child pipeline for that extraction. </li> </ol>"},{"location":"knowledge-base/adf-integration-using-webservices/#variables-and-parameters","title":"Variables and Parameters","text":"<p>Parameters and variables are used in both pipelines:</p> <ul> <li>Parameters provide constant values that are used in multiple activities. </li> <li>Variables provide dynamic values at runtime and are used to pass on data between different activities or pipelines.</li> </ul> <p>The following parameters and variables are used in the depicted scenario:</p> Parameter / Variable Name Data Type Defined in Description Parameter p_global_XU_HOST String global This parameter contains the base URL of the Xtract Universal webserver, here: <code>https://MyOnPremXuServer.theobald.local:8165</code>. The parameter is used in every Web Activity. Variable v_XU_extractions_array Array Master pipeline This variable stores the list of XU extractions returned by Web activity Get_List_of_XU_extractions. The variable's value is set in the Set variable activity Set variable_extraction array. Parameter p_extractionName_from_Master String Child pipeline This parameter takes on the value (extraction name) of the current iteration For Each activity *ForEach extraction in v_extraction array. As a default name, you assign a name of an extraction. This allows running the Child pipeline w/o being triggered from the Master pipeline. Variable v_TIMESTAMP String Child pipeline This variable stores the extraction's timestamp returned by Web activity XU_START_JOB. The variable\u2019s value is set in the Set variable activity TIMESTAMP. The variable is later used in Web activities CHECK_XU_JOB_STATUS and XU_Get_Extraction_Log. Variable v_JOB_STATUS String Child pipeline This variable stores the extraction's run status returned by Web activityCHECK_XU_JOB_STATUS. The variable\u2019s value is set in the Set variable activity JOB_STATUS. As long as the variable has the status \"Running\", the Until activity IS_JOB_RUNNING is executed. Other values this variable can can have are \"FinishedNoErrors\" and \"FinishedErrors\". Variable v_Log String Child pipeline This variable stores the extraction's log returned by Web activityXU_Get_Extraction_Log. The variable\u2019s value is set in the Set variable activity Set_variable_XU_Log. The value of this variable is appended to the log file in the Copy data activity Copy Extraction Log to Blob. <p>For more information on variables in ADF, see Call Dynamic Extractions with Variables in ADF.</p>"},{"location":"knowledge-base/adf-integration-using-webservices/#download-json-templates","title":"Download JSON Templates","text":"<p>Downloads for the child and master pipeline are provided below:</p> <p> Download CHILD pipeline as json  Download MASTER pipeline as json</p>"},{"location":"knowledge-base/adf-integration-using-webservices/#related-links","title":"Related Links","text":"<ul> <li>Call Dynamic Extractions with Variables in ADF.</li> <li>Integration in Azure Data Factory using Commandline</li> </ul>"},{"location":"knowledge-base/adjust-column-name-style/","title":"Post-Processing Column Name Style","text":"<p>The following section describes a common business scenario to rename column name styles within the Microsoft SQL-Server environment. The given example shows how to use Custom SQL in the Finalization step of the database transaction within the Xtract Universal destination settings.</p>"},{"location":"knowledge-base/adjust-column-name-style/#about-column-name-styles","title":"About Column Name Styles","text":"<p>Xtract Universal offers 4 different Column Name Styles for naming the SAP table columns in databases:</p> <ul> <li>Code - <code>[FieldName]</code></li> <li>Prefixed Code - <code>[TabName]~[FieldName]</code></li> <li>CodeAndText - <code>[FieldName]_[FieldDescription]</code></li> <li>TextAndCode - <code>[FieldDescription]_[FieldName]</code></li> </ul> <p>The depicted example uses the Column Name Style 'Prefixed Code', which connects each table field in the format [TabName][ColumnName] with the SAP standard separator '~'.  This naming of table columns is mainly used for table joins, because identical column identifiers exist in the different tables.  A typical example is the table join of 'EKKO' (Purchasing Document Header) and 'EKPO' (Purchasing Document Item).  Both tables have the following identical column descriptions: </p> <ul> <li>'MANDT'</li> <li>'EBELN'</li> </ul> <p>When selecting the standard Column Name Style 'Code' in the destination settings, the following error occurs on the SQL side when selecting these fields:</p> <pre><code>&gt; System.Data.SqlClient.SqlException (0x80131904): Column names in each table must be unique. Column name 'MANDT' in table 'EKKO_JOIN' is specified more than once.\n</code></pre>"},{"location":"knowledge-base/adjust-column-name-style/#adjust-standard-separator-using-custom-sql","title":"Adjust Standard Separator using Custom SQL","text":"<p>Follow the steps below to adjust SAP standard separator from '~' to '_':</p> <ol> <li>Adjust the Column Name Style e.g. 'PrefixedCode'  . </li> <li> <p>Insert the generic SQL Code below into the Finalization  step using [Edit SQL].</p> <pre><code>declare @table_name nvarchar(128) = '#{ Extraction.TableName }#'\ndeclare @old_name nvarchar(128)\ndeclare @new_name nvarchar(128)\n\ndeclare cur CURSOR LOCAL for\n    select COLUMN_NAME\n    from INFORMATION_SCHEMA.COLUMNS\n    where TABLE_NAME = @table_name\n\nopen cur\n\nwhile (1 = 1)\nbegin\n    fetch next from cur into @old_name\n    IF @@FETCH_STATUS != 0 BREAK\n\n    SET @new_name = REPLACE(@old_name, '~', '_')\n    SET @old_name = '[' + @table_name + '].[' + @old_name + ']'\n    EXEC sp_rename @old_name, @new_name, 'COLUMN'\nend\n\nclose cur\ndeallocate cur\n</code></pre> </li> <li> <p>Click [OK] to confirm your input  . </p> </li> <li>Run the extraction. </li> <li>Check the Column Name Style changes and results in SQL Server Management Studio (SSMS). </li> </ol>"},{"location":"knowledge-base/adjust-column-name-style/#create-stored-procedure-sp-using-ssms","title":"Create Stored Procedure (sp) using SSMS","text":"<p>Create a stored procedure that contains above mentioned T-SQL code for renaming column name styles and call this stored procedure in the Finalization step of the destination settings.  This approach allows to easily change the renaming logic within the DB or SQL server instance. You only have to adapt the stored procedure instead of each Finalization step.</p> <p>Follow the steps below to adjust the SAP standard separator from '~' to '_':</p> <ol> <li>Create T-SQL Stored Procedure using SQL Server Management Studio.  For more information, see Microsoft Documentation: Create a stored procedure.</li> <li>Assign a name for the Stored Procedure e.g., ColumnNameStyle. </li> <li> <p>Insert the SQL-Code below and [Execute] the statement to save the process.</p> <pre><code>CREATE PROCEDURE ColumnNameStyle \n    @table_name nvarchar(128)\nAS \n\nBEGIN\n\ndeclare @old_name nvarchar(128)\ndeclare @new_name nvarchar(128)\ndeclare cur CURSOR LOCAL for\n\n    select COLUMN_NAME\n        from INFORMATION_SCHEMA.COLUMNS\n        where TABLE_NAME = @table_name\n\nopen cur\nwhile (1 = 1)\nbegin\n\n    fetch next from cur into @old_name\n    IF @@FETCH_STATUS != 0 BREAK\n    SET @new_name = REPLACE(@old_name, '~', '_')\n    SET @old_name = '[' + @table_name + '].[' + @old_name + ']'\n    EXEC sp_rename @old_name, @new_name, 'COLUMN'\n\nend\nclose cur\ndeallocate cur\n\nEND\n</code></pre> </li> <li> <p>Open the destination settings in Xtract Universal and select a Column Name Style, e.g., 'PrefixedCode'  . </p> </li> <li> <p>Insert following SQL Code into the Finalization  step using [Edit SQL].</p> <pre><code>EXEC ColumnNameStyle '#{ Extraction.TableName }#'\n</code></pre> </li> <li> <p>Click [OK] to confirm your input. </p> </li> <li>Execute the selected extraction and check the Column Name Style changes and results in SSMS.</li> </ol>"},{"location":"knowledge-base/authentication-via-azure-ad-with-azure-storage/","title":"Authentication via Azure Active Directory for Azure Storage","text":"<p>The following article shows how to connect to the Azure Storage destination using Authentication via Azure Active Directory. The article leads you through the following process:</p> <ol> <li>Register a new app with your Azure AD tenant.</li> <li>Assign access rights for the new app in Azure Storage using the Storage Blob Data Contributor role.</li> <li>In Xtract Universal, connect to Azure Storage using the Azure Active Directory method.</li> </ol>"},{"location":"knowledge-base/authentication-via-azure-ad-with-azure-storage/#app-registration","title":"App Registration","text":"<p>Follow the steps below to register a new app with your Azure AD tenant:</p> <ol> <li>Open the Azure portal and navigate to App Registrations.</li> <li>Click [New registration] to register a new app with your Azure AD tenant.  </li> <li>Enter the name of the application.</li> <li>In the Redirect UI section, select Public Client /native (mobile and desktop) and assign <code>https://login.microsoftonline.com/common/oauth2/nativeclient</code> as the redirect URI.</li> <li>Click Register. </li> <li>Open the new application and navigate to API Permissions &gt; Add a permission &gt; Azure Storage. </li> <li>Click Grant admin consent. </li> </ol>"},{"location":"knowledge-base/authentication-via-azure-ad-with-azure-storage/#access-rights-in-azure-storage","title":"Access Rights in Azure Storage","text":"<p>Follow the steps below to assign access rights for the new Azure app in Azure Storage using the Storage Blob Data Contributor role:</p> <ol> <li>Open the Azure portal and navigate to Access Control (IAM). </li> <li>Click [Add role assignment]. </li> <li>Select the Storage Blob Data Contributor role and click [Next].  </li> <li>Click + Select members and add the new Azure app created in App Registration to the members. </li> <li>Click [Next] to continue, then click [Review + assign] to assign the access rights. </li> </ol>"},{"location":"knowledge-base/authentication-via-azure-ad-with-azure-storage/#connect-to-azure-storage","title":"Connect to Azure Storage","text":"<p>Follow the steps below to connect Xtract Universal to the Azure Storage destination using Authentication via Azure Active Directory:</p> <ol> <li>Open Xtract Universal and create a new Azure Storage destination or edit an existing destination.</li> <li>Select the connection type Azure active directory. </li> <li>Enter the name of your storage account. </li> <li>Copy and paste the Application (client) ID and the Directory (tenant) ID from the Azure app created in App Registration. </li> <li>Click [Connect]. The window \"Azure OAuth 2.0\" opens.</li> <li>When prompted, pass your Active Directory credentials and click [Accept].  </li> <li>If the connection is successful, a \"Connection successful\" message is displayed in a pop-up window.</li> </ol>"},{"location":"knowledge-base/authorize-access-to-specific-reports/","title":"Authorize Access to Reports via Authorization Groups","text":"<p>The following article shows how to set up access to reports by assigning authorization groups to reports. Access is then granted through the S_PROGRAM authorization object, see SAP Note 338177.</p>"},{"location":"knowledge-base/authorize-access-to-specific-reports/#set-up-access-to-specific-reports","title":"Set Up Access to Specific Reports","text":"<ol> <li>Log into SAP and use transaction code SE38 to open the ABAP Editor.</li> <li>Enter the name of the report you want to access and select Attributes as the Subobjects.</li> <li>Click [Change]. A window that contains the program attributes opens.</li> <li>Assign an authorization group. </li> <li>Edit or create a user role you want to grant access to (transaction code PFCG).</li> <li>Manually assign the authorization object S_PROGRAM to the user role. </li> <li>Select the actions SUBMIT and BTCSUBMIT in the S_PROGRAM object field P_ACTION.</li> <li>Assign the same authorization group that is assigned to the report to the S_PROGRAM object field P_GROUP. </li> <li>Save and generate the authorization.</li> <li>Assign the user role to users.</li> </ol>"},{"location":"knowledge-base/authorize-access-to-specific-reports/#related-links","title":"Related Links","text":"<ul> <li>Create the Custom Authorization Object Z_TS_PROG</li> <li>SAP Authorization Objects for Reports</li> <li>Documentation: Report</li> </ul>"},{"location":"knowledge-base/call-dynamic-extractions-with-variables-in-adf/","title":"Call Dynamic Extractions with Variables in ADF","text":"<p>The following article shows how to call Xtract Universal extractions dynamically from Azure Data Factory (ADF) using user-defined variables. The depicted example runs extractions daily to write data added or updated on the day before to the destination.</p>"},{"location":"knowledge-base/call-dynamic-extractions-with-variables-in-adf/#call-dynamic-extractions-with-variables","title":"Call Dynamic Extractions with Variables","text":"<p>The depicted example calls an extraction with a date parameter in ADF. The date parameter is set dynamically using a user-defined variable.</p> <ol> <li>Create an extraction in Xtract Universal that uses runtime parameters. The depicted example uses an extraction called 0COSTCENTER_0101_HIER with a date parameter called myDate. </li> <li>Create a pipeline in ADF that stores yesterday's date in a variable  . </li> <li>Format the date to the internal SAP date format (YYYYMMDD). The type and format of the input variable must match the type and format of the actual parameter in Xtract Universal. </li> <li>Add a web activity that calls extractions  . The URL used to call static extractions has the following format: <code>[Protocol]://[HOST or IP address]:[Port]/?name=[Name of the Extraction]</code></li> <li>To set runtime parameters of an extraction, add the corresponding variables to the extraction URL using the @concat command.  The concatenated string has the following format: <code>@concat('[Protocol]://[HOST or IP address]:[Port]/?name=[Name of the Extraction]&amp;[Name of the Parameter in XU]=',variables('[Name of the Variable in ADF'))</code> </li> <li>Run the pipeline and check the result.</li> </ol> <p>Tip</p> <p>You can copy the URL of an extraction from the Run window in Xtract Universal, see Documentation: Run an Extraction.</p>"},{"location":"knowledge-base/call-dynamic-extractions-with-variables-in-adf/#related-links","title":"Related Links","text":"<ul> <li>Run an ADF pipeline when an SAP extraction file is successfully uploaded to Azure storage</li> <li>Integration in Azure Data Factory using Webservices</li> <li>Integration in Azure Data Factory using Command Line</li> <li>Documentation: Web API</li> </ul>"},{"location":"knowledge-base/call-extraction-via-script/","title":"Call Extractions via Script","text":"<p>This section shows how to call an extraction from a Windows script (.bat) or PowerShell script using the command line tool xu.exe.</p>"},{"location":"knowledge-base/call-extraction-via-script/#call-via-windows-script-bat","title":"Call via Windows script (.bat)","text":"<p>Follow the steps below to run an extraction using a Windows script that calls the command line tool xu.exe.</p> <ol> <li>Create a new batch file.</li> <li> <p>Define the following variables:</p> <ul> <li>Standard output (XUOutputfile)</li> <li>Standard error output (XULogfile)</li> <li>Path to the command line tool (XUCmd)</li> <li>XU server name (XUServer)</li> <li>XU server port (XUPort) </li> <li>Name of the extraction (XUExtraction)</li> </ul> <pre><code>:: Execute an Xtract Universal extraction using the command tool xu.exe\n:: clear screen  \ncls\n:: Turns off the command echoing feature\n@echo off\n:: write the output to a file\nset XUOutputfile=\"C:\\Data\\xubatch\\output.txt\"\n:: write the log to a file\nset XULogfile=\"C:\\Data\\xubatch\\log.txt\"\n:: set the path to the installation folder\nset XUCmd=\"C:\\Program Files\\XtractUniversal\\xu.exe\"\n:: default is also localhost, so you skip it or change it  \nset XUServer=localhost\n:: default port is also 8065, so you skip it or change it  \nset XUPort=8065\nset XUExtraction=customers \n</code></pre> </li> <li> <p>If the extraction requires input parameters, set dynamic parameters, e.g., v_country for the language key.</p> <pre><code>:: the extraction has a variable Country that needs a country code of length 2, e.g. US\n:: Skip this block if you don't use variable  \nset v_country=US\n:: Turns on the command echoing feature\n@echo on\n</code></pre> </li> <li> <p>Run the extraction by calling the command line tool with the corresponding parameters.</p> Run a single extractionRun a list of extractions <pre><code>:: run the command tool with the right parameters\n%XUCmd% -s %XUServer% -p %XUPort% -n %XUExtraction% -o Country=%v_country% 1&gt;%XUOutputfile% 2&gt;%XULogfile%\n</code></pre> <pre><code>@echo off \n:: create an array with extraction names separated by empty space \n:: in this example there are two extractions named *customers* and *materials*.\nset extraction_list=customers materials \n:: alternative \n:: set extraction_list[0] = customers \n:: set extraction_list[1] = materials \n@echo on\n\nfor %%e in (%extraction_list%) do ( \n    %XUCmd% -s %XUServer% -p %XUPort% -n %%e 1&gt;&gt;%XUOutputfile% 2&gt;&gt;%XULogfile%\n)\n\n:: The output in this example is added to the existing file with &gt;&gt;.\n</code></pre> </li> <li> <p>Check the return code and write a corresponding message. The return code 0 indicates a successful execution. Other Return Codes indicate errors during execution.</p> <pre><code>:: check the last exit code\n:: 0: successful\n:: else unsuccessful\n@echo off \nIF %ERRORLEVEL% EQU 0 ( \n echo extraction %XUExtraction% is successful \n) ELSE (\n echo extraction %XUExtraction% is not successful. Error Code %ERRORLEVEL%. See log for details.\n)\n@echo on\n</code></pre> </li> <li> <p>Optional: extractions can be added to the Windows logs. They can be displayed in the Event Viewer.</p> </li> </ol>"},{"location":"knowledge-base/call-extraction-via-script/#call-via-powershell-script","title":"Call via PowerShell Script","text":"<p>Follow the steps below to run an extraction using a PowerShell script that calls the command line tool xu.exe.</p> <ol> <li> <p>Define the following variables:</p> <ul> <li>Standard output (XUOutputfile)</li> <li>Standard error output (XULogfile)</li> <li>Path to the command line tool (XUCmd)</li> <li>XU server name (XUServer)</li> <li>XU server port (XUPort) </li> <li>Name of the extraction (XUExtraction)</li> </ul> <pre><code># Execute an Xtract Universal extraction using the command tool xu.exe \n# clear screen  \nclear\n# write the output to a file\n$XUOutputfile = \"C:\\Data\\powershell\\output.txt\"\n# write the log to a file\n$XULogfile = \"C:\\Data\\powershell\\log.txt\"\n# set the path to the installation folder\n$XUCmd = 'C:\\Program Files\\XtractUniversal\\xu.exe'\n$XUServer = \"localhost\"\n$XUPort = \"8065\"\n$XUExtraction = \"SAPSalesCube\" \n</code></pre> </li> <li> <p>If the extraction requires input parameters, set dynamic parameters, e.g., myCalendarMonth for the current month in the format \"yyyyMM\".</p> <pre><code># the extraction has a variable CalendarMonth that needs a value in the format \"yyyyMM\", e.g. 201712\n# Skip this block if you don't use variable\n# generate the calender month from the current date to be used as a variable\n# e.g. Tuesday, December 19, 2017 10:40:32 AM\n\n$myyear = (Get-Date -format \"yyyy\")\n$mymonth = (Get-Date -format \"MM\")\n\n# 201712\n$myCalendarMonth = \"$myyear$mymonth\"\n# another option Get-Date -format \"yyyyMM\"\n# just if you use variables\n# the extraction has a variable CalendarMonth, its value has the format YYYYMM\n# set the variable for calendar month e.g. 201712\n</code></pre> </li> <li> <p>Run the extraction by calling the command line tool with the corresponding parameters.</p> <pre><code>:: run the command tool with the right parameters\n%XUCmd% -s %XUServer% -p %XUPort% -n %XUExtraction% -o Country=%v_country% 1&gt;%XUOutputfile% 2&gt;%XULogfile%\n</code></pre> </li> <li> <p>Check the return code and write a corresponding message. The return code 0 indicates a successful execution. Other Return Codes indicate errors during execution.</p> <pre><code># check the last exit code\n# 0: successful\n# else unsuccessful\nif($LASTEXITCODE -eq 0) {           \nwrite-host -f Green \"The last command executed successfully\"          \n} else {           \nwrite-host -f Red \"The last execution failed with error code $LASTEXITCODE!\"\nwrite-host $errorMessage\n}\n</code></pre> </li> </ol> <p>For more examples on how to use PowerShell scripts with Xtract Universal, see SAP Access with Xtract Universal and Powershell</p>"},{"location":"knowledge-base/call-extraction-via-script/#related-links","title":"Related Links:","text":"<ul> <li>Create Extractions via Commandline</li> <li>Insert Extraction Events into Windows Logs</li> <li>SAP Access with Xtract Universal and Powershell</li> </ul>"},{"location":"knowledge-base/change-data-capture-with-cdhdr/","title":"Change Data Capture with CDHDR","text":"<p>The following article shows how to load data incrementally (daily) from an SAP Table with no delta pointers / date fields. The depicted example scenario uses two tables:</p> <ul> <li>MAKT (Material Descriptions), which has no date fields.</li> <li>CDHDR (Change Documents Header), which holds the header information of the changed records. CDHDR is used to determine the delta information of MAKT and other tables.</li> </ul>"},{"location":"knowledge-base/change-data-capture-with-cdhdr/#prerequisites","title":"Prerequisites","text":"<ul> <li>Prepare a record for the delta extraction: Change the description of a material in MAKT, e.g., change the description \u201cABC\u201d of material 2593 to \"Test_delta\u201d. </li> <li>Check if CDHDR registered the change: Filter the field UPDATE for today's date. The change made in MAKT should be listed. </li> </ul>"},{"location":"knowledge-base/change-data-capture-with-cdhdr/#daily-data-extraction","title":"Daily Data Extraction","text":"<p>The following steps describe how to only extract the data from MAKT that has been changed on today\u2019s date.</p> <ol> <li>Create a new Table extraction.</li> <li>Look up the tables MAKT and CDHDR. </li> <li>Select the fields OBJECTID and UDATE from CDHDR for the output.<ul> <li>OBJECTID contains information about the Key on which the changes are made.  This field is used for joining the tables and to get the delta data from MAKT.</li> <li>UDATE contains the date on which updates occurred. This field is used to filter the data for specific dates.</li> </ul> </li> <li>Select the fields you want to extract from MAKT for the output (MATNR is mandatory).</li> <li>Open the tab Joins and click [Add]. The window \"Join\" opens.  </li> <li>Select the join type INNER_JOIN to combine the tables CDHDR and MAKT. The OBJECTID from CDHDR and MATNR from MAKT have same entries and thus form an inner join condition.</li> <li>Click [Add] and confirm your selection with [OK].</li> <li>Open the tab WHERE Clause and enter the following filter criteria: <pre><code>CDHDR~UDATE = '#{ DateTime.Now.ToString(\"yyyyMMdd\") }#'\n</code></pre> This criteria uses script expressions to get the current date in the SAP format (\"yyyyMMdd\"). </li> <li>Click [Load live review] to check the results. Only the data in MAKT that has been changed on today's date is extracted.</li> <li>Schedule the extraction daily. </li> </ol> <p>Tip</p> <p>To extract all changes of the day before, change the WHERE clause to <code>CDHDR~UDATE &gt;= '#{ DateTime.Now.AddDays(-1).ToString(\"yyyyMMdd\") }#'</code> and schedule the extraction every night at 1p.m. or later. </p>"},{"location":"knowledge-base/change-data-capture-with-cdhdr/#related-links","title":"Related Links","text":"<ul> <li>Delta Table Extraction</li> <li>Table: WHERE Clause</li> </ul>"},{"location":"knowledge-base/check-the-accessibility-to-an-sap-system/","title":"Check the Accessibility to an SAP System","text":"<p>The following article shows how to check the accessibility of an SAP system using the paping.exe tool. Alternatively, you can also use the Microsoft telnet tool.</p> <p>Sometimes the firewall is blocking the traffic to the SAP System. This could be a local firewall, but also a firewall in the destination network. You can use the paping.exetool to ping the port and to check if the firewall is open.</p>"},{"location":"knowledge-base/check-the-accessibility-to-an-sap-system/#ping-the-sap-system","title":"Ping the SAP System","text":"<p>Use the following syntax with paping.exe:</p> General SyntaxExample <pre><code>paping.exe SAPServer -p port -c 3*\n</code></pre> <pre><code>paping.exe 192.168.0.9 -p 3305 -c 3\n</code></pre> <p></p>"},{"location":"knowledge-base/check-the-accessibility-to-an-sap-system/#port-numbers","title":"Port Numbers","text":"<p>If an SAP-Router is used, the ports are 3299 and 3399.  If not, the ports are 32XX and 33XX. XX is usually the system number, e.g., 00, or 05.</p> <p>Some important port numbers:</p> <pre><code>sapdp00 3200/tcp # SAP Dispatcher. 3200 + Instance-Number\nsapgw00 3300/tcp # SAP Gateway. 3300 + Instance-Number\nsapsp00 3400/tcp # 3400 + Instance-Number\nsapms00 3500/tcp # 3500 + Instance-Number\nsapmsSID 3600/tcp # SAP Message Server. 3600 + Instance-Number\nsapgw00s 4800/tcp # SAP Secure Gateway 4800 + Instance-Number\n</code></pre>"},{"location":"knowledge-base/collation-sql-server/","title":"Collation Settings for MSSQL Server Destination","text":"<p>The following article describes a common problem that occurs when pushing SAP data into an SQL server database when collation is not set to case-sensitive. The depicted example shows how to customize the Drop &amp; Create SQL server statement within the Xtract Universal destination settings to accommodate these issues.</p>"},{"location":"knowledge-base/collation-sql-server/#about-collation-sql-server","title":"About Collation SQL Server","text":"<p>Collations in SQL Server provide sorting rules, case, and accent sensitivity properties for your data. Collations that are used with character data types, such as char and varchar, difine the code page and corresponding characters that can be represented for the corresponding data type. </p> <p>Collation can be set up on three different levels:</p> <ul> <li>Server collation</li> <li>Database collation</li> <li>Column collation</li> </ul> <p>MSSQL server offers different collation statements. The following excerpts provide the necessary adaptions for the given example:</p> Option Description Case-sensitive (_CS) Distinguishes between uppercase and lowercase letters. If this option is selected, lowercase letters sort ahead of their uppercase versions. If this option isn't selected, the collation is case-insensitive. Which means, SQL Server considers the uppercase and lowercase versions of letters to be identical for sorting purposes. You can explicitly select case insensitivity by specifying _CI. Accent-sensitive (_AS) Distinguishes between accented and unaccented characters. For example, \"a\" is not equal to \"\u1ea5\". If this option isn't selected, the collation is accent-insensitive. Which means, SQL Server considers the accented and unaccented versions of letters to be identical for sorting purposes. You can explicitly select accent insensitivity by specifying _AI. <p>For information on usable collations, see Microsoft Documentation: Collation and Microsoft Documentation: Collation and Unicode support.  The depicted example shows the column collation within Xtract Universal with a Custom SQL statement.</p>"},{"location":"knowledge-base/collation-sql-server/#sql-server-management-studio-ssms","title":"SQL Server Management Studio (SSMS)","text":"<p>Check the database settings. The following collation statement is displayed: <code>Latin1_General_100_CI_AI</code>.</p> Option Description _CI case-insensitive _AI accent-insensitive <p></p>"},{"location":"knowledge-base/collation-sql-server/#setup-in-xtract-universal","title":"Setup in Xtract Universal","text":"<p>Follow the steps below to extract the SAP table MAKT from SAP:</p> <ol> <li>Create a table extraction, see Documentation: Table. The look-up process loads the corresponding metadata from our SAP object MAKT: <ol> <li>The composite primary key consists of the table fields MANDT, MATNR, SPRAS, each with a unique constraint. </li> <li>The SAP field SPRAS is of data type LANG with a length 1. </li> </ol> </li> <li>Create a simple WHERE clause, e.g., <code>MATNR = '000000000000000038' AND ( SPRAS  = 'd' OR SPRAS = 'D' )</code>.</li> <li>Click [Load live preview].  The results show that the SAP database interprets the data records with upper-case 'D' and lower-case 'd' in the field SPRAS as different data records.</li> <li>Assign an MSSQL server destination to the extraction and click [Run].</li> </ol> <p>The MSSQL server returns the following error:</p> <pre><code>&gt; System.Data.SqlClient.SqlException (0x80131904): Violation of PRIMARY KEY constraint 'PK__makt__3483F06C110B42CD'. \n&gt; Cannot insert  duplicate key in object 'dbo.makt'.The duplicate key value is (800, 000000000000000038, d)\n</code></pre>"},{"location":"knowledge-base/collation-sql-server/#workaround","title":"Workaround","text":"<p>As shown in Setup in Xtract Universal the data of MAKT cannot be pushed into the MSSQL server destination due to the collation statement of the database.  In this case, the user has to customize the SQL statement Preparation of the MSSQL destination settings.</p> <ol> <li>Change the default value Drop &amp; Create to Custom SQL.</li> <li>Click [Edit SQL] to enter an SQl statement.</li> <li>Select the Drop &amp; Create entry from the drop-down menu and click on [Generate Statement] for table MAKT.</li> <li> <p>Customize the column collation for field SPRAS using the following code:</p> <pre><code>IF (object_id('MAKT') IS NOT NULL)\nBEGIN\n   DROP TABLE [MAKT];\nEND;\n\nCREATE TABLE [MAKT]  \n(\n   [MANDT] NATIONAL CHARACTER VARYING(3) NOT NULL,\n   [MATNR] NATIONAL CHARACTER VARYING(18) NOT NULL,\n   [SPRAS] NATIONAL CHARACTER VARYING(1) COLLATE Latin1_General_100_CS_AS NOT NULL,\n   [MAKTX] NATIONAL CHARACTER VARYING(40),\n   [MAKTG] NATIONAL CHARACTER VARYING(40),\n   PRIMARY KEY\n   (\n      [MANDT], \n      [MATNR], \n      [SPRAS]\n   )\n\n);\n</code></pre> </li> <li> <p>Click [OK] to confirm your input.</p> </li> <li>Run the extraction again. </li> </ol> <p>The MSSQL server now returns the following message: </p> <pre><code>Extraction finished successfully\n</code></pre>"},{"location":"knowledge-base/config-command-line-tool/","title":"Create Extractions via Commandline","text":"<p>The command line tool xu-config.exe tool creates extractions, sources and destinations outside of the Xtract Universal Designer. The tool is available in the installation directory of Xtract Universal, e.g. <code>C:\\Program Files\\XtractUniversal\\xu-config.exe</code>.</p> <p>The xu-config.exe tool supports the creation of the following extractions types and destinations:</p> Extraction Types Destinations Table Azure Storage Table CDC Amazon AWS S3 ODP DeltaQ"},{"location":"knowledge-base/config-command-line-tool/#prerequisites","title":"Prerequisites","text":"<ul> <li>As of Xtract Universal 5.0.0, the xu-config.exe tool must be run by the same Windows AD account that runs the Xtract Universal Service.  This means, you either run the Windows command prompt as the respective user or you use the <code>runas</code> command in the command prompt. This is necessary, because passwords are encrypted for the user account that runs the xu-config.exe tool and can only be decrypted by the same account.</li> <li>The execution of PowerShell scripts must be authorized on your system, see Microsoft Documentation: Managing the execution policy with PowerShell.</li> </ul>"},{"location":"knowledge-base/config-command-line-tool/#create-an-sap-source-using-windows-command-prompt","title":"Create an SAP Source using Windows Command Prompt","text":"<p>Note</p> <p>the xu-config.exe tool only supports SAP connections with plain authentication.</p> <ol> <li>Start the Windows command prompt application   with administrator rights  .  </li> <li>Navigate to the installation directory of Xtract Universal  .  </li> <li> <p>Run the following shell command to create an encrypted password for your SAP source : </p> <pre><code>powershell ./protect-password.ps1\n</code></pre> </li> <li> <p>Use the following command to select the <code>xu-config.exe</code> command line tool from the Xtract Universal installation directory and to create a new SAP source: </p> <pre><code>xu-config.exe --source &lt;name&gt; &lt;host&gt; &lt;instance-number&gt; &lt;client&gt; &lt;language&gt; &lt;user&gt; &lt;protected-password&gt;\n</code></pre> </li> <li> <p>Replace the parameters in <code>&lt; &gt;</code> with actual values . The parameters are not case sensitive.</p> </li> <li>Press [Enter] to run the command. </li> <li>Check the generated source in the Xtract Universal Designer or in the directory <code>C:\\Program Files\\XtractUniversal\\config\\sources</code>. </li> </ol>"},{"location":"knowledge-base/config-command-line-tool/#create-a-destination-using-windows-command-prompt","title":"Create a Destination using Windows Command Prompt","text":"<ol> <li>Start the Windows command prompt application   with administrator rights  .  </li> <li>Navigate to the installation directory of Xtract Universal  .  </li> <li> <p>Run the following shell command to create encrypted passwords or keys necessary for the destination : </p> <pre><code>powershell ./protect-password.ps1\n</code></pre> </li> <li> <p>Use one the following commands to select the xu-config.exe command line tool from the Xtract Universal installation directory and to create a new destination: </p> Azure StorageAmazon AWS S3 with user credentialsAmazon AWS S3 with authentication via IAM role <pre><code>xu-config.exe --azure &lt;account&gt; &lt;accesskey&gt; &lt;container&gt; &lt;folder(opt)&gt;\n</code></pre> <pre><code>xu-config.exe --s3 --auth user &lt;key&gt; &lt;secretkey&gt; &lt;bucket&gt; &lt;region&gt; &lt;folder(opt)&gt;\n</code></pre> <pre><code>xu-config.exe --s3 --auth iam &lt;bucket&gt; &lt;region&gt; &lt;folder(opt)&gt;\n</code></pre> </li> <li> <p>Replace the parameters in <code>&lt; &gt;</code> with actual values . The names of the parameters are not case sensitive.</p> </li> <li>Press [Enter] to run the command.</li> <li>Check the generated destination in the Xtract Universal Designer or in the directory <code>C:\\Program Files\\XtractUniversal\\config\\destinations</code>. </li> </ol>"},{"location":"knowledge-base/config-command-line-tool/#create-a-table-extraction-using-windows-command-prompt","title":"Create a Table Extraction using Windows Command Prompt","text":"<ol> <li>Start the Windows command prompt application   with administrator rights  .  </li> <li>Navigate to the installation directory of Xtract Universal.</li> <li> <p>Use the following command to select the xu-config.exe command line tool from the Xtract Universal installation directory and to create a new Table extraction: </p> <pre><code>xu-config.exe --extraction &lt;source&gt; &lt;destination&gt; --table &lt;table&gt;\n</code></pre> </li> <li> <p>Replace the parameters in <code>&lt; &gt;</code>with actual values  . </p> </li> <li>Enter a defined SAP Connection, Destination and an SAP Table object for the parameters \\&lt;source&gt;, \\&lt;destination&gt; and \\&lt;table&gt;.  The names of the parameters are not case sensitive.  </li> <li>Press [Enter] to run the command. </li> <li>Check the generated table extraction in the Xtract Universal Designer or in the directory <code>C:\\Program Files\\XtractUniversal\\config\\extractions</code>. </li> </ol> <p>Note</p> <p>The following table settings are set by default after creation: Package Size (50000), Extract data in background job (enabled), all columns are selected for output.</p> <p>Tip</p> <p>Use the command <code>xu-config.exe -h</code> to look up the syntax for Table, Table CDC, ODP and DeltaQ extractions.</p>"},{"location":"knowledge-base/config-command-line-tool/#examples-for-all-extraction-types","title":"Examples for all Extraction Types","text":"Extraction Type Command Table <code>xu-config.exe --extraction ec5 sql-server --table TCURR</code> DeltaQ <code>xu-config.exe --extraction ec5 sql-server --table TCURR</code> ODP (ABAP Core Data Services) <code>xu-config.exe --extraction bw2 sql-server --odp ABAP_CDS UCONRFC_ATTR$F</code> ODP (SAP NetWeaver Business Warehouse) <code>xu-config.exe --extraction bw2 sql-server --odp BW 0ADDR_SHORT$T</code> ODP (SAP HANA Information Views) <code>xu-config.exe --extraction S4H sql-server --odp HANA HCCT232H1KHY32F7UL59IH224$F</code> ODP (DataSources/Extractors) <code>xu-config.exe --extraction ec5 sql-server --odp SAPI 2LIS_11_VAITM</code> Table CDC (extract table on first run) <code>xu-config.exe --extraction ec5 csv --tablecdc KNA1 true 5000</code> Table CDC (do not extract table on first run) <code>xu-config.exe --extraction ec5 csv --tablecdc KNA1 false 10000</code>"},{"location":"knowledge-base/config-command-line-tool/#create-multiple-extractions-using-powershell","title":"Create Multiple Extractions using PowerShell","text":"<p>Multiple extractions can be generated semi-automatically using suitable scripts. The scripts for creating extractions can be used to contribute to the generation of an SAP data warehouse. </p> PowerShell Script to Create Multiple Tbale Extractions<pre><code># read table list\n$tableList = \"KNA1\",\"LFA1\",\"MARA\",\"CSKT\",\"SKA1\"\n# set the path to the installation folder\n$XUConfig = 'C:\\Program Files\\XtractUniversal\\xu-config.exe'\n# source sytem\n$source = \"ec5\"\n# destination\n$destination = \"sqlserver2019\"\n\n# loop the tables\nforeach ($tableName in $tableList) {\n    # create the extraction e.g.\n    # xu-config.exe --extraction ec5 sqlserver2019 --table KNA1 \n    Try {                   \n        write-host -f Green \"$tableName : Creation of Extraction is starting \"  (Get-Date)                      \n        &amp;$XUConfig --extraction $source $destination --table $tableName    \n\n        # check the last exit code\n        # 0: successful\n        # else unsuccessful\n        if($LASTEXITCODE -eq 0) {                           \n            write-host -f Green \"$tableName : Creation of Extraction  is successful\"  (Get-Date)            \n        } else {           \n            write-host -f Red \"$tableName : Creation of Extraction failed with error code $LASTEXITCODE!\"  (Get-Date)\n            #Write-Host $errorMessage\n        }                \n    }\n    Catch {\n        write-host -f Red \"$tableName : Creation of Extraction failed with Exception ! \" + (Get-Date)  $_.Exception.Message\n    }         \n}\n</code></pre>"},{"location":"knowledge-base/config-command-line-tool/#related-links","title":"Related Links","text":"<ul> <li>Documentation: SAP Connection</li> <li>Documentation: Define a Table Extraction</li> <li>Documentation: Run an Extraction</li> <li>Documentation: WHERE Clause</li> <li>Documentation: Schedule Extractions</li> </ul>"},{"location":"knowledge-base/configuring-anysql-maestro-to-manage-amazon-redshift/","title":"Configure AnySQL Maestro to Manage Amazon Redshift","text":"<p>The following article shows an example of how to configure and use AnySQL Maestro to manage your Amazon Redshift database. The example works for any other Database.</p>"},{"location":"knowledge-base/configuring-anysql-maestro-to-manage-amazon-redshift/#create-database-profiles","title":"Create Database Profiles","text":"<ol> <li>Download and install AnySQL Maestro.</li> <li>Download and install ODBC driver for PostgreSQL.</li> <li>Launch AnySQL Maestro.</li> <li>Click \"Create Database Profiles\".  </li> <li>In the Create Database Profiles Wizard, click the button next to the connection string field.  </li> <li>Navigate to Connection and enable Use connection string then click [Build...].</li> <li>Navigate to Machine Data Source and Click [New...].   </li> <li>Click [Next] and choose PostgreSQL Unicode, then click [Next] and [Finish].</li> </ol> <p>Note</p> <p>A warning might pop up. Close the warning and click [OK].</p> <p></p>"},{"location":"knowledge-base/configuring-anysql-maestro-to-manage-amazon-redshift/#test-the-result","title":"Test the result","text":"<ol> <li>Enter your credentials, click \"Test\" to check if they are correct and click \"Save\".  </li> <li>Choose your connection and click [OK].</li> <li>Enter the database name and your credentials and select SSL-Mode allow, then click [OK].</li> <li>Enter your credentials and select the initial catalog, then click [OK].  </li> <li>Click [Next], then [Ready]. Now the database is ready.</li> </ol>"},{"location":"knowledge-base/configuring-anysql-maestro-to-manage-amazon-redshift/#related-links","title":"Related Links","text":"<ul> <li>Amazon AWS: Getting Started - Backup &amp; Restore with AWS</li> <li>Amazon Redshift: Getting Started with Amazon Redshift</li> </ul>"},{"location":"knowledge-base/connect-to-power-bi-service/","title":"Connect Xtract Universal to Power BI Service","text":"<p>The following article shows how to connect Xtract Universal to Power BI Service via an on-premises data gateway.</p>"},{"location":"knowledge-base/connect-to-power-bi-service/#prerequisites","title":"Prerequisites","text":"<p>To connect Power BI Service with Xtract Universal, the following components are required:</p> <ul> <li>Power BI Account</li> <li>On-premises Data Gateway</li> </ul>"},{"location":"knowledge-base/connect-to-power-bi-service/#setup-on-premises-data-gateway","title":"Setup On-Premises Data Gateway","text":"<p>To set up the on-premises Data Gateway using the Power BI Custom Connector, follow the steps below.</p> <p>Note</p> <p>The connection to Xtract Universal can be created using Power Query M-script or Power BI Custom Connector.  Using Power Query M-script does not require further configuration.</p> <ol> <li>Install the on-premises Data Gateway on the Xtract Universal application server.</li> <li>Configure the Data Gateway, see Use the on-premises data gateway app.</li> <li>Switch to the Connectors tab and define the path to the Power BI Custom Connector e.g., in <code>[Documents]\\Power BI Desktop\\Custom Connectors</code>.  The XtractUniversalExtension will then be displayed as a Custom Data Connector. </li> </ol>"},{"location":"knowledge-base/connect-to-power-bi-service/#add-xtract-universal-as-a-data-source","title":"Add Xtract Universal as a Data Source","text":"<p>Note</p> <p>Make sure that enabling and usage of custom connectors in Power BI is activated. If the Xtract Universal entry is not available in the drop-down menu, check the configuration in the Setup On-Premises Data Gateway section.</p> <p>The configured on-premises data gateway is integrated into the Power BI service environment.  A DataSource to the Xtract Universal Server must then be set up:</p> <ol> <li>In the Power BI service, navigate to Settings &gt; Manage connections and gateways and click [New]. The \"New Connection\" window opens. </li> <li>Select the connection type On-premises (default setting). Fill in the necessary fields: </li> <li>Select the previously created \"Data Gateway\" from the Gateway cluster name drop-down menu. </li> <li>Assign a name to the connection under Connection name.</li> <li>Select Xtract Universal Extraction under Connection type.  If the Xtract Universal entry is not available in the drop-down menu, check the configuration in Setup On-Premisess Data Gateway.   </li> <li>Specify the Xract Universal Server URL under Server, see Connect to an Xtract Universal Server.</li> <li>Decide on the appropriate authentication method under Authentication Method, see Single Sign On and SAP Authentication.</li> <li>Click [Create]. A connection is created and an automatic connection test is performed.</li> </ol>"},{"location":"knowledge-base/connect-to-power-bi-service/#data-source-status","title":"Data Source Status","text":"<ol> <li>Check the Data Source Status and other settings, e.g., Schedules Refresh.</li> <li>Under [Workspace settings] navigate to the settings of the dataset [...]. </li> <li>Expand the entry Gateway and cloud connections. </li> <li>Configure the connection of the uploaded Power BI dataset.</li> <li>Select the defined gateway and select the name of the connection (here XtractUniversal) from the drop-down menu under the option Extention.  The status of the connection is checked and reported back in the Status field.</li> </ol>"},{"location":"knowledge-base/connect-to-power-bi-service/#related-links","title":"Related Links","text":"<ul> <li>Documentation: Power BI Connector</li> </ul>"},{"location":"knowledge-base/create-a-custom-cennector-in-matillion-data-loader/","title":"SAP Integration with Matillion Data Loader","text":"<p>The following article shows how to create a custom connector in Matillion Data Loader that loads SAP data via Xtract Universal into Snowflake. Matillion Data Loader is a cloud based data loading platform that extracts data from popular sources and loads it into cloud destinations, see Official Website: Matillion Data Loader.</p>"},{"location":"knowledge-base/create-a-custom-cennector-in-matillion-data-loader/#prerequisites","title":"Prerequisites","text":"<ul> <li>Matillion Hub Account, see Official Website. </li> <li>Snowflake Destination for the Matillion Data Loader pipeline, see Matillion Documentation: Destinations - Set up Snowflake.</li> <li>Xtract Universal must be accessible via the internet, e.g., by hosting Xtract Universal on a webserver with a static IP address or via third party tools like ngrock. </li> </ul>"},{"location":"knowledge-base/create-a-custom-cennector-in-matillion-data-loader/#setup-in-xtract-universal","title":"Setup in Xtract Universal","text":"<ol> <li>Create an extraction in Xtract Universal, see Getting Started: Create an Extraction.  The depicted example scenario extracts the SAP table KNA1 (General Data in Customer Master). </li> <li>Assign the <code>http-json</code> destination to the extraction, see Documentation: Assign Destinations.</li> </ol>"},{"location":"knowledge-base/create-a-custom-cennector-in-matillion-data-loader/#create-a-custom-connector-in-matillion","title":"Create a Custom Connector in Matillion","text":"<p>To extract SAP data via Xtract Universal you must define a custom connector that contains the connection details of Xtract Universal, see Matillion Documentation: Matillion Custom Connector Overview.</p> <ol> <li>Open the website https://create-connector.matillion.com/ and log in to create the custom connector.</li> <li>Click [Add Connector]  to create a new custom connector. </li> <li>Click   to change the name of the connector  .</li> <li>Copy the URL the extraction into the designated input field and select <code>GET</code> as the http method  .  The URL has the following format:  <code>&lt;Protocol&gt;://&lt;HOST or IP address&gt;:&lt;Port&gt;/?name=&lt;Name of the Extraction&gt;{&amp;&lt;parameter_i&gt;=&lt;value_i&gt;}</code> Example: the URL <code>https://6606-185-114-89-133.eu.ngrok.io/?name=kna1</code> calls the extraction \"kna1\" via ngrock. For more information about calling extractions via web services, see Web API. </li> <li>To test the connection, enter your authentication details and click [Send] .  If the connection is successful, the http response contains the SAP customer data extracted by Xtract Universal .</li> <li>Click   to edit the structure (names and data types) of the http response . The structure is used when loading data into your destination. This example scenario only extracts the KNA1 columns City_ORT01, Name 1_NAME1, Country Key_LAND1 and Customer Number_KUNNR. </li> <li>Optional: If your extraction uses parameters, open the Parameters tab and define the parameters. </li> <li>Click [Save]  to save the connector.</li> </ol> <p>The custom connector can now be used in a Matillion Data Loader pipeline.</p> <p>Note</p> <p>The Matillion Custom Connector must be set to the same region as Matillion Data Loader, e.g., US (N. Virginia).</p>"},{"location":"knowledge-base/create-a-custom-cennector-in-matillion-data-loader/#create-a-pipeline-in-matillion-data-loader","title":"Create a Pipeline in Matillion Data Loader","text":"<p>Create a pipeline that triggers the extraction and writes the data to a destination, see Matillion Documentation: Create a pipeline with custom connectors.</p> <ol> <li>Open the Matillion Data Loader dashboard.</li> <li>Click [Add Pipeline] to create a new pipeline  . </li> <li>Open the Custom Connectors tap to select the custom connector  , that contains the connection settings for Xtract Universal.  </li> <li>Select the endpoint that calls the Xtract Universal extraction and use the arrow buttons to add the endpoint to the list Endpoints to extract and load. Note that a custom connector can have multiple endpoints.</li> <li>Click [Continue with x endpoint] . </li> <li>In the General tab enter a name for the target table  under Data warehouse table name. </li> <li>Open the Authentication tab and enter the authentication details for the Xtract Universal webservice.</li> <li>Open the Behaviour tab and select the elements you want to include as columns in the target table. By default, all elements are selected.</li> <li>Optional: If your endpoint uses parameters, open the Parameters tab to define the parameters.</li> <li>Open the Keys tab and select a key column that is used to match existing data and prevent duplicates, e.g., Customer Number_KUNNR .</li> <li>Click [Continue] . </li> <li>Select the destination to which the data is written to, e.g., Snowflake .  For more information on how to connect to Snowflake, see Matillion Documentation: Connect to Snowflake. </li> <li>Configure the destination, see Matillion Documentation: Configure Snowflake.</li> <li>Click [Continue].</li> <li>Enter a name for the pipeline . </li> <li>Select at which interval pipeline is to be executed . The pipeline first runs after it is created and then continues with the specified frequency.</li> <li>Click [Create pipeline] to create and run the pipeline . The pipeline is now listed in your dashboard. </li> <li>Check if the data was successfully uploaded to the destination. </li> </ol> <p>The pipeline now runs automatically at the specified frequency. </p>"},{"location":"knowledge-base/create-a-custom-cennector-in-matillion-data-loader/#related-links","title":"Related Links","text":"<ul> <li>Matillion Documentation: Snowflake Destination</li> <li>Matillion Documentation: Matillion Custom Connector Overview</li> <li>Matillion Documentation: Create a pipeline with custom connectors.</li> </ul>"},{"location":"knowledge-base/create-generic-datasource-using-function-module-and-timestamps/","title":"Create Generic DataSources","text":"<p>This article shows how to create generic DataSources for delta extractions in SAP using a function module and timestamps.</p>"},{"location":"knowledge-base/create-generic-datasource-using-function-module-and-timestamps/#about-delta-functionality-with-delta-fields","title":"About Delta Functionality with Delta fields","text":"<p>To use the delta functionality, a delta field is required.  Some tables like VBAK (Sales Document: Header Data) do not have a timestamp field for creation/change that can be uses as a unique delta field, but they have separate fields for creation date (ERDAT), creation time (ERZET) and change date (AEDAT).  To get the delta data of the VBAK table, we create a generic DataSource using a custom function module that implements the necessary logic.</p> <p>This article leads you through all necessary steps to create an extraction structure that has a timestamp field that can be used to implement the delta functionality. </p> <p>There are two template function modules that can be copied and used:</p> <ul> <li>RSAX_BIW_GET_DATA_SIMPLE: A function module with simple interface for Full Load with no support of delta loads.</li> <li>RSAX_BIW_GET_DATA: : A function module with complete interface that supports Delta Load. </li> </ul>"},{"location":"knowledge-base/create-generic-datasource-using-function-module-and-timestamps/#step-1-create-an-extract-structure","title":"Step 1: Create an Extract Structure","text":"<p>Follow the steps below to create the extract structure for the DataSource:</p> <ol> <li>Use SAP transaction SE11 to create a new structure ZZVBAK. </li> <li>Insert the table VBAK as an include into the structure. </li> <li>Add a field ZTMSTMP(Data element: TZNTSTMPS, it is of datatype DEC with Length 15).  This field holds the timestamp and allow us to use the extraction for delta purposes. </li> <li>Save and activate the structure. </li> </ol>"},{"location":"knowledge-base/create-generic-datasource-using-function-module-and-timestamps/#step-2-create-the-function-module","title":"Step 2: Create the Function Module","text":"<ol> <li>Use SAP transaction SE80 to copy the function group RSAX to the new function group Z_RSAX and to copy the function module RSAX_BIW_GET_DATA to Z_RSAX_BIW_GET_DATA_VBAK.  </li> <li>Be sure to copy and activate all the related objects (interfaces, datatypes etc.). </li> <li>Use SAP transaction SE37 to open and edit the function module Z_RSAX_BIW_GET_DATA_VBAK.  In the tab Tables, set the parameter E_T_DATA to associated type ZZVBAK. </li> <li> <p>Navigate to the tab source code and paste the following ABAP Code.</p> ABAP Code for the Custom Function Module<pre><code>FUNCTION Z_RSAX_BIW_GET_DATA_VBAK.\n*\"----------------------------------------------------------------------\n*\"*\"Local Interface:\n*\"  IMPORTING\n*\"     VALUE(I_REQUNR) TYPE  SBIWA_S_INTERFACE-REQUNR\n*\"     VALUE(I_ISOURCE) TYPE  SBIWA_S_INTERFACE-ISOURCE OPTIONAL\n*\"     VALUE(I_MAXSIZE) TYPE  SBIWA_S_INTERFACE-MAXSIZE OPTIONAL\n*\"     VALUE(I_INITFLAG) TYPE  SBIWA_S_INTERFACE-INITFLAG OPTIONAL\n*\"     VALUE(I_UPDMODE) TYPE  SBIWA_S_INTERFACE-UPDMODE OPTIONAL\n*\"     VALUE(I_DATAPAKID) TYPE  SBIWA_S_INTERFACE-DATAPAKID OPTIONAL\n*\"     VALUE(I_PRIVATE_MODE) OPTIONAL\n*\"     VALUE(I_CALLMODE) LIKE  ROARCHD200-CALLMODE OPTIONAL\n*\"     VALUE(I_REMOTE_CALL) TYPE  SBIWA_FLAG DEFAULT SBIWA_C_FLAG_OFF\n*\"  TABLES\n*\"      I_T_SELECT TYPE  SBIWA_T_SELECT OPTIONAL\n*\"      I_T_FIELDS TYPE  SBIWA_T_FIELDS OPTIONAL\n*\"      E_T_ZZVBAK STRUCTURE  ZZVBAK OPTIONAL\n*\"  EXCEPTIONS\n*\"      NO_MORE_DATA\n*\"      ERROR_PASSED_TO_MESS_HANDLER\n*\"----------------------------------------------------------------------\n\n\n* THE INPUT PARAMETER I_DATAPAKID IS NOT SUPPORTED YET !\n\n\n* EXAMPLE: INFOSOURCE CONTAINING TADIR OBJECTS\n*  TABLES: TADIR.\n\n\n* AUXILIARY SELECTION CRITERIA STRUCTURE\n  DATA: L_S_SELECT TYPE SBIWA_S_SELECT,\n        STARTDATE LIKE SY-DATUM,\n        STARTTIME LIKE SY-UZEIT,\n        ENDDATE LIKE SY-DATUM,\n        ENDTIME LIKE SY-UZEIT,\n        TSTAMP LIKE TZONREF-TSTAMPS.\n\n\n* MAXIMUM NUMBER OF LINES FOR DB TABLE\n  STATICS: L_MAXSIZE TYPE SBIWA_S_INTERFACE-MAXSIZE.\n\n\n* SELECT RANGES\n  RANGES:  L_R_TMPSTMP FOR ZZVBAK-ZTMSTMP.\n\n\n* PARAMETER I_PRIVATE_MODE:\n* SOME APPLICATIONS MIGHT WANT TO USE THIS FUNCTION MODULE FOR OTHER\n* PURPOSES AS WELL (E.G. DATA SUPPLY FOR OLTP REPORTING TOOLS). IF THE\n* PROCESSING LOGIC HAS TO BE DIFFERENT IN THIS CASE, USE THE OPTIONAL\n* PARAMETER I_PRIVATE_MODE (NOT SUPPLIED BY BIW !) TO DISTINGUISH\n* BETWEEN BIW CALLS (I_PRIVATE_MODE = SPACE) AND OTHER CALLS\n* (I_PRIVATE_MODE = X).\n* IF THE MESSAGE HANDLING HAS TO BE DIFFERENT AS WELL, DEFINE YOUR OWN\n* MESSAGING MACRO WHICH INTERPRETS PARAMETER I_PRIVATE_MODE. WHEN\n* CALLED BY BIW, IT SHOULD USE THE LOG_WRITE MACRO, OTHERWISE DO WHAT\n* YOU WANT.\n\n\n* INITIALIZATION MODE (FIRST CALL BY SAPI) OR DATA TRANSFER MODE\n* (FOLLOWING CALLS) ?\n  IF I_INITFLAG = SBIWA_C_FLAG_ON.\n\n************************************************************************\n\n* INITIALIZATION: CHECK INPUT PARAMETERS\n*                 BUFFER INPUT PARAMETERS\n*                 PREPARE DATA SELECTION\n************************************************************************\n\n\n* THE INPUT PARAMETER I_DATAPAKID IS NOT SUPPORTED YET !\n\n\n* INVALID SECOND INITIALIZATION CALL -&gt; ERROR EXIT\n    IF NOT G_FLAG_INTERFACE_INITIALIZED IS INITIAL.\n\n      IF 1 = 2. MESSAGE E008(R3). ENDIF.\n      LOG_WRITE 'E'                    \"MESSAGE TYPE\n                'R3'                   \"MESSAGE CLASS\n                '008'                  \"MESSAGE NUMBER\n                ' '                    \"MESSAGE VARIABLE 1\n                ' '.                   \"MESSAGE VARIABLE 2\n      RAISE ERROR_PASSED_TO_MESS_HANDLER.\n    ENDIF.\n\n\n* CHECK INFOSOURCE VALIDITY\n    CASE I_ISOURCE.\n      WHEN 'ZDSVBAK' OR ''.\n      WHEN OTHERS.\n        IF 1 = 2. MESSAGE E009(R3). ENDIF.\n        LOG_WRITE 'E'                  \"MESSAGE TYPE\n                  'R3'                 \"MESSAGE CLASS\n                  '009'                \"MESSAGE NUMBER\n                  I_ISOURCE            \"MESSAGE VARIABLE 1\n                  ' '.                 \"MESSAGE VARIABLE 2\n        RAISE ERROR_PASSED_TO_MESS_HANDLER.\n    ENDCASE.\n\n\n* CHECK FOR SUPPORTED UPDATE MODE\n    CASE I_UPDMODE.\n      WHEN 'F' OR ''.\n      WHEN 'C'.  \"\n      WHEN 'R'.  \"\n      WHEN 'S'.  \" DELTA INITIALIZATION\n      WHEN 'I'.    \" DELTA INIT FOR NON CUMMULATIVE\n      WHEN 'D'.   \" DELTA UPDATE\n      WHEN OTHERS.\n        IF 1 = 2. MESSAGE E011(R3). ENDIF.\n        LOG_WRITE 'E'                  \"MESSAGE TYPE\n                  'R3'                 \"MESSAGE CLASS\n                  '011'                \"MESSAGE NUMBER\n                  I_UPDMODE            \"MESSAGE VARIABLE 1\n                  ' '.                 \"MESSAGE VARIABLE 2\n        RAISE ERROR_PASSED_TO_MESS_HANDLER.\n    ENDCASE.\n\n\n* CHECK FOR OBLIGATORY SELECTION CRITERIA\n*    READ TABLE I_T_SELECT INTO L_S_SELECT WITH KEY FIELDNM = 'ZTMSTMP'.\n*    IF SY-SUBRC &lt;&gt; 0.\n*      IF 1 = 2. MESSAGE E010(R3). ENDIF.\n*      LOG_WRITE 'E'                    \"MESSAGE TYPE\n*                'R3'                   \"MESSAGE CLASS\n*                '010'                  \"MESSAGE NUMBER\n*                'PGMID'                \"MESSAGE VARIABLE 1\n*                ' '.                   \"MESSAGE VARIABLE 2\n*      RAISE ERROR_PASSED_TO_MESS_HANDLER.\n*    ENDIF.\n\n    APPEND LINES OF I_T_SELECT TO G_T_SELECT.\n\n\n* FILL PARAMETER BUFFER FOR DATA EXTRACTION CALLS\n    G_S_INTERFACE-REQUNR    = I_REQUNR.\n    G_S_INTERFACE-ISOURCE   = I_ISOURCE.\n    G_S_INTERFACE-MAXSIZE   = I_MAXSIZE.\n    G_S_INTERFACE-INITFLAG  = I_INITFLAG.\n    G_S_INTERFACE-UPDMODE   = I_UPDMODE.\n    G_S_INTERFACE-DATAPAKID = I_DATAPAKID.\n    G_FLAG_INTERFACE_INITIALIZED = SBIWA_C_FLAG_ON.\n\n\n* FILL FIELD LIST TABLE FOR AN OPTIMIZED SELECT STATEMENT\n* (IN CASE THAT THERE IS NO 1:1 RELATION BETWEEN INFOSOURCE FIELDS\n* AND DATABASE TABLE FIELDS THIS MAY BE FAR FROM BEEING TRIVIAL)\n    APPEND LINES OF I_T_FIELDS TO G_T_SEGFIELDS.\n\n  ELSE.                 \"INITIALIZATION MODE OR DATA EXTRACTION ?\n\n************************************************************************\n\n* DATA TRANSFER: FIRST CALL      OPEN CURSOR + FETCH\n*                FOLLOWING CALLS FETCH ONLY\n************************************************************************\n\n\n* FIRST DATA PACKAGE -&gt; OPEN CURSOR\n    IF G_COUNTER_DATAPAKID = 0.\n      \"LOOP AT I_T_SELECT INTO L_S_SELECT WHERE FIELDNM = 'ZTMSTMP'.\n       LOOP AT G_T_SELECT INTO L_S_SELECT WHERE FIELDNM = 'ZTMSTMP'.\n\n        TSTAMP = L_S_SELECT-LOW.\n        CONVERT TIME STAMP TSTAMP TIME ZONE SY-ZONLO INTO DATE STARTDATE TIME STARTTIME.\n        TSTAMP = L_S_SELECT-HIGH.\n        CONVERT TIME STAMP TSTAMP TIME ZONE SY-ZONLO INTO DATE ENDDATE TIME ENDTIME.\n      ENDLOOP.\n\n* FILL RANGE TABLES FOR FIXED INFOSOURCES. IN THE CASE OF GENERATED\n* INFOSOURCES, THE USAGE OF A DYNAMICAL SELECT STATEMENT MIGHT BE\n* MORE REASONABLE. BIW WILL ONLY PASS DOWN SIMPLE SELECTION CRITERIA\n* OF THE TYPE SIGN = 'I' AND OPTION = 'EQ' OR OPTION = 'BT'.\n*      LOOP AT G_T_SELECT INTO L_S_SELECT WHERE FIELDNM = 'PGMID'.\n*        MOVE-CORRESPONDING L_S_SELECT TO L_R_PGMID.\n*        APPEND L_R_PGMID.\n*      ENDLOOP.\n\n\n*      LOOP AT G_T_SELECT INTO L_S_SELECT WHERE FIELDNM = 'OBJECT'.\n*        MOVE-CORRESPONDING L_S_SELECT TO L_R_OBJECT.\n*        APPEND L_R_OBJECT.\n*      ENDLOOP.\n\n\n* DETERMINE NUMBER OF DATABASE RECORDS TO BE READ PER FETCH STATEMENT\n* FROM INPUT PARAMETER I_MAXSIZE. IF THERE IS A ONE TO ONE RELATION\n* BETWEEN INFOSOURCE TABLE LINES AND DATABASE ENTRIES, THIS IS TRIVIAL.\n* IN OTHER CASES, IT MAY BE IMPOSSIBLE AND SOME ESTIMATED VALUE HAS TO\n* BE DETERMINED.\n      L_MAXSIZE = G_S_INTERFACE-MAXSIZE.\n      IF ENDDATE &lt;&gt; '00000000' AND ENDTIME &lt;&gt; '000000'.\n        OPEN CURSOR WITH HOLD G_CURSOR FOR\n    SELECT * FROM VBAK\n      WHERE\n      (\n        ( ERDAT &gt;= STARTDATE AND ERZET &gt;= STARTTIME AND ERDAT &lt;= ENDDATE AND ERZET &lt;= ENDTIME )\n        OR ( AEDAT &gt;= STARTDATE AND  AEDAT &lt;= ENDDATE )\n      ).\n      ELSE.\n        OPEN CURSOR WITH HOLD G_CURSOR FOR\n         SELECT * FROM VBAK.\n      ENDIF.\n    ENDIF.                             \"FIRST DATA PACKAGE ?\n\n\n* FETCH RECORDS INTO INTERFACE TABLE. THERE ARE TWO DIFFERENT OPTIONS:\n* - FIXED INTERFACE TABLE STRUCTURE FOR FIXED INFOSOURCES HAVE TO BE\n*   NAMED E_T_'NAME OF ASSIGNED SOURCE STRUCTURE IN TABLE ROIS'.\n* - FOR GENERATING APPLICATIONS LIKE LIS AND CO-PA, THE GENERIC TABLE\n*   E_T_DATA HAS TO BE USED.\n* ONLY ONE OF THESE INTERFACE TYPES SHOULD BE IMPLEMENTED IN ONE API !\n    FETCH NEXT CURSOR G_CURSOR\n               APPENDING CORRESPONDING FIELDS\n               OF TABLE E_T_ZZVBAK\n               PACKAGE SIZE 1000.\n* PACKAGE SIZE L_MAXSIZE.\n\n    IF SY-SUBRC &lt;&gt; 0.\n      CLOSE CURSOR G_CURSOR.\n      RAISE NO_MORE_DATA.\n    ENDIF.\n\n    G_COUNTER_DATAPAKID = G_COUNTER_DATAPAKID + 1.\n\n  ENDIF.              \"INITIALIZATION MODE OR DATA EXTRACTION ?\n\nENDFUNCTION.\n</code></pre> </li> <li> <p>Save and activate the function module.</p> </li> </ol>"},{"location":"knowledge-base/create-generic-datasource-using-function-module-and-timestamps/#step-3-create-the-datasource","title":"Step 3: Create the DataSource","text":"<ol> <li>Use SAP transaction RSO2 to create a new DataSource for transaction data and name it to ZDSVBAK. </li> <li>Set the Application component and the description texts. </li> <li>Click [Extraction by FM]. Enter the name of the function module Z_RSAX_BIW_GET_DATA_VBAK and the extract structure ZZVBAK. </li> <li>Click [Generic Delta]. Select the timestamp field ZTMSTMP and activate the option [Time stamp]. </li> <li>Optional: set the Safety Interval Lower Limit.</li> <li>Click [Save] twice. In the following screen you can set the selection fields.  The timestamp field is disabled, because it is automatically populated as part of the delta process. </li> <li> <p>Use SAP transaction RSA2 to see the details of our DataSource ZDSVBAK.  The extraction method is set to F2 (Simple Interface).  Change it to F1 (Complete Interface)  by executing the following ABAP code. </p> <p>Tip</p> <p>Use SAP transaction SE38 to create a new report with this ABAP code. Unfortunately this is not possible in the GUI. </p> <pre><code>REPORT ZABAPDEMO\nUPDATE roosource\nSET delta = 'E'\nexmethod = 'F1'\ngenflag = 'X'\nWHERE oltpsource = 'ZDSVBAK'\n</code></pre> </li> <li> <p>Use SAP transaction RSA2 to display the status of the DataSource ZDSVBAK. Confirm that the Extraction Mode is set to F1. </p> </li> <li>Check for errors. </li> <li> <p>Optional: use SAP transaction SE37 and call it twice to test the function module. The first call is for the initialization and the second call reads the data.  Make sure that the table E_T_DATA contains the data.</p> Call Function ModuleResult <p></p> <p></p> </li> </ol>"},{"location":"knowledge-base/create-generic-datasource-using-function-module-and-timestamps/#step-4-test-the-datasource","title":"Step 4: Test the DataSource","text":"<p>Use SAP transaction RSA3 to test the datasource. </p> <p>Now the DataSource is created and you can use the DeltaQ extraction type to read it.  Be sure to activate the DataSource using the [Activate] button in the main window of the extraction type.</p>"},{"location":"knowledge-base/create-generic-datasource-using-function-module-and-timestamps/#result","title":"Result","text":"<p>The DataSource supports the Full and Update Delta mode.  To use theUpdate Delta mode, the first call must have the update type C (Delta Initialization).  All following calls must have the update type D (Delta Update). </p> <p>The delta process of the DataSource can be monitored and maintained in SAP transaction RSA7 (Delta Queue).</p> <p></p>"},{"location":"knowledge-base/create-personal-security-environment/","title":"Create a Client PSE to connect to SAP Cloud Systems","text":"<p>The following article shows how to create a client PSE (Personal Security Environment) that can be used to connect to SAP cloud systems via WebSocket RFC.</p>"},{"location":"knowledge-base/create-personal-security-environment/#prerequisites","title":"Prerequisites","text":"<ul> <li>SAP Cloud API URL, e.g., <code>https://my123456-api.s4hana.ondemand.com</code>. The correct URL is displayed in the API-URL field of the communication arrangement set up for communication scenario SAP_COM_0193.</li> <li>Command line tool sapgenpse.exe. The tool can be downloaded as part of the SAP Cryptographic Library in the SAP Service Marketplace.</li> </ul>"},{"location":"knowledge-base/create-personal-security-environment/#creating-a-client-pse","title":"Creating a Client PSE","text":"<p>Follow the steps below to create a client PSE file that trusts the server certificate of the SAP cloud system. </p> <ol> <li>Enter the SAP Cloud API URL in a browser of your choice.</li> <li> <p>View the certificate in the browser.</p>  Chrome Firefox <p>Navigate to View site information &gt; Connection is secure &gt; Certificate is valid. </p> <p>Click the pad lock icon left of the URL, navigate to Connection secure &gt; More information, then click [View Certificate]. </p> </li> <li> <p>Download the certificate chain from the browser. The certificate chain contains all certificates that are signed by the server certificate.</p>  Chrome Firefox <p>Open the Details tab and click [Export...].  Make sure to save the file in the format Base64-encoded ASCII, certificate chain (*.pem;*.crt). </p> <p>Scroll to the Miscellaneous section of the certificate and in the download row, click PEM (chain). </p> </li> <li> <p>Use the sapgenpse tool to create a client PSE file: </p> <p><pre><code>sapgenpse.exe gen_pse -p client.pse -v [Distinguished name]\n</code></pre> Replace <code>[Distinguished name]</code> with the distinguished name of the server that runs the Xtract product, e.g., <code>\"CN=COMPUTER.theobald.local, C=DE, S=BW, O=TS, OU=DEV\"</code>. Optionally, replace <code>client.pse</code> with a custom file name for the .pse file.    The tool creates its own repository in a standard path, unless the path is changed by the environment variable SECUDIR or by specifying an absolute path. </p> <p>Warning</p> <p>Restricted Access The PSE must be created without a password/pin, otherwise reading is not possible. Make sure not to secure the PSE. </p> </li> <li> <p>Use the following command to add the certificate chain from step 3 to the client PSE:</p> <p><pre><code>sapgenpse.exe maintain_pk -a &lt;[chain.pem]&gt; -p &lt;client.pse&gt;\n</code></pre> Replace <code>[chain.pem]</code> with the name of the downloaded .pem file, e.g., <code>s4hana-cloud-sap-chain.pem</code>. For more information on how to use the sapgenpse.exe, run the command <code>sapgenpse -h</code>.</p> </li> </ol> <p>The .pse file can now be used to connect Xtract Universal to the SAP cloud.</p>"},{"location":"knowledge-base/create-the-custom-authority-object-z-ts-prog/","title":"Create the Custom Authorization Object Z_TS_PROG","text":"<p>The following article shows how to create the Z_TS_PROG authorization object for the custom function module Z_XTRACT_IS_REMOTE_REPORT.</p> <p>The Theobald Software custom function module Z_XTRACT_IS_REMOTE_REPORT enables the extractions of reports from SAP systems. If no authorization group is assigned to a report, Z_XTRACT_IS_REMOTE_REPORT uses a custom authorization object Z_TS_PROG to verify whether the SAP user is allowed to extract a report.  The access to reports is granted based on the name of the report.</p>"},{"location":"knowledge-base/create-the-custom-authority-object-z-ts-prog/#create-the-custom-authorization-object-z_ts_prog","title":"Create the Custom Authorization Object Z_TS_PROG","text":"<ol> <li>Use transaction SU21 to create a new authorization object.</li> <li>Expand the Create menu and click [Authorization Object]. The window \"Create Authorization Object\" opens. </li> <li>Enter the following values: Object: Z_TS_PROG Text: Theobald Software Report Authorization </li> <li>Click [Continue] to enable editing of the section Authorization fields.</li> <li>Manually enter S_NAME as the first entry in Authorization fields. </li> <li>Click [Save] to save the authorization object.</li> </ol>"},{"location":"knowledge-base/create-the-custom-authority-object-z-ts-prog/#related-links","title":"Related Links","text":"<ul> <li>Authorize Access to Reports via Authorization Groups</li> <li>SAP Authorization Objects for Reports</li> <li>Documentation: Report</li> </ul>"},{"location":"knowledge-base/delta-table-extraction/","title":"Delta Table Extraction","text":"<p>The following article shows how to load data incrementally (daily) from an SAP Table based on date fields. The depicted example scenarios use the table VBAK (SAP Sales Document Header), which has two date fields: </p> <ul> <li>ERDAT for creation date </li> <li>AEDAT for update date</li> </ul>"},{"location":"knowledge-base/delta-table-extraction/#extract-data-using-a-date-parameter","title":"Extract Data using a Date Parameter","text":"<p>The depicted example extracts data that was created or changed after a specific date. The date is provided as a parameter at runtime.</p> <ol> <li>Create a new Table extraction.</li> <li>Look up a table you want to extract data from, e.g., VBAK.  </li> <li> <p>Open the WHERE clause tab of theTable extraction type and enter the following criterion: </p> <pre><code>( VBAK~ERDAT GE @LastDate AND VBAK~AEDAT EQ '00000000' ) OR VBAK~AEDAT GE @LastDate\n</code></pre> <p>This criterion extracts data if one of the following conditions is true: </p> <ul> <li>The data was created (ERDAT) after the date provided by the parameter <code>@LastDate</code> and it has not been changed (AEDAT). </li> <li>The data has changed (ARDAT) after the date provided by the parameter <code>@LastDate</code>. </li> </ul> </li> <li> <p>Click [OK] to confirm your input.</p> </li> <li> <p>Open the Run Extraction menu and navigate to the Custom tab for runtime parameters. </p> </li> <li>Enter a value for the runtime parameter <code>@LastDate</code> in the format <code>YYYYmmDD</code>.</li> <li>Click [Run] and check the results.</li> </ol>"},{"location":"knowledge-base/delta-table-extraction/#daily-data-extraction","title":"Daily Data Extraction","text":"<p>The depicted example extracts data that was created or changed the day before. The depicted example uses script expressions to query the current date.</p> <ol> <li>Create a new Table extraction.</li> <li>Look up a table you want to extract data from, e.g., VBAK.  </li> <li> <p>Open the WHERE clause tab of the Table component and enter the following criterion: </p> <pre><code>(ERDAT &gt;= '#{ DateTime.Now.AddDays(-1).ToString(\"yyyyMMdd\") }#' AND AEDAT = '00000000') OR AEDAT &gt;= '#{ DateTime.Now.AddDays(-1).ToString(\"yyyyMMdd\") }#'` &lt;br&gt;\n</code></pre> <p>This criterion extracts data if one of the following conditions is true:</p> <ul> <li>The data was created (ERDAT) the day before the current date and it has not been changed (AEDAT).</li> <li>The data has changed (ARDAT) the day before the current date. </li> </ul> </li> <li> <p>Click [OK] to confirm your input.</p> </li> <li>Run the extraction.</li> </ol> <p>The extraction can be scheduled every night at 1p.m. or later to extract all changes of the day before. Providing extraction dates is not necessary.</p>"},{"location":"knowledge-base/delta-table-extraction/#related-links","title":"Related Links","text":"<ul> <li>Delta Table Extraction</li> <li>Table: WHERE Clause</li> </ul>"},{"location":"knowledge-base/deploy-extractions-using-Git-version-control/","title":"Deploy Extractions Using Git Version Control","text":"<p>The following article describes versioning of newly developed extractions using Git.</p> <p>Versioning ensures a seamless synchronization of new developments on a test environment with the productive system. The implementation of various Git security techniques ensures an error-free transfer without compromising the production system.</p> <p>Note</p> <p>Target audience: Customers who use a production and a development environment for SAP data replications.</p>"},{"location":"knowledge-base/deploy-extractions-using-Git-version-control/#prerequisites","title":"Prerequisites","text":"<ul> <li>Technically separate development and production environment.</li> <li>Developers have read-only access to Xtract Universal on the production system.</li> <li>Git client installed locally on the development environment and the production environment, e.g., Fork.</li> <li>The Xtract Universal installation must have the identical release version on the servers.</li> </ul>"},{"location":"knowledge-base/deploy-extractions-using-Git-version-control/#set-up-a-new-environment","title":"Set Up a new Environment","text":"<ol> <li>Create a new Git repository. How to do this depends on the technology you use, e.g.:<ul> <li>GitHub</li> <li>Azure DevOps</li> <li>Git-scm</li> </ul> </li> <li>Clone the repository into the installation directory of Xtract Universal, e.g., <code>C:\\Program Files\\XtractUniversal</code>. The repository directory must be named <code>config</code>!</li> <li>Use dedicated branches for test and development environments and the main branch for the production system.</li> </ol> <p>Tip</p> <p>The initial initialization of the Git version control requires an empty <code>config</code> folder.  For this reason, the existing <code>config</code> folder must first be renamed and then filled with the configuration files.</p>"},{"location":"knowledge-base/deploy-extractions-using-Git-version-control/#set-up-an-existing-environment","title":"Set Up an existing Environment","text":"<ol> <li>Create a new Git Repository. How to do this depends on the technology you use, e.g.:<ul> <li>GitHub</li> <li>Azure DevOps</li> <li>Git-scm</li> </ul> </li> <li>Initialize a local repository in the installation directory of Xtract Universal, e.g., <code>C:\\Program Files\\XtractUniversal\\config</code> using the command <code>git init</code>. </li> <li>Attach the remote repository to your local one using the command <code>git remote add origin [ssh/https]://git-server-address/path/to/repo.git</code>.</li> <li>Commit the current <code>config</code> folder.</li> </ol>"},{"location":"knowledge-base/deploy-extractions-using-Git-version-control/#advantage-usage-of-versioning","title":"Advantage &amp; Usage of Versioning","text":"<ul> <li>By using the main branch for the production environment and development branches for the development or test environments, the former is separated from the latter.</li> <li>Applying appropriate and established techniques such as pull requests and Git user rights allows the changes to be checked and corrected in advance.</li> <li>New developments can not cause fundamental damage to the data load of the productive landscape with this approach.</li> <li>Versioning enables quick and easy rollbacks of changes by reverting them in Git and is agnostic towards the use of containers (such as Docker).</li> </ul> <p>Note</p> <p>This scenario can also be implemented with Azure DevOps and Git. For more information, see Microsoft Documentation.</p>"},{"location":"knowledge-base/deploy-extractions-using-Git-version-control/#releated-links","title":"Releated Links","text":"<ul> <li>GitForWindows Download</li> <li>Manual Migration to a Different Machine</li> <li>Getting Started - About Version Control</li> <li>Git Basics - Getting a Git Repository</li> <li>Git Branching - Branches in a Nutshell</li> <li>Distributed Git - Distributed Workflows</li> <li>Git Basics - Undoing Things</li> </ul>"},{"location":"knowledge-base/dynamic-runtime-paramater-within-KNIME-workflow/","title":"Dynamic Runtime Parameters in KNIME Workflows","text":"<p>The following article describes how user-defined runtime parameters in Xtract Universal can be transferred to the SAP Reader (Theobald Software) of a KNIME workflow.  This reduces transaction costs and improves the use of a delta-mechanism on standard SAP tables. </p> <p>The depicted example uses the field AEDAT (Changed On) of the standard table EKKO (Purchasing Document Header).  A runtime parameter in KNIME is used to extract and process only the entries from the SAP table EKKO that changed since the last run. </p> <p></p>"},{"location":"knowledge-base/dynamic-runtime-paramater-within-KNIME-workflow/#requirements","title":"Requirements","text":"<p>Basic knowledge of T-SQL, KNIME Analytics Platform and the creation of table extractions in Xtract Universal is required. The following BI architecture must be available and configured:</p> <ul> <li>Latest version of the KNIME Analytics Platform.</li> <li>Installed KNIME extension SAP Reader (Theobald Software).</li> <li>Existing table object in a Microsoft database (SQL-Server).</li> <li>Latest version of Xtract Universal, obtained from the Theobald Software website.</li> <li>Existing table extraction of table EKKO - Purchasing Document Header in Xtract Universal, see Define a Table Extraction.</li> <li>Use of a WHERE condition with runtime parameters of the table extraction in Xtract Universal e.g.: <code>EKKO~AEDAT &gt; @maxAEDAT</code>.</li> </ul>"},{"location":"knowledge-base/dynamic-runtime-paramater-within-KNIME-workflow/#knime-analytics-platform-workflow","title":"KNIME Analytics Platform Workflow","text":"<ol> <li>Configuration of the KNIME node Microsoft SQL Server Connector: </li> <li>Configuration of the KNIME node DB Table Selector:  Use the following SQL statement   for the table object EKKO in the Custom Query . <pre><code>SELECT MAX(REPLACE(AEDAT, '-', '')) AS maxAEDAT FROM #table#\n</code></pre> </li> <li>Use the KNIME node DB Reader to read the result value of the Custom Query <code>maxAEDAT</code> and propagate the data to all connected nodes.</li> <li>Configuration of the KNIME node Table Row to Varaiable : </li> <li>Right-click on the node SAP Reader (Theobald Software) to display the flow variable ports. </li> <li>Configuration of the KNIME node SAP Reader (Theobald Software), see KNIME Integration via SAP Reader (Theobald Software): </li> <li>In the Parameters tab, use Fetch Parameters  to add the user-defined variable to the WHERE condition  and enter a default value . </li> <li>In the Flow Variables tab, select the variable <code>maxAEDAT</code> in the Custom Parameters section using the drop-down menu . </li> <li>Pass the results of the extraction in the workflow to the KNIME node DB Writer and execute the workflow.</li> </ol> <p>Tip</p> <p>Check the correct execution of the extraction with user-defined runtime parameters in the Extraction Log of Xtract Universal.</p>"},{"location":"knowledge-base/dynamic-runtime-paramater-within-KNIME-workflow/#releated-links","title":"Releated Links","text":"<ul> <li>Documentation: KNIME Destination</li> <li>Read or download documentation for KNIME Software</li> <li>KNIME Flow Control Guide</li> </ul>"},{"location":"knowledge-base/enable-snc-using-pse-file/","title":"Enable Secure Network Communication (SNC) via X.509 certificate","text":"<p>The following article describes how to establish an SNC connection to an SAP source system. </p> <p>The depicted approach uses an X.509 certificate that provides the logon data of the Windows AD user.  The correctness of this X.509 certificate is ensured via the company's internal certification authority (ca).</p>"},{"location":"knowledge-base/enable-snc-using-pse-file/#workflow","title":"Workflow","text":"<ol> <li>Upon connection start, the Secure Login Client retrieves the SNC name from the SAP NetWeaver AS ABAP.</li> <li>The Secure Login Client uses the authentication profile for this SNC name.</li> <li>The user unlocks the security token, for example, by entering the PIN or password.</li> <li>The Secure Login Client receives the X.509 certificate from the user security token. </li> <li>The Secure Login Client provides the X.509 certificate for single sign-on and secure communication between SAP GUI or Web GUI and the AS ABAP.</li> <li>The user is authenticated and the communication is secured.</li> </ol> <p>Tip</p> <p>The configuration of the X.509 certificate should be implemented by the network &amp; SAP Basis team and requires basic knowledge in this area.</p>"},{"location":"knowledge-base/enable-snc-using-pse-file/#requirements","title":"Requirements","text":"<p>The following system settings are a prerequisite for using this SNC solution:</p> <ul> <li>Install the Secure Login Client.</li> <li>The SAP application server is configured and activated for Secure Network Communication (SNC).</li> <li>The SNC standard library sapcryptolib is used as the SNC solution.</li> <li>The following SNC parameters are configured:</li> </ul> SNC parameter Value Example snc/gssapi_lib Path and file name where the SAP Cryptographic Library is located. $(DIR_EXECUTABLE)\\sapcrypto.dll snc/identity/as Application server's SNC name Syntax: p:\\&lt;Distinguished_Name&gt;  The Distinguished Name part must match the Distinguished Name that you specify when creating the SNC PSE. p:CN=saperp.theobald.local"},{"location":"knowledge-base/enable-snc-using-pse-file/#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li> <p>Generate certificate for the application server and AD-user context from common Certificate Authority (ca). </p> <p>Note</p> <p>The X.509 certificate is available when placed in folder Certmgr &gt; Personal &gt; Certificates within Windows certificate store (user).</p> </li> <li> <p>Convert pfx file to SAP PSE format e.g., <code>sapgenpse.exe import_p12 -p cert.pse cert.pfx</code>.</p> </li> <li>Import the created PSE file via TA STRUST &gt; Edit mode &gt; PSE Import &gt; PSE Save as SNC Libcrypto.</li> <li>Edit the SNC configuration of the corresponding SAP user via transaction SU01  , SNC , SNC Name  = p:\\&lt;Full Distinguished_Name&gt;  e.g., <code>p:EMAIL=\"RandomUser@domain\",CN=\"Random User\",OU=\"Users\",OU=\"TheobaldSoftware\",DC=\"theobald\",DC=\"local\"</code>. </li> <li>Set up SNC authentication in the Xtract Universal SAP connection settings.</li> </ol>"},{"location":"knowledge-base/enable-snc-using-pse-file/#releated-links","title":"Releated Links","text":"<ul> <li>SAP Help: Workflow with X.509 Certificate without Secure Login Server</li> <li>SAP Help: Secure Network Communications (SNC)</li> <li>SAP Help: Configuring SNC: External Programs AS ABAP Using RFC </li> <li>SAP Help: Setting the SNC Profile Parameters</li> <li>SAP Help: Configuring SAP GUI and SAP Logon for Single Sign-On</li> <li>SAP Help: Secure Login Client</li> <li>SAP Additional Content: List of SNC Error Codes</li> </ul>"},{"location":"knowledge-base/execute_all_defined_xu_extractions/","title":"Execute & Schedule all Extractions using an SSIS Package","text":"<p>The following article shows how to execute and schedule all defined extractions in the Xtract Universal Designer using an SSIS-package. </p> <p>The implementation of this scenario is realized via the integration of an SSIS-package.  The package uses the onboard SSIS tasks to sequentially call all existing extractions in Xtract Universal.  The execution is implemented via the SQL Server Agent as part of the SSIS-DB.</p>"},{"location":"knowledge-base/execute_all_defined_xu_extractions/#requirements","title":"Requirements","text":"<p>The following programs and tools are required for using the SSIS-package:</p> <ul> <li>Install the SQL Server</li> <li>Install the SQL Server Management Studio (SSMS)</li> <li>Configure the SQL Server Agent</li> <li>Install the Xtract Universal Server</li> <li>Download the following SSIS-package: </li> </ul> <p> Download XtractUniversalScheduler.dtsx</p> <p>The Xtract Universal Server must be installed on the same environment as the SQL-Server / SSIS-DB using the default directory <code>C:\\Program Files\\XtractUniversal</code>.</p>"},{"location":"knowledge-base/execute_all_defined_xu_extractions/#step-by-step-guide","title":"Step-by-Step Guide","text":"<ol> <li>Import the SSIS-package XtractUniversalScheduler.dtsx into the SSIS-DB using the Microsoft SQL Server Management Studio.</li> <li> <p>Adjust the SSIS parameters to the correct values of the customer installation. </p> SSIS Parameter Data type Sensitive Required Example Value Info XtractUniversalServer String False True [ServerName.theobald.local] Xtract Universal Server XtractUniversalServerPort String False True 8065 XU-Server Ports </li> <li> <p>Schedule the deployed SSIS package using the integrated SQL Server Agent execution jobs. For more information, see Microsoft Documentation: Create a SQL-Server Agent Job. </p> </li> <li>Execute the created step of the SQL Server Agent.</li> <li>Check the correct execution of all defined extractions within the Xtract Universal Designer window. </li> <li>Check the results in the extraction destination(s). </li> </ol>"},{"location":"knowledge-base/execute_all_defined_xu_extractions/#about-the-ssis-package","title":"About the SSIS Package","text":"<p>The SSIS package contains the following SSIS variables:</p> SSIS Variables Data type Example Value Expression extraction_arguments String -s todd.theobald.local -p 8065 -n ExtractionName <code>\"-s \" + @[$Package::XtractUniversalServer]  + \" -p \" + @[$Package::XtractUniversalServerPort]  + \" -n \" +  REPLACE ( SUBSTRING( @[User::extraction_folder_name], 53, LEN(@[User::extraction_folder_name]) - 52 ) , \"\\\\general.json\", \"\")</code> extraction_folder_name String C:\\Program Files\\XtractUniversal\\config\\ extractions\\ExtractionName\\general.json -"},{"location":"knowledge-base/execute_all_defined_xu_extractions/#releated-links","title":"Releated Links","text":"<ul> <li>Call via Commandline</li> <li>Run Integration Services (SSIS) Packages</li> <li>Create an SQL-Server Agent Job</li> </ul>"},{"location":"knowledge-base/google-cloud-storage-oauth/","title":"Set Up OAuth 2.0 for the Google Cloud Storage Destination","text":"<p>The following article shows how to enable OAuth 2.0 for the google cloud storage destination.</p> <p>Xtract Universal supports OAuth 2.0 for authentication with the Google servers. To enable the OAuth 2.0 protocol, configure an OAuth flow with the required access permissions to Xtract Universal.</p> <p>Note</p> <p>Google initially classifies third-party applications generally as unsafe and issues a warning. The verification process is optional. Official app verification involves ongoing charges.</p> <p>Note</p> <p>As of version 5.11.16 Xtract Universal also supports authentication via service account credentials, see Documentation: Google Cloud Storage - Destination Details.</p>"},{"location":"knowledge-base/google-cloud-storage-oauth/#gcp-console","title":"GCP console","text":"<p>The GCP console allows configuring of all resources and services.  To get to the overview dashboard, navigate to the Google Cloud Storage page and click [Console] or [Go to console]. </p> <p>To access all settings and services use the navigation menu on the upper left side.</p> <p></p>"},{"location":"knowledge-base/google-cloud-storage-oauth/#set-up-oauth-20","title":"Set Up OAuth 2.0","text":"<ol> <li>Open the GCP console. In the navigation menu, select APIs &amp; Services &gt; Credentials. </li> <li>In the \"Credentials\" section select Create Credentials &gt; OAuth client ID. </li> <li>Click [Configure consent screen]. The \"Configure consent screen\" is processed with the OAuth flow that is started when a connection is established in the Xtract Universal Designer. </li> <li> <ol> <li>If your account belongs to an organization, you can restrict the usage of Xtract Universal in combination with GCS to your organization.  To do so, select internal in the User Type option  . The restriction option is only available, if you are a GSuite user. </li> <li>Alternatively you can allow any user with access to the OAuth credentials to grant Xtract Universal the permission to write data to your GCS buckets.  To do so, select external in the User Type option. </li> </ol> </li> <li>Click [Create] to continue  .</li> <li>In the \"App information\" section enter an app name of your choice.  Support email and Developer contact information are also mandatory fields. Click [Save and continue] to get to the next section.  </li> <li>In the following section click [Add or remove scopes]. Xtract Universal needs read and write permissions for its operations, which are configured in the \"Scopes\" section. </li> <li>Enter <code>https://www.googleapis.com/auth/devstorage.read_write</code> under Manually add scopes and click [Add to table]. </li> <li>The newly added scope is the first entry in the table. Click [Update] to create the entry.  </li> <li>Click [Continue] when the \"Verfification required\" window is prompted.  </li> <li>Click [Save and continue] twice.  </li> <li>Click [Back to dashboard] to return to the dashboard. </li> <li>Return to the \"Credentials\" menu, click [Create credentials] and select OAuth client ID.  Select Desktop app as application type  , enter a name for the app  .  </li> <li>Click [Create] .</li> </ol> <p>Your OAuth client is now created.  The Client ID and the Client secret are needed for the destination configuration in Xtract Universal, see Documentation: Google Cloud Connection Settings. </p>"},{"location":"knowledge-base/google-cloud-storage-oauth/#related-links","title":"Related Links","text":"<ul> <li>Documentation: Google Cloud Storage Destination</li> <li>Google Cloud Storage Documentation: Cloud Storage-Authentifizierung</li> <li>Google Cloud Storage Documentation</li> </ul>"},{"location":"knowledge-base/import-an-sap-transport-request/","title":"Import an SAP Transport Request","text":"<p>The following article shows how to import transport requests for custom functions modules that are included in the installation directory of your product, e.g., <code>C:\\Program Files\\XtractUniversal\\ABAP</code>.</p>"},{"location":"knowledge-base/import-an-sap-transport-request/#upload-sap-transport-requests-to-sap","title":"Upload SAP Transport Requests to SAP","text":"<p>If you have access to the file system of SAP, you can copy and paste the files of your transport request directly into the <code>data</code> and <code>cofiles</code> folders of your SAP system. If you don't have access to the file system, follow the steps below to upload the files of your transport request using the SAP function module ARCHIVFILE_CLIENT_TO_SERVER:</p> <ol> <li>Unzip the transport request provided in the installation directory of your product, e.g., <code>C:\\Program Files\\XtractUniversal\\ABAP</code>.</li> <li>Open SAP and go to transaction AL11.</li> <li>Find the entry DIR_TRANS in the column Name of Directory Parameter. Note or copy the path shown in the column Directory. </li> <li>Go to SAP transaction SE37.</li> <li>Enter name of function module ARCHIVFILE_CLIENT_TO_SERVER and click [Test/Execute]. </li> <li>In the field PATH you select your request file from from step 1. The name of the file starts with an \"R\", e.g., R900472.</li> <li>In the field TARGET PATH you construct your target path using the following pattern: <code>{copied path from step 2}\\data\\{request file name}</code>.</li> <li>Enable case-sensitivity and click [Execute]. When prompted, confirm the upload.  </li> <li>In the field PATH you select your cofile from from step 1. The name of the file starts with a \"K\", e.g., K900472.</li> <li>In the field TARGET PATH you construct your target path using the following pattern: <code>{copied path from step 2}\\cofiles\\{cofile name}</code>.</li> <li>Enable case-sensitivity and click [Execute]. When prompted, confirm the upload. </li> </ol> <p>The files are now available in SAP.</p> <p>Note</p> <p>Another method for uploading files to SAP is the SAP transaction CG3Z. This transaction is only available on ERP systems. </p>"},{"location":"knowledge-base/import-an-sap-transport-request/#import-sap-transport-requests","title":"Import SAP Transport Requests","text":"<p>Follow the steps below to add the transport requests to the import queue and import them:</p> <ol> <li>Go to SAP transaction STMS to open the transport management system.</li> <li>Click [Import Overview] ( icon). </li> <li>Double click on the import queue in which you want to load the transport request into. </li> <li>Open the transport request selection dialog via More &gt; Extras &gt; Other Requests &gt; Add.</li> <li>Select the transport request and confirm. If prompted, confirm the import. </li> <li>Select your transport request from the list and click [Import Request] ( icon). The window \"Import Transport Request\" opens.</li> <li>Enter the target client.  If the version of the SAP system where the transport request was created differs from your SAP system version, select the option Ignore Invalid Component Version. </li> <li>Confirm your settings. </li> </ol> <p>The transport request is imported.</p>"},{"location":"knowledge-base/import-an-sap-transport-request/#check-the-status-of-transport-requests","title":"Check the Status of Transport Requests","text":"<p>The import overview of the transport management system (transaction STMS) lists all transport requests. The status of the transport requests is displayed in the column \"RC\".</p> <p>A green bar indicates that the import was successful. In case of warnings or errors, double click on the icon to view the error messages. </p>"},{"location":"knowledge-base/insert-extraction-events-into-the-windows-logs/","title":"Insert Extraction Events into Windows Logs","text":"<p>The following article shows how to create a batch file that writes extraction logs into the windows log. The logs can be viewed in the Windows Event Viewer.</p>"},{"location":"knowledge-base/insert-extraction-events-into-the-windows-logs/#create-a-batch-file-that-writes-events-into-the-windows-log","title":"Create a Batch File that Writes Events into the Windows Log","text":"<ol> <li>Create a new batch file, e.g., xubatch.bat.</li> <li> <p>Add the following code to the batch file:</p> Insert Extraction Events into the Windows Log<pre><code>echo off\n\"C:\\Program Files\\XtractUniversal\\xu.exe\" %1\nIF %ERRORLEVEL% == 0 eventcreate /ID 1 /L APPLICATION /T INFORMATION /SO \"XtractUniversal\" /D \"Extraction successfully executed\"\nIF %ERRORLEVEL% == 1001 eventcreate /ID 101 /L APPLICATION /T ERROR /SO \"XtractUniversal\" /D \"An undefined error occured\"\nIF %ERRORLEVEL% == 1002 eventcreate /ID 102 /L APPLICATION /T ERROR /SO \"XtractUniversal\" /D \"Could not find the specified file\"\nIF %ERRORLEVEL% == 1013 eventcreate /ID 113 /L APPLICATION /T ERROR /SO \"XtractUniversal\" /D \"Invalid input data\"\nIF %ERRORLEVEL% == 1014 eventcreate /ID 114 /L APPLICATION /T ERROR /SO \"XtractUniversal\" /D \"The number of arguments is invalid\"\nIF %ERRORLEVEL% == 1015 eventcreate /ID 115 /L APPLICATION /T ERROR /SO \"XtractUniversal\" /D \"The parameter name is unknown\"\nIF %ERRORLEVEL% == 1016 eventcreate /ID 116 /L APPLICATION /T ERROR /SO \"XtractUniversal\" /D \"The argument is not valid\"\nIF %ERRORLEVEL% == 1053 eventcreate /ID 153 /L APPLICATION /T ERROR /SO \"XtractUniversal\" /D \"Something is wrong with your URL\"\nIF %ERRORLEVEL% == 1087 eventcreate /ID 187 /L APPLICATION /T ERROR /SO \"XtractUniversal\" /D \"The parameter is invalid\"\nIF %ERRORLEVEL% == 404 eventcreate /ID 404 /L APPLICATION /T ERROR /SO \"XtractUniversal\" /D \"Extraction does not exist\"\n</code></pre> </li> <li> <p>If you do not use the default folder for your Xtract Universal installation, change the following line to your installation path:</p> <pre><code>\"C:\\Program Files\\XtractUniversal\\xu.exe\" %1\n</code></pre> </li> <li> <p>Execute the following line with the name of the extraction as an argument in the windows command line:</p> <pre><code>xubatch.bat -n MAKT\n</code></pre> <p></p> </li> <li> <p>You can see the following event in the Event Viewer:</p> Extraction is successfulExtraction returns an error <p></p> <p></p> </li> </ol>"},{"location":"knowledge-base/insert-extraction-events-into-the-windows-logs/#related-links","title":"Related Links","text":"<ul> <li>Schedule Extractions using xu.exe command line</li> </ul>"},{"location":"knowledge-base/integrate_sap_data_into_a_Snowflake_data_warehouse/","title":"Integrate SAP Data into a Snowflake Data Warehouse","text":"<p>The following article describes a customer use case, where Theobald Software has partnered with Snowflake to build a showcase for integrating SAP ERP data into a Snowflake cloud data warehouse. </p>"},{"location":"knowledge-base/integrate_sap_data_into_a_Snowflake_data_warehouse/#customer-need","title":"Customer Need","text":"<p>The customer was looking to replace an existing on-premises data warehouse and analytics toolset with primarily cloud-based services.  Critical business data would have to be sourced from SAP ECC (on-premise), in addition to data stored in various\u00a0databases or flat files.</p> <p>The customer was also looking for a fully automated process to bring data from different sources together in a Snowflake cloud data warehouse.  The data load process had to be fast and support incremental data loads, to ultimately enable near real-time insights for the business users.</p>"},{"location":"knowledge-base/integrate_sap_data_into_a_Snowflake_data_warehouse/#sap-data-extraction","title":"SAP Data Extraction","text":"<p>The customer used our Xtract Universal product to extract data from their SAP ECC system.  Incremental data feeds are enabled with the built-in Table and DeltaQ components in Xtract Universal.  The Table component is easy to configure and delivers performant data extraction from even the largest SAP tables.  The DeltaQ component provides reliable delta feeds, based on SAP\u2019s native DataSource extractors.</p> <p></p>"},{"location":"knowledge-base/integrate_sap_data_into_a_Snowflake_data_warehouse/#cloud-storage","title":"Cloud Storage","text":"<p>Once the data is extracted from SAP, it can be directly stored in one of currently 20+ supported target environments.  It\u2019s a direct pass-through of the data from SAP into the target. In the process, SAP data types are mapped to the data types of the target environment.   Xtract Universal can write SAP data directly into a database, cloud storage or data warehouse.  In this case, the customer wants to write the data from Xtract Universal to Azure blob storage first and then to Snowflake DB.  With this approach an additional staging datalake can be maintained in Azure. The DDLs to create the proper tables in Snowflake can be auto-generated in Xtract Universal.  An initial, full data load from the Blob container can be done in Snowflake with the COPY command.  Subsequent, incremental data loads can be done with a MERGE statement.</p>"},{"location":"knowledge-base/integrate_sap_data_into_a_Snowflake_data_warehouse/#automation","title":"Automation","text":"<p>The SAP data extractions and the load process in Snowflake can be fully automated, using utilities like the Windows Task Scheduler or Cron, or your scheduling tool of choice.  This can also work in conjunction with an existing ETL tool for centralized monitoring and management of all data movement processes.</p>"},{"location":"knowledge-base/integrate_sap_data_into_a_Snowflake_data_warehouse/#related-links","title":"Related Links","text":"<ul> <li>Xtract Universal Product Information </li> <li>Documentation: Snowflake Destination</li> </ul>"},{"location":"knowledge-base/like-operand-where-clause/","title":"Working with LIKE operand in WHERE-Clauses","text":"<p>The following article shows how to use the LIKE operand in WHERE-Clauses of the Table extraction type. The LIKE operand represents a pattern using the following wildcard characters:</p> <ul> <li>\"%\" is any character string (including an empty string)</li> <li>\"_\" is any character</li> </ul> <p>The pattern is case-sensitive. Trailing blanks in operands are ignored.  This also applies in particular to operands of the type string with trailing blanks that are otherwise respected in ABAP.</p>"},{"location":"knowledge-base/like-operand-where-clause/#examples","title":"Examples","text":"WHERE-Condition Description <code>MSEG~MJAHR LIKE '20__'</code> Filter back all fiscal years of the table column MSEG~MJAHR that start with 20. <code>MSEG~MBLNR LIKE '0049%'</code> Filters all records of the table column MSEG~MBLNR that return the Number of Material Document starting with the value 0049. <code>BKPF~BUKRS LIKE '__1_'</code> Filters all records that have a 1 in the third digit of the value for the company code (BKPF~BUKRS). <code>MSEG~MJAHR LIKE '00490002'</code><code>MSEG~MBLNR LIKE '0049%'</code><code>BKPF~BUKRS LIKE '__1_'</code>"},{"location":"knowledge-base/like-operand-where-clause/#related-links","title":"Related Links","text":"<ul> <li>SAP Help: Open SQL - Operands and Expressions - LIKE </li> </ul>"},{"location":"knowledge-base/link-bex-query-with-hierarchy/","title":"Link a BEx query with a Hierarchy in Tableau","text":"<p>This articles shows how to link BEx Query and BW Hierarchy extractions in the Tableau destination. By linking the extractions, you can blend data from both data sources into a single sheet. </p>"},{"location":"knowledge-base/link-bex-query-with-hierarchy/#setup-in-xtract-universal","title":"Setup in Xtract Universal","text":"<ol> <li>Create a BEx Query extraction, see Define a BW Cube Extraction. </li> </ol> <ol> <li>Open the extraction settings of the BEx Query and activate Formatted Values. </li> <li>Open the Destination Settings of the extraction and set Text as the Column Name Style for Tableau. Note that using Text as the column name style can result in column names that are not unique.</li> <li>Create a BW Hierarchy extraction, see Define a Hierarchy Extraction. </li> </ol> <ol> <li>Open the Extraction Settings of the Hierarchy and set Representation to Natural: </li> </ol>"},{"location":"knowledge-base/link-bex-query-with-hierarchy/#create-a-relationship-in-tableau","title":"Create a Relationship in Tableau","text":"<ol> <li>Load both extractions into Tableau  . </li> <li>Create a relationship between the data sources by dragging the sheets into the canvas  . The window \"Edit Relationships\" opens.</li> <li>Select one pair of fields that is to be matched. Add multiple field pairs to create a compound relationship. Matched pairs must have the same data type.  </li> <li>Close \"Edit Relationships\" and switch to the Worksheet view  .</li> </ol> <p>The data sources are now linked and data from both data sources can be blended in a single sheet. </p>"},{"location":"knowledge-base/link-bex-query-with-hierarchy/#not-assigned-nodes","title":"\"Not Assigned\" Nodes","text":"<p>In BW Hierarchies, the values that are not assigned to a hierarchy node are gathered under the \"Not Assigned\" node.</p> <p>In Tableau the \"Not Assigend\" node is labeled \"Null\" and is only displayed if the BEx Query extraction acts as the primary data source.  If the BW Hierarchy extraction is acts as primary data source, the \"Null\" node is not displayed.</p>"},{"location":"knowledge-base/link-bex-query-with-hierarchy/#related-links","title":"Related Links","text":"<ul> <li>How to Create Relationships in Tableau</li> <li>Tutorial: Relationships in Tableau</li> </ul>"},{"location":"knowledge-base/load-balancer/","title":"Load Balancing","text":"<p>The following article shows how to operate Xtract Universal with load balancing. In this context, load balancing means to distribute the network traffic across multiple Windows servers to avoid server overloads. </p>"},{"location":"knowledge-base/load-balancer/#about-load-balancing","title":"About Load Balancing","text":"<p>In today's highly interconnected world, a load balancer is a crucial tool for managing traffic effectively.  When distributing network traffic caused by Xtract Universal, it is necessary to use a load balancer between different Windows servers that run the Xtract Universal.</p> <p>Typical use cases for load balancing include:</p> <ul> <li>Improved Performance: By distributing network traffic across multiple Xtract Universal servers, a load balancer can significantly improve the performance of your network.  This can be especially important for high-traffic applications that require fast and reliable access.</li> <li>Increased Reliability: Load balancing can help increase the reliability of your network by ensuring that no one Xtract Universal server becomes overloaded with traffic.  This helps to prevent downtime and ensures that your network is always available.</li> <li>Scalability: A load balancer can also help improve the scalability of your network by allowing you to easily add or remove Xtract Universal servers as needed.  This can be important for businesses that experience sudden spikes in traffic.</li> <li>Centralized Management: A load balancer allows you to manage multiple servers from a single location.  This makes it easier to monitor your network, troubleshoot issues, and make necessary adjustments.</li> <li>Parallel Processing: A load balancer allows running multiple extractions at the same time on different servers.  This increases the amount of extractions that can be run in parallel.</li> </ul> <p></p>"},{"location":"knowledge-base/load-balancer/#prerequisites","title":"Prerequisites","text":"<ul> <li>Every Xtract Universal servers needs a server license, see Documentation: Licensing.</li> <li>All Xtract Universal servers must share the same configuration folder, e.g., by mapping the configuration folder to an external shared storage.  The configuration folder contains the settings for the destinations, extractions, sources, server and users. The folder is located in the installation directory of Xtract Universal, e.g., <code>C:\\Program Files\\XtractUniversal\\config</code>. For versioning of the configuration folder you can use git version control.</li> <li>All Xtract Universal installations must use the same software version to avoid any version compatibility issues.</li> </ul>"},{"location":"knowledge-base/load-balancer/#the-process","title":"The Process","text":"<p>A load balancer setup with two Xtract Universal servers uses the following process when processing requests:</p> <ol> <li>A client sends a web request to the load balancer. The load balancer is the entry point for all incoming web requests, so it's the first component to receive the request.</li> <li>The load balancer uses a predefined algorithm (e.g., round-robin, least connections, or IP hash) to select the Xtract Universal server to handle the incoming web request.</li> <li>The load balancer forwards the request to the selected Xtract Universal server.</li> <li>The selected Xtract Universal server processes the incoming request and sends the response back to the load balancer.</li> <li>The load balancer receives the response from the selected server and forwards the response to the client.</li> <li>For subsequent web requests, the load balancer repeats this process.</li> </ol> <p>This setting can also be configured to act as Active / Standby servers.  This means you have multiple servers with identical configurations and applications where only one server is active, while the others remain passive or on standby until a failover event occurs.</p> <p>The load balancer forwards incoming requests to the active server. If the active server fails, the load balancer will automatically switch to one of the passive servers. The goal is to provide redundancy and ensure high availability and reliability of critical applications and services.</p>"},{"location":"knowledge-base/load-balancer/#related-links","title":"Related Links:","text":"<ul> <li>Deploying Extractions Using Git Version Control</li> <li>Documentation: Installation and Update - Program Directory Files</li> </ul>"},{"location":"knowledge-base/log-access-via-http/","title":"Log Access via Web Service","text":"<p>The following contains the documentation of deprecated API calls.</p>"},{"location":"knowledge-base/log-access-via-http/#basic-url","title":"Basic URL","text":"<p>The basic URL for web calls uses the following format: <code>[protocol]://[host or IP address]:[port]/</code>. Make sure to use the correct protocol:</p> Protocol Syntax Example HTTP <code>http://[host].[domain]:[port]</code> <code>http://todd.theobald.local:8065</code> HTTP <code>http://[host]:[port]</code> <code>http://localhost:8065</code> HTTPS <code>https://[host].[domain]:[port]</code> <code>https://todd.theobald.local:8165</code> Requires a dedicated host name and X.509 certificate, see web server settings. <p>Note</p> <p>Make sure to use the correct ports, see Server Ports.</p>"},{"location":"knowledge-base/log-access-via-http/#before-version-600","title":"Before Version 6.0.0","text":""},{"location":"knowledge-base/log-access-via-http/#query-all-logs","title":"Query all Logs","text":"URL Description <code>http(s)://[host]:[port]/log/?req_type=all</code> Returns all extraction and server logs. <p>Note</p> <p>Server logs files are deleted after a defined period of days, see Server Setting - Web Server.</p>"},{"location":"knowledge-base/log-access-via-http/#response","title":"Response","text":"<p>The web call returns the following information:</p> <ul> <li>LineCount: row number of the log entry.</li> <li>Name:  name of the extraction / name of the server.</li> <li>Timestamp: timestamp of the extraction or server log.</li> <li>State: returns a number between 2 and 4 for a server extraction or the number 5 for a server log, see table below.</li> <li>StateDescr: description of the state, see table below.</li> <li>LogLevel: type of the log (\"Error\", \"Info\" \"Warning\" or \"Debug\").</li> <li>Source technical name of the component that generates the log info. </li> <li>Message: content of the log.</li> </ul> State StateDescr Description 2 Running The extraction is running. 3 FinishedNoErrors Extraction succeeded without errors. 4 FinishedErrors Extraction is finished with at least one error. 5 NotAvailable The status for a server log. <p>For information on how to read extractions logs, see Extraction Logs.</p>"},{"location":"knowledge-base/log-access-via-http/#example","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/log/?req_type=all</code> <pre>\nLineCount,Name,Timestamp,State,StateDescr,LogLevel,Source,Message\n1,MAKT,2023-02-20_09:49:23.941,3,FinishedNoErrors,Info,LiveDataExtraction,Product version 5.21.10.14\n2,MAKT,2023-02-20_09:49:23.942,3,FinishedNoErrors,Info,LiveDataExtraction,Using Theobald.Extractors Interface\n3,MAKT,2023-02-20_09:49:23.999,3,FinishedNoErrors,Info,LiveDataExtraction,Theobald.Extractors version 1.39.3.13\n4,MAKT,2023-02-20_09:49:23.999,3,FinishedNoErrors,Info,LiveDataExtraction,Executing Table extraction\n...\n1,[server],2023-02-20_09:49:10.208,5,NotAvailable,Warning,VersionStore,Configuration was created by a development build. This can lead to unexpected behaviour.\n2,[server],2023-02-20_09:49:10.258,5,NotAvailable,Info,AsyncTcpServer,Trying to listen on [::]:8065...\n3,[server],2023-02-20_09:49:10.258,5,NotAvailable,Info,AsyncTcpServer,Listening on [::]:8065\n4,[server],2023-02-20_09:49:23.353,5,NotAvailable,Info,AsyncTcpServer,Client [::1]:51531 connected\n...\n</pre>"},{"location":"knowledge-base/log-access-via-http/#query-logs-at-specific-timestamps","title":"Query Logs at Specific Timestamps","text":"URL Description <code>http(s)://[host]:[port]/log/?req_type=all&amp;timestamp=[yyyy-MM-dd]</code> Returns all logs of the specified date. <code>http(s)://[host]:[port]/log/?req_type=all&amp;timestamp=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns all logs of the specified timestamp. <code>http(s)://[host]:[port]/log/?req_type=all&amp;min=[yyyy-MM-dd]</code> Returns all logs after the specified date. <code>http(s)://[host]:[port]/log/?req_type=all&amp;min=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns all logs after the specified timestamp. <code>http(s)://[host]:[port]/log/?req_type=all&amp;max=[yyyy-MM-dd]</code> Returns all logs before the specified date. <code>http(s)://[host]:[port]/log/?req_type=all&amp;max=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns all logs before the specified timestamp. <code>http(s)://[host]:[port]/log/?req_type=all&amp;min=[yyyy-MM-dd]&amp;max=[yyyy-MM-dd]</code> Returns all logs between two dates. <code>http(s)://[host]:[port]/log/?req_type=all&amp;min=[yyyy-MM-dd_HH:mm:ss.SSS]&amp;max=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns all logs between two timestamps. <code>http(s)://[host]:[port]/log/?req_type=all&amp;past_days=[number_of_days]</code> Returns all logs since n days."},{"location":"knowledge-base/log-access-via-http/#response_1","title":"Response","text":"<p>The web call returns the following information:</p> <ul> <li>LineCount: row number of the log entry.</li> <li>Name:  name of the extraction / name of the server.</li> <li>Timestamp: timestamp of the extraction or server log.</li> <li>State: returns a number between 2 and 4 for a server extraction or the number 5 for a server log, see table below.</li> <li>StateDescr: description of the state, see table below.</li> <li>LogLevel: type of the log (\"Error\", \"Info\" \"Warning\" or \"Debug\").</li> <li>Source technical name of the component that generates the log info. </li> <li>Message: content of the log.</li> </ul> State StateDescr Description 2 Running The extraction is running. 3 FinishedNoErrors Extraction succeeded without errors. 4 FinishedErrors Extraction is finished with at least one error. 5 NotAvailable The status for a server log. <p>For information on how to read extractions logs, see Reading Extraction Logs.</p>"},{"location":"knowledge-base/log-access-via-http/#example_1","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/log/?req_type=all&amp;past_days=2</code> <pre>\nLineCount,Name,Timestamp,State,StateDescr,LogLevel,Source,Message\n1,MAKT,2023-02-20_09:49:23.941,3,FinishedNoErrors,Info,LiveDataExtraction,Product version 5.21.10.14\n2,MAKT,2023-02-20_09:49:23.942,3,FinishedNoErrors,Info,LiveDataExtraction,Using Theobald.Extractors Interface\n3,MAKT,2023-02-20_09:49:23.999,3,FinishedNoErrors,Info,LiveDataExtraction,Theobald.Extractors version 1.39.3.13\n4,MAKT,2023-02-20_09:49:23.999,3,FinishedNoErrors,Info,LiveDataExtraction,Executing Table extraction\n...\n1,[server],2023-02-20_09:49:10.208,5,NotAvailable,Warning,VersionStore,Configuration was created by a development build. This can lead to unexpected behaviour.\n2,[server],2023-02-20_09:49:10.258,5,NotAvailable,Info,AsyncTcpServer,Trying to listen on [::]:8065...\n3,[server],2023-02-20_09:49:10.258,5,NotAvailable,Info,AsyncTcpServer,Listening on [::]:8065\n4,[server],2023-02-20_09:49:23.353,5,NotAvailable,Info,AsyncTcpServer,Client [::1]:51531 connected\n...\n</pre> <code>https://todd.theobald.local:8165/log/?req_type=all&amp;min=2023-02-20_09:49:24.500&amp;max=2023-02-20_09:50:00.000</code> <pre>\nLineCount,Name,Timestamp,State,StateDescr,LogLevel,Source,Message\n22,MAKT,2023-02-20_09:49:24.500,3,FinishedNoErrors,Debug,TheoReadTableExtractor,Data will be extracted in dialog work process\n23,MAKT,2023-02-20_09:49:24.501,3,FinishedNoErrors,Debug,TheoReadTableExtractor,\"Fetching packages (50,000 rows per package)\"\n24,MAKT,2023-02-20_09:49:24.653,3,FinishedNoErrors,Debug,TheoReadTableExtractor,Z_THEO_READ_TABLE version 1.x\n25,MAKT,2023-02-20_09:49:24.653,3,FinishedNoErrors,Debug,TheoReadTableExtractor,Received package #1 (1 rows)\n26,MAKT,2023-02-20_09:49:24.657,3,FinishedNoErrors,Info,LiveDataExtraction,Starting to write 1 rows to destination...\n27,MAKT,2023-02-20_09:49:24.668,3,FinishedNoErrors,Info,LiveDataExtraction,Finished writing rows to destination\n28,MAKT,2023-02-20_09:49:24.712,3,FinishedNoErrors,Info,TheoReadTableExtractor,Extraction finished - received 1 rows in total\n29,MAKT,2023-02-20_09:49:24.714,3,FinishedNoErrors,Debug,LiveDataExtraction,Writing results to destination completed\n6,[server],2023-02-20_09:49:24.802,5,NotAvailable,Debug,ProcessAsync,Theobald.Xu.Web.Worker.exe (16240) exited with 0x0 - The operation completed successfully\n7,[server],2023-02-20_09:49:36.257,5,NotAvailable,Info,AsyncTcpServer,Client [::1]:51533 connected\n8,[server],2023-02-20_09:49:36.262,5,NotAvailable,Debug,ProcessAsync,Theobald.Xu.Web.Worker.exe (16368) started\n</pre>"},{"location":"knowledge-base/log-access-via-http/#query-all-server-logs","title":"Query all Server Logs","text":"URL Description <code>http(s)://[host]:[port]/log/?req_type=server</code> Returns a list of timestamps that correspond to server logs."},{"location":"knowledge-base/log-access-via-http/#response_2","title":"Response","text":"<p>The web call returns timestamps in the format <code>[yyyy-MM-dd_HH:mm:ss.SSS]</code>. Use the timestamps to query the content of the logs, see Query Server Logs at a Specific Timestamp.</p> <p>Note</p> <p>Server log files are deleted after a defined period of days, see Server Setting - Web Server.</p>"},{"location":"knowledge-base/log-access-via-http/#example_2","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/log/?req_type=server</code> <pre>\nTimestamp\n2023-02-20_09:49:10.055\n2023-02-15_13:49:38.401\n</pre>"},{"location":"knowledge-base/log-access-via-http/#query-server-logs-at-specific-timestamps","title":"Query Server Logs at Specific Timestamps","text":"URL Description <code>http(s)://[host]:[port]/log/?req_type=server&amp;timestamp=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns the server logs of the specified timestamp. <p>Tip</p> <p>Query timestamps that correspond to server logs, see Query all Server Logs. To query server logs before, after or between timestamps, see Query Logs at Specific Timestamps.</p> <p>Note</p> <p>Server log files are deleted after a defined period of days, see Server Setting - Web Server.</p>"},{"location":"knowledge-base/log-access-via-http/#response_3","title":"Response","text":"<p>The web call returns the following information:</p> <ul> <li>LineCount: row number of the log entry.</li> <li>Name:  name of the server.</li> <li>Timestamp: timestamp of the server log.</li> <li>State: returns the number 5 to indicate that the log is a server log.</li> <li>StateDescr: description of the state.</li> <li>LogLevel: type of the log (\"Error\", \"Info\" \"Warning\" or \"Debug\").</li> <li>Source technical name of the component that generates the log info. </li> <li>Message: content of the log.</li> </ul>"},{"location":"knowledge-base/log-access-via-http/#example_3","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/?req_type=server\u00d7tamp=2023-02-20_09:49:10.228</code> <pre>\nLineCount,Name,Timestamp,State,StateDescr,LogLevel,Source,Message \n1,[server],2023-02-20_09:49:10.258,5,NotAvailable,Info,AsyncTcpServer,Trying to listen on [::]:8065... \n2,[server],2023-02-20_09:49:10.258,5,NotAvailable,Info,AsyncTcpServer,Listening on [::]:8065 \n3,[server],2023-02-20_09:49:23.353,5,NotAvailable,Info,AsyncTcpServer,Client [::1]:51531 connected \n4,[server],2023-02-20_09:49:23.372,5,NotAvailable,Debug,ProcessAsync,Theobald.Xu.Web.Worker.exe (16240) started \n5,[server],2023-02-20_09:49:24.802,5,NotAvailable,Debug,ProcessAsync,Theobald.Xu.Web.Worker.exe (16240) exited with 0x0 - The operation completed successfully \n6,[server],2023-02-20_09:49:36.257,5,NotAvailable,Info,AsyncTcpServer,Client [::1]:51533 connected \n7,[server],2023-02-20_09:49:36.262,5,NotAvailable,Debug,ProcessAsync,Theobald.Xu.Web.Worker.exe (16368) started\n</pre>"},{"location":"knowledge-base/log-access-via-http/#query-a-list-of-all-defined-extractions","title":"Query a List of all Defined Extractions","text":"URL Description <code>http(s)://[host]:[port]/extractions</code> Returns a list of all defined extractions. <code>http(s)://[host]:[port]/config/extractions/</code> Returns a list of all defined extractions with more details and in JSON format."},{"location":"knowledge-base/log-access-via-http/#response_4","title":"Response","text":"<p>The web calls return the following information:</p> <ul> <li>Name: name of the extraction.</li> <li>Type: extraction type, e.g., Table, Report, ODP, etc.</li> <li>Source: name of the SAP source connection.</li> <li>Destination: name of the destination.</li> <li>LastRun: timestamp of the last execution. </li> <li>RowCount: number of the last extracted data records. </li> <li>LastChange: timestamp of the last change. </li> <li>Created: timestamp of the creation.</li> </ul>"},{"location":"knowledge-base/log-access-via-http/#example_4","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/extractions</code> <pre>\nName,Type,Source,Destination,LastRun,RowCount,LastChange,Created\nMAKT,Table,ec5,csv,2022-12-15_13:30:08.921,177318,2023-02-15_13:49:38.401,2022-12-12_08:39:27.407\n2LIS,ODP,ec5,AzureStorageAD,2022-08-18_10:55:00.189,59058,2023-01-20_11:26:05.641,2022-08-18_10:46:50.721\nCOUNTRY,Hierarchy,bw2,http-csv,2022-12-01_12:53:57.098,8,2022-12-01_12:53:53.599,2022-10-05_10:41:43.848\nRLT10010,Report,ec5,csv,2023-01-12_11:11:48.975,21,2022-12-13_11:07:36.437,2022-06-30_08:24:47.755\n</pre> <code>https://todd.theobald.local:8165/config/extractions/</code> <pre>\n{\n    \"extractions\": \n    [\n        {\n            \"name\": \"MAKT\",\n            \"type\": \"Table\",\n            \"source\": \"ec5\",\n            \"destination\": \"csv\",\n            \"latestRun\": {\n                \"started\": \"20221215T133008.921Z\",\n                \"duration\": \"PT00H00M02.850S\",\n                \"rowsCount\": 177318,\n                \"state\": \"FinishedNoErrors\"\n            },\n            \"lastChange\": {\n                \"machine\": \"SHERRI\",\n                \"user\": \"alice\",\n                \"timestamp\": \"20230215T134938.401Z\"\n            },\n            \"created\": {\n                \"machine\": \"SHERRI\",\n                \"user\": \"alice\",\n                \"timestamp\": \"20221212T083927.407Z\"\n            }\n        }\n    ]\n}           \n</pre>"},{"location":"knowledge-base/log-access-via-http/#query-a-specific-extraction-at-a-specific-timestamp","title":"Query a Specific Extraction at a Specific Timestamp","text":"URL Description <code>http(s)://[host]:[port]/log/?req_type=extraction&amp;name=[extraction_name]&amp;timestamp=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns detailed logs of the specified extraction at the specified timestamp. <p>Tip</p> <p>Query the timestamp of the last run of an extraction as described in Query a List of all Defined Extractions or query timestamps using the logs as described in Query all Logs.</p>"},{"location":"knowledge-base/log-access-via-http/#response_5","title":"Response","text":"<p>The web call returns the following information:</p> <ul> <li>LineCount: row number of the log entry.</li> <li>Name:  name of the extraction.</li> <li>Timestamp: timestamp of the extraction.</li> <li>State: returns a number between 2 and 4 for a server extraction, see table below.</li> <li>StateDescr: description of the state, see table below.</li> <li>LogLevel: type of the log (\"Error\", \"Info\" \"Warning\" or \"Debug\").</li> <li>Source technical name of the component that generates the log info. </li> <li>Message: content of the log.</li> </ul> State StateDescr Description 2 Running The extraction is running. 3 FinishedNoErrors Extraction succeeded without errors. 4 FinishedErrors Extraction is finished with at least one error. <p>For information on how to read extractions logs, see Extraction Logs.</p>"},{"location":"knowledge-base/log-access-via-http/#example_5","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/log/?req_type=extraction&amp;name=makt\u00d7tamp=2023-02-20_09:49:23.807</code> <pre>\nLineCount,Name,Timestamp,State,StateDescr,LogLevel,Source,Message\n1,makt,2023-02-21_11:04:33.765,3,FinishedNoErrors,Info,LiveDataExtraction,Product version 5.21.10.14\n2,makt,2023-02-21_11:04:33.766,3,FinishedNoErrors,Info,LiveDataExtraction,Using Theobald.Extractors Interface\n3,makt,2023-02-21_11:04:33.817,3,FinishedNoErrors,Info,LiveDataExtraction,Theobald.Extractors version 1.39.3.13\n4,makt,2023-02-21_11:04:33.817,3,FinishedNoErrors,Info,LiveDataExtraction,Executing Table extraction\n5,makt,2023-02-21_11:04:33.882,3,FinishedNoErrors,Info,LiveDataExtraction,Found license.\n6,makt,2023-02-21_11:04:33.930,3,FinishedNoErrors,Debug,R3ConnectorServerWindows,'Use SAPGUI' expert option is disabled\n7,makt,2023-02-21_11:04:33.931,3,FinishedNoErrors,Debug,R3ConnectorServerWindows,\"Connecting to SAP application server, using Classic RFC SDK\"\n8,makt,2023-02-21_11:04:33.931,3,FinishedNoErrors,Debug,R3ConnectorServerWindows,\"Client '800',  language 'EN'\"\n9,makt,2023-02-21_11:04:33.931,3,FinishedNoErrors,Debug,R3ConnectorServerWindows,\"User ALICE, Password has been provided\"\n10,makt,2023-02-21_11:04:33.931,3,FinishedNoErrors,Debug,R3ConnectorServerWindows,Using plain authentication\n11,makt,2023-02-21_11:04:34.291,3,FinishedNoErrors,Info,R3ConnectorServerWindows,\"Connected to SAP host 'sap-erp-as05.example.com', instance 00, release 740, codepage 4103, user 'ALICE'\"\n...\n</pre>"},{"location":"knowledge-base/log-access-via-http/#query-the-extraction-status","title":"Query the Extraction Status","text":"URL Description <code>http(s)://[host]:[port]/status/?name=[extraction_name]&amp;timestamp=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns the status of the specified extraction at the specified timestamp. <p>Follow the steps below to create a status check routine:</p> <ol> <li>Run your extraction in asynchronous mode using the following URL schema: <code>http(s)://[host]:[port]/?name=[extraction_name]&amp;wait=false</code> The asynchronous extraction immediately returns an HTTP-response, while the extraction is still running.  For more information on how to trigger extractions, see Execute and Automate - Call via Webservice.</li> <li>Copy the timestamp that is returned in the HTTP-response header and body of the asynchronous extraction. Example: X-XU-Timestamp: 2023-01-28_09:58:47.312.</li> <li>Use the extraction name and the timestamp to query the status of the extraction using the following URL schema: <code>http(s)://[host]:[port]/status/?name=[extraction_name]&amp;timestamp=[yyyy-MM-dd_HH:mm:ss.SSS]</code></li> <li>The status of an extraction changes in time.  Query the status of the extraction in a loop to trigger follow-up actions once the extraction is finished.</li> </ol> <p>Note</p> <p>Triggering an extraction in asynchronous mode and querying the extraction status is only possible with push-destinations, e.g.s databases or file destinations.</p>"},{"location":"knowledge-base/log-access-via-http/#response_6","title":"Response","text":"<p>The web call returns one of the following statuses:</p> State Description Running The extraction is running. FinishedNoErrors Extraction succeeded without errors. FinishedErrors Extraction is finished with at least one error."},{"location":"knowledge-base/log-access-via-http/#example_6","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/status/?name=makt\u00d7tamp=2023-02-21_11:06:16.314</code> <pre>\nFinishedNoErrors\n</pre> <code>https://todd.theobald.local:8165/status/?name=mara\u00d7tamp=2023-02-21_13:11:27.327</code> <pre>\nFinishedErrors\n</pre>"},{"location":"knowledge-base/log-access-via-http/#before-version-630","title":"Before Version 6.3.0","text":""},{"location":"knowledge-base/log-access-via-http/#query-all-server-logs_1","title":"Query All Server Logs","text":"URL Description <code>http(s)://[host]:[port]/log?req_type=server</code> Returns a list of timestamps that correspond to server logs."},{"location":"knowledge-base/log-access-via-http/#response_7","title":"Response","text":"<p>The web call returns timestamps in the format <code>[yyyy-MM-dd_HH:mm:ss.SSS]</code>. Use the timestamps to query the content of the server logs, see Query Server Logs at Specific Timestamps.</p> <p>Note</p> <p>Server log files are deleted after a defined period of days, see Server Setting - Web Server.</p>"},{"location":"knowledge-base/log-access-via-http/#example_7","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/log?req_type=server</code> <pre>\nTimestamp\n2023-09-08_09:49:10.055\n2023-09-08_13:49:38.401\n</pre>"},{"location":"knowledge-base/log-access-via-http/#query-server-logs-at-specific-timestamps_1","title":"Query Server Logs at Specific Timestamps","text":"URL Description <code>http(s)://[host]:[port]/log?req_type=server&amp;timestamp=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns the server logs of the specified timestamp. <p>Tip</p> <p>Query timestamps of all server logs, see Query All Server Logs or query timestamps that correspond to an extraction run, see Query Extraction Runs of a Specific Extraction.</p> <p>Note</p> <p>Server log files are deleted after a defined period of days, see Server Setting - Web Server.</p>"},{"location":"knowledge-base/log-access-via-http/#response_8","title":"Response","text":"<p>The web call returns the following information:</p> <ul> <li>LineCount: row number of the log entry.</li> <li>Name:  name of the server.</li> <li>Timestamp: timestamp of the server log.</li> <li>State: returns the number 5 to indicate that the log is a server log.</li> <li>StateDescr: description of the state.</li> <li>LogLevel: type of the log (\"Error\", \"Info\" \"Warning\" or \"Debug\").</li> <li>Source technical name of the component that generates the log info. </li> <li>Message: content of the log.</li> </ul>"},{"location":"knowledge-base/log-access-via-http/#example_8","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/log?req_type=server\u00d7tamp=2023-09-08_10:21:46.390</code> <pre>\nLineCount,Name,Timestamp,State,StateDescr,LogLevel,Source,Message\n1,[server],2023-09-06_08:38:12.519,5,NotAvailable,Info,XtractWebServer,Client [fe80::d3ac:77ba:ce0f:83b1%8]:54421\n2,[server],2023-09-06_08:38:12.823,5,NotAvailable,Info,XtractWebServer,Request URI: http://sherri.theobald.local:8065/?name=MARA&amp;quiet-push=true\n3,[server],2023-09-06_08:38:17.458,5,NotAvailable,Info,RunParameter,Parameter 'quiet-push' overriden (new value: 'true')\n4,[server],2023-09-06_08:38:17.553,5,NotAvailable,Info,XtractWebServer,Running Table extraction 'MARA'...\n</pre>"},{"location":"knowledge-base/log-access-via-http/#query-a-list-of-all-defined-extractions_1","title":"Query a List of all Defined Extractions","text":"URL Description <code>http(s)://[host]:[port]/config/extractions/</code> Returns a list of all defined extractions with more details and in JSON format."},{"location":"knowledge-base/log-access-via-http/#response_9","title":"Response","text":"<p>The web calls return the following information:</p> <ul> <li>Name: name of the extraction.</li> <li>Type: extraction type, e.g., Table, Report, ODP, etc.</li> <li>Source: name of the SAP source connection.</li> <li>Destination: name of the destination.</li> <li>LastRun: timestamp of the last execution. </li> <li>RowCount: number of the last extracted data records. </li> <li>LastChange: timestamp of the last change. </li> <li>Created: timestamp of the creation.</li> </ul>"},{"location":"knowledge-base/log-access-via-http/#example_9","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/config/extractions/</code> <pre>\n{\n    \"extractions\": \n    [\n        {\n            \"name\": \"MAKT\",\n            \"type\": \"Table\",\n            \"source\": \"ec5\",\n            \"destination\": \"csv\",\n            \"latestRun\": {\n                \"started\": \"20221215T133008.921Z\",\n                \"duration\": \"PT00H00M02.850S\",\n                \"rowsCount\": 177318,\n                \"state\": \"FinishedNoErrors\"\n            },\n            \"lastChange\": {\n                \"machine\": \"SHERRI\",\n                \"user\": \"alice\",\n                \"timestamp\": \"20230215T134938.401Z\"\n            },\n            \"created\": {\n                \"machine\": \"SHERRI\",\n                \"user\": \"alice\",\n                \"timestamp\": \"20221212T083927.407Z\"\n            }\n        }\n    ]\n}           \n</pre>"},{"location":"knowledge-base/log-access-via-http/#query-extraction-runs-of-a-specific-extraction","title":"Query Extraction Runs of a Specific Extraction","text":"URL Description <code>http(s)://[host]:[port]/log?req_type=extraction&amp;name=[extractionname]</code> Returns information and timestamps of extractions runs of the specified extraction."},{"location":"knowledge-base/log-access-via-http/#response_10","title":"Response","text":"<p>The web call returns timestamps in the format <code>[yyyy-MM-dd_HH:mm:ss.SSS]</code>. Use the timestamps to query the content of the extraction logs.</p> <p>The web call returns the following information:</p> <ul> <li>Timestamp: timestamp of the extraction.</li> <li>State: returns a number between 2 and 4 for a server extraction or the number 5 for a server log, see table below.</li> <li>StateDescr: description of the state, see table below.</li> <li>RequestID: ID that is specific to ODP extractions.</li> <li>RowCount number of the last extracted data records. </li> <li>WebLog: timestamp of the corresponding server log.</li> </ul>"},{"location":"knowledge-base/log-access-via-http/#example_10","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/log?req_type=extraction&amp;name=KNA1</code> <pre>\nTimestamp,State,StateDescr,RequestID,RowCount,WebLog\n2023-09-08_10:43:38.288,3,FinishedNoErrors,,9895,2023-09-08_10:43:37.832\n2023-09-08_10:53:27.074,3,FinishedNoErrors,,9895,2023-09-08_10:53:26.616\n</pre>"},{"location":"knowledge-base/log-access-via-http/#query-a-specific-extraction-at-a-specific-timestamp_1","title":"Query a Specific Extraction at a Specific Timestamp","text":"URL Description <code>http(s)://[host]:[port]/log?req_type=extraction&amp;name=[extractionname]&amp;timestamp=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns extraction logs of the specified extraction at the specified timestamp. <p>Tip</p> <p>Query the timestamp of extraction runs of a specific extraction, see Query Extraction Runs of a Specific Extraction. Query the timestamp of the last extraction run, see Query a List of all Defined Extractions.</p>"},{"location":"knowledge-base/log-access-via-http/#response_11","title":"Response","text":"<p>The web call returns the following information:</p> <ul> <li>LineCount: row number of the log entry.</li> <li>Name:  name of the extraction / name of the server.</li> <li>Timestamp: timestamp of the extraction or server log.</li> <li>State: returns a number between 2 and 4 for a server extraction, see table below.</li> <li>StateDescr: description of the state, see table below.</li> <li>LogLevel: type of the log (\"Error\", \"Info\" \"Warning\" or \"Debug\").</li> <li>Source technical name of the component that generates the log info. </li> <li>Message: content of the log.</li> </ul> State StateDescr Description 2 Running The extraction is running. 3 FinishedNoErrors Extraction succeeded without errors. 4 FinishedErrors Extraction is finished with at least one error. <p>For information on how to read extractions logs, see Reading Extraction Logs.</p>"},{"location":"knowledge-base/log-access-via-http/#example_11","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/log?req_type=extraction&amp;name=KNA1\u00d7tamp=2023-09-08_10:43:13.234</code> <pre>\nLineCount,Name,Timestamp,State,StateDescr,LogLevel,Source,Message\n1,KNA1,2023-09-08_10:43:13.241,3,FinishedNoErrors,Debug,Table,Attempting to load Theobald.Extractors.Table.TableExtractionDefinition information for extraction KNA1\n2,KNA1,2023-09-08_10:43:13.465,3,FinishedNoErrors,Info,Table,Starting extraction without cache.\n3,KNA1,2023-09-08_10:43:13.465,3,FinishedNoErrors,Info,TheoReadTableExtractor,Starting extraction - using function module Z_THEO_READ_TABLE\n4,KNA1,2023-09-08_10:43:13.465,3,FinishedNoErrors,Info,TheoReadTableExtractor,Extracting table KNA1\n...\n</pre>"},{"location":"knowledge-base/log-access-via-http/#query-the-extraction-status_1","title":"Query the Extraction Status","text":"URL Description <code>http(s)://[host]:[port]/status/?name=[extractionname]&amp;timestamp=[yyyy-MM-dd_HH:mm:ss.SSS]</code> Returns the status of the specified extraction at the specified timestamp. <p>Follow the steps below to create a status check routine:</p> <ol> <li>Run your extraction in asynchronous mode using the following URL schema: <code>http(s)://[host]:[port]/?name=[extraction_name]&amp;wait=false</code> The asynchronous extraction immediately returns an HTTP-response, while the extraction is still running.  For more information on how to trigger extractions, see Execute and Automate - Call via Webservice.</li> <li>Copy the timestamp that is returned in the HTTP-response header of the asynchronous extraction. Example: X-XU-Timestamp: 2023-01-28_09:58:47.312.</li> <li>Use the extraction name and the timestamp to query the status of the extraction using the following URL schema: <code>http(s)://[host]:[port]/status/?name=[extraction_name]&amp;timestamp=[yyyy-MM-dd_HH:mm:ss.SSS]</code></li> <li>The status of an extraction changes in time.  Query the status of the extraction in a loop to trigger follow-up actions once the extraction is finished.</li> </ol> <p>Note</p> <p>Triggering an extraction in asynchronous mode and querying the extraction status is only possible with push-destinations, e.g.s databases or file destinations.</p>"},{"location":"knowledge-base/log-access-via-http/#response_12","title":"Response","text":"<p>The web call returns the following information:</p> <ul> <li>Duration: duration of the extraction run.</li> <li>rowCount:  number of extracted rows.</li> <li>State: returns the status of the extraction run.</li> <li>WebLog Timestamp: timestamp of the corresponding server log.</li> </ul> State Description Running The extraction is running. FinishedNoErrors Extraction succeeded without errors. FinishedErrors Extraction is finished with at least one error."},{"location":"knowledge-base/log-access-via-http/#example_12","title":"Example","text":"Example URLExample Response <code>https://todd.theobald.local:8165/status/?name=KNA1\u00d7tamp=2023-11-22_10:15:32.390</code> <pre>\nFinishedNoErrors\n</pre>"},{"location":"knowledge-base/proxy-server-settings/","title":"Proxy Server Settings","text":"<p>The following article shows how to configure a proxy server in Xtract Universal.  Due to corporate network regulations there can be the requirement that all web traffic needs to go through a web proxy.  This means that Xtract Universal must also connect to a destination via a proxy server. </p>"},{"location":"knowledge-base/proxy-server-settings/#system-wide-configuration-in-the-windows-settings","title":"System-Wide Configuration in the Windows Settings","text":"<p>Open the settings Network and Internet in your Windows start menu and set the respective proxy server settings.  When connecting to a destination in Xtract Universal the connection is routed through the proxy server. </p> <p></p>"},{"location":"knowledge-base/proxy-server-settings/#application-specific-configuration","title":"Application Specific Configuration","text":"<p>Follow the Microsoft guidelines to configure the proxy server.</p> <p>Open the installation directory of Xtract Universal, e.g., <code>C:\\Program Files\\XtractUniversal</code>. Modify the files Theobald.Xu.Web.Worker.exe.config and XtractDesigner.exe.config.  Enter the following section in both files: </p> Proxy Configuration<pre><code>&lt;system.net&gt;\n    &lt;defaultProxy&gt;\n        &lt;proxy  proxyaddress=\"http://[My_Proxyserver]:3128\"\n                bypassonlocal=\"True\"\n        /&gt;\n    &lt;/defaultProxy&gt;\n&lt;/system.net&gt;\n</code></pre> <p>Examples:</p> Theobald.Xu.Web.Worker.exe.configXtractDesigner.exe.config <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;configuration&gt;\n    &lt;system.net&gt;\n        &lt;defaultProxy&gt;\n            &lt;proxy proxyaddress=\"http://[My_Proxyserver]:3128\" bypassonlocal=\"true\" /&gt;\n        &lt;/defaultProxy&gt;\n    &lt;/system.net&gt;\n    &lt;startup&gt;\n        &lt;!-- ... --&gt;\n    &lt;/startup&gt;\n    &lt;runtime&gt;\n        &lt;!-- ... --&gt;\n    &lt;/runtime&gt;\n&lt;/configuration&gt;\n</code></pre> <pre><code>&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\n&lt;configuration&gt;\n\u200b\n    &lt;configSections&gt;\n        &lt;!-- ... --&gt;\n    &lt;/configSections&gt;\n\u200b\n    &lt;system.net&gt;  \n        &lt;defaultProxy&gt;  \n            &lt;proxy  proxyaddress=\"http://[My_Proxyserver]:3128\"\n                bypassonlocal=\"true\"  \n        /&gt;  \n        &lt;/defaultProxy&gt;  \n    &lt;/system.net&gt;\n</code></pre>"},{"location":"knowledge-base/proxy-server-settings/#related-links","title":"Related Links","text":"<ul> <li>Microsoft Help: &lt; defaultProxy &gt; Element (Network Settings)</li> </ul>"},{"location":"knowledge-base/read-data-from-cluster-fields-in-the-tables-pcl1-and-pcl2-payroll/","title":"Read Data from Cluster Fields in Tables PCL1 and PCL2 (Payroll)","text":"<p>The following article shows how to extract data from the SAP HCM tables PCL1 and PCL2. The data can only be extracted using a custom function module and the BAPI extraction type.  Data extraction via Table extraction type is not supported.</p>"},{"location":"knowledge-base/read-data-from-cluster-fields-in-the-tables-pcl1-and-pcl2-payroll/#custom-function-module-z_hr_cluster_read","title":"Custom Function Module Z_HR_CLUSTER_READ","text":"<p>Create the following custom function module in SAP:</p> <ol> <li>Use SAP transaction SE37 to create a remote enabled custom function module Z_HR_CLUSTER_READ.  </li> <li> <p>Create the following parameters:</p> Import ParametersTable Parameters <pre><code>PERNR            TYPE=PC2B0-PERNR;\nACTIONID         TYPE=CHAR2; DEFAULT = 'P1'\nSTARTDATE        TYPE=DATS;\nENDDATE          TYPE=DATS;\n</code></pre> <p></p> <pre><code>ERT        LIKE=PC2B8\nST         LIKE=PC2B5\nCRT        LIKE=PC208\n</code></pre> <p></p> </li> <li> <p>Copy and paste the following ABAP source code into the source code area of the function module.</p> ABAP source code<pre><code>     FUNCTION Z_HR_CLUSTER_READ.\n\n     *\"----------------------------------------------------------------------\n\n     *\"*\"Local Interface:\n     *\"  IMPORTING\n     \"     VALUE(PERNR) TYPE  PC2B0-PERNR OPTIONAL\n     *\"     VALUE(ACTIONID) TYPE  CHAR2 DEFAULT 'P1'\n     *\"     VALUE(STARTDATE) TYPE  DATS OPTIONAL\n     *\"     VALUE(ENDDATE) TYPE  DATS OPTIONAL\n     *\"  TABLES\n     *\"      ERT STRUCTURE  PC2B8 OPTIONAL\n     *\"      ST STRUCTURE  PC2B5 OPTIONAL\n     *\"      CRT STRUCTURE  PC208 OPTIONAL\n     *\"----------------------------------------------------------------------\n     DATA : BEGIN OF it_pcl1 OCCURS 0,\n     srtfd TYPE pcl1-srtfd,\n     END OF it_pcl1.\n\n     DATA BEGIN OF b1_key.\n          INCLUDE STRUCTURE pdc10.\n     DATA END OF b1_key.\n\n     IF actionid = 'P1'.\n\n     SELECT srtfd\n     FROM pcl1\n     INTO TABLE it_pcl1\n     WHERE relid EQ 'B1'\n     AND srtfd EQ pernr\n     AND srtf2 EQ 0.\n\n    LOOP AT it_pcl1.\n      MOVE it_pcl1-srtfd TO b1_key.\n      IMPORT st ert FROM DATABASE pcl1(b1) ID b1_key.\n      IF sy-subrc EQ 0.\n      ENDIF.\n    ENDLOOP.\n\n     ENDIF.\n\n     IF actionid = 'P2'.\n     DATA : it_rgdir TYPE TABLE OF pc261 INITIAL SIZE 0,\n           wa_rgdir LIKE LINE OF it_rgdir,\n           it_crt TYPE pay99_result-inter-crt,\n           wa_crt LIKE LINE OF it_crt,\n           wa_payrollresult TYPE pay99_result,\n           v_molga TYPE molga.\n\n    DATA : BEGIN OF wa_out,\n            pernr TYPE pernr-pernr,\n            gross TYPE pc207-betrg, \"Amount\n            net TYPE pc207-betrg,\n           END OF wa_out,\n           it_outtab LIKE TABLE OF wa_out.\n\n     wa_out-pernr = PERNR.\n     CALL FUNCTION 'CU_READ_RGDIR'\n     EXPORTING\n     persnr          = PERNR\n     IMPORTING\n     molga           = v_molga\n     TABLES\n     in_rgdir        = it_rgdir\n     EXCEPTIONS\n     no_record_found = 1\n     OTHERS          = 2.\n     IF sy-subrc = 0.\n\n\n\n    LOOP AT it_rgdir INTO wa_rgdir\n                       WHERE fpbeg GE startdate AND\n                             fpend LE enddate AND\n                             srtza EQ 'A'.  \"Current result\n        CALL FUNCTION 'PYXX_READ_PAYROLL_RESULT'\n          EXPORTING\n            clusterid                    = 'RD'\n            employeenumber               = PERNR\n            sequencenumber               = wa_rgdir-seqnr\n            READ_ONLY_INTERNATIONAL      = 'X'\n          CHANGING\n\n            payroll_result               = wa_payrollresult\n          EXCEPTIONS\n            illegal_isocode_or_clusterid = 1\n            error_generating_import      = 2\n            import_mismatch_error        = 3\n            subpool_dir_full             = 4\n            no_read_authority            = 5\n            no_record_found              = 6\n            versions_do_not_match        = 7\n            error_reading_archive        = 8\n            error_reading_relid          = 9\n            OTHERS                       = 10.\n\n        IF sy-subrc = 0.\n          LOOP AT wa_payrollresult-inter-crt INTO wa_crt.\n            CASE wa_crt-lgart.\n              WHEN '/101'.  \" Gross\n                APPEND wa_crt TO crt.\n            ENDCASE.\n            CLEAR wa_out.\n          ENDLOOP.\n        ENDIF.\n      ENDLOOP.\n    ENDIF.\n  ENDIF.\nENDFUNCTION.\n</code></pre> </li> <li> <p>Save the function module.</p> </li> </ol>"},{"location":"knowledge-base/read-data-from-cluster-fields-in-the-tables-pcl1-and-pcl2-payroll/#how-to-use-the-custom-function-module","title":"How to use the Custom Function Module","text":"<p>Depending on the table you want to access, define the following parameters:</p> Extract the payroll results of PCL1Extract the payroll results of PCL2 <ol> <li>Look up the function module using the BAPI extraction type.</li> <li>Populate the import parameter ACTIONID with value P1.</li> <li>Populate the field PERNR with a value for Personnel Number. The personnel number has to be entered with leading zeroes.</li> <li>Run the extraction. The data is available in the table parameter ST.</li> </ol> <ol> <li>Look up the function module using the BAPI extraction type.</li> <li>Populate the import parameter ACTIONID with value P2.</li> <li>Enter a start date and end date using the parameters STARTDATE and ENDDATE. The date fields have the format YYYYmmDD.</li> <li>Run the extraction. The data is available in the table parameter CRT.</li> </ol>"},{"location":"knowledge-base/register-rfc-server-in-sap-releases-in-kernel-release-720-and-higher/","title":"Register an RFC Server in SAP with Kernel Release 720 and higher","text":"<p>The following article shows how to register an RFC server in SAP releases with Kernel release 720 and higher.</p> <p>As of SAP Kernel Release 720, you can use the parameter gw/acl_mode to set an initial security environment to start and register external programs, e.g., RFC Server required for DeltaQ processing / customizing check.  If this value is set to 1, the DeltaQ extraction type cannot register the RFC Server and the Customizing Check returns the following error message: </p> <pre><code>RFC server is not working, please check gateway info.\n</code></pre> <p>There are two options to avoid this error:</p> <ul> <li>Set the Profile Parameter gw/acl_mode to 0.</li> <li>Define a whitelist of programs that can register at the SAP Gateway.</li> </ul>"},{"location":"knowledge-base/register-rfc-server-in-sap-releases-in-kernel-release-720-and-higher/#change-the-profile-parameter-gwacl_mode","title":"Change the Profile Parameter gw/acl_mode","text":"<p>When setting the profile parameter gw/acl_mode to 0 (default is 1), all RFC destinations / RFC servers with different Program IDs can register.</p> <ol> <li>Use SAP transaction RZ10 to open the \"Edit Profile\" menu.</li> <li>Select the profile name Default and Extended Maintenance.</li> <li>Click [Change] and set the profile parameter gw/acl_mode value to 0 </li> </ol> <p>The Customizing Check now executes without error messages.</p>"},{"location":"knowledge-base/register-rfc-server-in-sap-releases-in-kernel-release-720-and-higher/#define-a-whitelist-of-programs-at-the-sap-gateway","title":"Define a Whitelist of Programs at the SAP Gateway","text":"<p>To define a whitelist of programs that can register at the SAP Gateway, create two files named secinfo and reginfo.  Both files don't exist per default.</p> <p>Warning</p> <p>Registration of the RFC-server fails. The content of both files secinfo and reginfo override the parameter gw/acl_mode.  Make sure that both files secinfo and reginfo allow the registration of the RFC-server.  </p> <p>See the example below.</p> <ol> <li> <p>Create the files secinfo and reginfo. The files must have the following content:</p> Content of \"secinfo\"Content of \"reginfo\" <pre><code>#VERSION=2\nP TP=* HOST=internal,local CANCEL=internal,local ACCESS=internal,local\n# the following line should be the LAST line in the secinfo file\nP TP=XTRACT01 USER=* USER-HOST=* HOST=* \n</code></pre> <pre><code>#VERSION=2\n# the following line should be the LAST line in the reginfo file\nP TP=XTRACT01\n</code></pre> </li> <li> <p>Copy both files to the following directory (data path): <code>/usr/sap/&lt;SID&gt;/&lt;INSTANCE&gt;/data/</code></p> </li> <li>Extend the following two parameters to the Profile Parameter in the SAP transaction RZ10:<ul> <li>gw/reg_info = $(DIR_DATA)/reginfo</li> <li>gw/sec_info = $(DIR_DATA)/secinfo</li> </ul> </li> <li>Restart the gateway or re-read the security parameters using SAP transaction SMGW (navigate to Menu -&gt; Goto -&gt; Expert Functions -&gt; External Security -&gt; Reread). </li> </ol> <p>The Customizing Check now executes without error messages.</p>"},{"location":"knowledge-base/register-rfc-server-in-sap-releases-in-kernel-release-720-and-higher/#related-links","title":"Related Links","text":"<ul> <li>SAP Help: Gateway Security Files secinfo and reginfo</li> <li>SAP Note 1850230</li> <li>SAP Community blog</li> </ul>"},{"location":"knowledge-base/run-an-ADF-pipeline-when-sap-extraction-file-is-successfully-uploaded-to-Azure-storage/","title":"Run an ADF Pipeline when an SAP Extraction File is uploaded to Azure Storage","text":"<p>The following article shows how to run an event-driven pipeline in Azure Data Factory to process SAP data extracted with Xtract Universal into an Azure Storage. </p>"},{"location":"knowledge-base/run-an-ADF-pipeline-when-sap-extraction-file-is-successfully-uploaded-to-Azure-storage/#about","title":"About","text":"<p>The depicted example extracts and uploads SAP customer master data to Azure Storage.   An event then triggers an ADF pipeline to process the SAP parquet file, e.g. with Databricks. Xtract Universal supports different file formats for Azure storage, the depicted example uses Apache Parquet, which is a column file format that provides optimizations to speed up queries and is more efficient than CSV or JSON.</p> <p>Target audience: Customers, who utilize Azure Data Factory (ADF) as a platform for orchestrating data movement and transformation. </p> <p>Note</p> <p>The following sections describe the basic principles for triggering an ADF pipeline. Keep in mind, this is not a best practice document or a recommendation.</p>"},{"location":"knowledge-base/run-an-ADF-pipeline-when-sap-extraction-file-is-successfully-uploaded-to-Azure-storage/#azure-storage","title":"Azure Storage","text":"<p>Xtract Universal extracts SAP data and loads it into an Azure Storage as a parquet file. An Azure Storage event trigger is used to run an ADF pipeline for further processing of the SAP file. </p>"},{"location":"knowledge-base/run-an-ADF-pipeline-when-sap-extraction-file-is-successfully-uploaded-to-Azure-storage/#adf-pipelines-and-storage-event-triggers","title":"ADF Pipelines and Storage Event Triggers","text":"<p>The Master pipeline is triggered by an Azure Storage event and calls a child pipeline for further processing. The Master pipeline has an event trigger based on Azure storage. </p> <p>The Master pipeline has 2 activities: </p> <ul> <li>write a log to an Azure SQL database (optional)</li> <li>call a Child pipeline to process the parquet file with Databricks</li> </ul> <p>This article focuses on the Master pipeline.  The Child pipeline processes the parquet file e.g., with Databricks. The Child pipeline in this example is a placeholder. </p>"},{"location":"knowledge-base/run-an-ADF-pipeline-when-sap-extraction-file-is-successfully-uploaded-to-Azure-storage/#use-azure-sql-for-logging-optional","title":"Use Azure SQL for logging (optional)","text":"<p>In the scenario depicted, the ADF pipeline executes a stored procedure to log various details of the pipeline run into an Azure SQL table. </p>"},{"location":"knowledge-base/run-an-ADF-pipeline-when-sap-extraction-file-is-successfully-uploaded-to-Azure-storage/#prerequisites","title":"Prerequisites","text":"<ul> <li>Basic knowledge of Xtract Universal, see Getting Started.</li> <li>Basic knowledge of Azure Storage.</li> <li>You can successfully run extractions from a web browser, see Web API: Run Extractions.</li> <li>Azure Storage Destination is set up and running. </li> <li>Access to Azure Data Factory; basic knowledge of building ADF pipelines.</li> <li>Basic knowledge of ADF pipeline triggers, especially triggering a pipeline in response to a storage event. </li> </ul>"},{"location":"knowledge-base/run-an-ADF-pipeline-when-sap-extraction-file-is-successfully-uploaded-to-Azure-storage/#procedure","title":"Procedure","text":"<ol> <li> <p>Define an SAP extraction and set the destination to Azure Storage. The depicted example uses a storage account xtractstorage and a container called ke-container:</p> Destination SettingsDestination Details: Azure StorageDestination Details: File Format <p></p> <p></p> <p></p> </li> <li> <p>Define two pipelines in ADF: </p> <ul> <li>The master pipeline ProcessBlogStorageFile contains 2 activities. The first activity sp_pipelinelog  executes an SQL stored procedure to write a log entry to an Azure SQL table.  The second activity runs a dummy subpipeline  . As both activities are out of the scope of this article, there are no further details.  </li> <li>The child pipeline ProcessWithDataBricks processes the parquet file e.g., with Databricks.</li> </ul> </li> <li>Define the following parameters: <ul> <li><code>fileName</code>: contains the file Name in the Azure Storage.</li> <li><code>folderPath</code>: contains the file path in the Azure Storage. </li> </ul> </li> <li>Click [New/Edit] to add a new Storage Event Trigger in the ADF Pipeline. </li> <li>Adjust the details and use the Storage account name and Container name defined in the Xtract Universal Azure Storage destination: </li> <li>Adjust the event trigger parameters that are used as input parameters for the Master Pipeline: <ul> <li><code>@triggerBody().fileName</code></li> <li><code>@triggerBody().folderPath</code></li> </ul> </li> <li>Publish the pipeline.</li> <li>Run the extraction in Xtract Universal.</li> <li>When the extraction finishes successfully, check the Azure Storage. </li> <li>Check the log table in Azure SQL. The log table contains an entry, each for the master and child pipeline. </li> <li> <p>Check the trigger and pipeline runs in ADF.</p> Trigger RunsPipeline Runs <p></p> <p> </p> </li> </ol>"},{"location":"knowledge-base/run-an-ADF-pipeline-when-sap-extraction-file-is-successfully-uploaded-to-Azure-storage/#download-json-templates","title":"Download JSON Templates","text":"<p>Downloads for the trigger and the master pipeline are provided below:</p> <p> Download Trigger as json  Download MASTER pipeline as json</p>"},{"location":"knowledge-base/run-xu-in-aws/","title":"Run Xtract Universal in a VM on AWS EC2","text":"<p>The following article shows how to run Xtract Universal in a virtual machine on an AWS EC2.</p> <p>AWS enables running virtual servers (instances) in the cloud, see AWS Documentation: EC2. Theobald Software offers Xtract Universal as an Amazon Machine Image (AMI) for the following customer purposes:</p> <ul> <li>Xtract Universal evaluation</li> <li>Hosting of Xtract Universal in the cloud</li> </ul> <p>The Xtract Universal AMI can be selected when launching an instance in AWS.</p>"},{"location":"knowledge-base/run-xu-in-aws/#pre-configured-settings","title":"Pre-Configured Settings","text":"<p>When starting an Xtract Universal instance, the following settings are pre-configured:</p> Configuration Resources License Xtract Universal is already installed and running with a 30-days trial license. You can replace the trial license with your regular license. Documentation: Licensing Software Updates The pre-installed version of Xtract Universal is displayed in the AWS marketplace. Make sure to keep Xtract Universal up-to-date with the latest software releases, see Xtract Universal Changelog. Documentation: Installation and Update Server Settings The webserver is pre-configured with a self-signed TLS certificate and can be accessed in a browser via <code>https://xtractuniversal:8165/</code> from within the rdp session. Documentation: Server Settings"},{"location":"knowledge-base/run-xu-in-aws/#prerequisites","title":"Prerequisites","text":"<ul> <li>Access to an AWS account</li> <li>Access to the EC2 console</li> </ul> <p>There are multiple ways to start using Xtract Universal in EC2:</p> <ul> <li>Set Up from the AWS Marketplace</li> <li>Set Up directly from the EC2 Console</li> </ul>"},{"location":"knowledge-base/run-xu-in-aws/#set-up-from-the-aws-marketplace","title":"Set Up from the AWS Marketplace","text":"<p>Follow the steps below to set up Xtract Universal from the AWS Marketplace:</p> <ol> <li>Log in to AWS marketplace and open the Xtract Universal product page in AWS.</li> <li>Click [Continue to Subscribe] to subscribe to Xtract Universal. </li> <li>Click [Continue to Configuration] to select a software version and a region for hosting Xtract Universal.</li> <li>Click [Continue to Launch].  </li> <li>In Choose Action select Launch through EC2 to access all options for configuring a virtual machine in EC2.  </li> <li>Click [Launch]. The EC2 console opens.</li> <li>Set up your virtual machine, see AWS Documentation: Launch an Instance. The Xtract Universal AMI is already selected. </li> <li>Start the EC2 instance and connect to it, see AWS Documentation: Connect to an Instance.</li> <li>When connected, the Xtract Universal Designer is located on the Desktop. Start the Xtract Universal Designer.</li> <li>Set up an SAP connection and extractions, see Documentation: Getting Started.</li> </ol>"},{"location":"knowledge-base/run-xu-in-aws/#set-up-from-the-ec2-console","title":"Set Up from the EC2 Console","text":"<p>Follow the steps below to set up Xtract Universal directly from the EC2 console:</p> <ol> <li>Open the Amazon EC2 console.</li> <li>In the EC2 console dashboard, click [Launch instance]. </li> <li>In the Quick Start tab of Application and OS Images (Amazon Machine Image) click [Browse more AMIs].  </li> <li>Enter \"Xtract Universal\" in the search bar. Xtract Universal is listed under AWS Marketplace AMIs.</li> <li>Click [Select]. The application returns to the EC2 console. </li> <li>Set up your virtual machine, see AWS Documentation: Launch an Instance. The Xtract Universal AMI is already selected. </li> <li>Start the EC2 instance and connect to it, see AWS Documentation: Connect to an Instance.</li> <li>When connected, the Xtract Universal Designer is located on the Desktop. Start the Xtract Universal Designer.</li> <li>Set up an SAP connection and extractions, see Documentation: Getting Started.</li> </ol> <p>Note</p> <p>If you are not already subscribed to Xtract Universal, launching the EC2 instance automatically adds the subscription.</p>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/","title":"SAP Access with Xtract Universal and Powershell","text":"<p>This article contains examples of PowerShell scripts that query information from Xtract Universal and run extractions. The following sample scripts are available:</p> <ul> <li>Run an Extraction</li> <li>Run an Extraction with a Parameter</li> <li>Run an Extraction with a Parameter using PowerShell Variables</li> <li>Run an Extraction with Multiple Parameters</li> <li>Create a Function to Run an Extraction</li> <li>Loop an Array with Different Parameter Values</li> <li>Run multiple Extractions in Sequence</li> <li>Run multiple Extractions in Parallel</li> <li>Get a List of Defined Extractions</li> <li>Get the Latest Log of Extractions</li> <li>Get the Metadata of Extractions</li> </ul>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/#run-an-extraction","title":"Run an Extraction","text":"<p>Execute an Xtract Universal extraction using the command tool xu.exe in a PowerShell script as shown below:</p> Run an Extraction<pre><code># execute an Xtract Universal extraction using the command tool xu.exe in a PowerShell script\n# 2&gt;&amp;1 redirects standard error (the 2) to the same place as standard output (the 1)\n&amp;'C:\\Program Files\\XtractUniversal\\xu.exe' -s \"localhost\" -p \"8065\" -n \"SAPSalesCube\" 1&gt;$null 2&gt;1\n</code></pre>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/#run-an-extraction-with-a-parameter","title":"Run an Extraction with a Parameter","text":"<p>Here is an example of an extraction with a variable CalendarMonth that needs a value in the format YYYYMM, e.g. 201712:</p> Run an Extraction with a Parameter<pre><code># the extraction has a variable CalendarMonth that needs a value in the format YYYYMM, e.g. 201712\n&amp;'C:\\Program Files\\XtractUniversal\\xu.exe' -s \"localhost\" -p \"8065\" -n \"SAPSalesCube\" -o CalendarMonth='200401' 1&gt;$null 2&gt;&amp;1\n</code></pre>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/#run-an-extraction-with-a-parameter-using-powershell-variables","title":"Run an Extraction with a Parameter using PowerShell Variables","text":"<p>Here is an example of an extraction with a parameter using a PowerShell variable:</p> Run an Extraction with a Parameter using PowerShell Variables<pre><code># set the path to the installation folder\n$XUCmd = 'C:\\Program Files\\XtractUniversal\\xu.exe'\n# XU server &amp;amp; port\n$XUServer = \"localhost\"\n$XUPort = \"8065\"\n# extraction name\n$XUExtraction = \"SAPSalesCube\"\n\n# Setting Calendar month variable\n# the extraction has a variable CalendarMonth that needs a value in the format YYYYMM, e.g. 201712\n$myCalendarMonth = (Get-Date -format \"yyyyMM\")\n\n# run an extraction with one parameter\n&amp;$XUCmd -s $XUServer -p $XUPort -n $XUExtraction -o CalendarMonth=$myCalendarMonth 1&gt;$null 2&gt;&amp;1\n</code></pre>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/#run-an-extraction-with-multiple-parameters","title":"Run an Extraction with Multiple Parameters","text":"<p>Here is an example of an extraction with multiple parameters:</p> Run an Extraction with Multiple Parameters<pre><code># run an extraction with multiple parameters\n&amp;$XUCmd -s $XUServer -p $XUPort -n $XUExtraction -o CalendarMonth=$myCalendarMonth -o clearBuffer=true 1&gt;$null 2&gt;&amp;1\n</code></pre>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/#create-a-function-to-run-an-extraction","title":"Create a Function to Run an Extraction","text":"<p>Here is an example of a function that runs an extraction, checks the exit code and writes an output:</p> Create a Function to Run an Extraction<pre><code># Function to run an XU extraction\nFunction XURun($XUCmd, $XUServer, $XUPort, $XUExtraction, $XUParameters)\n{\nTry {\n$parameters = $XUCmd + \" \" + $XUServer + \" \" + $XUPort + \" \" + $XUExtraction + \" \" + $XUParameters\n\nif([string]::IsNullOrEmpty($XUParameters)){\n&amp;$XUCmd -s $XUServer -p $XUPort -n $XUExtraction 1&gt;$null 2&gt;&amp;1\n} else {\n&amp;$XUCmd -s $XUServer -p $XUPort -n $XUExtraction -o $XUParameters 1&gt;$null 2&gt;&amp;1\n}\n\n# check the last exit code\n# 0: successful\n# else unsuccessful\nif($LASTEXITCODE -eq 0) {\n\nwrite-host -f Green \"The last command $parameters has been executed successfully\" (Get-Date)\n\n} else {\n\nwrite-host -f Red \"The last execution for $parameters failed with error code $LASTEXITCODE!\" (Get-Date)\nWrite-Host $errorMessage\n}\n}\nCatch {\n\nwrite-host -f Red \"Error running XU extraction!\" + (Get-Date) $_.Exception.Message\n}\n}\n\n# define error message\n$errorMessage = @'\nIf the command completes an operation successfully, it returns an exit code of zero (0).\nIn case of an error, it will return one of the following (http status) codes:\nHTTP Statuscodes (e.g. 404 when the extraction does not exist)\n1001 An undefined error occured\n1002 Could not find the specified file\n1013 Invalid input data\n1014 The number of arguments is invalid\n1015 The parameter name is unknown\n1016 The argument is not valid\n1053 Something is wrong with your URL\n1087 The parameter is invalid\n'@\n\n# run an extraction with multiple parameters\n$XUParameters = \"clearBuffer=True -o CalendarMonth=$myCalendarMonth\"\n$XUResult = XURun -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtraction $XUExtraction -XUParameters $XUParameters\n</code></pre>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/#loop-an-array-with-different-parameter-values","title":"Loop an Array with Different Parameter Values","text":"<p>The depicted example\u00a0uses a loop to run an extraction with different\u00a0parameter values. The parameters values are defined in an array.</p> Loop an Array with Different Parameter Values<pre><code>$Months = @(\"200401\",\"200402\",\"200403\")\nforeach($Month in $Months){\nXURun -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtraction $XUExtraction -XUParameters CalendarMonth=$Month\n}\n</code></pre>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/#run-multiple-extractions-in-sequence","title":"Run multiple Extractions in Sequence","text":"<p>The depicted example\u00a0uses a loop tu run multiple extractions in sequence.\u00a0The extraction names are defined in an array.</p> Run multiple Extractions in Sequence<pre><code>Function XURun-Multi ($XUCmd, $XUServer, $XUPort, $XUExtractions,$XUParameters){\n\nforeach($XUExtraction in $XUExtractions){\n\nXURun -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtraction $XUExtraction -XUParameters $XUParameters\n}\n}\n\n#$XUExtractions = \"SAPCustomers\", \"SAPPlants\";,\"PSSAPCustomers\", \"PSSAPPlants\";\n$XUResult = XURUN-Multi -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtractions $XUExtractions\n</code></pre>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/#run-multiple-extractions-in-parallel","title":"Run multiple Extractions in Parallel","text":"<p>There are multiple ways to run parallel commands in PowerShell. One of them is using PowerShell Workflow.  Here is an example of using a ThrottleLimit to limit the number of parallel running extractions.</p> Run multiple Extractions in Parallel<pre><code># Define Workflow 1\n# Run multiple Extractions in parallell using powershell workflow\nWorkflow XURun-Parallel { param ($XUCmd, $XUServer, $XUPort, $XUExtractions, $XUParameters, $ThrottleLimit)\n\nforeach -parallel -throttlelimit $ThrottleLimit ($XUExtraction in $XUExtractions){\n\nInlineScript{\nif([string]::IsNullOrEmpty($XUParameters)){\n&amp;$Using:XUCmd -s $Using:XUServer -p $Using:XUPort -n $Using:XUExtraction 1&gt;$null 2&gt;&amp;1\n} else {\n&amp;$Using:XUCmd -s $Using:XUServer -p $Using:XUPort -n $Using:XUExtraction -o $Using:XUParameters 1&gt;$null 2&gt;&amp;1\n}\n}\n\n}\n}\n\n# 4 parallel extractions\n$ThrottleLimit = 4\nXURun-Parallel -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtractions $XUExtractions -XUParameters $XUParamters -ThrottleLimit $ThrottleLimit\n\n# Define Workflow 2\n\n# Run multiple Extractions using PowerShell workflow\n\nWorkflow XURun-Parallel2{ param ($XUCmd, $XUServer, $XUPort, $XUExtractions, $XUParameters, $ThrottleLimit)\n\nforeach -parallel -throttlelimit $ThrottleLimit ($XUExtraction in $XUExtractions){\n\nInlineScript{\n\nTry {\n$parameters = $Using:XUCmd + \" \" + $Using:XUServer + \" \" + $Using:XUPort + \" \" + $Using:XUExtraction + \" \" + $Using:XUParameters\n\nif([string]::IsNullOrEmpty($Using:XUParameters)){\n&amp;$Using:XUCmd -s $Using:XUServer -p $Using:XUPort -n $Using:XUExtraction 1&gt;$null 2&gt;&amp;1\n} else {\n&amp;$Using:XUCmd -s $Using:XUServer -p $Using:XUPort -n $Using:XUExtraction -o $Using:XUParameters 1&gt;$null 2&gt;&amp;1\n}\n\n# check the last exit code\n# 0: successful\n# else unsuccessful\nif($LASTEXITCODE -eq 0) {\n\nwrite-host -f Green \"The last command $Using:parameters has been executed successfully\" (Get-Date)\n\n} else {\n\nwrite-host -f Red \"The last execution for $Using:parameters failed with error code $LASTEXITCODE!\" (Get-Date)\nWrite-Host $errorMessage\n}\n}\nCatch {\n\nwrite-host -f Red \"Error running XU extraction!\" + (Get-Date) $_.Exception.Message\n}\n}\n\n}\n}\n\n# 4 parallel extractions\n$ThrottleLimit = 4\nXURun-Parallel2 -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtractions $XUExtractions -XUParameters $XUParamters -ThrottleLimit $ThrottleLimit\n</code></pre>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/#get-a-list-of-defined-extractions","title":"Get a List of Defined Extractions","text":"<p>Xtract Universal offers an HTTP API to access the defined extractions, their metadata and log, the server log and further information. The following function\u00a0queries the list of extractions from the repository. The output will have the following format for each extraction.</p> <p>Name : BWCubeFIGL Type : BWCube Source : sapbw Destination : tableau LastRun : 2018-04-25_12:44:02.422 RowCount : 2733787 LastChange : 2018-02-16_12:18:29.475 Created : 2018-02-14_11:25:47.718</p> Get a List of Defined Extractions<pre><code>&lt;pre&gt;Function XUGet-Extractions($XUServer, $XUPort){\n$XUExtractions= (Invoke-WebRequest \"http://$XUServer`:$XUPort\").Content | ConvertFrom-CSV\nreturn $XUExtractions\n}\n$XUExtractions = XUGet-Extractions $XUServer $XUPort\n[code lang='powershell']\nThe following functions gets the list of extraction names from repository. This list can be then used e.g. to run the extraction or to check their logs.\n[code lang='powershell']\nFunction XUGet-ExtractionNames($XUServer, $XUPort){\n$XUExtractions = XUGet-Extractions $XUServer $XUPort\n$XUExtractionNames = $XUExtractions | foreach { $_.Name } #| where{$_ -like \"*PSSAP*\"}\nreturn $XUExtractionNames\n}\n$XUExtractionNames = XUGet-ExtractionNames $XUServer $XUPort\n\n# run all the extractions in the list\nXURun-Parallel2 -XUCmd $XUCmd -XUServer $XUServer -XUPort $XUPort -XUExtractions $XUExtractionNames\n</code></pre>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/#get-the-latest-log-of-extractions","title":"Get the Latest Log of Extractions","text":"<p>The following script gets the latest log of the extractions and writes a colorful output depending on the log status.</p> Get the Latest Log of Extractions<pre><code>Function XUGet-Log($XUServer, $XUPort){\n$XUExtractionNames = XUGet-ExtractionNames $XUServer $XUPort\n$XULog = @{}\nforeach ($XUExtractName in $XUExtractionNames) {\n# concatenate URL\n$XUURL = \"http://$XUServer`:$XUPort/log/?req_type=extraction&amp;amp;name=$XUExtractName\"\n# get log, convert it to csv, sort by timestamp and select the newest log\n$newestLog = (Invoke-WebRequest $XUURL).Content | ConvertFrom-CSV | Sort-Object Timestamp | Select-Object -Last 1\n# chech log status\nSwitch ($newestLog.StateDescr) {\n\"FinishedNoErrors\"{ write-host -f Green $XUExtractName $newestLog}\n\"FinishedErrors\" {write-host -f Red $XUExtractName $newestLog}\n\"Running\" {write-host -f Yellow $XUExtractName $newestLog}\n\"NotAvailable\"{write-host -f Blue $XUExtractName $newestLog}\n}\n$XULog.Add($XUExtractName, $newestLog)\n}\nreturn $XULog\n}\n$XULog = XUGet-Log $XUServer $XUPort\n</code></pre>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/#get-the-metadata-of-extractions","title":"Get the Metadata of Extractions","text":"<p>This function gets the metadata of the extractions, including field names, data types etc. The output has the following format for each extraction:</p> <p>POSITION,NAME,DESC,TYPE,LENGTH,DECIMALS 0,WERKS,Plant,C,4,0 1,NAME1,Name,C,30,0 2,KUNNR,Customer number of plant,C,10,0 3,NAME2,Name 2,C,30,0</p> Get the Metadata of Extractions<pre><code># Get Metadata\n# http://[host]:[port]/metadata/?name=[extractionName]\nFunction XUGet-Metadata($XUServer, $XUPort){\n$XUExtractionNames = XUGet-ExtractionNames $XUServer $XUPort\n$XUMetadata = @{}\nforeach ($XUExtractName in $XUExtractionNames) {\n# concatenate URL\n$XUURL = \"http://$XUServer`:$XUPort/metadata/?name=$XUExtractName\"\n# get log, convert it to csv, sort by timestamp and select the newest log\n$tmpmeta = (Invoke-WebRequest $XUURL).Content | ConvertFrom-CSV\n$XUMetadata.Add($XUExtractName, $tmpmeta)\n}\nreturn $XUMetadata\n}\n$XUMetadata = XUGet-Metadata $XUServer $XUPort\n</code></pre>"},{"location":"knowledge-base/sap-access-with-xtract-universal-and-powershell/#related-links","title":"Related Links:","text":"<ul> <li>Get all scripts from GitHub</li> <li>Documentation: Call extractions from command line</li> <li>Microsoft PowerShell Documentation</li> </ul>"},{"location":"knowledge-base/sharepoint-lists-notification-using-intelligent-merge-procedure/","title":"SharePoint Lists Notification using Intelligent Merge Procedure","text":"<p>The following article shows how to create SharePoint lists notification using intelligent merge procedure of Xtract Universal. </p>"},{"location":"knowledge-base/sharepoint-lists-notification-using-intelligent-merge-procedure/#procedure","title":"Procedure","text":"<p>As the first step, customer master data fron table KNA1 is be loaded into SharePoint.</p> <p></p> <p>Afterwards, the alert functionality for the SharePoint list is set up.</p> <p>Note</p> <p>The alert setup functionality is available in SharePoint 2013 as well as SharePoint Online by default.</p> <p></p> <p>As the following step, the customer master data is changed in SAP:</p> <p></p> <p>Next, the complete table KNA1 is extracted again.</p> <p>Xtract Universal's intelligent merge procedure identifies and sends only those data records to SharePoint that have either been changed or added since the last upload.</p> <p></p>"},{"location":"knowledge-base/sharepoint-lists-notification-using-intelligent-merge-procedure/#results","title":"Results","text":"<p>The SharePoint list now contains the updated data set.</p> <p></p>"},{"location":"knowledge-base/sharepoint-lists-notification-using-intelligent-merge-procedure/#email-notification","title":"Email Notification","text":"<p>The embedded user in the SharePoint alert is informed by email about the changed records.</p> <p></p>"},{"location":"knowledge-base/sharepoint-lists-notification-using-intelligent-merge-procedure/#related-links","title":"Related Links","text":"<ul> <li>Documentation: SharePoint</li> </ul>"},{"location":"knowledge-base/skip-rows-in-reports/","title":"Skip Rows in Reports","text":"<p>This articles shows how to skip certain rows when extracting SAP reports. The depicted example uses the Row Skip Pattern option of the Report extraction type to remove rows that contain the * symbol in extracted reports.</p> <p>Tip</p> <p>To skip header and/or footer rows that do not follow any pattern, use the Row settings in the main window of the extraction type. Row settings allows skipping rows at the top and at the bottom of the report.</p>"},{"location":"knowledge-base/skip-rows-in-reports/#about-row-skip-patterns","title":"About Row Skip Patterns","text":"<p>The extraction settings of the Report extraction type include the Row Skip Pattern option. Row Skip Pattern allows you to define a pattern that removes all rows that contain the pattern from the extracted report. This can be used to remove header rows that are repeated in the output body of reports.</p> <ul> <li>Regular expressions are supported</li> <li>Multiple row skip patterns can be entered, separated by the pipe symbol \"<code>|</code>\", e.g., <code>2020|2021|-|Sum</code> removes all rows containing \u20182020\u2019, \u20182021\u2019, \u2018-\u2018 and \u2018Sum\u2019.</li> <li>To process special symbols, add <code>\\</code>before the symbol, e.g., <code>\\*</code> removes rows that contain the sum symbol * .</li> <li>Only works with database destinations, e.g., Azure Storage, Snowflake, SQl Server, etc.</li> </ul> <p>Note</p> <p>The live preview in the Report extraction type does not include the Row Skip Pattern option, because the rows are only removed after the report data is extracted from SAP. </p>"},{"location":"knowledge-base/skip-rows-in-reports/#how-to-use-row-skip-patterns","title":"How to Use Row Skip Patterns","text":"<p>In the depicted example, the report RFITEMGL contains rows that total the content of the rows before.  These rows are marked with an * symbol: </p> Report without Row Skip PatternsReport with Row Skip Patterns <p></p> <p></p> <p>To remove all rows that contain the * symbol, follow the instructions below:</p> <ol> <li>Look up a report. The depicted example uses the report RFITEMGL. </li> <li>Define the columns of the report automatically or manually, see Documentation: Define the Report Extraction Type.  </li> <li>Click Extraction Settings. The window \"Extraction Settings\" open. </li> <li>Enter a Row skip pattern. To process special symbols, add <code>\\</code>before the symbol, e.g., to remove rows that contain the sum symbol * , enter <code>\\*</code>. </li> <li>Click [OK] to close the extraction settings. </li> <li>Click [OK] to save the extraction. Note that the live preview of the report component does not include the Row skip pattern option.  Row skip pattern is only executed when running the extraction.</li> <li>Run the extraction. </li> </ol> <p>The extracted report does not include rows that contain a * symbol.</p>"},{"location":"knowledge-base/sso-with-client-certificates/","title":"SSO with Client Certificates","text":"<p>The following article describes the process of setting up Single-Sign-On (SSO) via Secure Network Communication (SNC) with SAP client certificates.  For more information on using SSO with Xtract Universal (XU), see Documentation: SAP Single-Sign-On.</p>"},{"location":"knowledge-base/sso-with-client-certificates/#requirements","title":"Requirements","text":"<p>The usage of SSO Certificate requires the correct characteristics of the architecture:</p> <ul> <li>Implement SAP SSO  with X.509 certificates without Secure Login Server, see SAP-Documentation: Authentication Methods without Secure Login Server.</li> <li>Implement Microsoft Certificate Store and Active Directory Certificate Templates for SAPGUI/RFC, see Microsoft TechNet: Certificate Template.</li> <li>Set up an enrollment agent for Xtract Universal in AD, see Microsoft TechNet: Establish Restricted Enrollment Agents.</li> <li>Install the SAP Secure Login Client on the server that runs Xtract Universal, see SAP-Documentation: Secure Login Client. The Secure Login Client ensures that the correct SNC library is available for SSO Certificate. This library is used to open the SAP connection.</li> <li>The Xtract Universal service must run under a Windows AD Service account, see Run the Xtract Universal Service under a Windows Service Account.</li> <li>Set up access restrictions for the Xtract Universal Designer and the XU server, see Restrict Access to Windows AD Users (Kerberos Authentication).</li> </ul>"},{"location":"knowledge-base/sso-with-client-certificates/#process","title":"Process","text":"<p>The following graphic illustrates the process of authentication via SSO Certificate:</p> <p></p> <ol> <li>The user of the BI tool (caller) triggers an extraction by calling the XU webservice of your Xtract product. The caller uses their Active Directory identity to authenticate against the XU webservice via HTTPS and SPNEGO.</li> <li>The XU server checks if a certificate for the caller is available in the Windows Certificate Store. If no certificate is available for the caller, a new certificate is issued by the Windows enrollment agent.<ol> <li>The XU server requests the Client certificate from the Windows Certificate Store via the Windows API. If a certificate is available, the process continues with step 3. If no certificate is available steps 2b) to 2e) are executed.</li> <li>The XU server requests an enrollment agent certificate from the Windows Certificate Store via the Windows API. The enrollment agent certificate can be used to issue client certificates.</li> <li>The XU server receives the enrollment agent certificate from the Windows Certificate Store.</li> <li>If the requested certificate from 2a) is not found in the Windows Certificate Store, the XU server enrolls a new client certificate for the caller using the enrollment agent certificate.</li> <li>The Windows Certificate Store receives the new client certificate from the Active Directory Services via MSRPC.</li> </ol> </li> <li>The XU server receives the client certificate of the caller from the Windows Certificate Store.</li> <li>The XU server configures the SAP Secure Login Client via the Windows Registry.</li> <li>The Secure Login Client receives the caller's client certificate as specified by the XU server in step 4 from the Windows Certificate Store.</li> <li>The Secure Login Client uses the client certificate of the caller to authenticate the caller's identity via SNC against SAP.</li> <li>The XU server extracts data with the identity and privileges of the caller.</li> <li>The XU server loads the extracted data from 7 to the tool that triggered the extraction.</li> </ol>"},{"location":"knowledge-base/sso-with-client-certificates/#setting-up-sso-and-snc-with-client-certificates","title":"Setting up SSO and SNC with Client Certificates","text":"<p>Create a new SAP source system in Xtract Universal to set up SSO with client certificates:</p> <ol> <li>Navigate to [Server &gt; Manage Sources] in the main menu of the Designer. The window \"Manage Sources\" opens.</li> <li>Click [Add] to create a new SAP source.</li> <li>Open the tab General and enter the connection details of your SAP system.  </li> <li>Open the tab Authentication and activate the option SNC. </li> <li>Enter the path to the 64bit version of the SAP Crypto Library in the field SNC library, e.g., <code>C:\\Program Files\\SAP\\FrontEnd\\SecureLogin\\lib\\sapcrypto.dll</code>. The SAP Crypto Library is installed as part of the SAP Secure Login Client.</li> <li>Enter the SNC partner name of the SAP system in the field SNC partner name.  This is the same partner name as the SNC name used to set up the SAP GUI.</li> <li>Activate the option Enroll certificate on behalf of caller (Certificate SSO). </li> <li>Enter the technical name of the Active Directory Certificate Template used to authenticate SAP users.</li> <li>Enter the thumbprint of the certificate of the enrollment agent. If you don't know the name or thumbprint, consult the IT department that manages the Active Directory Certificate Services.</li> <li>Click [Test Designer Connection] to test your connection settings.</li> <li>Click [OK] to confirm your input.</li> </ol> <p>Tip</p> <p>Create new extractions in the test environment with an SAP connection that uses Plain Authentication. Change the SAP source when moving the extraction to the productive environment.</p>"},{"location":"knowledge-base/sso-with-client-certificates/#related-links","title":"Related Links","text":"<ul> <li>SAP Help: Secure Network Communications</li> <li>SAP Help: Secure Login Client</li> <li>SAP Help: Logging on with Secure Login Client Using SNC</li> </ul>"},{"location":"knowledge-base/sso-with-external-id/","title":"SSO with External ID","text":"<p>The following article shows how to set up Single Sign-On (SSO) with Secure Network Communication (SNC) and External ID. SSO with External ID uses a Personal Security Environment (PSE) to create a trust relationship between the SAP application server and the service account that runs Xtract Universal. This allows Xtract Universal to impersonate any SAP user.</p>"},{"location":"knowledge-base/sso-with-external-id/#requirements","title":"Requirements","text":"<p>The usage of SSO with External ID requires:</p> <ul> <li>The Xtract Universal service must run under a Windows AD Service account, see Run the Xtract Universal Service under a Windows Service Account.</li> <li>Set up access restrictions for the Xtract Universal Designer and the XU server, see Restrict Access to Windows AD Users (Kerberos Authentication).</li> <li>Windows AD users must be mapped to SAP users in the SAP table USRACL, see SAP Help: User Authentication and Single Sign-On. </li> <li>The SAP CommonCryptoLib must be installed on the machine that runs Xtract Universal, see SAP Note 1848999. Copy the library (sapcrypto.dll) and the command line tool (sapgenpse.exe) to a local directory, e.g. <code>C:\\PSE\\</code>. For more information, see SAP Help: Downloading and Installing the SAP Cryptographic Library.</li> <li>The environment variables SECUDIR and SNC_LIB must be set to the PSE directory that contains the SAP CommonCryptoLib.</li> </ul> <p>For more information on PSE, see SAP Help: Creating PSEs and Maintaining the PSE Infrastructure.</p>"},{"location":"knowledge-base/sso-with-external-id/#the-process","title":"The Process","text":"<p>SSO with External ID uses an X.509 certificate &amp; PSE to create a trust relationship between the SAP application server and the service account that runs Xtract Universal. This allows Xtract Universal to impersonate any SAP user.</p> <p></p> <ol> <li>Users authenticate themselves against Xtract Universal via Active Directory (Kerberos) and request data from SAP.</li> <li>Xtract Universal opens an RFC connection via SNC and uses PSE &amp; External ID for authentication.</li> <li>Xtract Universal reads the SAP table USRACL to determine the SAP user that is mapped to the Active Directory user from step 1.</li> <li>Xtract Universal then impersonates the mapped SAP user to request the SAP data via SNC.</li> <li>Xtract Universal retrieves the requested SAP data with the privileges of the caller.</li> <li>Xtract Universal loads the extracted SAP data to the tool that triggered the extraction.</li> </ol>"},{"location":"knowledge-base/sso-with-external-id/#setup-in-sap","title":"Setup in SAP","text":"<ol> <li>Use the SAPGENPSE command line tool to generate an X.509 certificate for the Windows service account that runs Xtract Universal.  Use the following command to create the certificate:  <pre><code>sapgenpse gen_pse -p theo-xu.pse\n</code></pre> The distinguished name of the PSE owner can be the fully qualified hostname of the Xtract Universal server, e.g., <code>CN=xuserver.example.com</code>. </li> <li>Use the the following command to export the certificate: <pre><code>sapgenpse export_own_cert -v -p theo-xu.pse -o theo-xu.crt\n</code></pre></li> <li>Use SAP transaction STRUST to add the certificate to the list of trusted PSE certificates, see SAP Help: Adding Certificates to PSE Certificate Lists.</li> <li>Use SAP transaction SNC0 to create an access control list item that allows RFC and external IDs for the Common Name (CN) of the certificate created in step 1. </li> <li>Use SAP transaction STRUST to export the server certificate of the SAP server, see SAP Help: Exporting the AS ABAP's Public-Key Certificate.</li> <li>Copy the exported server certificate to the PSE directory of the machine that runs Xtract Universal.</li> <li>Use the SAPGENPSE command line tool to import the server certificate to the client PSE. Example:  <pre><code>sapgenpse maintain_pk -v -a server.crt -p theo-xu.pse\n</code></pre></li> <li>Use the following command to create a credentials file (cred_v2), see SAP Help: Creating the Server's Credentials Using SAPGENPSE.  <pre><code>sapgenpse seclogin -p theo-xu.pse \u2013O SAPServiceUser\n</code></pre> The credentials file gives Xtract Universal access to the PSE without providing the password for the PSE.</li> </ol> <p>The PSE directory should now contain the following files:</p> <ul> <li>the client PSE <code>theo-xu.pse</code></li> <li>the client certificate <code>theo-xu.crt</code></li> <li>the server certificate that was exported from your SAP system <code>[server].crt</code></li> <li>the credentials file <code>cred_v2</code></li> </ul>"},{"location":"knowledge-base/sso-with-external-id/#setup-in-xtract-universal","title":"Setup in Xtract Universal","text":"<p>Create a new SAP source system in your Xtract Universal to set up SSO with External ID:</p> <ol> <li>Navigate to [Server &gt; Manage Sources] in the main menu of the Designer. The window \"Manage Sources\" opens.</li> <li>Click [Add] to create a new SAP source.</li> <li>Open the tab General and enter the connection details of your SAP system.</li> <li>Open the tab Authentication and activate the option Secure Network Communications (SNC). </li> <li>Enter the name of an SAP user in the field User for the initial login with Xtract Universal.  This user must be a technical user (SAP user with security policy set to Technical User) and must have privileges to read the SAP table USRACL via the function module RFC_READ_TABLE. </li> <li>Enter the complete path to the SAP cryptographic library in the field SNC Library, e.g. <code>C:\\PSE\\sapcrypto.dll</code>.</li> <li>Enter the SPN of the SAP service account in the field SNC partner name. Use the following notation: <code>p:[SPN]@[Domain-FQDN-Uppercase]</code>. </li> <li>Enable the option SSO - Log in as caller via External ID.</li> <li>Click [Test Connection] to verify your connection settings.</li> <li>Click [OK] to save your changes. </li> </ol>"},{"location":"knowledge-base/sso-with-external-id/#related-links","title":"Related Links","text":"<ul> <li>Documentation: SAP Single-Sign-On</li> <li>Documentation: Run the Xtract Universal Service under a Windows Service Account.</li> <li>SAP Help: Creating PSEs and Maintaining the PSE Infrastructure</li> </ul>"},{"location":"knowledge-base/sso-with-kerberos-snc/","title":"SSO with Kerberos SNC","text":"<p>The following article describes the required steps for setting up Single Sign-On (SSO) with Secure Network Communication (SNC) and Kerberos encryption.</p> <p>Note</p> <p>SAP officially does not support the Kerberos Wrapper Library (gx64krb5.dll) anymore.</p> <p>Warning</p> <p>Single Sign-On availability.   ABAP application server has to run on a Windows OS and SNC with Kerberos encryption setup on SAP.</p>"},{"location":"knowledge-base/sso-with-kerberos-snc/#requirements","title":"Requirements","text":"<ul> <li>The SAP ABAP application server runs under a Windows operating system. </li> <li>The BI client (which calls the extraction) runs under Windows.</li> <li>The SAP Kerberos Wrapper Library (gsskrb5) is used as the SNC solution.</li> </ul>"},{"location":"knowledge-base/sso-with-kerberos-snc/#double-hop-issue","title":"\"Double Hop\" Issue","text":"<p>SNC solution must support the Windows credentials being passed on by Xtract Universal.  Since Active Directory is based on Kerberos, a \"Double Hop\" issue may occur.  Here is a possible solution to the \"Double Hop\" issue:</p> <p>For security reasons, Kerberos does not allow credentials to be passed on.  But there are Kerberos extensions from Microsoft (S4U extensions) that allow credentials passing on.  These extensions are also known as \"constrained delegation\". Because the Kerberos Wrapper Library uses the Microsoft extensions for Kerberos to work around the \"Double Hop\" issue, it is only available for Windows.  It therefore only works with SAP application servers under Windows and clients under Windows.</p> <p>Unlike the Kerberos Wrapper Library (gsskrb5) from SAP (according to SAP), SAP's Common Crypto Library does not explicitly support credentials to be passed on.  The Kerberos Wrapper Library (gsskrb5) used by multiple customers of Theobald Software.</p> <p>When using an SNC solution from a third-party vendor, use either the Kerberos Wrapper Library or a corresponding solution of the third-party vendor.</p>"},{"location":"knowledge-base/sso-with-kerberos-snc/#activation-of-https","title":"Activation of HTTPS","text":"<ol> <li>Enable access control protocol HTTPS   within the tab Web Server settings.</li> <li>Reference an existing X.509 certificate .</li> <li>Click [OK] to confirm  . </li> </ol> <p>Make sure to check the default ports, e.g., 8165 in Xtract Universal.</p>"},{"location":"knowledge-base/sso-with-kerberos-snc/#configuration-of-windows-ad-service-account","title":"Configuration of Windows AD Service Account","text":"<p>Using SSO with Kerberos SNC, the Xtract Universal service must run under a dedicated service account. To do so, follow the steps as outlined in Run the Xtract Universal Service under a Windows Service Account.</p> <p>Note</p> <p>As of Xtract Universal version 5.0 SAP passwords are encrypted with a key that is derived from the Windows account that runs the XU service. The passwords can only be accessed from the same service account, when restoring a backup or moving the files to a different machine.  If the service account changes, passwords need to be re-entered manually.</p>"},{"location":"knowledge-base/sso-with-kerberos-snc/#server-settings","title":"Server Settings","text":"<p>Warning</p> <p>Incompatible library.  Xtract Universal runs on 64bit OS only. Kerberos Wrapper Library gx64krb5.dll(64-Bit version) is required.  Download <code>gx64krb5.dll</code> from SAP Note 2115486.</p> <ol> <li>Copy the Kerberos Wrapper Library to the file system, e.g., to <code>C:\\SNC\\gx64krb5.dll</code> of the machine, where the service is running.</li> <li>Place the downloaded .dll file on each machine, where the Designer is running.</li> <li>Open \"Computer Management\" by entering the following command: <code>compmgmt.msc</code>.</li> <li>Under Local Users and Groups select Groups &gt; Administrators.</li> <li>Click [Add]  to add the service account to the local admin group . </li> <li>Open \"Local Security policy\" by entering the following command: <code>secpol.msc</code>.  </li> <li>Select [Local Policies &gt; User Rights Assignment]:<ul> <li>Act as part of the operating system </li> <li>Impersonate a client after authentication</li> </ul> </li> <li> <p>Change the registry settings of the server machine:</p> Field Registry Entry SubKey HKEY_LOCAL_MACHINE\\SOFTWARE\\SAP\\gsskrb5 Entry ForceIniCredOK Type REG_DWORD Value 1 </li> </ol>"},{"location":"knowledge-base/sso-with-kerberos-snc/#sap-source-settings","title":"SAP Source Settings","text":"<p>Note</p> <p>An existing SAP connection to a Single Application Server or Message Server is the prerequisite for using SSO with SNC.</p> <ol> <li>In the main menu of the Xtract Universal Designer, navigate to [Server &gt; Manage Sources]. The window \"Source Details\" opens. </li> <li>Select an existing SAP source and click [Edit] (pencil symbol).</li> <li>Enable the SNC option   in the subsection Authentication. </li> <li>Enable the checkbox Impersonate authenticated caller (SSO) .</li> <li>Enter the complete path of the Kerberos library in the field SNC library e.g., <code>C:\\SNC\\gx64krb5.dll</code> .</li> <li>Enter the SPN of the SAP service account in the field Partner name . Use the following notation: <code>p:[SPN]@[Domain-FQDN-Uppercase]</code>.  </li> <li>Click [Test Connection] to verify your connection settings.</li> <li>Click [OK] to confirm.</li> </ol> <p>Note</p> <p>The SAP Logon Pad SNC settings for partner name differ from the ones used in Xtract Universal. SAP Logon Pad uses the UPN of the SAP service accounts and Xtract Universal uses the Service Principal Name (SPN).  Use the following notation: p:[SAP Service Account]@[domain]. SPN's are case sensitive in the SNC partner name.</p>"},{"location":"knowledge-base/sso-with-kerberos-snc/#snc-activation-in-sap","title":"SNC Activation in SAP","text":"<p>In SAP, apply the Kerberos SNC settings as described in the SAP Help: Single Sign-On with Microsoft Kerberos SSP.</p>"},{"location":"knowledge-base/sso-with-kerberos-snc/#related-links","title":"Related Links","text":"<ul> <li>Documentation: X.509 certificate</li> <li>Documentation: Run the Xtract Universal Service under a Windows Service Account.</li> </ul>"},{"location":"knowledge-base/sso-with-logon-ticket/","title":"SSO with Logon-Ticket","text":"<p>The following article describes the process of running extractions for pull destinations using Single-Sign-On (SSO) with SAP Logon-Ticket. For more information on using SSO with Xtract Universal (XU), see Documentation: SAP Single-Sign-On.</p>"},{"location":"knowledge-base/sso-with-logon-ticket/#requirements","title":"Requirements","text":"<ul> <li>The Xtract Universal server must be set up to use HTTPS, see SSO with Kerberos SNC. </li> <li>The Xtract Universal service must run under an XU Service Account.</li> <li>The XU service account must be configured for Constrained Delegation to the SPN of the AS Java in AD.</li> <li>An Application Server Java (AS Java) must be set up as a Ticket Issuing System, see SAP Help: Configuring the AS Java to Issue Logon Tickets.</li> <li>The AS Java must be configured for SPNEGO/Kerberos.</li> <li>A mapping between Windows AD users and SAP identities must be configured in the AS Java.  The AS Java must be configured to generate SAP Logon Tickets. Consult with your SAP Basis team for more information.</li> <li>The SAP AS ABAP must be configured to trust SAP Logon Tickets from the AS Java, see SAP Help: Using Logon Tickets with AS ABAP.</li> </ul>"},{"location":"knowledge-base/sso-with-logon-ticket/#process-of-authentication","title":"Process of Authentication","text":"<p>The following graphic illustrates the process of calling an extraction with SSO via Ticket Issuer: </p> <ol> <li>The BI tool user starts an extraction by calling the XU web service.  They authenticate against the XU web service with their Active Directory identity, using HTTPS and SPNEGO.</li> <li>The XU server contacts the Active Directory Domain Controller via Kerberos and tries to impersonate the web service caller (BI tool user) against the SAP AS Java (ticket issuer).</li> <li>The XU server receives a Kerberos ticket from the DC that allows it to impersonate the caller against the AS Java.</li> <li>The XU server uses the Kerberos ticket from 3. to authenticate against the AS Java as the caller via HTTPS and SPNEGO.Prerequisite: The AS Java has been configured for SPNEGO/Kerberos.</li> <li>The AS Java maps the caller's AD identity to an SAP user and generates an SAP Logon Ticket for this SAP user.  The AS Java sends the SAP Logon Ticket to the XU server via HTTPS as the value of the MYSAPSSO2 cookie.</li> <li>The XU server takes the SAP Logon Ticket that it has received from the AS Java and uses it to authenticate against the AS ABAP as the caller via RFC.</li> <li>The XU server extracts data from the AS ABAP using the identity and privileges of the caller (BI tool user) via RFC.</li> <li>The XU server sends the extracted data from 7. to the caller.</li> </ol>"},{"location":"knowledge-base/sso-with-logon-ticket/#related-links","title":"Related Links","text":"<ul> <li>Set Up an XU Service Account</li> <li>SAP Help: Kerberos and SAP NetWeaver AS for Java</li> <li>SAP Help: Using Logon Tickets with AS ABAP</li> <li>Youtube-Tutorial: Kerberos-Based Single Sign-On to Application Server Java Unlisted</li> </ul>"},{"location":"knowledge-base/supported-sap-and-hana-versions/","title":"Supported SAP S/4HANA Versions","text":"<p>The following article gives an overview of all SAP S/4HANA systems supported by Xtract Universal.</p>"},{"location":"knowledge-base/supported-sap-and-hana-versions/#supported-sap-s4hana-cloud-systems","title":"Supported SAP S/4HANA Cloud Systems","text":"<p>The following table lists all available SAP S/4HANA cloud systems and their compatibility with Xtract Universal. SAP S/4HANA Cloud Edition (Cloud ERP):</p> S/4HANA Public Cloud S/4HANA Private Cloud RISE with SAP S/4HANA Cloud Private Edition Compatible with Xtract Universal:  (with limitations)   (with limitations) Limitations: Only Remote Function Modules (BAPIs) released via Communication Scenario are supported. The installation of custom function modules is restricted.Transport requests for Table and Report cannot be imported by customers, but must be requested from SAP. -"},{"location":"knowledge-base/supported-sap-and-hana-versions/#sap-s4hana-on-premise-systems","title":"SAP S/4HANA On Premise Systems","text":"<p>The following table lists all available SAP S/4HANA on premise systems and their compatibility with Xtract Universal. SAP S/4HANA AnyPremise (ERP in the Cloud):</p> S/4HANA Private Cloud Managed by SAP (HEC) SAP HEC Customer Edition S/4HANA AnyPremise S/4HANA AnyPremise Additional Information: On-Premises Edition Runs on Customer Data Center Public Cloud (AZURE/AWS/GCP) called ERP in DC (Customer Data Center) Compatible with Xtract Universal:"},{"location":"knowledge-base/table-cdc-initial-table-load/","title":"Initial Table Load in SAP Versions < 7.10","text":"<p>The following article shows how to handle the initial table load for delta extractions using the Table CDC extraction type. The article applies in the following situation:</p> <ul> <li>The Table CDC extraction type is run on SAP releases &lt; 7.10</li> <li>The option Extract table on first run (Delta initialization) is activated.</li> <li>The delta initialization takes longer than the maximum processing time specified in the SAP profile parameter rdisp/max_wprun_time.</li> <li>The extraction aborts with an error message, e.g., <code>ERPConnect.ABAPRuntimeException: RfcInvoke failed(RFC_ABAP_RUNTIME_FAILURE): TIME_OUT - Time limit exceeded</code>.</li> </ul> <p>Note</p> <p>The custom function module /THEO/READ_TABLE used by the Table CDC component to extract the table does not support background mode on SAP releases &lt; 7.10. The background mode avoids the timeout mentioned above.</p>"},{"location":"knowledge-base/table-cdc-initial-table-load/#recommended-workflow","title":"Recommended Workflow","text":"<ol> <li>Create a Table CDC extraction. Make sure the option Extract table on first run is deactivated. </li> <li>Run the Table CDC extraction to initialize the delta extractions. This ensures that no data is missed between table extraction and delta initialization.</li> <li>Create a regular Table extraction using the Table extraction type.  Make sure to select an SAP standard function module, e.g., RFC_READ_TABLE. </li> <li>Run the Table extraction.</li> </ol>"},{"location":"knowledge-base/table-cdc-initial-table-load/#related-links","title":"Related Links","text":"<ul> <li>Documentation: Define the TableCDC Extraction Type.</li> <li>Documentation: Define the Table Extraction Type.</li> <li>Documentation: Table Extraction Settings - Function modules and Background Jobs</li> </ul>"},{"location":"knowledge-base/table-cdc-mechanism/","title":"Delta Mechanism of Table CDC","text":"<p>The following article illustrates the process of delta table extractions using the Table CDC extraction type.</p>"},{"location":"knowledge-base/table-cdc-mechanism/#table-cdc-process","title":"Table CDC Process","text":"<p>The delta mechanism of Table CDC includes the following processes:</p> <ul> <li>Lookup process to read SAP metadata for the definition of the Table CDC extraction.</li> <li>CDC watch process to create a database trigger on the source table and to create the corresponding log table in SAP.</li> <li>Synchronize data process to run Table CDC extractions regularly.</li> </ul> <p>The depicted graphic illustrates the processes in both Xtract Universal and in SAP. Click the graphic to zoom in.</p> <p></p> <p>Tip</p> <p>Use the SAP transaction DB02 to view all triggers of Table CDC log tables in SAP. </p>"},{"location":"knowledge-base/target-principal-TPN/","title":"Target Principal Field (TPN)","text":"<p>The following article describes how to use the Target Principal field when connecting the Xtract Universal Designer to an Xtract Universal Server.</p> <p>The use of a Target Principal Name (TPN) is required to use Kerberos transport encryption or to authenticate Active Directory users.  The Target Principal Name (TPN) can be either a User Principal Name (UPN) or a Service Principal Name (SPN).</p> <p>Note</p> <p>The Target Principal Name only needs to be changed in the Xtract Universal Designer login screen if the service account of the Xtract Universal Windows service is changed.</p>"},{"location":"knowledge-base/target-principal-TPN/#about-target-principal-name-tpn","title":"About Target Principal Name (TPN)","text":"<p>By default, the Xtract Universal Service is executed under the Local System Account. </p> <p></p> <p>In the Active Directory (AD), this user acts as a computer account.  When dialing into a remote server where the service is not used in the local environment, both an UPN and an SPN can be used in the following form:</p> Field Syntax Example XU Server <code>[host].[domain]:[port]</code> theosoftw2012r2.theobald.local:8064 Target Principal as UPN <code>[AD-user]@[domain]</code> svc_xusrv@theobald.local Target Principal as SPN <code>[service class]/[host]@[domain]</code> HTTP/theosoftw2012r2.theobald.local@THEOBALD.LOCAL <p>The Target Principal Name must correspond either to the UPN of the user under which the Xtract Universal Windows service is running, or to an SPN assigned to this user. The UPN or SPN of the Xtract Universal Windows service executes the write processes for the target environments in this context.   Accordingly, this user must have the necessary write permissions for the database.</p> <p>Note</p> <p>Xtract Universal can be used as a distributed application on a central application instance in the company network using appropriate UPNs or SPNs.   All users connect to this remote server in the company network using the locally installed Xtract Universal Designer.</p>"},{"location":"knowledge-base/target-principal-TPN/#example","title":"Example","text":"Local System AccountUser Principal Name (UPN)"},{"location":"knowledge-base/target-principal-TPN/#user-principal-name-upn","title":"User Principal Name (UPN)","text":"<p>A User Principal Name identifies users in a domain. For more information, see Microsoft Documentation: User Principal Name.  A UPN is assigned in the following form:</p> Field Syntax Example XU Server <code>[host].[domain]:[port]</code> TODD.theobald.local:8064 (or localhost:8064) Target Principal <code>[AD-user]@[domain]</code> steffan@theobald.local <p>Note</p> <p>After changing the user context of the Windows service, the UPN or SPN for logging in to the Xtract Universal Server must also be adjusted.</p> <p>Follow the steps below to configure the service to use with UPN:</p> <ol> <li>Open Windows Services (Local).</li> <li>Right-click the Xtract Universal service to open the service Properties. </li> <li>Open the Log-on tab and switch to This Account.</li> <li>Click  [Browse] to look up Windows AD users.</li> <li>Click [Locations] and select Entire Directory. </li> <li>Select an existing UPN or SPN and confirm with [OK]. </li> <li>Apply the changes by restarting the Xtract Universal service.</li> <li>Adjust the UPN in the Target Principal field when logging on to the Xtract Universal Designer. </li> </ol>"},{"location":"knowledge-base/target-principal-TPN/#service-principal-name-spn","title":"Service Principal Name (SPN)","text":"<p>A Service Principal Name is an identifier for services within an authentication domain. For more information, see Microsoft Documentation: Service Principal Names.  An SPN is assigned in the following form:</p> Field Syntax Example XU Server <code>[host].[domain]:[port]</code> TODD.theobald.local:8064 (or localhost:8064) Target Principal <code>HOST/[hostname]@[domain]</code> HOST/TODD.theobald.local@THEOBALD.LOCAL <p>The service class and host name are required for authenticating a service instance to a logon account.  Domain Admin rights are required for processing Manage Service Accounts in Active Directory Users and Computers. </p> <p></p>"},{"location":"knowledge-base/target-principal-TPN/#windows-service-does-not-start","title":"Windows Service does not Start","text":"<p>When a service does not start, configure the service to use a user account with the following rights:</p> <ul> <li>Local Security Policy &gt; Local Policies &gt; User Right Management: Log on as a service</li> <li>Permissions for the installation folder and subfolders: Modify</li> <li>HTTP URL Access Control List e.g.,  <code>urlacl url=https://+:80/MyUri user=DOMAIN\\user</code> </li> </ul>"},{"location":"knowledge-base/target-principal-TPN/#related-links","title":"Related Links","text":"<ul> <li>Microsoft Documentation: User Principal Name</li> <li>Microsoft Documentation: Service Principal Names</li> <li>Enable Secure Network Communcation (SNC)via X.509 Certificate</li> </ul>"},{"location":"knowledge-base/where-clause-editor-lists/","title":"Working with Lists in the WHERE-Clause Editor","text":"<p>The following article shows how to use lists and SELECT statements in the WHERE Clause Editor of the Table extraction type. Lists can contain multiple values separated by commas e.g., <code>1,10</code> or <code>\u201c1\u201d, \u201c10\u201d</code>.</p>"},{"location":"knowledge-base/where-clause-editor-lists/#list-parameters","title":"List Parameters","text":"<ol> <li>Create a Table extraction. </li> <li>Look up the table KNA1, see Documentation: Define the Table Extraction Type.</li> <li>Click Edit runtime parameters. The window \"Edit runtime parameters\" opens.</li> <li>Click [Add List] to define a list parameter, e.g., Parameter0.  </li> <li>Click [OK] to save the parameter. The window \"Edit runtime parameters\" closes.</li> <li>Open the WHERE Clause tab and click [Editor Mode] to open the WHERE clause editor.  </li> <li>Click [Add criteria], then [Default with parameter] to create an empty template in the WHERE clause editor.</li> <li>Select the column ORT01 from KNA1 as the data you want to filter.</li> <li>Select IN as the operator. IN is the only operator that can be used for lists.</li> <li>Click [Select parameter] in the static parameter component of the WHERE clause. A drop down list opens.</li> <li>Select an existing list parameter from the drop down list, e.g., Parameter0. </li> <li>Click [OK] to confirm your input.</li> <li>Click [Load live preview] or run the extraction to check the output. When providing values for the list parameter, use multiple values separated by commas e.g., <code>1,10</code> or <code>\u201c1\u201d, \u201c10\u201d</code>.</li> </ol>"},{"location":"knowledge-base/where-clause-editor-lists/#static-lists","title":"Static Lists","text":"<p>The depicted example statement returns all active customers (rows in the table KNA1) that have an address in one of the following cities: Berlin, Stuttgart, Paris, Seattle, Hong Kong or Dongguan.</p> <ol> <li>Create a Table extraction. </li> <li>Look up the table KNA1, see Documentation: Define the Table Extraction Type.</li> <li>Open the WHERE Clause tab and click [Editor Mode] to open the WHERE clause editor.  </li> <li>Click [Add criteria], then [Default with literal] to create an empty template in the WHERE clause editor.</li> <li>Select the column ORT01 from KNA1 as the data you want to filter.</li> <li>Select IN as the operator. IN is the only operator that can be used for lists.</li> <li>Select List as the type of the static filter value. </li> <li>Click [Press to Edit] in the static value component of the WHERE clause. The window \"Edit List\" opens.</li> <li>Select String as the Type of the list. When working with numbers, select Number.</li> <li>Click [Add] to add items to the list. You can edit items via double-click. </li> <li>Click [OK] to confirm your input.</li> <li>Click [Load live Preview] or run the extraction to check the output. When providing values for the list parameter, use multiple values separated by commas e.g., <code>1,10</code> or <code>\u201c1\u201d, \u201c10\u201d</code>.</li> </ol>"},{"location":"knowledge-base/where-clause-editor-lists/#select-statement","title":"SELECT Statement","text":"<p>SELECT statements can be used to select data from SAP tables, see ABAP Documentation: Open SQL SELECT. The depicted example statement returns all active customers (rows in the table KNA1) that have a sales document in the table VBAK for sales document header data.</p> <p>Note</p> <p>The usage of SELECT statements is only possible as of SAP Release 7.40, SP05.</p> <ol> <li>Create a new Table extraction. </li> <li>Look up the table KNA1, see Documentation: Define the Table Extraction Type.</li> <li>Open the WHERE Clause tab and click [Editor Mode] to open the WHERE clause editor.  </li> <li>Click [Add criteria], then [Default with literal] to create an empty template in the WHERE clause editor.</li> <li>Select the column KUNNR from KNA1 as the data you want to filter.</li> <li>Select IN as the operator. IN is the only operator that can be used for lists.</li> <li>Select List as the type of the static filter value. </li> <li>Click [Press to Edit] in the static value component of the WHERE clause. The window \"Edit List\" opens.</li> <li>Select SELECT as the Type of the list. </li> <li> <p>Enter the following SELECT statement to create a list that contains all items of the column KUNNR from the SAP table VBAK:</p> <pre><code>SELECT KUNNR FROM VBAK\n</code></pre> <p></p> </li> <li> <p>Click [OK] to confirm your input.</p> </li> <li>Click [Load live Preview] or run the extraction to check the output.</li> </ol>"},{"location":"knowledge-base/where-clause-editor-lists/#related-links","title":"Related Links","text":"<ul> <li>Documentation: WHERE Clause Editor</li> </ul>"},{"location":"knowledge-base/xu-ssrs-parameterizing-in-vs/","title":"Use Computed Query Parameters for SSRS","text":"<p>This article shows how to parameterize extractions in Visual Studio as part of Xtract Universal's Power BI Report Server destination. The depicted example demonstrates how to pass multiple parameters using a single computed query parameter.</p>"},{"location":"knowledge-base/xu-ssrs-parameterizing-in-vs/#about-parameters","title":"About Parameters","text":"<p>When creating reports with Visual Studio you can incorporate dynamic and optional parameters to let users filter data without having to change the report itself. To do this, the parameters are passed as runtime parameters to Xtract Universal. In Visual Studio's Report Server Projects, the runtime parameters of Xtract Universal allow only one parameter input for each runtime parameter. To pass multiple parameters, a computed query parameter can be used.</p> <p>The depicted example uses the WHERE clause of a Table extraction to demonstrate how to pass multiple parameters using a single computed query parameter.</p>"},{"location":"knowledge-base/xu-ssrs-parameterizing-in-vs/#prerequisites","title":"Prerequisites","text":"<ul> <li>Create a Table extraction with an SSRS destination in Xtract Universal. The depicted example uses the SAP standard table MAKT. </li> <li>Add the extraction as a data source in Visual Studio and create a report as described in Power BI Report Server in Visual Studio.</li> </ul>"},{"location":"knowledge-base/xu-ssrs-parameterizing-in-vs/#computed-query-parameters","title":"Computed Query Parameters","text":"<p>The depicted example uses data from the columns SPRAS and MATNR to filter the SAP standard table MAKT. Input parameters for both columns are combined in a single computed parameter that is passed to the WHERE clause.</p> <p>Follow the steps below to create a parameter that encapsulates 2 other parameters:</p> <ol> <li>Create 3 query parameters: <ol> <li>Right-click the data set in the Report Data section and select Dataset Properties. The window \"Dataset Properties\" opens. </li> <li>Switch to the tab Parameters and click [Add].</li> </ol> </li> <li>Name two parameters after the columns you want to filter, e.g., \"spras\" and \"matnr\". Name the other parameter \"where\". This will be the computed query parameter.  </li> <li>Click the [f(x)] button next to the \"where\" parameter. The window \"Expressions\" opens.</li> <li>Enter the name of the two columns and the value of their respective query parameter. Link them together using \" and \": <pre><code>= \"spras \" &amp; Parameters!spras.Value &amp; \" and \" &amp; \"matnr \" &amp; Parameters!matnr.Value\n</code></pre> </li> <li>Confirm your input with [OK].</li> <li>Switch to the Query tab and click [Query Designer\u2026]. The window \u201cQuery Designer\u201d opens.</li> <li>Select Parameterized as the Behaviour for Xtract Universal's \"where\" parameter. </li> <li>Enter the name of the new query parameter under Value.</li> <li>Confirm your input with [OK].</li> </ol> <p>Use the Preview mode of Visual Studio to test the inputs.</p>"},{"location":"knowledge-base/xu-ssrs-parameterizing-in-vs/#optional-computed-query-parameters","title":"Optional Computed Query Parameters","text":"<p>To use parameters as optional input, the expression for the \"where\" parameter needs to be edited to accommodate optional parameters. Follow the steps below to make input parameters optional:</p> <ol> <li>Right-click the input fields of the 2 parameters you want to be optional and select Parameter Properties. The window \"Report Parameter Properties\" opens. </li> <li>In the General tab, activate the checkbox Allow null value.</li> <li>Confirm your input with [OK]. A checkbox appears next to the input field.</li> <li>If the checkbox is activated, the parameter will send Nothing and thus be ignored at runtime.</li> </ol>"},{"location":"knowledge-base/xu-ssrs-parameterizing-in-vs/#optional-parameters-in-expressions","title":"Optional Parameters in Expressions","text":"<p>In this example we add an optional WHERE clause to the query. The goal is to filter the SPRAS and MATNR columns, with both filters being optional. This results in 4 possible behaviors for the WHERE clause:</p> <ul> <li>no filtering, the runtime parameter \"where\" is ignored</li> <li>filter by \"spras\" only, e.g., <code>spras &gt; 5</code></li> <li>filter by \"spras\" and \"matnr\", e.g., <code>spras &gt; 5 and matnr = TG0012</code></li> <li>filter by \"matnr\" only, e.g., <code>matnr = TG0012</code></li> </ul> <p>Follow the steps below to edit the \"where\" expression to accommodate optional input:</p> <ol> <li>Navigate to the query parameters:<ol> <li>Right-click the data set in the Report Data section and select Dataset Properties. The window \"Dataset Properties\" opens. </li> <li>Switch to the tab Parameters.</li> </ol> </li> <li>Click the [f(x)] button next to the \"where\" parameter. The window \"Expressions\" opens.</li> <li>Edit the expression to include the following conditions: Optional Parameters<pre><code>= IIf (IsNothing(Parameters!spras.Value) And IsNothing(Parameters!matnr.Value),Nothing,\nIIf(IsNothing(Parameters!spras.Value),\"\",\"spras \" &amp; Parameters!spras.Value) &amp;\nIIf(Not(IsNothing(Parameters!spras.Value)) And Not(IsNothing(Parameters!matnr.Value)),\" and \",\"\") &amp;\nIIf(IsNothing(Parameters!matnr.Value),\"\",\"matnr \" &amp; Parameters!matnr.Value))\n</code></pre><ol> <li>The first line checks if both parameters are Nothing. In this case the expression returns Nothing and the evaluation is complete. Else either one or both parameters are not set to Nothing.</li> <li>The second line checks if \"spras\" is not Nothing and adds the column name and value to the expression if that is the case. Else the expression is left unchanged (\"\").</li> <li>The third line checks if both \"spras\" and \"matnr\" are not Nothing and adds an \" and \" to the where clause. Else the expression is left unchanged (\"\").</li> <li>The last line checks if the \"matnr\" is not Nothing and adds the column name and value to the expression if that is the case. Else the expression is left unchanged (\"\").</li> </ol> </li> <li>Confirm your input with [OK].</li> </ol> <p>Use the Preview mode of Visual Studio to test the inputs.</p>"}]}